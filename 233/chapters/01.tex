\chapter{Introduction}\label{sec:1}
Many common questions asked by researchers about the interrelationships between language varieties can be framed in terms of the direction of influence. A sociolinguist might ask which social group inside a society has introduced a certain usage of a word and how this usage spread, a dialectologist is often faced with the question where a certain phonetic innovation originated, and a historical linguist will be interested in whether a group of clearly related words from two otherwise unrelated languages can be explained by borrowing, and if so, in which direction they are likely to have been borrowed.

Across these domains, data tends to be available only in terms of discrete features assigned to each variety (perhaps with frequency information), or continuous measures of distance or similarity. We can only observe the distribution of these features at certain points, and often only a single point, in time, whereas the focus of our interest is on the processes generating the data we see. The challenge is to develop and test theories about these processes post-hoc based only on observable data. 

Historical linguists are often faced with data from a set of languages about which little is known, and need to develop a coherent set of reconstructions and sound changes to explain how the observed data most likely came about. This book explores the idea of using causal inference for this purpose, a comparatively recent approach that is designed to systematically extract evidence about the directionality of influence between statistical variables based on observational data alone, whereas in classical statistics, the direction of causality between pairs of variables can only be determined by experiment.

The conditional independence tests which are necessary for constraint-based causal inference can be generalized with the help of information theory, a mathematical framework which provides a systematic way of analyzing the knowledge provided by sources of information, quantifying how informative a certain piece of information is if we already know the information from a different source, and most crucially, to offset the shared information different pairs of sources provide about each other in very complex ways in order to answer the question whether some source of information (e.g. some language) will provide any new knowledge if we already know the information coming from a set of other sources (e.g. related languages).

Causal inference has the advantage of being able to infer very general graph structures, whereas the bulk of efforts in automated inference of linguistic history has been on inferring tree structures, a very common simplified model of the historical developments shaping the linguistic landscape. In recent years, work on automated inference of phylogenetic trees has quite successfully been performed on many language families whose internal structure was found to be difficult to determine based on the classical methods. In these works, contact between languages is usually only seen as causing noise which complicates inference of the inheritance tree, and sometimes needs to be corrected for. In contrast, methods for explicitly determining a set of likely contact events are still in their infancy. A still largely open question is whether it is possible to determine algorithmically not only which languages form a genetic unit by offspring, but also which contacts have taken place, and in which direction the lexical material was transmitted.

The problem that I am setting out to solve in the present volume can be described as the inference of lexical flow. The basic metaphor is that lexical material flows into a language either by inheritance from an earlier ancestral language (much as water flowing down a river), or through borrowing (spillovers into adjacent waterways). To stay with the metaphor, the challenging task of lexical flow inference is then equivalent to measuring the composition of the water on various outlets of a large delta, and infer a structure of sources, brooks and spillovers which may have produced this pattern.

Starting only with parallel wordlists for a set of languages, the first step is to determine which of the words from different languages are cognates, i.e.\ related by common ancestry. Given a model specifying which words in a set of neighboring languages are cognates, the next step is to build a theory of which languages are genetically related (offspring of a common ancestral language), and how the languages influenced each other during their history. In this book, I show that building on state-of-the-art methods from computational linguistics to perform automated cognate detection, and then performing novel algorithmic methods inspired by causal inference on the cognate data, it is possible to come rather close to a good solution for two types of lexical flow inference problem. My algorithms are evaluated both against real data derived from a new large-scale lexicostatistical database, and against synthetic data which were generated by a new simulation model which allows me to generate any amount of realistic cognacy data for simulated linguistic areas.

While the evaluations I perform show that the structure of the flow networks is quite good, and that a lot of the expected directional signal is indeed found, some erroneous directional arrows are invariably inferred as well. This gives my current algorithms the character of tools for exploratory data analysis, which can be used to quickly infer an initial picture of relevant connections in very large datasets, and to locate questions of interest which would then be investigated further using classical or fully probabilistic methods. 

Because it is based on very general mathematical and algorithmic ideas, the framework I propose and explore for a single application in this book is much more widely applicable to a range of problems of similar structure covering other types of linguistic entities. 

For instance, many varieties of interest to dialectology and sociolinguistics could be modeled as information-theoretic variables as well. Where I build on a measure of lexical overlap to determine lexical flow between languages in the contact history of a linguistic area, diatopic variation, as between the dialects spoken in different villages, could instead be measured based on phonetic or phonotactic features, and my infrastructure could be applied in order to determine the direction in which certain phonetic innovations spread through a language area.

Diastratic variation, e.g. between the elevated code of an educated elite and the everyday usage of different professions, might be modeled in terms of the usage patterns of near-synonyms, and analyzed using causal inference in order to determine in which social group a lexical replacement is likely to have originated, even if diachronic data is not available.

Diaphasic variation, e.g. between the language variants used by the same person in different communicative settings, as the language used in formal keynotes, at press conferences, and by politicans in discussions with voters, could be modeled based on the usage of certain constructions, in order to conclude from the overlaps which familiar setting tends to be used as a model for usage in less familiar settings.

Diamesic variation, e.g. between the language used in televised political debates as opposed to the language used in written manifestos and the language of opinion pieces in newspapers, could be modeled in terms of phraseological features. The resulting overlap patterns would likely provide hints about which medium is most influential, and the pathways by which new phrases tend to be adopted across the different media.

\largerpage
In addition to language varieties modeled on different levels of description, the general framework can also be applied to other types of linguistic entities. For instance, in previous work \citep{dellert2016b}, I have started to explore an analogous overlap measure which treats concepts as variables, and treats their colexifications in different languages as observations. The resulting model summarizes how often knowledge about the realizations of a certain concept in some language allows to predict the word used for other concepts, and causal inference on this model provides some evidence about likely pathways of semantic change.

Coming to the structure of this book, Chapters \ref{sec:2} and \ref{sec:3} serve as introductions to the core terminology and issues of computational historical linguistics and causal inference, and give an overview of the previous work in both areas that my work is building on. I have tried my best to provide low-level entry points to both subjects in order to make the book self-contained for readers who are familiar with either the problem or the method, but I will sometimes need to use linguistic or mathematical terminology that will not be understandable without some background in either field. \chapref{sec:3} also introduces the PC and FCI algorithms that form the basis for my lexical flow algorithms in Chapters \ref{sec:6} and~\ref{sec:7}.

\chapref{sec:4} then describes the long process by which I arrived at my test data. It starts by describing the NorthEuraLex database, which was compiled under my supervision as part of the dissertation project that ultimately became this book. Comparing my own infrastructure to existing approaches to the same problems, I then describe how the sound correspondences and cognacy relations between the words for 1,016 concepts across 107 languages contained in the database were estimated. The chapter concludes with a detailed look at the contact histories of four subregions of Northern Eurasia, summarizing the findings into the gold-standard of language contacts necessary for evaluating the lexical flow methods.

Complementing the first set of test data derived from actual language data and the literature, \chapref{sec:5} describes and motivates the simulation model which I am using to generate large amounts of additional test data. In essence, in addition to the four continent-sized areas where I have access to language data and the language's histories, I use the simulation model to investigate the performance of my methods on 50 additional generated histories of multiple interacting language families sharing virtual continents.

\largerpage
\chapref{sec:6} describes the core of my contribution to the field, detailing how causal inference can be applied to cognacy overlap data in order to generate evolutionary networks by what I call the Phylogenetic Lexical Flow Inference (PLFI) algorithm. The core idea is to define a measure of conditional mutual information between sets of languages, which quantifies how much of the lexical overlap between two groups of languages can be accounted for by transmission through a third set of languages. While I explore several ways in which the explanation of overlap can be modeled, the ultimately best-performing variant has an intuitive interpretation in terms of lexical flow, building on possible paths by which each word can have arrived in the languages where it is currently attested according to the inferred cognacy data. Variants of the PC algorithm are then applied to systematically perform conditional independence tests in order to progressively remove links from a network representing pairwise overlaps, until we arrive at a skeleton of links that are minimally necessary to explain the observable pattern of lexical overlaps. In order to be able to apply the PC algorithm, ancestral languages (the source of cognate vocabulary) need to be modeled explicitly, which requires reconstruction of cognate presence or absence on all nodes of a predefined language tree, and then treating the reconstructed states as if they were actually observed data.

\chapref{sec:7} then explores what happens if we want to avoid building on reconstructed states as an additional layer of inference, but want to build what I call a contact flow network that only involves the attested languages. Inferring a contact flow network is a somewhat less ambitious goal because tree inference and ancestral state reconstruction are not required any longer, but it also means that we are now faced with performing causal inference in the presence of hidden common causes which act as confounders. This more complex type of causal inference problem presupposes the use of the more complex FCI algorithm as the basis for my own Contact Lexical Flow Inference (CLFI) algorithm. Since FCI is even more sensitive to wrong conditional independence judgments than the PC algorithm, applying it to the quite noisy automatically inferred cognate data requires a specialized test of directionality.

Chapters \ref{sec:6} and \ref{sec:7} both include detailed discussions of the many problems these approaches are still facing in a range of test cases, and conclude with a quantitative evaluation of the new algorithms against both the gold standard and the simulated language data.

The book concludes with a final chapter summarizing the results, and providing an outlook on future research. Much of this research will be focused either on improving the quality of the input data (e.g. by improving automated cognate detection), or on steps towards assigning confidence scores to the links in the lexical flow networks, which is not yet possible based on the infrastructure presented in this book. Beyond the possibility of quickly deriving a first approximation of the history of a region from raw data, such refined versions of my framework could potentially provide plausibility checks for existing theories as well. In cases where the methods of classical historical linguistics cannot decide between two alternative theories of contact history (usually due to difficulties in weighting the evidence), the results of a statistical model could provide evidence in the absence of hard proof.
