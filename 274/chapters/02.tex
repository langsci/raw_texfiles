\chapter{Internalism}

In this chapter I detail the internalist approach, which is taken by generative linguistics as well as the broader, generative-oriented, biolinguistics programme. Language is regarded by biolinguists as an internal computational system that produces a set of hierarchically structured expressions that are employed by the systems of thought and the sensorimotor systems to yield language production and comprehension. I discuss the work in internalist semantics of Paul Pietroski and others according to which linguistic meanings are computational instructions to build monadic concepts.

Internalism, as the name suggests, studies internal states, including those that in philosophy are regarded as mental states. \citet{Chomsky2003} makes clear that internalism is not the doctrine that denies that mental states are individuated by reference to the subject’s environment, nor is it the doctrine that holds that subjects in the same internal states are therefore in the same mental states. That is, as mentioned above, internalism is not the same as individualism. Rather, internalism is “an explanatory strategy that makes the internal structure and constitution of the organism a basis for the investigation of its external function and the ways in which it is embedded in an environment” \citep[139]{Hinzen2006}. In other words, internalism “is primarily a \textit{conjecture about a proper object of the scientific study of language }(which internalists claim to be \textit{I-language})" \citep[324, emphasis in original]{LohndalNarita2009}. This chapter will outline what this amounts to in the case of semantics.

\section{E-language and I-language}

Biolinguistics treats language as an internal computational system, a recursive mechanism that produces a potentially infinite set of hierarchically structured expressions that are employed by the conceptual-intentional systems (systems of thought) and the sensorimotor systems to yield language production and comprehension. As I detail below, this particular functional design of the language faculty is strongly shaped by its interface with the systems of thought, rather than by the peripheral process of externalisation inherent in the link with the sensorimotor systems \citep{Chomsky2013,Hinzen2013,Asoulin2016,BerwickChomsky2016}. Biolinguistics takes its object of study to be the underlying mechanisms of language, which are a subsystem of our cognitive system and are composed of a computational system (called an I-language) that is encoded in individual brains. The subject matter of biolinguistics (and internalism) is thus competence, as opposed to performance. As Chomsky put it in an oft-quoted phrase, generative linguistics is primarily concerned with an ideal speaker/hearer who resides in “a completely homogeneous speech-community, who knows its language perfectly and is unaffected by such grammatically irrelevant conditions as memory limitations, distractions, shifts of attention and interest, and errors (random or characteristic) in applying his knowledge of the language in actual performance” \citep[3]{Chomsky1965}. Competence, then, refers to the speaker/hearer’s knowledge of his/her language, whereas performance refers to the actual use of this knowledge by a particular person. The actual use of one’s linguistic knowledge in language production and comprehension involves many other factors, only one of which is one’s competence, and it is only under strict idealisation conditions that performance might be seen as reflecting competence. \citet{Chomsky1986} developed a different characterisation of the competence/performance distinction, a clearer and more useful distinction that is still used today: I-language versus E-language. 

Externalised (E-) language refers to actual or potential speech events. From the E-language point of view, a grammar is a collection of descriptive statements concerning performance; the grammar describes or taxonomises the corpus of linguistic performance data. This is the way language is studied in structural and descriptive linguistics, behavioural psychology, and some branches of cognitive science, where language is viewed as a collection of linguistic forms (words or sentences) that are paired with meanings. Even though this description glosses over the subtleties of and the differences between specific E-language approaches, the main thread of them all is the view of language as “the totality of utterances that can be made in a speech community” \citep[155]{Bloomfield1926}; or language as a pairing of sentences and meanings over an infinite range, where the language is used by a population when certain regularities hold among the population with respect to the language and are sustained by an interest in communication \citep{Lewis1975}.\footnote{See \citet[19]{Chomsky1986} for discussion and more references.} What the E-language approaches share is the view that language can be understood (indeed it is often claimed that it exists) independently of the properties of the mind/brain. That is, language is understood as a collection of actions or behaviours, and “a grammar is a collection of descriptive statements concerning the E-language, the actual or potential speech events (perhaps along with some account of their context of use or semantic content)” \citep[20]{Chomsky1986}. 

In other words, this approach sees a grammar as a function that enumerates the elements of the E-language. But this function need not be unique. From the E-language perspective, there need not be one \textit{real} or \textit{correct} grammar that corresponds to the corpus data: as long as it yields a correct description of the corpus data, any number of grammars could in principle apply. Lewis, for example, says that he can find no way to “make objective sense of the assertion that a grammar Γ is used by population \textit{P} whereas another grammar Γ$'$, which generates the same language as Γ, is not” \citep[20]{Lewis1975}. Lewis believes that a language is an abstract, formal system that a population selects by convention \citep{Lewis1969}. Another manifestation of E-language can be seen in \citet{DevittSterelny1989}, who argue that rather than being about competence, linguistics is about the properties and relations of \textit{observable} linguistic symbols (see also \citealt{Devitt2006}). According to the E-language conception, then, language is, as it were, out there, it is not intimately related to the mind. Deacon, for example, argues that in contrast to the claim of generative linguistics that support for language acquisition originates inside the brain (in the language faculty), “the extra support for language learning is vested neither in the brain of the child nor in the brains of parents or teachers, but \textit{outside brains}, \textit{in language itself}” \citep[105, emphasis mine]{Deacon1997}. The E-language is the real object of study here, not the grammar which generated it, which is a derivative notion because it is assumed that any grammar is suitable so long as it correctly generates the observable corpus.

On the internalised (I-) language perspective, however, there is a particular grammar that generates and is responsible for the observable corpus of utterances. More precisely, it generates a set of structural descriptions that provide the basis for interpretation. It is the generative grammar that is the object of study (as opposed to the set generated by the grammar), and this grammar \textit{qua} generative computational device is instantiated in the brain. Language is thus conceived as some real structure in the brain of the speaker/hearer that is responsible for (indeed, it \textit{is}) the language that that speaker/hearer knows. So, unlike the E-language conception of language, a generative grammar \textit{qua} I-language is a theory of a real mental structure to which “questions of truth and falsity arise […] as they do for any scientific theory” \citep[22]{Chomsky1986}. As we'll see in detail below, the I-language approach, which biolinguistics takes, sees the proper subject matter of a scientific linguistics to be the knowledge a speaker/hearer has of his or her language, the knowledge that underlies and makes possible, along with other factors, that speaker/hearer’s language production and comprehension. This is also the research programme of internalism.

Let us be clear about the relation between the internalism/externalism distinction and the I-language/E-language distinction. Internalism in the sense understood by the authors discussed in this book is clearly and explicitly rooted in the I-language approach to semantics. The E-language approach, on the other hand, is exemplified by several externalists that are cited in \citet{Chomsky1986} where the I-language/E-language distinction was first articulated. The labels I-language and E-language, then, denote approaches to the study of language and meaning. There is more than one way to flesh out an I-language or E-language approach to semantics, and I discuss some of these variations below. I should also note that there are other criticisms of the externalist position apart from those offered here from the point of view of internalist semantics, so it of course does not follow that one must agree with biolinguistics in order to see the problems with externalist semantics of the Putnam or Davidson sort. For example, as discussed below, Paul Horwich offers both a critique of externalist semantics and an alternative semantic theory. But his theory still clings to an externalist (in the E-language sense) understanding of meaning. 

With that in mind, let us now explore the nature of I-language before moving on to internalist semantics. An I-language is a computational system that is in the mind of individual language users. It is a generative procedure that outputs structural descriptions that provide the basis for interpretation. There is a stress here on the intensional nature of particular I-languages, meaning that there is a specific procedure encoded in the mind that generates the structural descriptions; this is in contrast to the extensional nature of E-language grammars. Another way to put the matter is in terms of formal mathematics, in which a sequence can be defined extensionally by listing its members, say ${0, 1, 1, 2, 3, 5, 8, 13…}$, or intensionally by providing a formula that generates the members of the sequence, say the formula $F_n = F_{(n-1)} + F_{(n-2)}$ that generates all and only the numbers of the Fibonacci sequence. An intensional definition is much more useful for large sets and is essential for potentially infinite sets like the ones associated with natural languages. This analogy should not be taken too literally, for as we'll see in Chapter 3, there are crucial differences between formal languages and natural languages.

The internal computational processes of the language faculty generate linguistic objects that are employed by the conceptual-intentional systems (systems of thought) and the sensorimotor systems. Lexical items, then, and all expressions generated from them, must have properties that are interpretable at both these interfaces. Notice that on this view the language faculty is embedded within, but separate from, the performance systems. So an I-language is a device that generates structured expressions of the form $Exp = <Phon,Sem>$, where $Phon$ provides the sound instructions of which the sensorimotor systems make use, and $Sem$ provides the meaning instructions of which the systems of thought make use. $Phon$ contains information relating to linear precedence, stress, temporal order, prosodic and syllable structure, and other articulatory features. $Sem$ contains information relating to event and quantification structure, and certain arrays of semantic features. The term \textit{instructions} is here used in a technical sense, so that to say that

\begin{quote}
    […] phonetic features are “instructions” to sensorimotor systems at the interface is not to say that they have the form “Move the tongue in such-and-such a way” or “Perform such-and-such analysis of signals”. Rather, it expresses the hypothesis that the features provide information in the form required for the sensorimotor systems to function in language-independent ways. \citep[91]{Chomsky2000a}\footnote{For more on the phonetic implementation of phonological features, see \citet{Halle1983,Halle1995}. See \citet{Kenstowicz1994,HaleReiss2008,VolenecReiss2020} for an overview of generative phonology.}
\end{quote}

The same is true for the semantic features at the $Sem$ interface, which are not “instructions” to the conceptual-intentional systems of the form “this pronounced word means such-and-such” or “link this pronounced phrase with this concept”. Rather, as detailed in the next section, the $Sem$ interface is part of the procedure that generates instructions to build new mental representations.

The expression $Exp$ is generated by the operation Merge, which takes objects already constructed and constructs from them a new object. So, for example, $Merge (X,Y)$ will yield the unordered set $\{X,Y\}$. The structure-building operation Merge follows the principle of Minimal Computation (compute and articulate as little as possible), for it is the simplest possible computational operation for the task at hand (\citealt{BerwickChomsky2016,Chomsky2016a}). There are two cases of Merge: External Merge refers to the operation where two syntactic objects are merged but where neither one is part of the other. Internal Merge, on the other hand, refers to the operation where one of the syntactic objects is part of the other. For example, Internal Merge takes place when a syntactic object is combined with the set that contains it: so if $Merge (X,Y)$ yields $Z=\{X,Y\}$, then $Merge (Z,X)$ yields $\{X,Y\{X\}\}$. For concreteness, take the following simplified example of External Merge. \textit{The silver saucer broke yesterday} is produced by Merge as follows: lexical items are merged to (separately) create \textit{The}, \textit{silver}, and \textit{saucer}. Then \textit{silver} and \textit{saucer} are merged to create the Noun Phrase (NP) \textit{silver saucer}. Then \textit{silver saucer} is merged with \textit{the} to create the NP \textit{the silver saucer}. Then that NP is combined with the Verb Phrase (VP) \textit{broke yesterday} (which was produced by Merge when lexical items were merged to create \textit{broke} and lexical items were merged to create \textit{yesterday}, and then \textit{broke} and \textit{yesterday} were merged together to create the VP \textit{broke yesterday}).\footnote{For recent discussion of Merge, see \citet{Collins2017,ChomskyGallegoOtt2019}.}

As for Internal Merge, suppose we merged \textit{which saucer} with \textit{John broke which saucer} to produce \textit{which saucer John broke which saucer}, which via further computations is then externalised as \textit{which saucer did John break}. Before externalisation, there are two copies of the same linguistic object (X): the original one and the displaced one. They are both essential for interpretation. As Chomsky has remarked in various places when he discusses Merge, this is an example of the ubiquitous phenomenon of displacement in language, where phrases are heard in only one place but are interpreted both there and in another place. So we interpret the above sentence to mean “for which X, John broke the saucer X”. Merge, then, defined as recursive set-formation, produces hierarchical structures and allows for the unbounded embedding of these structures (namely, it allows for discrete infinity).

There are independent reasons to believe that cognitive processes satisfy the principle of Minimal Computation (see \citealt{Cherniak1994,CherniakNodelman2002}), and since Merge satisfies this principle and is able to account for the underlying mechanisms of language, we have strong grounds for its existence as a core computational principle of human language. Furthermore, as \citet{Chomsky2013} shows with examples such as the above where two copies of \textit{which saucer} are required for the interpretation of the sentence, Merge yields structures suited for interpretation at the conceptual-intentional interface but “these are clearly the wrong structures for the SM [sensorimotor] system: universally in language, only the structurally prominent copy is pronounced” \citep[41]{Chomsky2013}. That is, the second copy must be deleted when it is transferred to the sensorimotor interface resulting in articulated sentences having gaps that create problems for language comprehension and communication but that are necessary for interpretation at the conceptual-intentional interface. These are the so-called filler-gap problems,\footnote{See \citet{SprouseHornstein2013} for a recent collection of work on long-distance filler-gap dependencies.} where the hearer has to figure out where the unarticulated element is in order to parse and interpret the sentence correctly. There is thus an asymmetry between the interfaces in favour of the semantic side, pushing externalisation (via $Phon$) to the periphery. If the language faculty is structured in this way then it follows that the underlying computational mechanisms of language “will provide structures appropriate for semantic-pragmatic interpretation but that yield difficulties for perception (hence communication)” \citep[41]{Chomsky2013}. In other words, the design of language favours minimal computation, often at the expense of ease of communication (\citealt{Sigurðsson2004,Burton-Roberts2011,Asoulin2016,Asoulin2020}).


\section{Internalist semantics}
The expression $Exp$ is of course not the same as a linguistic utterance but rather provides the information required for the sensorimotor systems and the systems of thought to function, largely in language-independent ways. Since these two systems operate independently of (but at times in close interaction with) the faculty of language, a mapping to each interface is necessary, for these two systems have different and often conflicting requirements. The systems of thought require a particular sort of hierarchical structure in order to, for example, calculate relations such as scope; the sensorimotor systems, on the other hand, often require the elimination of this hierarchy because, for example, pronunciation must take place serially. The instructions at the $Sem$ interface that are interpreted by the performance systems are used in acts of talking and thinking about the world – in, say, reasoning or organising action. Linguistic expressions, then, provide a perspective (in the form of a conceptual structure) on the world, for it is only via language that certain perspectives are available to us and to our thought processes. This is in line with a long rationalist tradition in the philosophy of language and linguistics \citep{Chomsky1966}, most famously articulated by Humboldt in the nineteenth century, according to which language provides humans with a \textit{Weltansicht} or worldview that allows us to form the concepts with which we think certain kinds of thought (but, crucially, not \textit{all} kinds of thought: for we share many kinds of thought processes with animals that do not have language). 
	
In his recent study of Humboldt, \citet{Underhill2009} remarks that Humboldt’s “rich and dynamic model of language” is one “in which the individual both shapes and is shaped by the \textit{organ of speech}” \citep[xi, emphasis in original]{Underhill2009}. The worldview concept of \textit{Weltansicht}, which forms the cornerstone of Humboldt’s linguistic philosophy, is understood as “the configuration of concepts which allow conceptual thought” of a certain kind \citep[56]{Underhill2009}. Language is an instrument of thought in this sense \citep{Asoulin2016}, but note that this is not a Whorfian claim of linguistic determinism, for thought is certainly independent of particular natural languages, and what can be expressed or thought by a speaker of one language can certainly be expressed or thought by a speaker of a very different language. As \citet[57]{Underhill2009} remarks, Whorfian claims are merely “weak echoes of Humboldt’s voice”. Language provides us with a unique way of thinking and talking about the world that is unavailable to non-linguistic animals. Animals of course have thoughts of many kinds (many of which are shared with humans), but since they lack the language faculty there is a specific kind of thought that they lack (\citealt{Hinzen2013,Asoulin2019}). Let us see how this rationalist understanding of the role of language in cognition is manifested in current biolinguistics and internalist semantics.

As mentioned, an I-language is a device that generates structured expressions of the form $Exp = <Phon,Sem>$ with a double interface property: they have phonological and semantic features through which the linguistic computations can interact with other cognitive systems. But the link to and influence of each interface is not symmetrical, for there is mounting evidence that there is an asymmetry between the interfaces in favour of the semantic side, pushing externalisation via $Phon$ to the periphery (\citealt{Chomsky2013,BerwickChomsky2016,Chomsky2016a,Asoulin2016,Asoulin2020}). Merge implements the basic properties of I-language (\citealt{ChomskyGallegoOtt2019}). \citet{CollinsStabler2016} show that all the essential syntactic operations, such as c-command, can be formally defined in terms of Merge. For reasons of computational efficiency, the computations of Merge should apply freely so that the only constraints imposed on them are those derived from the interfaces with the external systems. There are independent reasons to believe that cognitive processes satisfy this principle of Minimal Computation (\citealt{Cherniak1994,CherniakNodelman2002,Chomsky2016a}). Investigation of the structures generated by Merge shows that they are well suited to the $Sem$ interface (hence, for internal thought), but cause predictable problems at the $Phon$ interface. In other words, the normal course of the derivation generated by Merge simply proceeds towards $Sem$, then at some point in the derivation some parts of the expression are sent to $Phon$ for externalisation. The ‘point’ of the derivation is the generation of interpretable structures: its externalisation via sound or sign is secondary at best. 

The $Sem$ interface is the way in which biolinguistics and internalism explain meaning in natural language. A theory of $Sem$ must satisfy three basic conditions of adequacy: in order to capture what the language faculty determines about the meaning of an expression, $Sem$ must “be universal, in that any thought expressible in a human language is representable in it; an interface, in that these representations have an interpretation in terms of other systems of the mind/brain involved in thought, referring, planning, and so on; and uniform” \citep[21]{Chomsky1995}. $Sem$ must be uniform “for all languages, so as to capture all and only the properties of the system of language as such” \citep[21]{Chomsky1995}. In other words, the way in which the meanings at $Sem$ are generated (and then sent to the conceptual-intentional interface) is uniform in the sense that any meaning generated via the language faculty is expressible in any natural language. Note the stress on all and only the properties of the system of language: the language faculty allows humans to use available concepts (some of which are shared with other animals) to generate formally new concepts. The claim is not that $Sem$ is the interface of all conceptual content or of all thought.
	
\citet{Pietroski2008,Pietroski2010,Pietroski2018} has developed one of the most interesting and detailed accounts of an internalist semantics, the leading idea of which is that “in the course of language acquisition, humans use available concepts to \textit{introduce} formally new concepts that can be fetched via lexical items and combined via certain operations that are invoked by the human faculty of language” \citep[247, emphasis in original]{Pietroski2010}. That is, meanings are (internal, and unconscious) instructions for how to access and assemble concepts of a special sort. Meaning is here understood not in an extensional sense but rather in terms of the cognitive resources (the computational procedures) that humans deploy in \textit{generating} the meanings. So, for example, the $Sem$ of \textit{white sheep} is an instruction to fetch a concept from each lexical address and then conjoin them. There are a number of steps and notions here that require unpacking: (i) What is a concept? (ii) How is a concept lexicalised? and (iii) How are these lexicalised concepts conjoined? I discuss each in turn below.
	
Concepts are, roughly, constituents of mental states. To give Fodor’s favourite example, believing that cats are animals is a paradigmatic mental state, and the concept \textit{animal} is a constituent of the belief that \textit{cats are animals}. The latter is a proposition, and propositions are generally understood to be structured objects of which concepts are the constituents. As Fodor has discussed in his various works on concepts (for example, \citealt{Fodor1998,Fodor2003,FodorPylyshyn2015}; see \citealt{Murphy2002} for an overview), some concepts are structured and some are primitive. So the concept \textit{white} \textit{cat} is a structured concept that might include the two primitive concepts \textit{white} and \textit{cat}. The meaning of a structured concept depends on its primitive elements and on the way in which they are combined. But not all combinations are possible: there is a syntax that determines how concepts can (and cannot) be combined. Frege’s famous metaphor of saturating concepts is helpful here. Statements can be thought of in the same way as mathematical equations, argued \citet{Frege1892}, in that they are split into two parts (a function and an argument). Consider the sentence \textit{Caesar conquered Gaul}. The first part (\textit{Caesar}) is the subject expression, which can stand on its own, but the second part (\textit{conquered Gaul}), which is the predicate expression, is in need of supplementation or saturation for it contains an empty place that needs to be filled in. So a proper name like \textit{Caesar} is said to saturate the function \textit{conquered Gaul }by filling in the empty place, giving a complete sense. 

As \citet[249]{Pietroski2010} discusses, singular concepts are able to saturate concepts like \textsc{arrived(x)} and \textsc{saw(x, y)}, which are used to classify and relate represented individuals, allowing humans to form sentential representations like \textsc{arrived(Brutus)} and \textsc{saw(Brutus, Caesar)}. Abstracting away from completed concepts leaves what Pietroski calls a sentence frame “that can be described as an unsaturated concept whose adicity is the number of saturable positions: \textsc{arrived(x)} is monadic, \textsc{saw(x, y)} is dyadic, \textsc{give(x, y, z)} is triadic, etc.” \citep[249]{Pietroski2010}. There is of course a limit to the saturable positions that natural language concepts can possess, and Pietroski argues that tetradic concepts may be common (compare the difference between selling and giving). As he puts it, “we seem to have higher-order numeric/set-theoretic/quantificational concepts that can be saturated by monadic concepts, as in \textsc{three/include/most[brown(x), cow(x)]}. In short, concepts compose and exhibit a limited hierarchy of types” \citep[249]{Pietroski2010}.

Now, as \citet{Fodor1975} famously argued, there are parallels between propositions and sentences and between words and concepts (and thus between thought and language). That is, “propositions are what (declarative) sentences \textit{express}, and (excepting idioms, metaphors, and the like), \textit{which} proposition a sentence expresses is determined by its syntax and its inventory of constituents” \citep[8, emphasis in original]{FodorPylyshyn2015}. But how far do these parallels go? How much of conceptual thought is influenced, constructed, or determined by the computational procedures of I-language? If our concepts are parallel to linguistic expressions in their systematicity and productivity, how did these concepts emerge? There is perhaps a spectrum of answers to these questions, but in general there are two answers: either the concepts were there prior to lexicalisation or else the process of lexicalisation introduced new sets (or new types) of concepts. The biolinguistic and internalist semantics claim is of the latter sort. To put the matter in Pietroski’s terms, already existing concepts (many of which we share with other animals) are lexicalised and in the process distinctively new concepts are produced that we are then able to combine to form linguistic expressions. This process of lexicalisation and concatenation is part of the  explanation of the creative aspect of language use \citep{Chomsky1966,McGilvray2001,McGilvray2005,Asoulin2013}.
	
So how is a concept lexicalised? There are several ways in which to flesh out the idea that lexicalisation is the process by which pre-existing concepts are used to introduce formally new concepts. The externalist answer, which will be discussed in the next chapter, is a compositional theory of meaning modelled on the work of \citet{Davidson1967,Davidson1973} and \citet{Montague1974}; but let us continue with an internalist answer. Pietroski’s answer moves away from the Fregean idea that combining expressions is an instruction to saturate a concept and towards a Conjunctivist account of linguistic composition (\citealt{HornsteinPietroski2009,Pietroski2018}). According to the latter account, lexicalisation is not a process in which a previously available concept is merely labelled using a lexical item that inherits its content from the concept itself. Rather, lexicalisation is a device for accessing previously available concepts which become lexical items that are used as input to I-language operations that combine the lexical items in specific ways to introduce new formally distinct concepts. Accordingly, the $Sem$ of any expression $Exp = <Phon,Sem>$ is not a concept that is paired with a pronunciation. Indeed, as Pietroski puts it, “evaluating SEMs as if they were concepts may be a category mistake, like evaluating \textit{an instruction to fetch} a rabbit as male or female” \citep[252, emphasis in original]{Pietroski2010}. So a $Sem$ is an instruction to fetch (i.e., lexicalise) a previously available concept that is then used to build a formally new concept(s). This formally new concept will be stored in the mind somehow (and perhaps be recombined with other concepts to create yet more formally new concepts), but the $Sem$ itself is not a concept. 
	
Another way to put the matter is as follows. Humans possess a great variety of pre-lexical mental representations (many of which we share with other animals). On the Conjunctivist account, these pre-lexical mental representations are linked to formally distinct but analytically similar concepts. The latter are sometimes referred to as I-concepts \citep{Jackendoff1989,Jackendoff1990} to signal that the way in which these concepts are to be studied is on the model of the study of language signalled by the use of I-language as opposed to E-language. Thus, “the repertoire of I-concepts expressed by sentences cannot be mentally encoded as a list, but must be characterized in terms of a finite set of mental primitives and a finite set of principles of mental combination that collectively describe the set of possible I-concepts expressed by sentences” \citep[9]{Jackendoff1990}. I-concepts, then, are a uniquely human \textit{subset of concepts} that humans can use to think about the world. It thus follows that “there may be many human concepts that cannot be fetched or assembled via SEMs: acquirable I-languages may not interface with all the concepts that humans enjoy”, for “there may be ways of assembling concepts that SEMs cannot invoke” \citep[325]{LohndalPietroski2018}.
	
If lexical meanings are understood to be instructions to fetch concepts, then phrases are understood to be instructions to combine these fetched concepts in specific ways. So how are these lexicalised concepts conjoined? Let us look at two simple examples taken from \citet[249--250]{Pietroski2010} (see also \citealt{Pietroski2018}). Consider the phrase \textit{kick a brown cow}. The acquisition of \textit{kick} might involve the process in which a dyadic concept like \textsc{kick(x, y)} is paired with a $Phon$, stored in the mind, and then used to introduce a concept of events, \textsc{kick(e)}. The latter is then fetched and conjoined with the other concepts of the phrase \textit{kick a brown cow}, which were fetched in a similar way. \textit{Kick a brown cow} can then be analysed in terms of the instructions to build concepts like \textsc{•[kick(e), ∃•[patient(e, x), •[brown(x),  cow(x)]]]}. “•” indicates a conjunction operator, \textsc{patient(e, x)} is a concept of a thematic relation exhibited by certain events and participants affected in those events, and “∃” existentially closes the participant variable (x). In the same way, \textit{kick a carrot to a cow} can be analysed in terms of the instructions to build a monadic concept like \textsc{•[•[kick(e), ∃•[patient(e, x), carrot(x)]], ∃•[recipient(e, x), cow(x)]]}, which applies to kicks that have carrots as patients and cows as recipients.
	
The internalist semantics claim, then, is that understanding an expression of I-language (or perceiving its meaning) is a matter of (unconsciously) recognising that that expression is an instruction to construct concepts of a special kind. This explanation of meaning also offers an explanation for the creative aspect of language use, for it suggests the procedure by which we combine concepts in recursively productive ways to yield formally new concepts and phrases.
	
It may seem as if this internalist explanation of meaning merely passes the buck to concepts, thus avoiding the crucial explanatory question as to what makes a proposition mean what it does. That is, claiming that lexical meanings have as primitives non-lexical concepts that are then combined to produce formally new concepts might be criticised for assuming (and thus leaving unexplained) the meanings of the primitives. This criticism is unwarranted for several reasons. First, we cannot expect a theory of semantics to explain every primitive, for at some point we move beyond semantics and into cognitive or perceptual psychology (and ultimately into neuroscience). We are concerned here with lexical meaning, not with pre-lexical or non-linguistic conceptual structure. If biolinguistics is on the right track, then it follows that lexical concepts are a distinct subset of concepts available to humans, a subset that is uniquely human in comparison to what is shared with other animals. But what about the other concepts? That depends on what one takes non-linguistic (or pre-lexical) concepts to be, if one takes them to be concepts at all. Since concepts are constituents of thoughts, the debate about the nature of non-linguistic concepts often overlaps with the debate about whether animals can think or with the debate about whether language is the medium of thought. There is a tradition in philosophy that argues that all thought requires language \citep{Malcolm1972,Davidson1975,Davidson1982,Dummett1989,McDowell1994}, whereas others have agreed with \citet[56]{Fodor1975} that the “obvious (and, I should have thought, sufficient) refutation of the claim that natural languages are the medium of thought is that there are nonverbal organisms that think” (see also \citealt{Ryle1968,Slezak2002,deWaal2016}). I do not want to weigh in on the debate of whether cognitive processes as understood by cognitive scientists are the same as what philosophers such as Malcolm and Davidson understand to be thought processes. Whether or not we should conceive of the cognitive processes that humans share with animals as thoughts with concepts is orthogonal to my concerns here. I’ve argued elsewhere that humans share with animals a great deal of thought processes but that humans also possess a unique type of thought that is not available to animals without a language faculty \citep{Asoulin2016,Asoulin2019} (see also \citealt{Gallistel1991,Gallistel2011}).
	
An interesting corollary of the internalist understanding of meanings as effectively tools for reformatting pre-lexical concepts to yield formally new concepts is that these newly created concepts lie at a greater remove from the environment than the concepts that get lexicalised. As \citet{Pietroski2010} discusses, pre-lexical concepts are different to the newly created concepts because the latter require an additional kind of abstraction. That is, the creative aspect of lexicalisation “promotes cognitive integration, by giving us new concepts that fit together in recursively productive ways, [but] at the cost of giving us concepts that fit the world less well than the concepts initially lexicalized” \citep[250]{Pietroski2010}. The process of lexicalisation results in the new concepts (and the phrases of which they form the constituents) not fitting the world in the same way that pre-lexical concepts do. The pre-lexical concepts that we share with other animals, on the other hand, do exhibit “a functioning isomorphism between processes within the brain or mind […] and an aspect of the environment to which those processes adapt the animal’s behavior” \citep[1--2]{Gallistel1990}. There is “a mapping from external entities or events (temporal intervals, numerosities of sets, rates of food occurrence, shapes of patterns, chemical characteristics of foods, members of a matrilineal family within a monkey group, and so on) to mental or neural variables that serve as representatives of those entities” \citep[2]{Gallistel1990}. In other words, in the case of pre-lexical concepts, “the concept of a mental or neural representation depend[s] on the demonstration of a mapping from world variables to mental or neural variables and on a formal correspondence between operations in the two domains” \citep[4]{Gallistel1990}. But lexical concepts and natural language sentences do not work that way. As we’ll see later on, however, the kind of referential semantics that externalism proposes assumes that natural language is referential in this problematic way. But if “our ‘I-fetchable’ concepts are systematically combinable because they were \textit{introduced} to ensure such combinability, then these concepts lie at greater remove from the environment than the concepts we lexicalize” \citep[266, emphasis in original]{Pietroski2010}.
	
There is thus a lack of one-to-one (or even one-to-many) relations between mental representations and things in the world when it comes to natural language. As \citet{Chomsky2000,Chomsky2003a,Chomsky2003b,Chomsky2016} has discussed at length, this is clear for even the simplest words. Take Chomsky’s famous example of London. He remarks that London is of course not a fiction created by our minds but “considering it as London – that is, through the perspective of a city name, a particular type of linguistic expression – we accord it curious properties” such as the following:

\begin{quote}
[…] we allow that under some circumstances, it could be completely destroyed and rebuilt somewhere else, years or even millennia later, still being London, that same city. [….] We can regard London with or without regard to its population: from one point of view, it is the same city if its people desert it; from another, we can say that London came to have a harsher feel to it through the Thatcher years, a comment on how people act and live. Referring to London, we can be talking about a location or area, people who sometimes live there, the air above it (but not too high), buildings, institutions, etc., in various combinations (as in \textit{London is so unhappy, ugly, and polluted that it should be destroyed and rebuilt 100 miles away}, still being the same city). \citep[37]{Chomsky2000}
\end{quote}

We use terms such as \textit{London} to talk and think about the mind-external world but “there neither are nor are believed to be things-in-the-world with the properties of the intricate modes of reference that a city name encapsulates” \citep[37]{Chomsky2000}. Another way to put the matter is as follows. The properties that we attribute to the world via our everyday language are for science the products of our minds (see \citealt{McGilvray2002} for discussion). That does not mean that the mind creates all objects and thus there are no mind-independent objects out there. Rather, it means that the scientific study of the way in which humans conceive the world will be internalist. 
	
Language--world relations are assumed in externalist semantics, and it is almost as often assumed that generative linguistics can provide an analysis of the language side of this relation. \citet[214]{DAmbrosio2019} understands semantics to be a “theory of the contents of natural-language expressions, where such contents are ultimately found in the world, or constructed mathematically out of pieces of reality". On this view, “semantics makes use of lexical postulates that express genuine relations between words and objects or collections of objects, and from these premisses, semanticists derive theorems about what the world must look like for natural-language sentences to be true". Semantics is thus “partly a metaphysical theory—it is a version of the theory of truthmaking". D'Ambrosio claims that his account of the verbs we use in our semantic theorising (such as \textit{refers (to)}, \textit{applies (to)}, and \textit{is true (of)}) shows that internalist and externalist semantics are compatible. \citet{King2007,King2018} also argues that one can accept the central features of generative linguistics and still endorse an externalist semantics for I-languages. But this is far from clear. \citet[805]{Collins2007} discusses the “presumption that -- at some level and in some way -- the structures specified by [generative] syntactic theory mesh with or support our conception of content/linguistic meaning as grounded in our first-person understanding of our communicative speech acts". This presumption is currently shared by many top philosophers of language, but Collins shows that generative syntactic structure both provides too much and too little to serve as the structural basis for the notion of content as understood in philosophy. He argues that the philosopher’s “content, as it were, is the result of a massive cognitive interaction effect as opposed to an isomorphic map onto syntactic structure” \citep[806]{Collins2007}.

\section{What about mind-world relations?}

A common misconception of internalist semantics is that it sees as irrelevant or ignores the environment of the speaker or the context of the speech act. A corollary of this misconception is the argument favouring externalism that argues that, if meaning is construed as “an internal phenomenon” that rejects relations to the extra-mental world, then there will be “a wholesale and incomprehensible relativism concerning truth” and “a total collapse in our belief in the existence of the external world” (\citealt[299]{Ferguson2009}, quoting \citealt[7]{Millikan1984}). That is, “if meaning is equated with intensions of the individual language users” then the extension of words is derivative of these mental representations and thus “this derived extension is never actually put into direct correspondence with external objects but only with each user’s \textit{concepts} of such objects” \citep[299-300, emphasis in original]{Ferguson2009}. In other words, the externalist worry is that if meaning is construed as an internal phenomenon then words are not connected to the external world but only to the user’s internal mental representations of them and thus there is no way to distinguish between a true representation of something in the world and a misrepresentation. 
	
Moreover, it is claimed that realism itself is at stake. Millikan is explicit about this point: she argues that the “assumption that whatever meaning is, it must determine reference or extension is the very essence of realism” \citep[329]{Millikan1984}. She takes issue with what she calls meaning rationalism, with the claim that we can know \textit{a priori} “that in seeming to think or talk about something we \textit{are} thinking or talking about – \textit{anything at all}”. That is, meaning rationalism claims that we can “know a priori \textit{that we mean}” and “know a priori or with Cartesian certainty \textit{what} it is that we are thinking or talking about” \citep[10, emphasis in original]{Millikan1984}. This is “a view of meaning that is completely internal”, a “theory of meaning that sees the extension of words as a function of the intensions of individual speakers, with no way to ensure that these intensions actually correspond to anything in the external world” \citep[299]{Ferguson2009}. Millikan claims that meaning rationalism “permeates nearly every nook and cranny of our philosophical tradition” and that “[i]n order even to come to \textit{comprehend} what meaning rationalism is, what various forms it can take, it is necessary forcefully to fling down on the table something with which to contrast it” \citep[92, emphasis in original]{Millikan1984}. Millikan’s own externalist theory of meaning (teleosemantics) is then offered as a contrast. I should note that meaning rationalism, understood as an internalist (individualist) view of meaning, certainly no longer “permeates nearly every nook and cranny of our philosophical tradition”: the tables have completely turned. As noted above, it is now claimed that “externalism has been so successful that the primary focus of today’s debate is not so much on whether externalism is right or wrong, but rather on what its implications are” \citep[158]{Wikforss2008}.

The externalist worry, then, is that an internalist view of meaning and concepts either leads to an attack on realism or to an idealism of some sort. Millikan remarks that
\begin{quote}
The important thing is that meaning rationalism led to the conclusion that all our genuine concepts are of things that have a most peculiar ontological status. They are things that \textit{are} and that can be \textit{known} to be, yet that have no necessary relation to the actual world. They are things that do not need the world about which we make ordinary judgements in order to be. They must be Platonic forms, or reified “concepts” or reified “meanings” or things having “intentional inexistence” or reified “possibilities” – or else they must be \textit{nothing at all!} \citep[328, emphasis in original]{Millikan1984}
\end{quote}

This worry about internalist theories of meaning of any sort is not limited to Millikan. \cite{Fodor2007}, for example, worries that Chomsky’s internalist semantics is a sort of idealism about meaning. Fodor understands internalist semantics to eschew relations between concepts and the world in favour of relations among the concepts themselves, and he believes that Chomsky is motivated by epistemological concerns. So, as Fodor paints the scene, since knowledge involves representation, the question is how we can know the world independently of the ways in which we represent it. That is, “if representation is itself a kind of mind-world relation, we can’t know whether we ever do succeed in thinking about the world. (/about what our words mean, etc.)” \citep[6]{Fodor2007}. Fodor thinks that one of the motivations for an internalist semantics is this epistemological question, which would then be answered by internalists by holding that representation is constituted by relations among our thoughts only. And since “we can know about such relations [among our thoughts] (by introspection for example) we likewise can know for sure such putatively analytic truths as that bachelors are unmarried, that cats are animals, and so forth” \citep[6]{Fodor2007}. In other words, Fodor argues that internalist semantics is in effect a proposal “to avoid skepticism about knowledge by adopting a sort of Idealism about meaning: all our ideas are ideas about ideas” \citep[6]{Fodor2007}.

Fodor’s misconstrual of Chomsky’s internalism is revealing in several respects, and so it is worth exploring in detail. It shows the intuitive pull that externalist theories of meaning have as well as the theoretical motivations driving externalism (see also the discussion in the next chapter and \citealt{Slezak2002,Slezak2004,Slezak2018}). It is especially revealing in Fodor’s case because he was one of the first to present (along with Jerrold Katz in 1963) a version of semantics that was compatible with generative syntax. Even though Fodor admits that he’s “not at all sure that this is Chomsky’s view”, he gives a long list of reasons for why “succumbing to representational Idealism strikes [him] as a strategy that is to be avoided at all costs” \citep[6]{Fodor2007} (Fodor at times refers to Chomsky’s internalist semantics as semantic Idealism or representational Idealism). Let us look at some of those reasons, addressing each in turn below. \citet[7--8]{Fodor2007} claims the following:
\begin{quote}
(i) That it “is wildly implausible that we don’t, at least some of the time, think about the world. Semantic Idealism seems to deny this and hence to be false on the face of it”. That is, the claim is that internalist semantics “rejects the notion of mind-world correspondence”. 

(ii) The view of meaning Fodor supposes Chomsky endorses apparently requires that there be a great deal of analytic propositions and thus “avoids skepticism about whether bachelors are unmarried; we really can know that they are; in fact, anyone who has the concept BACHELOR must know that they are”. But Fodor counters that “it’s very unclear how this is supposed to work for knowledge of ‘contingent’ propositions (for example the case of one’s perceptually grounded true belief that the cat is on the mat.) In such cases, our knowledges simply can’t come from our grasp of relations among ideas: It’s not part of the idea CAT that this one (the one I’m, just now looking at) is on a mat; and it’s not part of the idea MAT that this one now has a cat that’s on it”. If we suppose that empirical knowledge is a mind-world relation, then “[s]emantic Idealism avoids skepticism about ‘conceptual truths’ only at the cost of making a total mystery of empirical truth”. That is, the claim is that there is something wrong with a semantics that implies that “our concepts are constrained by their relations to one another but not by their relations to the world”.

(iii) Fodor also claims that “semantic Idealism can’t account for the fact that, at least some times, we are able to make rational choices among conflicting beliefs; in particular, among conflicting scientific theories”. That is, it seems to follow that “theories can’t be rationally compared because what their terms in a theory mean is determine[d] internal to the theory. If I think dogs have tails and you think they don’t, then we must ‘mean something different’ by ‘dog’ so there’s no way of settling what appears to be the disagreement between us”.
\end{quote}

In summary, Fodor’s belief is that if we take internalist semantics to claim that semantic relations hold only among ideas, then it follows that we can only think about mind-dependent things. But, he says, “it’s simply untrue that whatever we can think about is mind dependent”, for “we can think about The Grand Canyon, which surely was around before there were any minds and presumably will continue to be when all the minds are gone. The world (consider[ed] as the potential object of indefinitely many thoughts) is prior to the mind. A fortiori, the objects of thought can’t themselves all be mental”. Fodor concludes that it is “an infallible sign of bad semantics that it leads to bad metaphysics” \citep[8]{Fodor2007}.

Note that Fodor (like Millikan and others) is here importing epistemological and metaphysical worries into the debate about the fecundity of a semantic theory. It is not at all clear that epistemological worries about the nature of knowledge and skepticism thereof are relevant to semantic theories. \citet[6]{Fodor2007} takes such relevance to be “more or less truistic” but herein lies the problem. That the externalist conception of semantics is taken to be a truism is perhaps too strong, but the underlying intuition is clear and goes back to the early days of externalist critiques of internalist semantics. In the oft-quoted words of \citet[18]{Lewis1970}, “[s]emantics with no treatment of truth conditions is not semantics”. Lewis was very critical of the internalist semantics of the 1960s \citep{KatzFodor1963,KatzPostal1964}, a version of which is still held by biolinguists today, because he felt that it dealt with “nothing but symbols”. That is, assigning to sentences particular symbols or semantic markers that are supposed to explain their semantic interpretation (what Lewis called Semantic Markerese) is  supposed to be invalid because it “amounts merely to a translation algorithm from the object language to the auxiliary language Markerese” \citep[18]{Lewis1970}. The translation of sentences into symbolic (but, crucially, non-linguistic) expressions is seen by Lewis to be “at best a substitute for real semantics, relying either on our tacit competence (at some future date) as speakers of Markerese or on our ability to do real semantics at least for the one language Markerese” \citep[18]{Lewis1970}. Lewis erroneously takes Markerese to be akin to a natural language and jokes that, given certain qualifications, we might as well translate the sentences into Latin. In other words, Markerese semantics cannot deal “with the relations between symbols and the world of non-symbols – that is, with genuinely semantic relations” \citep[19]{Lewis1970}.

But we do not (and cannot) speak Markerese as a natural language (any more than we can see the visual mental representations our brain uses to process edges in a visual scene, or speak physics) and the fact that Lewis thought this was implied by internalist semantics is again revealing. Internalist semantics proposes a mental structure (an I-language) in virtue of which language production and comprehension are made possible. This structure is not available to introspection, nor is it anything like a natural language that we can understand \textit{qua} speakers, and so the claim that internalist semantic theories rely on the tacit competence of us as speakers of Markerese is a misunderstanding of the internalist project. The claim that internalist semantics is “nothing but” symbol manipulation has remained a consistent misconstrual of internalism since Lewis, the main reason being that semantics is taken to essentially make epistemological and metaphysical claims. Fodor and Lewis are far from alone in this. \citet{Ludlow2003}, for example, argues for a certain kind of language--world isomorphism. As he puts it, “[t]he idea is that the linguistic representations will indeed underwrite our metaphysical intuitions, but that because of this we can expect our metaphysical intuitions to shed some light on the nature of I-language” \citep[154]{Ludlow2003}. In other words, a “plausible hypothesis […] is that any grasp we have on metaphysics is by virtue of our having the linguistic representations that we do” \citep[155]{Ludlow2003}. Ludlow tries to have it both ways. He assumes with Fodor that there exists a language--world relation such that studying the structure of language can shed light on the structure of the world, but he wants to do this within an internalist semantic picture. That is, Ludlow argues that our metaphysical intuitions can be underwritten by the structure of I-language. As he puts it, we “do have substantial independent knowledge of the language faculty, and we can use that knowledge to gain insight into the nature of reality” \citep[154]{Ludlow2003}.

It appears that Ludlow has the same motivation as Fodor in trying to preserve a certain conception of mind-world relations so that it could underwrite truth and realism. It is thus no coincidence that both Fodor and Ludlow (albeit in different ways) propose an externalist referential semantics that could be compatible with I-languages. However, the language--world relation that is posited by externalists is problematic at best (see Chapter 3 for discussion). We can understand language in the language--world relation as either the surface structure of sentences and their pronunciations or as the underlying structure. Both are unhelpful in regard to metaphysics or epistemology. If we take the relation to be between the surface forms of language (say the words \textit{sheep} or \textit{river}) and the world then we can of course agree that those words are used to refer to sheep or rivers and thus agree that there exist sheep and rivers. That is, Ludlow’s and the externalists’ claim is that the word \textit{sheep} refers to sheep in the world, and that we can use this fact to underwrite our metaphysical theory about what can exist in the world. In other words, the claim is that we can successfully refer to sheep in the world because sheep exist in the world. If we can refer to them then they exist, and so we can infer the latter from the former. However, as Chomsky quips, we “can accept all this at the level at which we abandon curiosity about language and mind, about human action and its roots and properties” \citep[290]{Chomsky2003a}. If we take the relation to be between the surface forms of language and the world, are we making a metaphysical claim about the constituents of the world? It is difficult to see how this position can be substantiated, at least if we construe the metaphysical claim to be part of natural science. Alternatively, if we take the relation to be between the underlying structures of language (say the logical forms of linguistic expressions) and the world then we also run into difficulties. If the underlying structures in the relation are couched in terms of logical form or in terms of syntax, then as \citet{Collins2007} shows, there is both too much and too little structure in syntax to serve the purposes that philosophers want them to serve. 

The language--world relation, however, is problematic in a deeper sense. As \citet{Chomsky1995a,Chomsky2000,Chomsky2003a,Chomsky2003b} argues, the analogous argument in regard to sound is absurd, so why is it taken seriously in regard to meaning? That is, suppose (following the thought experiment in \citealt[270 ff.]{Chomsky2003b}) that we talk of a denotational or referential phonology as parallel to a denotational or referential semantics. So that instead of $Phon$ in the expression $Exp = <Phon,Sem>$, which explains the sound properties of the expression in an internalist fashion, the expression lacks $Phon$ and is instead said to \textit{P-denote} some object that is external to the speaker, call it a phonetic value (PV). One can even suppose that some computation on the phonetic value yields the sound component of expressions. The phonetic value could be said to be a construction from physical sound waves and the proposal could be elaborated by taking into account the social context of the speakers. Given this denotational account of phonology, we could then offer an account of communication, translation, and perhaps even language acquisition. So, for example, we could say that “Peter is able to communicate with Tom because the same PV is denoted by their expressions in the language they share (but only partially know)” \citep[271]{Chomsky2003b}. However, as should be clear, a denotational phonology gets us nowhere and in fact “leaves all problems where they were, adding a host of new ones”,  for we “understand nothing more than before about the relation of [the expression] E to its external manifestations” and thus such an “account of communication and other processes is worthless” \citep[271]{Chomsky2003b}. The introduction of phonetic values is completely unhelpful in regard to explaining how humans interpret the sounds of linguistic expressions and there are of course no serious theories of denotational phonology.

The language--world relation in regard to sound is never postulated, whereas the language--world relation in regard to meaning is commonplace. These two postulated relations are of course not identical, but they are similar enough to present a puzzle as to why denotational phonology is rightly dismissed as absurd whereas denotational semantics is taken seriously. One could perhaps make the counter argument that sound doesn’t “mean” anything, but doing so begs the question. Indeed, semantics doesn’t “mean” anything in the everyday sense either. Semantics explains interpretation but does not provide interpretation per se. More on this in the next chapter.

Fodor’s misconstrual of semantic internalism claims that Chomsky’s language faculty hypothesis is an epistemological proposal. As \citet{Collins2004} has argued at length, Fodor reads into Chomsky’s work (and into internalist semantics in general) epistemological proposals about what speakers know about their language. This type of knowledge is said by Fodor to be propositional knowledge as understood in philosophy, thus bringing in issues of truth, belief, and justification. It follows, then, contrary to explicit pronouncements by internalists and biolinguists, that the hypothesis of the language faculty is not a proposal about a specific mental structure in the mind. But as we saw above, the biolinguistic claim is just that. Indeed, whether it is true or not, internalist semantics does not make epistemological claims at all. Jackendoff's remarks in regard to the tension that exists between fundamental questions in a theory of mind are relevant here. There is what he calls a philosophical approach that “grows out of questions of epistemology”, and there is the psychological approach that “grows out of issues in perception” and questions of how the brain functions. Jackendoff argues that this tension has “the flavor of a paradigm split in the sense of Kuhn” \citep[411-412]{Jackendoff1991}. 

Moreover, Jackendoff points to the fact that Fodor “embodies in a single person \textit{both} sides of the paradigm split” \citep[412, emphasis in original]{Jackendoff1991}. On the one hand, Jackendoff notes, Fodor wants to be a psychologist by insisting that an organism’s behaviour is determined by its computational mental representations. On the other hand, Fodor is “holding on very hard to his roots as a philosopher in his use of the terms \textit{true} and \textit{false} with respect to mental representations” \citep[412]{Jackendoff1991}. Jackendoff argues that since the philosophical approach “leads to uncomfortable metaphysical problems”, one should abandon it in favour of the psychological approach that “permits – at least in principle – a revealing account of the phenomena” \citep[416]{Jackendoff1991}. The conclusions Jackendoff draws are supposed to apply to the philosophical and psychological approaches to perception, but as we’ll see in the next chapter, the same “paradigm split” exists in the debate between semantic internalism and externalism where externalists insist upon the relevance of truth and reference to semantic theories whereas internalists reject the relevance of such notions to a scientific semantic theory. 

\begin{center}
***
\end{center}

The next chapter will discuss the externalist approach to semantics, but before moving on let me briefly outline where the argument is going. After detailing the externalist approach, I will argue in the final chapter that internalism is a promising solution to the problem of constructing a scientific theory of semantics, a solution that does not include the various additives that externalists demand be included in a semantic theory. Externalism will thus be discussed from the point of view of explanatory scientific theories. I will ask what externalism is supposed to explain, and whether such an explanation is or can be a scientific one. In contrast to leading externalists, I argue that whatever merits externalism may possess, it is unable to provide a fruitful explanatory framework for a scientific theory of meaning. I argue that an externalist explanation of meaning is concerned with \textit{ascription }and \textit{description }of meaning rather than the \textit{mechanisms} of meaning. That is, externalism is not concerned with the mental mechanisms in virtue of which humans produce and comprehend meaning. In Chapter 4, I argue that a fruitful scientific explanation is one that aims to uncover the underlying mechanisms in virtue of which the observable phenomena are made possible, and that a scientific semantics should be doing just that. Therefore, externalist explanations are not part of the psychological explanation of the mechanisms in virtue of which meaning is made possible. Rather, externalist explanations are an interpretive and hermeneutic explanatory project.

In his discussion of externalism, Chomsky remarks that it faces a choice: if it is conceived as part of ethnoscience, “it is making the factual claim that people (in our culture, or universally) attribute thoughts, beliefs, etc., which they individuate by reference to environment or social context, and then faces the task of clarifying and defending that empirical thesis”. If, on the other hand, externalism is conceived as part of psychology, then “it is making the claim that among the entities of the world, alongside of complex molecules and (maybe) I-languages, are mental states individuated by environment and social context, and it will again have to explain what these entities are, show how they function, and provide empirical confirmation for its conclusions about these matters” \citep[269-270]{Chomsky2003}. Chomsky is skeptical about the prospects for \textit{both} of these construals of externalism and stresses that whichever way we understand externalism, the normal criteria for scientific theory evaluation should be satisfied. Chomsky’s skepticism is warranted in regard to externalism understood as part of psychology, but there is still value to the way in which externalism approaches semantics as hermeneutics in that (if sharpened and explicitly understood in this way) it can help in shedding light on the way in which language users ascribe meanings to words or phrases in particular contexts. But this should not be confused with the scientific task of unearthing the mechanisms in virtue of which language production and comprehension are made possible.

This is where the argument is headed. But first let us explore the externalist approach to semantics.
