\chapter{Writers' aids}
\label{ch:writers-aids}

\section{Introduction}
\label{sec:intro}



%In the early days of the internet (2003), the following text was circulated as an email meme:

%\begin{quote}
%Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae. The rset can be a toatl mses and you can sitll raed it wouthit porbelm. Tihs is bcuseae the huamn mnid deos not raed ervey lteter by istlef, but the wrod as a wlohe.
%\end{quote}


%These claims are not entirely true (in reality, people are slower to read scrambled text, the difficulty increases as the letters move further from their proper place within the word).  But the text caught people's interest because it raises intriguing questions about how writing is processed visually and mentally, and about why standardized spelling exists in the first place if nonstandard spellings are easily understood.  Is standard spelling just a meaningless hurdle invented by middle school teachers?


English has been written in the Latin alphabet since around the ninth century of the Common Era, first by hand, then via the printing press (starting in the 1470s), the typewriter (1860s), and ASCII (1960s) as discussed in \chapref{ch:encodings}.
But English spelling was not standardized until the mid-1600s into the 1700s, influenced by the publication of the King James Bible as well as some of the first dictionaries.  In other words, English was written for seven hundred years without standardized spelling.  The playwright William Shakespeare (1564-1616) did not use a standard spelling of his own name, writing it in different places as \textit{Willm Shakp, William Shaksper, Wm Shakspe, William Shakspere, Willm Shakspere,} and  \textit{William Shakspeare}.


Not only did the most famous English writer not need standardized spelling, but it is also often easy to understand slightly misspelled text: to what extent do the spellling errers in this setnence dirsupt your undertsanding?  It's often argued (e.g., \citealt{White-etal:2008}) that readers focus mostly on the first letter and the overall shape of a word rather than processing each character sequentially.  So why have standardized spelling at all?  What if we let everyone use whatever spelling they want, as  in Shakespeare's time?  Or, if that sounds too chaotic, what if we replaced the notoriously confusing English orthography with the unambiguous International Phonetic Alphabet introduced in \chapref{ch:encodings}?

On the one hand, if English were written in the IPA, it might be easier to pronounce.  We'd no longer have to wonder why \exword{epitome} doesn't rhyme with \exword{tome}, and we'd know from the spelling that \exword{tough} rhymes \exword{cuff} -- eliminating the  silent letters which might be especially confusing to children or people learning English as a second language.

But on the other hand, imagine a person named Leslie, pronounced by different people as  [lɛzli]  `Lez-lee' and  [lɛsli] `Less-lee'.  When you want to find Leslie's voter registration, tax documents, or publications, which spelling do you use?  How could you search using control-F for Leslie in a long document when their name is spelled inconsistently?
When you want to search for a \exword{pen} on Amazon, what if you find different listings under  {[pɛn]}  (the pronunciation used in the Northern United States) versus  [pɪn]  (used in the Southeastern region)?  How would you find the movie \exword{Harry Potter} online if the word \exword{Potter} is listed in some places with the non-rhotic (r-less) British pronunciation  and in other places with the rhotic (r-ful) version?   As you can see, standardized spelling is useful for record-keeping (which is why dictionaries contributed to spelling standardization) and for communicating across groups who use different pronunciations for the same language.

%\mad{I wonder about including examples of words we can read without knowing how to say? (I keep fluctuating, e.g., in the first vowel of \exword{immunocompromised})}


%Actually, there are a number of reasons why standard spelling is useful and

%Consider, for instance, the spelling of family names:  for this, spelling is not standardized, so you will encounter variation.  You are looking for someone called \exsent{Vladimir Zygmunt}, so you look him up online, but you can't find him. Eventually, you find him under the spelling \exsent{Wladimir Siegmund}. Without an agreed-upon standard spelling for English words, similar detective work would be necessary every time we want to look up a word in a dictionary or a thesaurus. A word such as \exsent{itinerary} might, for example, be found under \exword{I}, but without standard spelling it might also be listed as \exsent{etinerary} under \exword{E}, or as \exsent{aitinerary} under \exword{A}. Standard spelling makes it possible to find a word in a single, predictable place.

%Standard spelling also makes it easier to provide a single text that is accessible to a wide range of readers.  People of different social or regional backgrounds speaking different dialects can still read and write a common language by using the same spelling conventions. The word that a Northern US speaker pronounces as {[pɛn]} (so with a vowel as in \exword{bet}) might sound more like {[pɪn]} (a vowel as in \exword{bit}) when in the mouth of a Southern speaker. Since both agree that the standard spelling of this word is \exsent{pen}, it becomes possible to write texts in a general way, independent of the particular pronunciations used in a given region or by a specific social group.

  
%Related to the last point, in the US today being able to use standard spelling is an expected consequence of being well-educated.  This was not the case in England in the age of Shakespeare. But today, anyone wanting to make a good impression in formal social interactions such as in job applications had better be prepared to spell words correctly. Of course, the spelling conventions that are suitable for informal settings such as text messages are quite different from those of formal letters: \exsent{CU L8R} (\exword{see you later}), \exsent{ROTFL} \exword{rolling on the floor laughing}), \exsent{AFAIK} (\exword{as   far as I know}).

%Finally, standard spelling is also important for computer software that deals with written text. Optical character recognition (OCR) software, for instance, can use knowledge about standard spelling to recognize scanned words even for hardly legible input. It can piece together the letters that it cannot read by noticing that they are next to letters that it can read, and using knowledge of spelling to fill in the gaps.

So whether you find English spelling annoying or fun, it is here to stay. 
Luckily, for those of us who can never remember how to spell
\exword{Massachusetts} or \exword{onomatopoeia}, or who consistently
confuse \exword{they're} and \exword{there}, there is technological
support in the form of spell checkers, grammar checkers, and predictive auto-completion of words and sentences.


These tools,  referred to collectively as \keyword{writers'  aids},  aim to automate the tedious part of writing, allowing the writer to focus on the ideas.
   We start our exploration of language technology applications
in this book with writers' aids because we assume they are familiar to
everybody, and they also provide a platform to introduce many
linguistic concepts which recur in the rest of the book. 

\section{Kinds of spelling errors}

 We begin our tour of writers' aids with spelling correction, in which a computer automatically identifies spelling errors and suggests correctly-spelled replacements.  It is always useful in natural language processing (NLP) to start by exploring examples: here, the types of spelling errors that we may encounter.  %This is common in natural language processing (NLP): we may have an intuitive sense of the problem -- in this case, spelling errors -- but the first step of developing technology is to understand the examples or data to be covered and the concepts which are relevant for capturing the properties we see exemplified by the data.

\subsection{Nonword errors}
\label{nonword:errors}
So-called \keywordAs{nonword errors}{nonword error} are errors resulting in
words which do not exist in the language, for example, when the
article \exword{the} is erroneously written as the nonword
\exword{hte}. 

One can characterize such errors in two ways: on the one hand, we can
try to determine why the error happened. On the other hand, we can describe how the word that was written
differs from the target word that the writer
intended.

Looking first at the \keyword{error cause}, nonword errors are usually
caused either by mistakes in typing or by spelling confusions.
\keywordAs{Typographical errors (typos)}{typographical error} arise when the person knew how to spell the word correctly,
but mistyped.  Perhaps they accidentally pressed an adjacent key (so the keyboard layout makes certain typos more likely than others); perhaps they pressed the right keys in the wrong order (typing \exword{hte} for \exword{the}); or perhaps they misused the space bar (typing \exword{myphone} for \exword{my phone}).  Spacing errors are tricky because they disrupt the default assumption that spaces demarcate word boundaries: rather than just turning a single non-word into a single real word, a spell-checker would have to find a way to split the non-word string at the right place into two different real words.

%Although determining where a word starts and where it ends, so-called \keyword{tokenization}, is a tricky issue for languages other than English (and even sometimes in English), most spell checkers assume that the string which needs to be checked is contained between two spaces.  (See chapter~\ref{sec:adding-linguistic-analysis} for more on tokenization.)  Thus, split errors are often difficult to correct because the checker cannot tell that the adjacent strings are intended as part of the same word.  

%On the other hand, \keywordAs{run-ons}{run-on   error} are errors which are caused by under-use of the space bar between two words, turning two separate words into one, such as \exsent{space bar} becoming \exsent{spacebar}. Finally, there can be errors where the space between words is present, but not in the correct place, as can happen if you write \exsent{atoll way} instead of \exsent{a tollway}. This is going to be especially hard to correct if, as here, the  results of the mistake turn out to be real words.


%Errors can happen for other reasons, such as a particular keyboard layout that causes certain typos to be more likely than others. Yet, while one can hypothesize a wide range of such underlying error causes, it is difficult to obtain definitive evidence about what was in someone's mind when they typed what they did. 



\keywordAs{Spelling confusions}{spelling confusion}
occur when the person does not know how to spell the word.
A writer might, for example, never have seen the word
\exword{onomatopoeia} or might remember that \exword{scissors} has
something odd about it, but not be able to recall the details.  For
the record, \exword{scissors} has at least three odd features: it's unusual for \exword{sc} to represent the {[s]} sound; for  \exword{ss} to represent {[z]}, and for \exword{or} to spell the suffix that is more often written \exword{er}.
%\mad{There may be an opportunity to point to the CALL material, as nonnative speakers will struggle even more (or maybe it's better just to point backwards in the CALL chapter to here?) $\to Detmar$} - I added a reference to the CALL chapter when I mention writers aids adapted to specific populations. - LG
In
English, when you know how to say a word, you may still be a long way
from knowing how to spell it correctly.

Spelling confusions based on sound are obviously a major problem; not
only do we spell based on analogy with other words, but we often use
typical sound-to-letter correspondences when a word is unknown.  Thus,
spellers may leave out silent letters (\exword{nave} for
\exword{knave}), may replace a letter with a similar sounding one
(\exword{s} for \exword{c} in \exword{deside} in place of
\exword{decide}), or may forget nuances of a rule \exword{recieve}:
\exword{i} before \exword{e} except after \exword{c}). In languages with a more \keyword{transparent writing system} such as Finnish or Korean, writers face fewer opportunities for such spelling confusions than in a language like English which preserves various spelling irregularities as historical vestiges.

%In some languages, such as Finnish and Korean, but not English, if you know how to say a word, you will be able to make a very good guess about how to spell it. We say that languages like this have a transparent writing system.  In these languages, we do not expect to see very many errors caused by confusions about the sound-to-letter correspondence.

%When finding many substitutions of one specific letter for another, the mistyped letter is often right next to the correct one on a common keyboard layout. For example, one finds many examples where the letter \exword{s} has been mistyped for the letter \exword{a}, which is related to the fact that the \exword{a} and \exword{s} keys are next to another on a typical English QWERTY keyboard. We can, of course, also turn this correspondence around and use the information about keyboard proximity to calculate the likelihood of particular substitutions and other error types to occur. This will be useful when we look at ranking candidates for spelling correction in section~\ref{sec:candidate-ranking}.

 %As a result, the analysis of spelling errors and the development of spell checking technology has primarily focused on a second method for classifying errors. Instead of trying to identify the underlying cause of the error, we try to describe the way in which the misspelled word differs from the target word that the writer was aiming for.  There is an obvious problem with this approach if the reader cannot tell what the intended target was, but this turns out to be rare, at least given enough context.

In motivating the need for spell-checkers, it helps to understand why people misspell words.  It may also be useful to distinguish typos versus spelling confusions in order to provide the most helpful feedback to a user. 
 But for the practical task of spelling correction, we can often ignore the error cause and focus instead on how the error would need to be fixed. What modifications would we need to  turn the misspelled word into the correct spelling? With that perspective, there are four types of operations that we can perform to get to the correctly spelled word:
% \mad{I added a few words here, as "insertion" and "deletion" need to be overly clear (in my opinion) in which direction they're going} - sounds good thanks- LG.

%Research on spelling errors distinguishes four classes of \keyword{target modification} errors on the basis of the surface form of the misspelled word and the likely target word. 

\begin{itemize}
\item \mbox{}\keywordAs{Insertion}{insertion}: Add a letter.
  \begin{itemize}
  \item[]   \exword{aquire}  $\rightarrow$ \exword{a\uline{c}quire}
  \end{itemize}
  \item \mbox{}\keywordAs{Deletion}{deletion}: Delete a letter.
    \begin{itemize}
    \item[] \exword{argu\uline{e}ment} $\rightarrow$ \exword{argument}
    \end{itemize}

\item \mbox{}\keywordAs{Substitution}{substitution}: Replace one letter with another.
  \begin{itemize}
  \item[] \exword{cal\uline{a}ndar} $\rightarrow$ \exword{cal\uline{e}ndar}
  \end{itemize}
  \item \mbox{}\keywordAs{Transposition}{transposition}: Swap the position of two adjacent letters.
    \begin{itemize}
    \item[]  \exword{con\uline{cs}ious} $\rightarrow$ \exword{con\uline{sc}ious}
    \end{itemize}
\end{itemize}



The \keyword{edit distance} between two strings quantifies the minimum
number of operations that it takes to transform one string into the other.  For example, the edit distance between \exword{Catherine} and \exword{Kathryn} is four: 

\begin{enumerate}

\item Replace \exword{C} with \exword{K} [\exword{Katherine}]

\item Delete the first \exword{e} [\exword{Kathrine}]

\item Delete the second \exword{e} [\exword{Kathrin}]

\item Replace \exword{i} with \exword{y} [\exword{Kathryn}]

\end{enumerate}

Note that the order of these operations often does not matter; we would get the same result if we deleted the first \exword{e} before swapping the initial \exword{C} for a \exword{K}.  But the order may matter in the case of transpositions, where two adjacent letters are swapped: whether two letters are adjacent or not may depend on what other operations have come before.

Actually, some people consider a transposition to have a cost of two, involving one substitution and then another. And some give a substitution  a cost of two, equivalent to a deletion followed by an insertion. These decisions are arbitrary value judgments based on what works best for a specific purpose.
%we will return to such considerations in section~\ref{sec:ranking-candidates}.

%\faye{Added a clarification for Levenshtein distance because I believe it only applies when these three operations are used: \href{https://nlp.stanford.edu/IR-book/html/htmledition/edit-distance-1.html}{Source}} - sounds good thank you. -LG.

When the operations used to calculate edit distance are insertion, deletion, and substitution, the edit distance is sometimes called the \keyword{Levenshtein distance}, after the Soviet mathematician who proposed it  \citep{Levenshtein:1966}. This flexible idea has also been used \citep{Sankoff:1992} to quantify the similarity between DNA sequences (represented as strings of letters).  It is sometimes specified to refer to the \emph{minimum} number of operations in order to rule out perversely inefficient pathways from one word to another, for example mapping \exword{Catherine} to \exword{Kathryn} by replacing the letter \exword{a} with \exword{x}, then replacing it again with \exword{a}.  

We can use edit distance to quantify the steps to turn a misspelled word into the correctly spelled target word, or inversely to quantify how the target word can be turned into the misspelling.  The edit distance should be the same in either direction.


%You may feel that this way of describing errors is quite mechanical. We agree that the underlying causes described a few paragraphs ago are more interesting for understanding why errors arise. But we will see in the following discussion and when we introduce techniques for automatically identifying and correcting errors in section~\ref{sec:isolated-word} that viewing errors in terms of target modification is a very useful perspective in practice.

%Similarly, to rank candidate corrections, we can assign each a distance value telling how far off it is from the misspelling.  A specific instance of this is called \keyword{edit distance}, which measures how many operations -- insertions, deletions, substitutions, and (sometimes) transpositions -- it would take to convert a misspelling into the correct spelling.  (Transpositions are often not used in calculating edit distance and instead are treated as two substitutions.)  The candidate with the minimum edit distance is the top-ranked one, where by \subkeyword{minimum edit distance}{edit   distance} -- often referred to as \subkeyword{Levenshtein   distance}{edit distance} -- we mean the minimum number of operations it would take to convert one word into another.  For example, for the misspelling \exsent{obe}, the word \exsent{oboe} has a distance score of one (insertion of \exword{o}), while \exsent{hobo} has a score of two (insertion of \exword{h}, deletion of \exword{e}), so \exsent{oboe} is the better choice.


%Classifying errors in terms of target modification allows us to identify the most common kinds of insertions, deletions, substitutions and transpositions people make. By looking at long lists of examples of actual errors that people made (\url{https://en.wikipedia.org/wiki/Commonly_misspelled_English_words}), it then also becomes possible to infer some likely error causes. 



\subsection{Real-word errors}
\label{sec:real-word-errors}

 Normally, when the result of an
error is a nonword, an automated system can detect the error by trying
to look up the word in a dictionary: if the word is not found, it may
very well be the result of a spelling error. But this will not work
for errors that result in real words, because real words are listed in the dictionary.  Here, the error can only be identified by considering the surrounding context of the word.
%\mad{I tweaked this sentence because "There are three different kinds" seemed strong for a semi-arbitrary distinction} - ok sounds good thanks - LG
We can distinguish three different kinds of \keyword{real-word errors}, each involving 
context in different ways.  As the appeal to context becomes more elaborate, the task of
detecting the error becomes progressively more difficult.

First, there are \keywordAs{local syntactic errors}{local syntactic error}.  Generally
speaking, syntactic errors are errors in how words are put together in
a sentence: different parts-of-speech are used in the wrong places in a
sentence or the wrong grammatical forms are used.  Local
syntactic errors are those where the syntactic violation is
detectable by looking within one or two words in the sentence.  In \ref{ex:local}, for example, a possessive pronoun
(\exword{their}) takes the place where a noun phrase (\exword{there})
should be.  Normally, we expect a possessive pronoun to be followed by a possessed noun, as in \exword{their book}, but here it is incorrectly followed by the past-tense singular verb \exword{was}.


\ea \label{ex:local} *Their was a problem yesterday.
\z 
  
To flag that this is an example of an ungrammatical
sentence, we mark it with an \keyword{asterisk (*)}. This is a common
convention in linguistics -- and also alerts editors that the error in the example is intended, rather than
something to be corrected for publication.

In contrast to local syntactic errors, 
\keywordAs{long-distance syntactic errors}{long-distance syntactic
  error} are those involving syntactic
violations spread out over many more words.  In 
\ref{ex:long}, \exword{the cabinets are} is a perfectly coherent string of English, but it is incorrect here because \exword{are}  corresponds grammatically to the singular subject noun \exword{key}, not to the plural noun \exword{cabinets}, which actually sits within a prepositional phrase.  The erroneous agreement with plural \exword{cabinets} is called \keyword{agreement attraction} \citep{BockMiller:1991}, and raises questions for psycholinguists about how people represent sentence structure in their minds.

\ea \label{ex:long} The key to the cabinets are on the counter.
\z

Finally, there are \keywordAs{semantic errors}{semantic error}.  These are errors in meaning: The
sentence structure is grammatical, but the problem is that the
sentence does not mean what was intended.  In \ref{ex:semantic},
\exsent{brook} is a singular noun -- just like the presumably intended
\exsent{book} -- but it does not make sense in this context.


  \ea \label{ex:semantic}I checked out a brook  from the library.
\z 

It is not always easy to decide whether an error is a spelling error
or an error in grammar.
Example \ref{ex:long}, for instance, 
could be thought of as a real-word spelling error in which the typist
simply omitted the \exword{s} of \exword{keys}, or it could be thought
of as a
problem with subject-verb agreement, which is  a
grammatical error.
Without knowing what is going on in the writer's mind,
%\mad{Again, could be an opportunity to point ahead to CALL, where learner models are employed?}  - I am not sure if this really relates super clearly to learner modeling? -LG
it is hard 
to know which explanation is better.
In \ref{ex:local}, on the other hand, it seems obvious that the
writer was trying to produce \exword{There}, so it is safer
to classify this as a real-word spelling error.   The distinction between spelling errors and grammatical errors is not just philosophical: A writers' aid may be designed to give different feedback to writers depending on the presumed nature of the error.

\subsection{How common are spelling errors?}

When proficient adult English speakers type on a full-size keyboard, they misspell approximately two to three percent of all typed words \citep{Flor-etal:2015}. 
%Of course, people don't type entire words on a mobile phone the way they do on a keyboard; they often rely on the phone to auto-complete and auto-correct the word for them.
About eighty percent of misspellings are non-word \keyword{single-error misspellings}, with an edit distance of 1 from the intended spelling.  In most cases, the writer is correct about the first letter of the word and the total number of letters. About twelve percent of spelling errors involve real words \citep{Flor-etal:2015}.


On a mobile phone, however, as much as 42 percent of English words are misspelled (according to 2022 blog post by the writers' aid company Grammarly\footnote{\url{https://www.grammarly.com/blog/mobile-communication-study/}, accessed 2024-07-01.}), and the rate is even higher for languages such as French where some letters are accented (\textit{caf\'e}), which people often rely on the phone to correct.  In general, people rely more heavily on auto-complete and auto-correct when typing on phones.
One might, therefore, expect a far greater proportion of \keyword{multi-error misspellings} -- those that are further in edit distance from the intended spelling.  The rate of \keyword{real-word errors} may also be higher if auto-complete inserts real words that the author did not intend.


These trends are based on the writing of adults who are proficient in English; but the pattern of errors may look different for children learning to write,  people with dyslexia, or people learning English as a second language (see \chapref{ch:call} for more on language learning).  Such groups may benefit from writers' aids designed for their specific needs.


% Note that these trends are based on data from good writers of native speak\-er English, so will not be fully applicable to other populations, such as non-native speakers or people with dyslexia. Because traditional spell checkers are built on the assumption that the writers are native speakers, they are not good at correcting non-native errors.  Fortunately, it can sometimes help to know that we are dealing with non-native speakers: there are specific types of spelling errors which are characteristic of people learning English as a foreign language.  For example, it is common to use an English word which looks like a word from their native language.  An example for such so-called \keywordAs{false friends}{false friend} or \keyword{lexical transfer} would be the common use of \exsent{become} in place of \exsent{get} by German-speaking learners of English -- an error which arises since the German word \exsent{bekommen} means \exsent{get} in English.  Chapter~\ref{ch:call} discusses the use of computers in foreign language teaching and learning in more detail.

%but the rate of errors is higher for less-proficient writers of English, including English language learners and schoolchildren.

%Looking at where such errors arise, we can distinguish \keywordAs{single-error mis\-spell\-ings}{single-error mis\-spell\-ing}, in which a word contains one error, from \keywordAs{multi-error misspell\-ings}{multi-error misspell\-ing}, where a single word contains multiple instances of errors.

% Research has found upwards of 80\% of misspellings to be single-error misspellings, and ``most misspellings tend to be within two characters in length of the correct spelling'' \citep{kukich:92}.  The two observations naturally are connected: both insertion and deletion errors only change the length of a word by one character, whereas substitution and transposition errors keep the length the same.



%The insight that writers primarily make single-error misspell\-ings within two characters in length of the correct spelling means that spell checkers can focus on a much smaller set of possible corrections and therefore can be relatively efficient. A related observation further narrowing down the search for possible corrections is that the first character in a word is rarely the source of the misspelling  so that the misspelled word and the correct spelling usually have the same first letter.




\section{How to build a simple spell-checker}


Having explored the sources and types of errors, we can now look at how to
automatically detect and correct them.  We take inspiration from a classic 2007 blog post \citep{Norvig:2007} by Google Research director Peter Norvig, who presents a spell-checker in 22 lines of Python.   For simplicity, we follow Norvig in illustrating the \keyword{context-independent} correction of non-word errors (e.g., correcting \exword{hte} to \exword{the}) -- setting aside real-word errors and those that can only be corrected with reference to the surrounding context (e.g., \exword{too computers}).



\begin{tblsfilledsymbol}{\underthehoodsubsection{Peter Norvig's spell-checker}}{glass}
\begin{underthehood}

Here is Norvig's spell-checker.  To run it, you would need a big text file of correctly-spelled English called ``big.txt'' saved in the same directory as your Python code.

\begin{lstlisting}[language=Python]

import re
from collections import Counter

def words(text): 
    return re.findall(r'\w+', text.lower())
    #Split text into words, lower-case all words.

WORDS = Counter(words(open('big.txt').read()))
#Read in a big txt file of correctly-spelled words.

def P(word, N=sum(WORDS.values())): 
    #Probability of `word`.
    return WORDS[word] / N

def correction(word): 
    #Most probable spelling correction for word.
    return max(candidates(word), key=P)

def candidates(word): 
    #Generate possible spelling corrections for word.
    return (known(
            [word]) or known(
            edits1(word)) or known(
            edits2(word)) or [word])

def known(words): 
    #The subset of `words` that appear in the dictionary of WORDS.
    return set(w for w in words if w in WORDS)

def edits1(word):
   #All edits that are one edit away from `word`.
   letters    = 'abcdefghijklmnopqrstuvwxyz'
   splits     = [(word[:i], word[i:])  
                 for i in range(len(word) + 1)]
   deletes    = [L + R[1:]             
                 for L, R in splits if R]
   transposes = [L + R[1] + R[0] + R[2:] 
                 for L, R in splits if len(R)>1]
   replaces   = [L + c + R[1:]  
                 for L, R in splits if R for c in letters]
   inserts    = [L + c + R               
                 for L, R in splits for c in letters]
   return set(deletes + transposes + replaces + inserts)

def edits2(word): 
    #All edits that are two edits away from `word`.
    return (e2 for e1 in edits1(word) for e2 in edits1(e1))

### Now try it:

>>> correction('speling')
'spelling'

>>> correction('korrectud')
'corrected'

\end{lstlisting}
\end{underthehood}
\end{tblsfilledsymbol}


A simple spell-checker uses three steps: It detects errors; generates candidate corrections; and ranks the candidate corrections to choose the best one.  

\subsection{Detecting errors} \label{sec:detecting-errors}

To detect a non-word error, we can compare a typed word to a list of correctly spelled words.  If the typed word is on the list, it is a real word.  If it is not on the list, it is taken as a non-word misspelling.

Where do we get the list of correctly spelled words?  We could get it from a dictionary, in which case we might have to generate rules to allow all inflections (\exword{runs, running}) of the word that appears in the dictionary as a \keyword{lemma} (\exword{run} -- the uninflected form).   We'd also have to decide what to do about capitalization: Should \exword{Cat} be considered the same word as \exword{cat} or \exword{CAT}?  What about \exword{alex} versus \exword{Alex} versus \exword{ALex}?  We could also get our list of correctly spelled words from a \keyword{corpus}, a large dataset of (hopefully correctly-spelled) text from sources such as books, news articles, and magazines. 

Wherever we get the list of correctly spelled words, people who use the spell-checker might feel that it includes too many or too few.  Maybe the corpus contains some typos that should actually be corrected (certainly true of corpora based on Wikipedia, which is not professionally copy-edited).  Maybe it includes some words that technically appear in the dictionary but are so rare that they are usually actually typos, such as \exword{teg} (a sheep in its second year, but only sheep farmers know this).   Maybe it seems racist for the  list of correctly spelled words to include traditional Anglophone names such as \exword{Jane} but not names from other parts of the world such as \exword{Shalini} (exemplifying the larger issue that text data can perpetuate racial biases). And of course, new words are invented all the time (for example, \exword{monoamorous} -- antonym to \exword{polyamorous}), so any static word list is inherently incomplete.  The dictionary is sometimes invoked as the ultimate authority on language (\exsent{look it up in the dictionary!}), but dictionaries have to keep up with the usage and creativity of the wider society.

Once we have curated this list of correctly spelled words, detecting a
non-word error is straightforward: If a word appears on the list, then
it is a real word and should not necessarily be spell-checked.  If it
is not on the list, then it is likely a misspelling and so we want to
generate candidates for the correct spelling.



%\subsubsection{Dictionary methods} \label{sec:dictionary-methods}
%The simplest way to detect nonwords is to have a dictionary, or spelling list; if the word is not in the dictionary, it is a misspelling.  While this is a good general idea, there are several issues to consider in constructing a dictionary and looking words up in the dictionary.

% Firstly, one must realize that a dictionary is inherently incomplete, due to the productivity of language.  New words are continually entering the language, such as \exsent{d'oh}, while at the same time, old words are leaving.  For example, in Shakespeare's time, \exsent{spleet} (`split') was a word and thus correctly spelled, while today it is not.  Foreign words, differently hyphenated words, words derived from other words, and proper nouns continually bring new words to a language.  A dictionary can never be exhaustive.

% Even if we could include every word, however -- such as all proper nouns -- it is not clear that this would be desirable.  Including the proper last name \exsent{Carr} may result in not finding the misspelling in \exsent{The Great American Carr Show}.  A writer does not use every possible word in existence, and no two writers use exactly the same set of words.  Should each person then have their own private dictionary?

%The answer is both yes and no.  For the majority of writers, we can narrow down a dictionary by considering only words which are ``common''.  Rare words are likely to become confused with actual misspellings, and so they should not be included.  For example, \exsent{teg} is a word (`a sheep in its second year'), but most English writers do not know or use this word. So, if it is found in a document, it is probably a misspelled form of, for instance, \exsent{tag} or \exsent{beg}. The few writers who do intend to use \exsent{teg} will probably not  mind if the spell checker flags it.

% We can also focus on specific populations of users by utilizing separate dictionaries for particular domains.  For the average user, \exsent{memoize} is a misspelling of \exsent{memorize}, but for computer scientists it is a technical term. We can take this to the level of the individual user, too.  Many spell checkers allow users to update their own personal dictionaries, by adding words or spelling rules.  The free checker Hunspell (\url{http://hunspell.sourceforge.net/}), for instance, allows users to easily change the dictionary they use.

%Turning to the internal parts of a dictionary, a general issue for dictionary construction is whether or not to include inflected words as separate entries in the dictionary.  For example, should \exsent{car} and \exsent{cars} be stored separately?  Or should \exsent{car} be stored as a single entry, indicating its part-of-speech (noun) and that it is regular, i.e., takes the standard \exword{-s} plural ending?

%The choice is one of efficiency and where to allocate the work.  If they are separate entries, then the lookup mechanism is straightforward, but there is an increase in the number of words.  If inflected words are not stored as separate entries, on the other hand, then there are far fewer lexical entries, but we have to strip the affixes (prefixes and suffixes) from the words before looking them up. Furthermore, one has to handle exceptions to affix rules in some systematic way, possibly by separate entries for, e.g., \exsent{ox} and \exsent{oxen}.  And we have to prevent overgeneralizations of affix rules: there must be a special note on \exsent{ox} that it is irregular, so that \exsent{oxs} or \exsent{oxes} will not be accepted as correct spellings.

% When dealing with thousands of words, looking up those words becomes non-trivial; each word has to be stored somewhere in computer memory, and finding where the word is takes time.  Thus, an efficient storage and lookup technique is also needed -- just as efficient storage is needed for webpages (see \emph{Search engine indexing} under chapter~\ref{sec:web-internal}).  Words could be stored alphabetically, and each word encountered could be searched for, but the words in English are not evenly distributed over the entire alphabet: e.g., the number of words which start with \exword{x} are much fewer than those that start with \exword{s}.  Computer scientists study and develop various ways to evenly distribute word entries in memory, in order to, for example, more quickly find words in the middle of the \exword{s} section.

\subsection{Generating candidates}
\label{sec:generating-candidates}

% \mad{In general, the excised material in this chapter was good to cut, but I wonder about including something about SOUNDEX, even a sentence or two on the main concept? It captures the idea that what we're creating bins of reasonable candidates (edit distance of 1 is a great way to do it; soundex simply does it differently) - and a person could think about, e.g., increasing edit distance if it meets, e.g., phonetic criteria or whatever?}
% \mad{New thought: Put SOUNDEX in as part of an exercise??}

We can generate candidates for the correct spelling using edit distance.  In particular, given that most misspellings are single-error misspellings, we can compute all alternative spellings that are within an edit distance of 1 from the misspelled word -- restricting our attention to those that actually appear on the list of correctly spelled words.  For the misspelling \exword{catt}, our candidate corrections would be \exword{cat, cats, catty, cast, cart},  and perhaps (depending on how we handle capitalization) the names \exword{Cato} or \exword{Matt} or the abbreviation \exword{Capt}.  We could also consider alternatives within an edit distance of 2 (for \exword{catt}, we'd entertain possibilities such as \exword{hat} or \exword{sat}) or even 3 (\exword{can't}); but most misspellings are within an edit distance of 1, and the more edits we allow, the more alternatives we have to sort through (to check them against the list of correct spellings).


\subsection{Ranking candidates}
\label{sec:ranking-candidates}

The candidate spellings also have to be ranked so that, depending on the user interface of the spell-checker, the ``best'' candidate can be chosen automatically or presented as the top suggestion for the writer to approve.  Of course, the ``best'' candidate is what the writer most likely intended.  But we can't see inside their mind, so how do we figure out what they intended?

To answer this question, we introduce some ideas that will be
important throughout this book, namely the \keyword{noisy channel
  model} inspired by \keyword{information theory} and \keyword{Bayes'
  Rule}.  We will take a close look at the mathematics in Chapter
\ref{ch:text-classification}, but for now we will keep the explanation
intuitive.  The writer has an intended message in their mind (the word
they want to write), but this message is sent through a ``noisy
channel'' -- typing it imperfectly on a keyboard.  As the receiver of
this message, we (as readers and spell-checkers) have to make our best
guess of what this person meant (the clean message) given what they
typed (the distorted message that comes out of the noisy channel).  We
can use two different pieces of information: The likelihood of a given
intended message overall; and the likelihood of typing what they typed
given that particular intended message.  We combine these two pieces
of information to arrive at the most likely intended clean message
(the correct spelling) given the noisy typed signal (the misspelling)
that we observe.

For example, imagine that someone types the word \exword{Dsvid}.  Let's imagine three candidates for what this person intended to type: \exword{Dsvid, Sara,} and \exword{David}.  First: did they mean to type \exword{Dsvid}?  On the one hand, it is extremely unlikely that anyone would intend to send the message \exword{Dsvid}, as we can infer from the fact that \exword{Dsvid} does not appear on any list of correctly-spelled English words and also is extremely infrequent in corpora of English text: The probability of \exword{Dsvid} being the intended message is quite low.   On the other hand, if someone did for some reason want to type the word \exword{Dsvid}, it is very likely that they would indeed type the string \exword{Dsvid} (as reflected by the fact that the edit distance from \exword{Dsvid} to \exword{Dsvid} is 0); the probability of observing the noisy message \exword{Dsvid} given the intended message \exword{Dsvid} is pretty high.  But overall, combining these two pieces of information, \exword{Dsvid} is probably not what they meant because \exword{Dsvid} is a very unlikely string for the writer to have intended.

Next, did they mean to type the word \exword{Sara}?  On the one hand, \exword{Sara} is a pretty likely string of English: It's in a dictionary and it occurs somewhat frequently in corpora of English text, so the probability of \exword{Sara} being the intended message is quite high.  On the other hand, if someone intended to type the word \exword{Sara}, it is extremely unlikely that they would have typed the string \exword{Dsvid} (as reflected by the fact that the edit distance from \exword{Dsvid} to \exword{Sara} is 4): The probability of observing the noisy message \exword{Dsvid} given the intended message \exword{Sara} is very low.  So \exword{Sara} is probably not what they meant.

Finally, did they mean to type \exword{David}?  On the one hand, \exword{David} is a pretty likely string of English, appearing in dictionaries and corpora: The probability of \exword{David} being the intended message is high.  Additionally, if someone intended to type the word \exword{David}, it's quite likely that they would have typed \exword{Dsvid} (as reflected by the fact that the edit distance between them is 1): The probability of observing the noisy message \exword{Dsvid} given the intended message \exword{David} is high.  To arrive at \exword{David}, we combined two pieces of information: The probability of the intended message, and the probability of the noisy signal that we observe given that intended message.  We choose the intended message that is most probable according to these two combined metrics.

To automate this reasoning process, Norvig's simple spell-checker first considers all correctly-spelled candidates within an edit distance of 1 from the misspelled word, and chooses as the ``best'' correction the one that is most frequent in a corpus of English text.  If there are no correctly-spelled candidates within an edit distance of 1, then Norvig's spell-checker considered all candidates within an edit distance of 2, and again chooses as the ``best'' correction the one that is most frequent in an English corpus.  If there is no word within an edit distance of 2, the spell-checker gives up and just returns the misspelling.  Edit distance reflects the probability of the misspelling given the intended message, and corpus frequency reflects the probability of the intended message.

If we wanted to make Norvig's spell-checker more sophisticated, we
might note that \exword{s} and \exword{a} are right next to each other
on a keyboard, so that \exword{Dsvid} is a particularly likely
mistyping of \exword{David}, more likely than \exword{Dbvid} even
though they are both equidistant from \exword{David} by edit distance.
Another way to improve this spell-checker is to consider the
linguistic context of each word, which we explore in
Section~\ref{correction-with-ngrams}.


%\mad{Would it be worth pointing out that substituting \textit{s} for \textit{a} is also quite likely, getting in some mention of probabilities of different kinds of errors?}  So \exword{David} is the most likely correction.
%\faye{Probability of different kind of errors could be interesting to discuss in a different section, but as a student-reader distracting here and make it harder to understand the two metrics}
% i added this in as the final paragraph of the section - LG.

%Starting with the misspelling, we use probabilities of insertions, deletions, substitutions, and transpositions to find the correct spelling in our candidate set with the highest probability (see discussion on distance measurements below).  We refer to this as the correction which \keywordAs{maximizes the probability}{maximize probability}, i.e., is the most likely correction given the misspelling.


%In general, we could design either an \keyword{interactive spell checker} or an \keyword{automatic spelling corrector}.  Interactive spell checkers try to detect errors as the user types,  suggesting corrections that the user can choose to use or ignore. Automatic spelling correctors detect and correct errors  without input from the user -- which is designed to be helpful, but can sometimes be annoying if one's intended message is auto-corrected to something else. 

% ADD SOMETHING LATER about user-interface questions -- suggested corrections, automatic corrections, red line under it, etc etc etc, this can go LATER BELOW. 

%Spelling correction involves the following three tasks,  in increasing order of difficulty. First, there is \subkeyword{nonword   error detection}{nonword errors}, the task of finding strings in a text which are not words.   For some purposes, simply detecting a misspelling is enough for the writer, who can then fix it by hand.  Sometimes, however, we require the second task, \keyword{isolated-word error  correction}.  As the name implies, the goal of this task is to correct errors or suggest corrections, but without taking the surrounding context into account.  For example, a corrector could correct \exsent{hte} to \exsent{the}.  Because it does not look at context, however, such a checker could not change \exsent{there} when it is used incorrectly in place of \exsent{their}.  For these errors, we need \keyword{context-dependent word correction}, the task of correcting errors based on the surrounding context.  For instance, \exsent{too} used as a numeral will be corrected to \exsent{two} when it is surrounded by a determiner and a plural noun (e.g., \exword{the too computers}).


%\subsubsection{$N$-gram methods}
%\label{sec:n-gram-methods}

%What might be surprising is that, to some extent, words can be determined to be misspelled without using an explicit dictionary.   One technique uses $n$-gram analysis. An \subkeywordAs{$n$-gram}{character $n$-gram}{$n$-gram}, in this context, refers to a string of $n$ characters in a row; for instance, \exword{thi} is a 3-gram (or trigram).  Note that \exword{thi} is a possible trigram of English (\exword{this}), whereas \exword{pkt} is not.  Thus, one can store all possible $n$-grams, for a certain $n$, for a given language and determine whether each new string encountered has only valid $n$-grams.  The set of valid (or likely) $n$-grams can be collected from a long piece of text,  assuming it has few or no misspellings.

% $N$-gram analysis is more popular for correcting the output of optical character recognition (OCR) than for detecting writers' errors.  This is due to the fact that OCR errors are more likely to result in invalid character sequences.  Still, the technique is useful to examine because it shows us how to detect spelling errors using very little knowledge or computational resources.

% Specifically, one can use 2-grams (bigrams) and store them in a table. This can be done with positional information included or not.  A \keyword{non-positional bigram array} is an array, or table, of all possible and impossible bigrams in a language, regardless of the bigram's position in a word, as shown in Figure~\ref{fig:np-array}.

%\begin{figure}[htbp!]
%\begin{center}
%\begin{tabular}{l | llll}
%  \ldots & k & l & m & \ldots\\
%  \hline
%  k & 0 & 1 (\exword{tackle}) & 1 (\exword{Hackman}) & \ldots\\
%  l & 1 (\exword{elk}) & 1 (\exword{hello}) & 1 (\exword{alms}) & \ldots \\
%  m & 0 & 0 & 1 (\exword{hammer}) & \ldots\\
%\end{tabular}
%\caption{Excerpt of a non-positional bigram array}
%\label{fig:np-array}
%\end{center}
%\end{figure}

%A \keyword{positional bigram array}, on the other hand, is an array of possible and impossible bigrams for a particular position in a word, e.g., word ending.  Figure~\ref{fig:pos-array} illustrates this, with 1 indicating possible and 0 impossible.

%\begin{figure}[htbp!]
%\begin{center}
%\begin{tabular}{l|llll}
%  ... & k & l & m & ...\\
%  \hline
%  k & 0 & 0 & 0 & ...\\
%  l & 1 (\exword{elk}) & 1 (\exword{hall}) & 1 (\exword{elm}) & ...\\
%  m & 0 & 0 & 0 & ...\\
%\end{tabular}
%\caption{Excerpt of a positional bigram array (word ending)}
%\label{fig:pos-array}
%\end{center}
%\end{figure}
 
%Thus, \exword{kl} is a possible bigram in English (e.g.,  \exword{knuckles}), so it is in the non-positional array (Figure~\ref{fig:np-array}), but it is not a valid word ending and so is not in the positional bigram array for word endings.

%To detect errors, the spell checker first breaks the input word into bigrams and labels them with their positions. Then it looks up the bigrams in the non-positional array for the given language. If \emph{any} of the bigrams is missing from the array, it knows that the word has been misspelled. If all the bigrams were present, it goes on to check the positional bigram arrays. If it finds all the bigrams in the correct positions, the word is probably good. But if any of the checks fail, that means that one of the bigrams is out of place, so the input form is probably an error.

%\subsection{Isolated-word spelling correction}
%\label{sec:isolated-word}

%Taking nonword errors which have been detected, isolated-word spelling correction attempts to correct a misspelled word in isolation, i.e., irrespective of surrounding words.  

%Notice that one cannot expect 100\% correctness from isolated-word spelling correctors.  When humans were given the same task -- correcting nonwords without knowing the context -- their accuracy was around 74\%.

% In general, there are three main steps for a spell checker to go through in correcting errors: 1) error detection (described above), 2) generation of candidate corrections, and 3) ranking of candidate corrections.  Isolated-word spelling correction deals with effective generation and ranking of candidate corrections.  Such a task is useful in interactive spelling correctors, where a human makes the ultimate decision, because the higher up in the list the correct spelling is, the more quickly the user can approve the correction.

%\subsubsection{Generation of candidate corrections}
%\label{sec:generate}

% Rule-based techniques generate candidate corrections by means of written rules, which encode transformations from a misspelling to a correct spelling.  For example, a rule will look for the misspelling \exsent{hte} and generate \exsent{the} as a potential correction, or perhaps the only possible correction.  Such rules can be highly effective, especially for high-frequency errors like \exsent{hte}, where the intended word is clear.  There are two downsides to rule-based generation of candidates, however.  

% The first is that someone has to write the rules, and they may not capture all the spelling errors humans might make.  There have been advances in computational linguistics, allowing for automatic derivation of rules, but a second problem remains: the rules are often very narrow and specific, such as the \exsent{hte} rule mentioned, which covers only one single misspelling of \exsent{the}.  A separate rule has to be written for \exsent{teh}, and, as mentioned, generally only high frequency errors are covered.

% A second technique for generating candidates is actually an old method that was developed for working with variant names in U.S. census data.  Similarity-key techniques store words (correct spellings) with similar properties in the same bucket.  That is, words with similar spellings or similar sounds and of about the same length are put together into one slot. When a misspelling is encountered, the similarity key for it is  automatically calculated, and the bucket of similar but correctly spelled words is retrieved.  

% For example, in the SOUNDEX system, one of several well-known similarity-key systems, the word \exsent{nub} would be assigned the code N2: an N for the first letter, a 0 (zero) (which is later eliminated) for the vowel, and a 2 for a bilabial (\exword{b}, \exword{p},  \exword{f}, \exword{v}).  The words \exsent{nob}, \exsent{nap}, and so on will also be in this bucket.  If we encounter the misspelling \exsent{nup}, we retrieve bucket N2 -- which contains \exsent{nub}, \exsent{nob}, \exsent{nap}, and other words -- as our set of potential corrections.
%\label{nup:example}

%\subsubsection{Ranking of candidate corrections}
%\label{sec:candidate-ranking}

%Once the list of candidate corrections is generated, we want to rank these candidates to see which is the best.  Again, we will look at two basic ideas.

% Methods that use probabilities give us a mathematically precise way of indicating which corrections we want a checker to choose. But where do the probabilities come from? If we could directly measure the probability that one word will be typed when another is intended, and do this for every possible pair of words, we could use the probabilities as a way of ranking candidates. Unfortunately, because there are so many possible words, and a correspondingly large number of probabilities, this approach is not practical. It would take too long and use too much computer memory.
%Fortunately, it turns out that there is an effective approximate approach based on character-level \keywordAs{transition  probabilities}{transition probability} and character-level \keywordAs{confusion probabilities}{confusion probability} of \emph{individual characters}. The idea is to use character-level statistics  to build an adequate approximation to the word-level confusion statistics that we would ideally like but cannot obtain.

%Transition probabilities represent the probability of one character following another; for instance, the probability of \exword{q} being followed by \exword{u} in English is essentially 100\%.  Transition probabilities are used to model how likely it is that a character has been inserted or deleted.  Confusion probabilities, on the other hand, represent the probability of one character being substituted for another; for example, using the standard English QWERTY keyboard, the chance of an \exword{m} being typed for an \exword{n} is higher than a \exword{q} being typed for an \exword{n} because \exword{m} and \exword{n} are nearby on the keyboard (and are similar phonetically). Starting with the misspelling, we use probabilities of insertions, deletions, substitutions, and transpositions to find the correct spelling in our candidate set with the highest probability (see discussion on distance measurements below).  We refer to this as the correction which \keywordAs{maximizes the probability}{maximize probability}, i.e., is the most likely correction given the misspelling. We will take a close look at the mathematics behind this in the under the hood section on the noisy channel model in Under the Hood~\ref{sec:noisy-channel-model}.


%Of course, a word like \exsent{robe} also has a distance score of one, so to give a ranking with no ties, we need to add more information. This is where we can combine the edit distance method with the probabilistic method mentioned above.  Instead of giving a value of one for each operation, we can use probabilities to determine which operations are more likely.  For example, a greater edit distance would be given to substituting \exword{q} for \exword{n} than would be given to substituting \exword{m} for \exword{n}.

%The minimum edit distance problem is technically challenging. The problem is this: there are many different sequences of edit operations that change a misspelling into a word. For instance, it is possible, although strange, to derive \exsent{oboe} from \exsent{obe} by deleting \exword{o}, changing \exword{b} to \exword{o}, inserting \exword{b}, and finally inserting \exword{o}, taking four steps.  Worse, there are hundreds of sequences that do it in five steps: one is to start by deleting \exword{o}, then change \exword{b} to \exword{o}, then insert \exword{b}, then insert \exword{a}, and finally change \exword{a} to \exword{o}.  It is pretty obvious that the \exword{a} was a pointless diversion, but not immediately obvious how to fix this in a systematic way.

%Both human beings and computers can solve this problem, but they do so in different ways. Human beings can often use intuition to seek out and follow a single promising possibility. They can even solve a harder version of the problem in which all the intermediate stages are real words. Computers do not have intuition, so it works better for them to  pursue multiple alternatives simultaneously. Fortunately they are very good at the bookkeeping required for systematically working through a range of possibilities. The technique which makes this efficient is called  \keyword{dynamic programming}, which is explained in detail  in the Under the Hood below.

\begin{tblsfilledsymbol}{\underthehoodsubsection{Dynamic programming}}{glass}
\begin{underthehood}

The minimum edit distance problem (also referred to as the edit distance problem) is to decide
the minimum number of edit operations needed in order to transform 
a misspelling into a candidate correction.
It is an example of an important class of problems
that look hard but turn out, when correctly viewed, to
have exceedingly efficient solutions. 
\keywordAs{Dynamic programming}{dynamic programming} is the technique that makes
this possible. 

The reason why minimum edit distance looks hard is that there are many different sequences of edit operations that need to be
considered. However, by breaking up the problem into smaller
sub-problems, it is possible to find enough opportunities for saving
time and space that the problem becomes feasible. The key idea is
to avoid solving the same sub-problem over and over again. This
can be done by solving each sub-problem once, then storing away the
answer in case it is needed again.
For an in-depth discussion on calculating minimum edit
distances, see chapter eight of \citet{mitton:96}, on which we base
this discussion.

To set up the problem, we need to ensure that there is a finite number
of steps involved. 
How could we end up with an infinite number of
steps?  
%We saw a hint of the problem in the discussion of \exword{oboe}
%above. There we saw that there is a five-step path involving a
%diversion via \exword{a}.
Consider the misspelling \exword{obe} and the correction \exword{oboe}: While we can intuitively see a distance of one between these strings,  it is possible to derive \exsent{oboe} from \exsent{obe} by 
%deleting \exword{o}, changing \exword{b} to \exword{o}, inserting \exword{b}, and finally inserting \exword{o}, taking four steps.  Worse, there are hundreds of sequences that do it in five steps: one is to start by
(i) deleting \exword{o}, (ii) changing \exword{b} to \exword{o}, (iii) inserting \exword{b}, (iv) inserting \exword{a}, and (v) finally changing \exword{a} to \exword{o}.  It is pretty obvious that the \exword{a} (step iv) was a pointless diversion, but not immediately obvious how to fix this in a systematic way.

Things are actually far worse than this: We can also do it in six
steps. We again start by
deleting \exword{o}, then change \exword{b} to
\exword{o}, then insert \exword{b}, then insert \exword{a}, change
\exword{a} to \exword{b}, then finally \exword{b} to \exword{o}.  
Or in seven, by converting the \exword{b} \emph{back} into an \exword{a}
before turning it into an \exword{o}. In fact, since you can make the
pointless diversion as long as you want, there are an \emph{infinite}
number of different ways of doing the conversion. Enumerating all these
possibilities is not just inefficient, it is actually impossible.
We need to
set up the problem to avoid this pointless and infinitely repetitive
enumeration of silly possibilities.  

Given this, we want the algorithm for edit distance calculation to satisfy two
requirements:
\begin{enumerate}
\item Letters cannot be changed back and forth a potentially infinite
  number of times.
\item The number of changes that we consider should be reasonable. In
  fact, we will show that the total number of changes we need to consider -- to find the single best sequence of changes -- is
  proportional to the product of the sizes of the two words being
  compared. This is definitely efficient enough for spell checking,
  because words are typically quite short (in English, 40 characters is a really
  long word). If the sequences were much longer, as happens when
  comparing strands of DNA in biology, we might need a still cleverer
  algorithm.
\end{enumerate}

In other words, we never want to deal with a character in either word
more than once.  With our basic operations, it is obvious that we
could get the desired result by deleting each character in the first
word and then inserting each character of the second word.  This
establishes an \keyword{upper bound} on the number of edit operations
we need. 
Mathematically, we can state this upper bound as $length(word_1) +
length(word_2)$.  In other words, if we compare a three-letter word
like \exsent{tap} to a misspelling like \exsent{step}, we know that
the edit distance cannot be more than 7.  In fact, if you think a
little more, you will be able to find a better upper bound on the
number of operations needed.  We will explain this tighter upper bound
at the end of this section.

To calculate minimum edit distance, we set up a \keyword{directed,
  acyclic graph}, where a \emph{graph} -- which is a mathematical
object that models relationships between items -- is represented by a
set of nodes (circles) and arcs (arrows).  Arcs capture relationships
between nodes: \emph{Directed} means that the arcs have a direction to
them, pointing from one node to another, and \emph{acyclic} means that
there are no loops: We cannot return to a node once we have left it.

We will set up the graph in the following way, as shown.  Horizontal arcs correspond to deletions;
vertical arcs correspond to insertions; and diagonal arcs correspond
to substitutions (including the option that a letter is substituted
for itself).  We leave out transpositions here, although the graphs can
be extended to allow them.

\tikzstyle{state without output}= [circle,draw,minimum
size=1.3em,every state] \tikzstyle{every state}=[font=\scriptsize\itshape]

  \begin{center}
    \begin{tikzpicture} [node distance=5.5em,>=stealth,initial text=]
      \node[state] (0) {}; \node[state] (1) [right of=0] {};
      \node[state] (2) [below of=1] {}; \node[state] (3) [below of=0]
      {}; \path [->] (0) edge node [above] {Delete \exword{x}} (1) (0)
      edge node [right] {Substitute \exword{y} for \exword{x}} (2) (0)
      edge node [left] {Insert \exword{y}} (3) ;
    \end{tikzpicture}
  \end{center}

Let us start by assuming that the user types in \exsent{fyre}.  Given
that \exsent{fry} is one of the possible corrections, we want to
calculate how far away \exsent{fry} is.  In other
words, we want to calculate the minimum edit distance (or minimum edit
cost) from \exsent{fyre} to \exsent{fry}.  As the first step, we draw
the directed graph:

  \begin{center}
    \begin{tikzpicture} [node distance=4em,>=stealth,initial text=]
      \node[state] (1) {}; \node[state] (2) [right of=1] {};
      \node[state] (3) [right of=2] {}; \node[state] (4) [right of=3]
      {}; \node[state] (5) [right of=4] {}; \node[state] (6) [below
      of=1] {}; \node[state] (7) [below of=2] {}; \node[state] (8)
      [below of=3] {}; \node[state] (9) [below of=4] {}; \node[state]
      (10) [below of=5] {}; \node[state] (11) [below of=6] {};
      \node[state] (12) [below of=7] {}; \node[state] (13) [below
      of=8] {}; \node[state] (14) [below of=9] {}; \node[state] (15)
      [below of=10] {}; \node[state] (16) [below of=11] {};
      \node[state] (17) [below of=12] {}; \node[state] (18) [below
      of=13] {}; \node[state] (19) [below of=14] {}; \node[state] (20)
      [below of=15] {}; \path [->]
      (1) edge node [above] {f} (2) (2) edge node [above] {y} (3) (3)
      edge node [above] {r} (4) (4) edge node [above] {e} (5)

      (6) edge (7) (7) edge (8) (8) edge (9) (9) edge (10)

      (11) edge (12) (12) edge (13) (13) edge (14) (14) edge (15)

      (16) edge (17) (17) edge (18) (18) edge (19) (19) edge (20)

      (1) edge (7) (2) edge (8) (3) edge (9) (4) edge (10)

      (6) edge (12) (7) edge (13) (8) edge (14) (9) edge (15)

      (11) edge (17) (12) edge (18) (13) edge (19) (14) edge (20)

      (1) edge node [left] {f} (6) (2) edge (7) (3) edge (8) (4) edge
      (9) (5) edge (10)

      (6) edge node [left] {r} (11) (7) edge (12) (8) edge (13) (9)
      edge (14) (10) edge (15)

      (11) edge node [left] {y} (16) (12) edge (17) (13) edge (18)
      (14) edge (19) (15) edge (20) ;
    \end{tikzpicture}
    \label{fig:directed2}
  \end{center}

The worst case of first deleting every letter in \exsent{fyre} and
then inserting every letter of \exsent{fry} would be achieved by
starting in the upper left, taking every arc across the top, and then
following the arcs down the right side.

What is missing from the graph are identifiers for the nodes.  If we
add an identifier to each state, that will allow us to define a
\keyword{topological order}.

  \begin{center}
    \begin{tikzpicture}[node distance=4em,>=stealth,initial text=]
      \node[state] (1) {A}; \node[state] (2) [right of=1] {E};
      \node[state] (3) [right of=2] {F}; \node[state] (4) [right of=3]
      {G}; \node[state] (5) [right of=4] {H}; \node[state] (6) [below
      of=1] {B}; \node[state] (7) [below of=2] {I}; \node[state] (8)
      [below of=3] {J}; \node[state] (9) [below of=4] {K};
      \node[state] (10) [below of=5] {L}; \node[state] (11) [below
      of=6] {C}; \node[state] (12) [below of=7] {M}; \node[state] (13)
      [below of=8] {N}; \node[state] (14) [below of=9] {O};
      \node[state] (15) [below of=10] {P}; \node[state] (16) [below
      of=11] {D}; \node[state] (17) [below of=12] {Q}; \node[state]
      (18) [below of=13] {R}; \node[state] (19) [below of=14] {S};
      \node[state] (20) [below of=15] {T}; \path [->]
      (1) edge node [above] {f} (2) (2) edge node [above] {y} (3) (3)
      edge node [above] {r} (4) (4) edge node [above] {e} (5)

      (6) edge (7) (7) edge (8) (8) edge (9) (9) edge (10)

      (11) edge (12) (12) edge (13) (13) edge (14) (14) edge (15)

      (16) edge (17) (17) edge (18) (18) edge (19) (19) edge (20)

      (1) edge (7) (2) edge (8) (3) edge (9) (4) edge (10)

      (6) edge (12) (7) edge (13) (8) edge (14) (9) edge (15)

      (11) edge (17) (12) edge (18) (13) edge (19) (14) edge (20)

      (1) edge node [left] {f} (6) (2) edge (7) (3) edge (8) (4) edge
      (9) (5) edge (10)

      (6) edge node [left] {r} (11) (7) edge (12) (8) edge (13) (9)
      edge (14) (10) edge (15)

      (11) edge node [left] {y} (16) (12) edge (17) (13) edge (18)
      (14) edge (19) (15) edge (20) ;
    \end{tikzpicture}
  \end{center}\unskip

A topological order means that the nodes are loosely ordered, but not
completely ordered.  Here, for example, node
I comes after nodes A, B, and E, but there is no ordering between
nodes C and I: There is no way to get from C to I or from I to C.  The
crucial property of this topological ordering, as we will see below,
is that at every node, all the incoming arcs come from nodes prior in
the ordering.

In order to actually calculate a distance, we need to add the costs
involved to the arcs.  In the simplest case, the cost of deletion,
insertion, and substitution is 1 each (and substitution with the same
character is free), as shown.  In other
words, there is a distance of one for any non-trivial operation.  For
example, from node A to node I, we substitute \exword{f} for
\exword{f} with a cost of zero, and from I to J, we delete \exword{y}
with a cost of one:

  \begin{center}
    \begin{tikzpicture}[node distance=4em,>=stealth,initial
      text=,cost/.style={font=\scriptsize}]
      \node[state] (1) {A}; \node[state] (2) [right of=1] {E};
      \node[state] (3) [right of=2] {F}; \node[state] (4) [right of=3]
      {G}; \node[state] (5) [right of=4] {H}; \node[state] (6) [below
      of=1] {B}; \node[state] (7) [below of=2] {I}; \node[state] (8)
      [below of=3] {J}; \node[state] (9) [below of=4] {K};
      \node[state] (10) [below of=5] {L}; \node[state] (11) [below
      of=6] {C}; \node[state] (12) [below of=7] {M}; \node[state] (13)
      [below of=8] {N}; \node[state] (14) [below of=9] {O};
      \node[state] (15) [below of=10] {P}; \node[state] (16) [below
      of=11] {D}; \node[state] (17) [below of=12] {Q}; \node[state]
      (18) [below of=13] {R}; \node[state] (19) [below of=14] {S};
      \node[state] (20) [below of=15] {T}; \path [->]
      (1) edge node [above] {f} node [cost,below] {1}(2) (2) edge node
      [above] {y} node [cost,below] {1} (3) (3) edge node [above] {r}
      node [cost,below] {1} (4) (4) edge node [above] {e} node
      [cost,below] {1} (5)

      (6) edge node [cost,below] {1} (7) (7) edge node [cost,below]
      {1} (8) (8) edge node [cost,below] {1} (9) (9) edge node
      [cost,below] {1} (10)

      (11) edge node [cost,below] {1} (12) (12) edge node [cost,below]
      {1} (13) (13) edge node [cost,below] {1} (14) (14) edge node
      [cost,below] {1} (15)

      (16) edge node [cost,below] {1} (17) (17) edge node [cost,below]
      {1} (18) (18) edge node [cost,below] {1} (19) (19) edge node
      [cost,below] {1} (20)

      (1) edge node [cost,below] {0} (7) (2) edge node [cost,below]
      {1} (8) (3) edge node [cost,below] {1} (9) (4) edge node
      [cost,below] {1} (10)

      (6) edge node [cost,below] {1}(12) (7) edge node [cost,below]
      {1}(13) (8) edge node [cost,below] {0}(14) (9) edge node
      [cost,below] {1}(15)

      (11) edge node [cost,below] {1} (17) (12) edge node [cost,below]
      {0}(18) (13) edge node [cost,below] {1} (19) (14) edge node
      [cost,below] {1} (20)

      (1) edge node [left] {f} node [cost,right] {1} (6) (2) edge node
      [cost,right] {1} (7) (3) edge node [cost,right] {1} (8) (4) edge
      node [cost,right] {1} (9) (5) edge node [cost,right] {1} (10)

      (6) edge node [left] {r} node [cost,right] {1} (11) (7) edge
      node [cost,right] {1} (12) (8) edge node [cost,right] {1} (13)
      (9) edge node [cost,right] {1} (14) (10) edge node [cost,right]
      {1} (15)

      (11) edge node [left] {y} node [cost,right] {1} (16) (12) edge
      node [cost,right] {1} (17) (13) edge node [cost,right] {1} (18)
      (14) edge node [cost,right] {1} (19) (15) edge node [cost,right]
      {1} (20) ;
    \end{tikzpicture}
  \end{center}\unskip

Now, we are ready to get into dynamic programming.  We want to find
the path from the start (A) to the end (T) with the least cost.  How
do we do that efficiently?

We will start with a simple but inefficient way of doing it:

\begin{enumerate}
\item Follow every path from start (A) to finish (T).
\item See how many changes we have to make for each path.
\end{enumerate}

This is very time-consuming, however.  There are too many different
paths to check.  We would have to check the path
A$\to$I$\to$J$\to$O$\to$S$\to$T and then separately check the path
A$\to$I$\to$J$\to$O$\to$P$\to$T, and so on.  

But note that these paths have a lot in common!
In fact, based on this insight, we should instead re-use results, which is
exactly what dynamic programming  does:

\begin{enumerate}
\item Follow the topological ordering.
\item Calculate the \keyword{least cost} for each node:
  
  \begin{itemize}
  \item Add the cost of an arc to the cost of reaching the node this
    arc originates from.
    
  \item Take the minimum of the costs calculated for all arcs pointing
    to a node and store it for that node.
  \end{itemize}
\end{enumerate}

The key point is that we are storing partial results along the way,
instead of recalculating everything every time we compute a new path.
For example, let's say that we have already processed the nodes A-H
and stored the minimum cost at each of those nodes.  We are now at
node I and need to examine all three incoming arcs (from nodes A, B,
and E).  Node A has a cost of zero; node E has a minimum cost of one,
and node B also has a minimum cost of one.  We add the cost of each
node to the cost of each incoming arc:

  \begin{center}
    \begin{tabular}{c  c@{\,+\,}c@{\,=\,}c}
    \lsptoprule
      Source to Target & Cost at Source & Cost of Arc & Total Cost \\
      \midrule
      A to I & 0 & 0 & 0 \\
      B to I & 1 & 1 & 2 \\
      E to I & 1 & 1 & 2 \\
    \lspbottomrule
    \end{tabular}
    \label{fig:node-i}
  \end{center}

The least cost of these three incoming arcs is the one from node A,
with a cost of 0.  So, we store 0 as the minimum cost at node I.  This
corresponds to the fact that the paths going through nodes B or E are
useless for us to arrive at node I: They cost too much, and from this
point onwards, we can ignore those paths, keeping only the cheapest
one.  Now, anytime node I is needed, we give the cost of 0, and
we never have to re-calculate its cost.

In this way, the minimum cost at every node is computed, following the
ordering from A to T.  At node T, we calculate costs in the same way.
You can verify that node O has a minimum cost of 1, node P a minimum
cost of 2, and node S a minimum cost of 2.  Each of the arcs (O$\to$T,
P$\to$T, S$\to$T) has a cost of 1, and so the final cost at node T
will be 2, taking the path coming from O.  Since T is the final node
in the graph, this means that 2 is the minimum cost between
\exsent{fyre} and \exsent{fry}.

Dynamic programming avoids infinite repetition, because it looks
things up in a table rather than recomputing them.  It also satisfies
the stronger requirement to avoid all unnecessary repetition, because
each intermediate result is entered into the table exactly once.  The
number of letter-letter comparisons needed is exactly the size of the
table, which is the product of the lengths of the source and target
string, so we also satisfy the second requirement that the cost of the
computation be in reasonable proportion to the size of the problem.

The better upper bound which we mentioned earlier is a simple one.
Let us decide to work on transforming the longer word into the shorter
word.  First we replace all the letters of the shorter word with the
corresponding letters of the longer word, then we insert the leftover
letters from the longer word.  To illustrate this, imagine that the
shorter word is \exsent{take} and the longer word is \exsent{intake}.
The old upper bound said that we might need \(4 + 6 = 10\) operations.
The new upper bound says that we should first replace \exsent{take}
with \exsent{inta}, then insert \exsent{ke}, for a total cost of 6. We
say that the new upper bound is tighter than the old one.  The total
number of operations required by the new, tighter upper bound is the
length of the longer word.

Obviously, the real minimum cost path for this example is to first
insert \exsent{in} then match up \exsent{take}, for a minimum cost of
2.  Dynamic programming will indeed find this path.  The upper bound
is still useful, because it can be calculated using nothing more than
the length of the words.  If a node has a cost greater than the upper
bound, it cannot possibly contribute to the minimum cost path, so we
could design a slightly cleverer version of the minimum cost algorithm
that avoids filling in nodes that would cost more than the upper
bound.  The answer would still be correct, and some time and effort
would be saved, at the expense of making the algorithm more
complicated. It is usually better to keep algorithms simple, because
complicated ones are more likely to have bugs, but in this case,
especially if we are dealing with a language with very long words, the
extra complexity might be warranted.

\end{underthehood}
\end{tblsfilledsymbol}

\section{$n$-grams} 

Imagine that someone types the phrase \exsent{You put the catt before the horse}.  The word \exword{catt} is misspelled, as we can tell from the fact that it does not appear in a dictionary or corpus.  Out of context, the simple spell-checker that we sketched above would consider all candidates that appear on the list of correctly spelled words and are within an edit distance of 1 from \exword{catt}, and then choose the most frequent one: \exword{cat}.  But of course, the most likely correction should actually be \exword{cart}, which is less frequent than \exword{cat} but more likely in this particular context.

%Let's walk through a simple technique for how our spell-checker could be upgraded to take such context into account:  using $n$-grams of words to tell us which strings are more or less likely.

 Intuitively, \exsent{put the cart before the horse} is more likely  as a string of English than \exsent{put the cat before the horse}. We can quantify that intuition using the idea of \keywordAs{$n$-grams}{n-grams@$n$-grams}, which
  are sequences of elements.  The elements can be characters or words (both have their uses, but we use word-level $n$-grams here); and $n$ is a number that goes up as high as you want.  Unigrams are one-element sequences (for word-level $n$-grams, this means single words: \exword{the}). Bigrams are two-element sequences (\exword{the cat}), trigrams are three-element sequences (\exword{put the cat}), and so on.

If you've ever managed to correctly guess the next word before you turn the page of a book, you are already an expert in reasoning about $n$-gram probabilities.  Let's illustrate using bigrams (\exword{the cat}).  The idea is that we count all of the two-word strings (bigrams) in a corpus, such as \exword{of the, to be,} and so on.  These simple counts are surprisingly useful.

To start, take a look at Tables \ref{fig:1grams}--\ref{fig:3grams}.  Here, we've extracted the top twenty most common unigrams, bigrams, and trigrams in two corpora of English text -- one million words of comments from January 2018 on the AskReddit web forum  \citep{Baumgartner-etal:2020}, and the comparably-sized Brown corpus \citep{FrancisKucera:1979}, which gathers data from a balanced blend of sources (newspapers, novels, academic journals, and so on) from the 1960s.  By comparing $n$-grams drawn from two different corpora, you can investigate which $n$-gram statistics are common to all English text, and which ones are specific to a particular corpus, genre, or time period.  

Consistent across both corpora, we see that the START token (indicating the beginning of a sentence) appears in more of the most frequent $n$-grams than the END token, which shows that the beginnings of English sentences tend to be more predictable than the end.  Distinguishing the two corpora, \exword{I} is far more frequent in Reddit than in the Brown Corpus, perhaps reflecting different patterns in  personal narratives versus expository writing.  Notably, \exword{he} and \exword{his} are among the top 20 unigrams in the 1960s-era Brown corpus, but not among the top 20 unigrams from 2018 Reddit data -- perhaps indicating a historical shift in the sociology of gender as well as a difference in genre.

\begin{table}
\begin{tabular}{lrc  lrc}
\lsptoprule
{Reddit 1-gram} & {Count} & {\%} & {Brown 1-gram} & {Count} & {\%}  \\ \cmidrule(r){1-3} \cmidrule(l){4-6}
the & 32404 & 3.34 & the & 69971 & 6.97 \\
i & 28940 & 2.99 & of & 36412 & 3.63 \\
to & 27629 & 2.85 & and & 28853 & 2.87 \\
a & 26724 & 2.76 & to & 26158 & 2.60 \\
and & 24795 & 2.56 & a & 23308 & 2.32 \\
of & 16106 & 1.66 & in & 21341 & 2.13 \\
in & 13649 & 1.41 & that & 10594 & 1.05 \\
that & 13636 & 1.41 & is & 10109 & 1.01 \\
it & 13281 & 1.37 & was & 9815 & 0.98 \\
my & 12412 & 1.28 & he & 9548 & 0.95 \\
you & 11404 & 1.18 & for & 9489 & 0.94 \\
for & 10350 & 1.07 & it & 8760 & 0.87 \\
is & 9790 & 1.01 & with & 7289 & 0.73 \\
was & 9541 & 0.98 & as & 7259 & 0.72 \\
but & 7313 & 0.75 & his & 6996 & 0.70 \\
have & 7295 & 0.75 & on & 6741 & 0.67 \\
on & 6588 & 0.68 & be & 6377 & 0.64 \\
with & 6446 & 0.67 & at & 5372 & 0.53 \\
not & 5912 & 0.61 & by & 5306 & 0.53 \\
they & 5794 & 0.60 & i & 5180 & 0.52 \\
\lspbottomrule
\end{tabular}
\caption{Top-20 common unigrams (lower-cased, stripped of punctuation) in two English corpora -- one million words of comments from AskReddit, and the one-million-word Brown corpus.}
\label{fig:1grams}
\end{table}



\begin{table}
\begin{tabular}{lrc  lrc}
\lsptoprule
{Reddit 2-gram} & {Count} & {\%} & {Brown 2-gram} & {Count} & {\%}  \\ \cmidrule(r){1-3} \cmidrule(l){4-6}
START i & 10917 & 1.04 & of the & 9750 & 0.92 \\
in the & 3093 & 0.30 & START the & 7055 & 0.66 \\
i was & 2831 & 0.27 & in the & 6092 & 0.57 \\
START the & 2529 & 0.24 & to the & 3503 & 0.33 \\
of the & 2458 & 0.23 & START he & 3040 & 0.29 \\
START my & 2277 & 0.22 & on the & 2476 & 0.23 \\
it END & 1939 & 0.19 & and the & 2269 & 0.21 \\
START it & 1877 & 0.18 & START it & 2122 & 0.20 \\
and i & 1775 & 0.17 & for the & 1854 & 0.17 \\
it was & 1694 & 0.16 & START i & 1827 & 0.17 \\
to be & 1634 & 0.16 & START in & 1795 & 0.17 \\
START its & 1579 & 0.15 & to be & 1718 & 0.16 \\
in a & 1566 & 0.15 & at the & 1658 & 0.16 \\
START im & 1478 & 0.14 & with the & 1541 & 0.15 \\
START you & 1472 & 0.14 & START but & 1513 & 0.14 \\
to the & 1464 & 0.14 & of a & 1492 & 0.14 \\
START and & 1414 & 0.14 & it is & 1475 & 0.14 \\
when i & 1367 & 0.13 & in a & 1425 & 0.13 \\
on the & 1326 & 0.13 & from the & 1419 & 0.13 \\
i have & 1293 & 0.12 & that the & 1399 & 0.13 \\
\lspbottomrule
\end{tabular}
\caption{Top-20 common bigrams (lower-cased, stripped of punctuation) in two English corpora -- one million words of comments from AskReddit, and the one-million-word Brown corpus.}
\label{fig:2grams}
\end{table}



\begin{table}
\begin{tabular}{lrc  lrc}
\lsptoprule
{Reddit 3-gram} & {Count} & {\%} & {Brown 3-gram} & {Count} & {\%}  \\\cmidrule(r){1-3} \cmidrule(l){4-6} 
a lot of & 851 & 0.09 & START it is & 604 & 0.06 \\
START i was & 697 & 0.07 & START it was & 579 & 0.06 \\
when i was & 602 & 0.06 & START in the & 471 & 0.05 \\
START i dont & 538 & 0.06 & one of the & 404 & 0.04 \\
START i have & 531 & 0.05 & START he was & 367 & 0.04 \\
START it was & 470 & 0.05 & the united states & 340 & 0.03 \\
START i think & 456 & 0.05 & START this is & 259 & 0.03 \\
START if you & 407 & 0.04 & START there was & 258 & 0.03 \\
START i had & 391 & 0.04 & START he had & 249 & 0.02 \\
START when i & 381 & 0.04 & as well as & 238 & 0.02 \\
START this is & 376 & 0.04 & START there is & 204 & 0.02 \\
i grew up & 361 & 0.04 & some of the & 179 & 0.02 \\
one of the & 289 & 0.03 & out of the & 174 & 0.02 \\
START i know & 275 & 0.03 & the fact that & 167 & 0.02 \\
START i am & 251 & 0.03 & START but the & 166 & 0.02 \\
to go to & 245 & 0.03 & START on the & 156 & 0.02 \\
i was a & 244 & 0.03 & the end of & 149 & 0.01 \\
START i grew & 244 & 0.03 & part of the & 145 & 0.01 \\
i have a & 242 & 0.02 & START at the & 145 & 0.01 \\
START i just & 234 & 0.02 & it was a & 143 & 0.01 \\
\lspbottomrule
\end{tabular}
\caption{Top-20 common trigrams (lower-cased, stripped of punctuation) in two English corpora -- one million words of comments from AskReddit, and the one-million-word Brown corpus.}
\label{fig:3grams}
\end{table}



To guess the next word of a book before turning the page, you might take into account the last word on the previous page and then look at the most common bigrams that begin with that word.  For example, the most common bigram beginning with \exword{you} is \exword{you are}, so when you see the word \exword{you}, the most likely following word is \exword{are}. The same idea can be used in a simple implementation of auto-complete for texting: The phone can suggest \exword{are} as the next word following \exword{you}.

\subsection{Spelling correction in context with $n$-grams} \label{correction-with-ngrams}

By telling us which strings of English are more or less likely, $n$-grams can also be used to upgrade our simple spell-checker, which will help us  identify \exword{cart} as the best correction for \exsent{put the catt before the horse}.  Specifically, we can use $n$-grams as a simple \keyword{language model} --  a way of computing the probability of a sequence of words, given a lot of data about the language.

In an $n$-gram language model, the probability of a full sentence is obtained by breaking it down into strings of length $n$ and then multiplying the probabilities of each $n$-gram.  
%\mad{Is the parenthetical distracting? I wonder about bumping it into some kind of exercise, to help with numeracy?}\faye{did find this a little distracting, so commented out and bumped to an exercise} The tokens \exword{START} and \exword{END} are added to compute the probability of the first word being at the beginning of the sentence, and of the last word being at the end.  For example, the probability of \exword{you} given \exword{START} is the percentage of all bigrams starting with \exword{START} that end in \exword{you}. \mad{These sentences about conditional probabilities seem pretty nice to me, but it wouldn't hurt to discuss if it's too much information for novice readers?} \faye{from a student perspective, I think these are okay! and a helpful visualization}
% what parenthetical are you all talking about? - LG.
The probability of \exword{put} given \exword{you} is the percentage of all bigrams starting with \exword{you} that end in \exword{put}.  The probability of \exword{the} given \exword{put} is the percentage of all bigrams starting with \exword{put} that end in \exword{the}.
 
%(In actual implementation, the probabilities are usually log-transformed and then added rather than multiplied; these operations are equivalent because the log of a product is the sum of the logs)

In a bigram language model, the probability of \exsent{You put the cat before the horse} would be: 

\ea  \label{ex:bi-correct} $P(\mbox{you}|\mbox{START}) \times
  P(\mbox{put}|\mbox{you}) \times P(\mbox{the}|\mbox{put}) \times P(\mbox{cart}|\mbox{the}) \times P(\mbox{before}|\mbox{cart}) \times P(\mbox{the}|\mbox{before}) \times P(\mbox{horse}|\mbox{the}) \times
  P(\mbox{END}|\mbox{horse})$
  \z 

%According to the trigram language model, the bigram \exword{cart before} is more likely than \exword{cat before}, so the language model should give the sentence \exword{You put the cart before the horse} a higher probability than \exword{You put the cat before the horse}. 



And in a trigram language model it would be:

\ea  \label{ex:tri-correct} $P(\mbox{put}|\mbox{START, you}) \times
  P(\mbox{the}|\mbox{you, put}) \times P(\mbox{cart}|\mbox{put, the}) \times P(\mbox{before}|\mbox{the, cart}) \times P(\mbox{the}|\mbox{cart, before}) \times P(\mbox{horse}|\mbox{before, the}) \times P(\mbox{END}|\mbox{the, horse})$
  \z 



According to the trigram language model, the trigrams \exword{the cart before} and \exword{cart before the} should be more likely than \exword{the cat before} and \exword{cat before the}, so the language model should give the sentence \exword{You put the cart before the horse} a higher probability than \exword{You put the cat before the horse}.  Such a  language model therefore would allow our spell-checker to take the context into account and choose the desired correction, \exword{cart}.

Now that you understand the idea of $n$-grams, you can see how it encompasses even our simple context-free spell-checker from above.  There, we computed the frequency of each single word in a corpus and used it to decide which candidate word is most likely.  In the terminology of $n$-grams, we considered unigram frequency and used it to estimate the probability of the writer's intended message.  Here, we are using bigram or trigram frequency to do the same thing, but the larger $n$ incorporates information about the linguistic context of the misspelling.

\newpage
But is a larger $n$ always better?  Not necessarily: The larger the $n$, the fewer examples of each $n$-gram will appear in a corpus, so that many 5-grams or 6-grams (\exsent{Samantha works as a party juggler}) are totally unique and thus absent from corpus statistics.   
You can see this yourself by considering the counts in Tables \ref{fig:1grams}--\ref{fig:3grams} (unigrams are more frequent than bigrams, which are more frequent than trigrams), or by searching the web in quotation marks for various two-word string of English (\exsent{Samantha works}, \exsent{works as}) versus various six-word strings (\exsent{Samantha works as a party juggler}), and comparing the number of hits.  Language is fascinating in part because it is \keyword{productive}: We can use it to generate and understand infinite new utterances that we have never heard before.  These utterances will include $n$-grams that don't appear in any corpus.  


The productive power of language leads to  \keyword{data sparsity}: No matter how much data we use, there will always be something we have never seen before.
Data sparsity is a core problem in much of statistical NLP, where the behavior is guided by the data one has
seen.  Part of the solution to data sparsity is to look at more data, from a larger corpus; but even then, you're guaranteed to encounter new $n$-grams that you've never seen before.  Another solution is to pretend you've seen more data than you really have, for example using \keyword{plus-one-smoothing} -- adding 1 to the count of every single possible $n$-gram, pretending that you've seen any novel $n$-grams once in the past instead of never.  This way, you can avoid multiplying by zero, or trying to take the log of zero (undefined), both of which could cause problems for the calculation of a language model.  A third solution is to \keyword{back off} to $n-1$ if you have never seen a particular $n$-gram of length $n$: If you've never seen  the trigram \exword{my glamorous elephant}, you can at least estimate its probability in your language model by taking the probability of the bigrams \exword{my glamorous} and \exword{glamorous elephant}. 

Stepping back, $n$-gram techniques are a simple form of \keyword{machine learning}, where a computer is able to generalize knowledge ($n$-gram counts from a corpus) to solve a new problem (deciding the probability of a sentence it has not seen before). We will see much more of machine learning in Chapter~\ref{ch:text-classification}.

We can also use $n$-grams to correct real-word errors.  For example, encountering a sentence such as \exsent{You put the cat before the horse}, we could calculate the $n$-gram probability of this original sentence and compare it to alternative versions where we alter each word by an edit distance of 1 (restricting our attention to edits that also appear on a word-list of correct spellings).  This way, \exsent{You put the cart before the horse} would be identified as a string with higher probability than \exsent{You put the cat before the horse}.


% \mad{It might be a useful exercise for students to think about how much higher the probability should be to make the correction?} \faye{added in exercises}

% However, this presents another issue: the number of observed trigrams will be large (e.g., 20 million trigrams), so the data structures used need to be efficient.  With a large corpus and efficient data structures in place, the method is intuitive, easy to implement, and effective at suggesting error corrections.

% $N$-gram techniques for grammar checking are a form of \keyword{machine learning}, where a computer learns in what contexts a word might be misspelled.  Machine learning involves a computer program learning from data such that it can apply what it learns to a new task (see more in chapter~\ref{ch:text-classification} on document classification).

% Another, similar set-up is to view the context-dependent spelling correction task as a \keyword{word disambiguation} task: we want to find which of a set of words is the correct word in a given context.  We base much of the following discussion on \citet{golding:roth:99}.



%The probabilities are obtained by breaking the sentence down into trigrams and then multiplying the probabilities of each trigram. For the ultimately correct sentence, we will multiply the probabilities in (\ref{ex:tri-correct}), which correspond to the trigrams \exsent{John came from}, \exsent{came from the}, and \exsent{from the house}, in addition to including START and END tags to model the likelihood of particular words starting or ending a sentence.







% language model. probability of a sequence of text.



%Complementing the rule-based techniques described in the previous sections, statistical techniques can be used to detect and correct words in context, be it real-word spelling errors or grammatical errors.  One such technique is to use $n$-grams of words to tell us what are likely and unlikely sequences of words.  By \keyword{$n$-gram}, we mean sequences of elements: in the context of spell checking in section~\ref{sec:n-gram-methods}, we used sequences of characters, but here we use sequences of \emph{words}.  A simple statistical bigram grammar corrector would use the probability of one word following another to determine if an error has been made.  For example, \exsent{conducted be} is not a likely word bigram in English, but \exsent{conducted by} is.  We will base our discussion here on \citet{wilcox-et-al:06}.


% Real-word spelling correctors which use $n$-grams typically use trigrams of words. Before we get to how trigrams are used, let us look at the overall scheme.  1) For each word in a sentence (real-word or misspelling), a set of candidate corrections, or spelling variants, is obtained.  2) A set of candidate corrections for the \emph{sentence} is obtained by changing exactly one word in the original sentence to a spelling variant.  3) The sentence with the highest probability is selected. This probability calculation is where trigrams are used, as will be discussed in a moment.

% Let's walk through an example, given in (\ref{ex:form}).

%\begin{exe}
%\ex\label{ex:form} John came \uline{\textbf{form}} the house.
%\end{exe}

%1) We first find all candidates for each word, typically real words which are one operation away.  For \exsent{came}, this will be \exsent{come}, \exsent{lame}, \exsent{cane}, etc.; for \exsent{form}, we will have \exsent{from}, \exsent{dorm}, etc.  2) Taking these candidates, we generate candidate sentences, as in (\ref{ex:candidates}) by changing one and only one word.  3) Finally, we find the sentence from this set with the highest probability.

%\begin{exe}
%\ex\label{ex:candidates}
%\begin{xlist}
%  \ex John \uline{come} form the house.
%  \ex John came \uline{from} the house.
%  \ex John come form the \uline{hose}.
%\end{xlist}
%\end{exe}


Instead of using sets of candidates based only on spelling differences,
these techniques often use \keywordAs{confusion sets}{confusion set} consisting of words that
are potentially confounded with one another in any relevant property.  
For example, a set could
contain the words \exsent{there}, \exsent{they're}, and
\exsent{their}, so that the task of the system is to figure out which of
the words was meant in a given context.  The advantage of such
confusion sets is that the words do not have to be similar-looking:
\exsent{amount} and \exsent{number}, for instance, can make up a set
of words which are commonly confused.
The disadvantage of these confusion sets is that someone usually has to
define them in advance.

\subsection{$n$-grams as a generative language model} \label{sec:ngramgen}

So far, we have used $n$-grams to create a language model that represents the probability of different strings, for example capturing the fact that \exsent{put the cart before} is a more likely string of English than \exsent{put the cat before}.  But we can also use $n$-grams as the basis for a very simple \keyword{generative} language model which creates probable strings of text.  The term \keyword{language model} is used in both senses.

To create a generative language model using bigrams, we would store all the bigrams in our corpus along with their frequency.  To generate a new sentence, we begin with the START token.  Next we choose a word $w$ to complete the bigram $\langle \text{START}, w\rangle$.  We could choose the word that appears in the most frequent bigram beginning with START, which would be \exword{The} in the Brown Corpus (there are 6544 instances of $\langle \text{START}, the\rangle$ in Brown).  But if we always chose the most likely word, then our language model would generate the same sentence repeatedly, beginning every sentence with \exword{The}.  If we want a bit more novelty, we can choose a random bigram beginning with START, such as $\langle \text{START}, \text{But}\rangle$.  We have seen this bigram 1271 times in the corpus, so it's a plausible way to a sentence, but it's not the most frequent one, so our language model will generate more interesting and less predictable text.  Next, we choose a new word $w$ to complete the bigram $\langle \text{START}, \text{But}\rangle$.  Again, we could choose the most frequent bigram beginning with \exword{But} (which is \exword{the}), or a random one for more creative text (such as \exword{there}).  And so on.  We keep generating text until we reach a final punctuation mark; then the sentence ends.

Here is an example of some text generated by this bigram language model:

\ea But there should always particularly when finally convinced that nobody else?
\z 


Here, the individual bigrams are reasonable (\exword{but there, there should, should always,} and so on), but the full sentence is not very coherent.


When we use trigrams instead of bigrams in our language model, we generate text like this:


\ea But with the increasing complexity of markets and their importance in the north, where no one had been allowed to go.  
\z 

Here, individual pieces of the sentence make sense (\exsent{increasing complexity of markets and their importance}), but again the full sentence seems to lose track of itself partway through.

Already, we have created a very simple example of a generative language model, which distills insights from a corpus to produce new sentences.  We have also seen that the sentences are more interesting if we add some randomness to the process.  These insights persist in modern generative language models, to be introduced below.



%The task for the machine, then, is to learn which member of a confusion set is the most appropriate in a context.  To do this, we break down the surrounding context into different \keywordAs{features}{feature}. Common kinds of features are context words and collocations. \keywordAs{Context words}{context word} capture a broad semantic context because they are simply words within a certain distance of the word in question.  For example, it is useful for the spelling corrector to know whether \exsent{cloudy} appears within the surrounding ten words if the confusion set we are dealing with is composed of \exsent{whether} and \exsent{weather}.  Collocations, on the other hand, capture a very local context -- essentially $n$-gram information.  With the same set of \exsent{whether} and \exsent{weather}, for instance, a collocation might be the word in question followed by \exsent{to} and a following verb (e.g., \exword{whether to run}).

%We will not discuss the actual machine learning mechanisms here, but the idea is that the computer learns which contextual features tend to go with which member of the confusion set.  When a new text is encountered with a member of the confusion set, the machine checks to see whether that word is the best one from the set in this context.  

%\subsubsection{Meaning-based techniques}

% Many real-word errors result in a word which does not seem to fit the meaning of the rest of the sentence or paragraph.  We refer to this as being \subkeyword{semantically inappropriate}{semantics}.  For example, in the example (\ref{ex:hole}) given by \citet{hirst:budanitsky:05}, the word \exsent{hole} does not fit very well with \exsent{sincere}.

%\begin{exe}
%  \ex\label{ex:hole} It is my sincere \uline{\textbf{hole}} that you will recover swiftly.
%\end{exe}

%The word \exsent{hope}, on the other hand, fits rather well with surrounding words like \exsent{sincere}.  This leads to a two-step procedure for identifying semantically inappropriate words:
%\begin{enumerate}
%\item Find words which are semantically unrelated to their  surroundings.
%\item Find a candidate correction which is a better semantic fit in the sentence.
%\end{enumerate}

%For this to work, we then require two things: a way to identify words which are semantically unrelated to their surroundings, and a way to obtain a set of candidates for correction.  We already discussed in section~\ref{sec:generate} how to obtain candidate corrections, so we just have to define \keyword{semantic relatedness} between words, i.e., how far apart they are.  In other words, we want to know which words are likely to appear in the same topic: relevant here are synonyms (e.g., \exword{couch} and \exword{sofa}), antonyms (e.g., \exword{good} and \exword{bad}), and any sort of meaningful relationship (e.g., \exword{penguin} and \exword{Antarctica}) (see also chapters~\ref{sec:call-aware} and \ref{mt:lexical-semantics}).

%One way to define semantic relatedness is to use a \keyword{semantic  hierarchy}, such as WordNet (\url{http://wordnet.princeton.edu/}). A semantic hierarchy defines relationships between words, such as: a \exsent{hatchback} is a type of \exsent{car}, which is a type of \exsent{vehicle}.  For English, at least, WordNet is an impressive resource which lays out the relationships between many different words and concepts (and more information on it can be found in the \emph{Further reading}).  Roughly speaking, we can use the distance two words have in WordNet to tell us how semantically related they are: \exsent{hatchback} and \exsent{boat} are somewhat related, in that both are \exsent{vehicles}, but \exsent{hatchback} and \exsent{terrier} are not particularly related, as the only commonality is that they are \exsent{physical objects}.

% As a side note, the techniques we have been discussing are very similar to techniques used for a related task, that of \keyword{word   sense disambiguation (WSD)}.  In that task, a system takes a semantically-ambiguous word like \exsent{bank} and attempts to determine which word sense the word has in a given context (e.g., a financial institution vs.~the sloped land next to a river). Hierarchies like WordNet are useful for defining the senses.

\section{Grammar checkers}




%The problem of \keyword{context-dependent word correction} is that of detecting and correcting errors which result in existing words, the \keyword{real-word errors} we first mentioned in section \ref{sec:real-word-errors}.  For example, the use of \exsent{There} in \exsent{There house is nice.} is wrong, but it is still a word.  The problem is not the word itself but the particular way that it is used. We say that the word does not fit the context.

When our context-sensitive spell-checker replaces \exword{their} with \exword{there}, we are moving from spell-checking into \keyword{grammar checking} -- automatically identifying errors in sentence structure.  
Before we investigate how to correct grammatical errors, we first must unpack the idea of grammar.

%\mad{I don't love this sentence, but felt like something more was needed.}Let's dig into how to check for grammatical errors!
% replaced with what used to be the first sentence of the following section -LG.

% This kind of spelling correction is closely related to \keyword{grammar checking}.  Consider \exsent{The teams was   successful}: here there is a grammatical error because the subject \exsent{teams} is plural while the verb \exsent{was} is singular. Again, the words used are wrong in context. Without looking further, we cannot be sure whether the writer intended \exsent{The   teams were successful} or \exsent{The team was successful}, but we know that something is wrong with the sentence.

% In the first case of the misspelled \exsent{there}, we know that the word \exsent{there} is not a possessive, yet this is the function it has in the sentence.  In order to know that this is a misspelling, we have to do some grammatical analysis, and some reasoning about what the writer probably intended. Indeed, it could be that the writer actually meant to say either \exsent{Their house is nice} or \exsent{The house there is nice}. To detect, classify and correct these issues we will need to do some guessing, and we want this guessing to be well-informed and based on evidence.  Thus, we will treat context-dependent word correction and grammar correction together here. 

\subsection{What is grammar?}
\label{sec:what-is-grammar}


 Linguists view \keyword{grammar} as the implicit set of rules that speakers use to create and understand sentences in their language.  For linguists, grammar is \keyword{descriptive}: The rules describe how people \emph{actually} use language, not how they \emph{should} use it.  Thus, for linguists, \exsent{I ain't got no books} is a perfectly grammatical sentence in some varieties of English, because people use it and understand it, whereas \exsent{*book the read student} is ungrammatical (indicated by the asterisk) because no one would ever use it.  

In contrast, educators and editors view grammar as a \keyword{prescriptive} set of rules for how people should create sentences in a way judged by some authority to be clear and correct.  Educators and editors may argue that \exsent{I ain't got no books} does not follow the prescriptive rules of standardized English, likely because this pattern is not used by the people who have historically had the power to declare their usage conventions as the correct ones.  \exsent{*Book the read student} would in some sense be prescriptively incorrect too, but prescriptive grammarians might not even think to prescribe against a sentence that no one would ever use.

While linguists focus on descriptive rules, in reality, commercially available grammar checkers correct text according to both descriptive and prescriptive rules, as writers generally want to adhere to both. To understand how such rules can be represented on a computer, we need to take a long look at how linguists analyze grammar. In linguistics, grammar -- as we have been using the term -- is studied largely within the subfield of \keyword{syntax}, where the goal is to identify rules and representations that explain how speakers create and understand new sentences. 

% \mad{I feel like we should give more indication that the rest of the paragraph is forward pointing to what we're going to discuss. Like, "as it turns out [or as we're going to see], words are represented ..."}

Sentences are interesting because on the one hand, words are ordered linearly: They are written from left to right (in the Latin alphabet used to write English), and spoken one after another over time.  But on the other hand, words are represented in the mind in a hierarchical structure, as we're going to see: Groups of words form chunks called \keyword{constituents}, and words are mentally and structurally ``closer'' to their constituent-mates than to other words that may be equidistant linearly.  Such hierarchical structure is not directly reflected in speech or writing, but is important for understanding the   meaning of a sentence, so a syntactic theory must find a way to represent it.


 In \REF{ex:reindeer}, for example, it seems like \exsent{most of the reindeer} forms a unit, and this intuition is backed up by the fact that we can replace it with other, similar phrases, like \exsent{the children} or \exsent{two of the students}.  We can also ask questions about this sentence for which \exsent{most of the reindeer} is the answer, such as: Who plays extremely fun games?  And we can conjoin \exsent{most of the reindeer} with other constituents using \exword{and}: \exsent{Most of the reindeer and some of the dogs play extremely fun games}. So, according to these \keyword{constituency tests} (replacement, answering of a question, conjoining), \exsent{most of the reindeer} forms a constituent.
 

\ea \label{ex:reindeer} Most of the reindeer play extremely fun games.
\z 


Large constituents are made up of smaller constituents.  \exsent{The reindeer} is a constituent too: We can replace it with other noun phrases (\exsent{the puppies, my students}).  It might be hard to phrase a question about \REF{ex:reindeer} such that \exsent{the reindeer} is the answer, but we can conjoin \exsent{the reindeer} with other similar constituents: \exsent{Most of the reindeer and the puppies play extremely fun games}.  So \exsent{the reindeer} is a constituent.

Just like \exsent{most of the reindeer}, \exsent{reindeer play extremely fun} is a four-word string of the same sentence, but this one does not behave like a meaningful unit.  It cannot serve as the answer to a question, nor can it be conjoined with or replaced by any other constituent.

In syntax, one of the goals is to articulate a set of rules for how to assemble words into constituents and constituents into sentences.  Starting at the word level, the way a given word fits into constituents and sentences is determined by its \keyword{part of speech} (also known as its \keyword{lexical category}).  The main parts of speech are verbs
(\exword{play}, \exword{run}, \exword{believe}, \exword{throw}), nouns
(\exword{reindeer}, \exword{game}, \exword{government}, \exword{basketball}), adjectives
(\exword{fun}, \exword{beautiful}, \exword{potential}, \exword{brown}), adverbs
(\exword{heavily}, \exword{well}, \exword{later}), prepositions
(\exword{on}, \exword{at}, \exword{into}), articles (also known as determiners: \exword{a},
\exword{the}, \exword{this}, \exword{some}), and conjunctions
(\exword{and}, \exword{or}, \exword{since}).


\label{part:of:speech}
To identify a word's part of speech, the clearest evidence comes from its \keywordAs{distribution}{distribution   of a word}: Where does it typically appear in a sentence?  For instance, nouns like \exword{reindeer} can appear after articles, like \exword{some} or \exword{the}, while a verb like \exword{eat} cannot. The second criterion is the word's morphology, the kinds of  prefixes or suffixes a word can have.  For example, verbs like \exword{play} can take an \exword{-ed} ending to mark them as past tense; but a noun like \exword{reindeer} cannot.  Words that share a similar distribution and similar morphological patterns are put into the same lexical category. Linguists use categories that are finer-grained than the familiar parts-of-speech, but they still talk about nouns, verbs, adjectives and so on, meaning roughly the same thing as an educated non-linguist does.  

Turning to a word's meaning, there is some truth to the common intuition that nouns (\exword{reindeer}) prototypically describe people, places, and things, while verbs (\exword{play}) describe actions and events, and adjectives (\exword{cute}) describe properties.  But there are also exceptions: \exword{Earthquake} is a noun but seems to describe an event almost like a verb, and \exword{beauty} is a noun but describes a property almost like an adjective.  Therefore, a word's distribution distinguishes its part-of-speech more clearly than its meaning.

In any case, a word's part-of-speech determines the syntactic behavior of the constituents built from it.
Constituents are built by assembling words according to their part-of-speech.  \exsent{The reindeer} is a constituent because it follows a general English rule of combining an article (\exword{the}) with a noun (\exword{reindeer}).  \exsent{Play games} is a constituent because it follows a general English rule of combining a transitive verb (\exword{play}) with a noun phrase (\exword{games}).
Just as words combine according to their lexical category, constituents  combine with other constituents according to their \exword{phrasal category}: A \keyword{noun phrase} (\exword{the reindeer}) can combine with a \keyword{verb phrase} (\exword{run, play extremely fun games, eat lunch}) to create a sentence.

When we know a word's part of speech, we know that we can substitute  other words from the same part of speech into a sentence in such a way that  --  even if its meaning changes or even becomes nonsensical -- the sentence still follows the rules of English.  Similarly, when we know the phrasal category of a constituent, we can substitute any other constituent from the same category. (Try it!) 


\citet{Chomsky:1957} observed that the sentence \exsent{Colorless green ideas sleep furiously} is nonsense in terms of its meaning and contains very unlikely $n$-grams such as \exword{colorless green}, but still grammatical as a sentence of English because all of the lexical and phrasal categories are combined correctly.  From his perspective, this example shows that the rules for building grammatical sentences are based on the sentence's structure rather than its meaning.


All of this prose discussion of syntax can be more explicitly formalized and visualized in a particular syntactic framework.  We introduce two such frameworks, phrase structure grammar (influential in American formal linguistics) and dependency grammar (widely used in NLP).%
%\mad{The way we phrase this may irritate some dependency grammarians, who might argue that PSG is influential in linguistics developed in the USA}
% reworded to say that Phrase Structure Grammar is popular in "American formal linguistics"

\subsection{Phrase structure grammar}

If you have taken a linguistics class, you have most likely encountered phrase structure grammar, introduced by Noam Chomsky in the 1960s.  Phrase structure grammar is a set of rules for breaking down a sentence into its constituents. For this, we need two things -- a lexicon of words categorized by their parts-of-speech; and a set of syntactic rules for how to assemble or decompose constituents according to the parts-of-speech of the words therein.

Of course, the lexicon is huge and new words are added all the time, so we can't write down all the words in every lexical category.  And there are lots of different syntactic rules for handling complex structures such as relative clauses, questions, and so on.  But we don't necessarily need to engage with all of that complexity to get started on a phrase structure grammar.  Instead, we can start with a \keyword{grammar fragment}, a toy grammar that includes just a few words and a few syntactic rules, which serves as a \keyword{model} for how we think grammar works more generally.

In our grammar fragment, we keep it simple with just a few words and phrase structure rules:


\begin{table}
\begin{tabular}{ll}
\emph{Lexicon:}\\
N   $\to$ \exword{reindeer, dragon, lunch, game, evening, morning}\\
V(trans)  $\to$ \exword{play, eat}\\
V(intrans)  $\to$ \exword{run, swim, dance}\\
Adj $\to$ \exword{fun, beautiful, interesting}\\
Det $\to$ \exword{the, a, some, many}\\ 
P $\to$ \exword{for, in, to, at} \\
\\
\emph{Phrase structure rules:}\\
S  $\to$ NP VP\\
VP $\to$ V(trans) NP\\
VP $\to$ V(intrans)\\
NP $\to$ Det (A*) N\\
NP  $\to$ N(plural)\\ 
NP $\to$ NP PP \\
PP $\to$ P NP \\
\end{tabular}
\caption{An English grammar fragment, including rules for words.}
\end{table}


``S  $\to$ NP VP'' is a rule meaning that a sentence (S) can be broken into a Noun Phrase constituent and a Verb Phrase constituent.  ``NP $\to$ Det N'' means that a Noun Phrase constituent can be broken into a determiner followed by a noun.  ``Det $\to$ \exword{the, a, some, many}'' means that a Determiner constituent 
can be instantiated by these words.  ``N   $\to$ \exword{reindeer, dragon, lunch, game}'' means that a Noun node can be instantiated by these nouns.  When a constituent is manifested by an individual word, we call that constituent a \keyword{terminal node}, because it doesn't contain any further constituents.

``NP $\to$ Det (A*) N'' means that a Noun Phrase can consist of a determiner; an optional (indicated by parentheses) and unlimited (indicated by the asterisk) number of adjectives; and a noun.  This rule parsimoniously captures Noun Phrases with and without adjectives, such as \exsent{the reindeer, the beautiful reindeer, the beautiful interesting reindeer}, and so on.

We can use these phrase structure rules to draw a \keyword{phrase structure tree} (\figref{fig:pstree}) for our sentence.   These rules are meant to indicate both syntactic hierarchy and linear order.  Concerning hierarchical structure, the idea is that if a given string of words is a constituent, that string of words will share a so-called \exword{parent} node not shared by other words (e.g., \exword{the reindeer} with its parent NP, \exword{played games} with its parent VP).  If a string of words is not a constituent (e.g., \exword{reindeer played}), then these words will not share a parent node to the exclusion of other words.  As for linear order, the terminal nodes can be read from left to right to arrive at the word order of the sentence.


\begin{figure}
\Tree[ .S [ .NP [  .Det  the ] [ .N reindeer  ] ]   [ .VP [ .V played ] [ .NP [ .N games ] ] ] ]
\caption{Phrase structure tree for \exsent{The reindeer played games.}}
\label{fig:pstree}
\end{figure}

% each phrase structure unit is a constituent.
% we can represent this in a tree....


% While this tree captures the intuition that words group into phrases, more can be done with the labels on the \keywordAs{tree nodes}{tree   node}. It is not helpful to use uninformative names such as \exword{a}, \exword{b}, and so forth.  What linguists do is to add categorical information, to tell us something about the \emph{kinds} of phrases there are.  For example, \exsent{most of the ducks} and \exsent{extremely fun games} are the same type of phrase, and \exsent{of the ducks} is something else entirely in that substituting it for one of the other two expressions does not result in a grammatical sentence.

% If you go further in the study of syntax, you will encounter sharp debates about specific aspects about how words and phrases really work. What we are presenting here is the basic terminology and ideas that all linguists use, especially those linguists who primarily \emph{use} syntax (as opposed to researching it), as many computational linguists do.

%Thus, we examine word and phrase categories.  Since phrases are built from words, we start with word categories: \keywordAs{lexical   categories}{lexical category} are simply word classes, or \keywordAs{parts-of-speech}{part-of-speech}. 



%Moving from lexical categories to \keywordAs{phrasal  categories}{phrasal category}, we can also look at distribution. For the sentence in (\ref{ex:duck2}), we can replace the phrase \exsent{most of the ducks} with \exsent{a bear}; \exsent{Barack   Obama}; \exsent{many a dog}; or \exsent{people who have won the   Super Bowl}.  All of these phrases contain nouns as their key component, so we call them noun phrases, abbreviated as NP.

%\begin{exe}
%  \ex\label{ex:duck2} Most of the ducks swam a lap.
%\end{exe}

%Being able to replace a phrase with another is a test of replacability, and it is one of several different \keywordAs{constituency tests}{constituency test} to determine which sequences of words make up constituents.  At any rate, if we give each phrasal category a name and an abbreviation, we can now draw a more complete tree, as shown in Figure~\ref{fig:tree-labeled}.

%\begin{figure}
%\begin{center}
%\begin{tikzpicture}[parent anchor=south, child anchor=north,
%  t/.style={font=\it\strut}, n/.style={font=\rm\strut}]
%\tikzstyle{level 1}=[level distance=12mm, sibling distance=20mm]
%\node[n] {S} 
%child [sibling distance=15mm] {node[n] {NP}
%       child {node[n] {Pro} child {node[t] {Most}}}
%       child {node[n] {PP}
%              child {node[n] {P} child {node[t] {of}}}
%                    child {node[n] {NP}
%                          child {node[n] {D} child {node[t] {the}}}
%                          child {node[n] {N} child {node[t] {ducks}}}}}}
%child [xshift=35mm] {node[n] {VP} 
%       child {node[n] {V} child {node[t] {play}}}
%       child  {node[n] {NP}
%              child {node[n] {AdjP}
%                    child {node[n] {Adv} child {node[t]  {extremely}}}
%                    child {node[n] {Adj} child {node[t]  {fun}}}}
%              child [xshift=5mm] {node[n] {N} child {node[t] {games}}}}}
%; 
%\end{tikzpicture} 
%\caption{A tree with linguistically meaningful labels}
%\label{fig:tree-labeled}
%\end{center}\unskip
%\end{figure}



%Our phrase structure rules indicate both hierarchy and linear order. The hierarchy aspect is that a sentence (S) constituent is composed of a noun phrase (NP) and a verb phrase (VP).  The linear order is indicated by the order of mention of the labels on the left hand side: the NP must precede the VP. 


%Syntax trees and phrase structure rules are really two sides of the same coin.  You can build a syntax tree by piecing together a set of phrase structure rules, and you can also go the other way, reading off the phrase structure rules that are used in a particular syntax tree. For instance, if we have the rule set in Figure~\ref{fig:dragon-rules}, we can give an analysis to \exsent{the young boy saw a dragon}.  The analysis is shown in Figure~\ref{fig:the-young-boy}.


%\newcommand{\Supp}{\rule{0em}{2ex}}
%\newcommand{\cat}[2]{\Supp#1$_{#2}$}

%\begin{figure}[htb!]
%\begin{center}
%  \Tree [.S 
%          [.NP 
%            [.Det $the$ ]
%            [.N 
%              [.Adj $young$ ] 
%              [.N $boy$ ]               
%            ]
%          ]
%          [.VP 
%            [.Vt $saw$ ]
%            [.NP 
%              [.Det $a$ ]
%              [.N $dragon$ ]
%            ]
%          ]
%        ]
%\end{center}
%\caption{Analyzing \exsent{the young boy saw a dragon} using the given grammar fragment}
%\label{fig:the-young-boy}
%\end{figure}

%A grammar, then, is simply a collection of these rules. Grammars with rules of the form shown here are referred to as \keyword{context-free   grammars}.  A grammar which accurately describes a language is able to model all and only those sentences which actually appear in the language.  It is very useful to try to build such grammars for human languages, but the complexity of human language makes it unlikely that anyone will ever manage to build a complete grammar for the whole of a human language. Instead, we build \keywordAs{grammar fragments}{grammar fragment}. These are simplified working models of a human language. Like other working models, they can serve several purposes. In this book, grammar fragments are important because they may be built into spelling and grammar checkers. For other linguists, they may be just as important as a tool for checking out scientific ideas about how human language works in general. If an idea works in a grammar fragment, maybe that idea reflects what is really going on in the mind of a human speaker. To prove that, detailed experimental or theoretical work would be needed, but a grammar fragment is a good start in the same way that a balsa-wood model in a flotation tank is a good start on the design of an oil tanker.  In this book, when we talk about a \keyword{model}, it is always a mathematical or a computational model: an abstract system designed to capture key apects of how we believe language to work. 

In a formal sense, each phrase structure rule must have a left-hand side, which is a single \keyword{non-terminal} element; non-terminals are defined as (phrasal and lexical) categories.  So, \exword{NP} is an acceptable left-hand side, while \exword{reindeer} is not.  Secondly, each rule must have a right-hand side, which is a mixture of non-terminal and terminal nodes -- \keyword{terminal} because they are the final nodes in a tree.  So, \exword{Det Noun} or \exword{Noun and Noun} or \exword{Preposition NP} are acceptable right-hand sides.

%In general, the left hand side of a phrase structure rule corresponds to the parent, and the labels in the right hand side of the rule correspond to its children -- the subtrees or terminal nodes nested under it.  

Note one other property of phrase structure rules: the label of the parent (i.e., the left-hand side) reflects the lexical category of the word within it that is in some sense the most important for determining its distribution. This most important word is called the \keyword{head}. So, a Noun Phrase is headed by a noun, a Verb Phrase is headed by a verb, and so on.

A grammar built on phrase structure rules is called a \keyword{context-free grammar}, which means that each rule is designed to be independent of the others.  These rules can be used wherever the category on their left-hand side occurs. Rules cannot be further restricted to only apply in some places but not others.  So, for example, you cannot have a rule which specifies that ``PP $\to$   P NP'' is applicable only when there is a verb phrase (VP) above the PP.  The real reason for this is complexity: by having simple rules that apply everywhere, you can reason in a modular fashion about what is going on inside a particular constituent.  (For more information on this topic, see the Under the Hood box on \emph{complexity of languages}.)

%With these rules put together into a grammar, this grammar has several properties which are useful for modeling natural language.  First, the grammar is \keyword{generative}; it is a schematic strategy that characterizes a set of sentences completely.  Secondly, as mentioned, the rules license \keyword{hierarchical} structure.  Thirdly, and crucially for natural languages, the rules allow for potential \subkeyword{struc\-tural ambiguity}{ambiguity}, where a sentence can have more than one analysis.  This is clearly needed when we examine sentences like (\ref{ex:leaders}), which can have multiple meanings. In this case, \exsent{more} can either group with \exsent{intelligent} (\ref{ex:leaders1}) or can modify \exsent{leaders} (\ref{ex:leaders2}).

%\begin{exe}
%  \ex\label{ex:leaders} We need more intelligent leaders.  
%  \ex \textup{Paraphrases:}
%  \begin{xlist} 
%    \ex\label{ex:leaders1} We need leaders who are more intelligent.  
%    \ex\label{ex:leaders2} Intelligent leaders? We need more of them!
%  \end{xlist}
%\end{exe}

The larger idea is that a phrase structure grammar represents an attempt to write down the implicit rules that explain how speakers create and understand new sentences, and thus to represent the infinite possibilities of language using a finite set of rules.

Note that these rules are \keyword{one-to-many}, meaning that the same left-hand side can correspond to multiple right-hand sides of a rule.   ``VP $\to$ V NP'' captures the behavior of transitive (object-taking) verbs such as \exword{throw} (\exsent{throw the ball}), while ``VP $\to$ V'' captures intransitive (object-less) verbs (\exsent{swam}).   ``NP $\to$ Det N'' captures \exsent{the reindeer}; ``NP $\to$ N'' captures bare (determiner-less) noun phrases such as \exsent{reindeer}; and ``NP $\to$ Det A N'' handles \exsent{the beautiful reindeer}.  One-to-many rules give phrase structure this important flexibility.

Phrase structure rules are also \keyword{recursive}, meaning that a rule can be used in building its own sub-structure. In \REF{ex:recursion}, for instance, NP is
reapplied within itself, thus allowing for phrases such as 
 [ [ \textit{the dog} ] [ \textit{on} [ \textit{the porch} ] ] ]: the whole thing is an NP, with smaller NPs (\exword{the dog, the porch}) inside it.

\ea \label{ex:recursion} 
	\ea NP $\to$ NP PP
	\ex PP $\to$ P NP
	\ex NP $\to$ Det N
 \z 
 \z


Similarly, the top S node can be embedded within a larger S node, for example to deal with sentence-level conjunction (\exsent{I ran and I swam}) as well as embedded clauses (\exsent{They know that you won}).  Recursion is another way that phrase structure rules capture the infinite possibilities of language.

Finally, phrase structure rules are \keyword{language-specific}, meaning that each language has not just its own lexicon but also its own grammar.  The English rule ``S $\to$ NP VP'' will not work for Irish, which uses \keyword{VSO} word order -- meaning that the verb comes first, the subject second, and the object last (\exsent{Threw Alice the ball}).

With phrase structure grammar, it is not obvious how to deal with sentences where there is a \keyword{long-distance dependency} between words, meaning that the linear position of a word and its structural role seem to come apart.  In \REF{ex:wh}, the word \exword{what} is far away from the word \exword{eat}, and yet there is a sense in which \exword{what} is extremely closely related to \exword{eat}, asking a question about the syntactic object of the eating.  Different syntactic theories handle long-distance dependencies in different ways.  Phrase structure grammar uses additional rules known as \keyword{transformations} or \keyword{movement}, which allow a word to essentially be in two places at once (one reflecting its linear order, another reflecting its structure); other theories add special rules for words like \exword{what}.  Such rules are interesting for linguists, but tricky for practitioners because things get more complicated when the structure and the form of the sentence don't match up.


\ea \label{ex:wh} What did you eat?
\z 

In sum, phrase structure grammar is a hugely influential idea in linguistics, and it can help build systems for grammar checking.  However, in many NLP applications, the grammar of a sentence is actually represented in a different framework: dependency grammar.


% the important thing to realize about syntax is that there is no One True Theory and in general actually you should try to keep apart theories versus facts....


\subsection{Dependency grammar}

\keywordAs{Dependency grammar}{dependency grammar}, introduced by the French linguist Lucien Tesni\`ere \citep{Tesniere:1929}, inspired by ideas dating back to the ancient Indian linguist Panini, provides a way of representing the syntactic relations between words in a sentence as a series of \keyword{binary}, \keyword{asymmetric} relations between pairs of words, a \keyword{head} and a \keyword{dependent}.  

For example, \figref{fig:reindeerdep} shows a dependency representation for \exsent{The reindeer played games}.  Each word is labeled for its part of speech (\exword{the} is labeled as DT, which refers to determiners; \exword{reindeer} is NN, which refers to a singular common noun) and is linked to another word via a dependency relation.  Here, \exword{the} is the dependent of the head word \exword{reindeer}.  The relation between these words is asymmetric, in that \exword{the} is the dependent of \exword{reindeer} (and not vice versa), as represented by the unidirectional arrow from \exword{reindeer} to \exword{the}.  Each relation is also \keyword{typed}, meaning that it has a specific label: here, the determiner relation (labeled \exword{det}).  In turn, \exword{reindeer} is the dependent of  the head \exword{played} (a past-tense verb, with the part of speech VBD); in particular, the dependency relation between \exword{reindeer} and \exword{played} is labeled with the  \exword{nsubj} relation type, which indicates a subject noun.  Finally, \exword{games} (a plural common noun, with the part-of-speech tag NNS) is also the dependent of the head \exword{played}, related through the \exword{obj} relation (for the syntactic objects of transitive verbs).  The verb \exword{played}  is the \keyword{root} of the sentence because it does not depend on anything else.


\begin{figure}
  \begin{dependency}[theme = simple, label style={scale=1.5}]   \begin{deptext}[column sep=1em]
    The \& reindeer \& played \& games\\
    DT \& NN \& VBD \& NNS\\
   \end{deptext}
   % MD: the \sc is a bit of a hack here (changed to \textsc by dm, though still a hack ;-)
    \depedge{2}{1}{\textsc{det}}
    \depedge{3}{2}{\textsc{nsubj}}
    \depedge{3}{4}{\textsc{obj}}
   \end{dependency}
\caption{Dependency graph for \exsent{The reindeer played games}.}
\label{fig:reindeerdep}
\end{figure}


One big advantage of dependency grammar is that the same set of dependencies can be used for any language, regardless of the word order.  In a language such as Irish where the verb comes first, the Irish equivalent of the verb \exword{played} could come at the beginning of the sentence, with both its \exword{nsubj} dependency and its \exword{obj} dependency coming afterward.  Indeed, dependency grammar was inspired in part by Lucien Tesni\`ere's work on Slavic languages such as Russian, which allow many different word order options and use word endings known as \keyword{case markings} rather than word order to indicate the relations between words in a sentence.  As a result, dependency grammar can be used to generate consistent representations for sentences in a multilingual corpus.  The \keyword{Universal Dependencies} project \citep{Nivre-etal:2016}, based at Stanford, offers a corpus of sentences in over 100 languages, all  annotated for their dependency relations in a consistent framework.  Such a corpus  helps linguists to study \keyword{language typology} -- the ways in which different languages are similar or different -- and empowers technologists to build robustly multilingual language technologies.



Because dependency grammar can reflect dependencies between non-adjacent words, it also offers a more transparent representation of long-distance syntactic dependencies.  Whereas a phrase structure grammar needs special rules to handle sentences like \exsent{What did you eat?}, a dependency grammar can just allow a long-distance dependency, with an \exword{obj} relation between \exword{what} and \exword{eat} (\figref{fig:whateat}).  Moreover, because dependency grammars are based on words (\exword{eat}) rather than phrases (\exword{VP}), these representations invoke fewer invisible abstractions and stay closer to what you see in the text.
 
 
\begin{figure}
\begin{dependency}[theme = simple] %, arc angle=45]
   \begin{deptext}[column sep=1em]
    What \& did \& you \& eat \& ?\\
    WP \& VBD \& PRP \& VB \& .\\
   \end{deptext}
   % MD: the sc is a bit of a hack here: I think there's a way to define a node style, but don't remember it
    \depedge{4}{1}{\textsc{\normalsize obj}}
    \depedge{4}{2}{\textsc{\normalsize aux}}
    \depedge[label style={below}]{4}{3}{\textsc{\normalsize nsubj}}
    \depedge{4}{5}{\textsc{\normalsize punct}}
   \end{dependency}
   \caption{Dependency graph for \exsent{What did you eat?}}
   \label{fig:whateat}
\end{figure}


In dependency grammar, the root of a sentence is generally a verb, reflecting the idea that verbs are very important.  The main verb of a sentence determines many elements of its syntax: if it  is a transitive verb such as \exword{throw}, the sentence will contain at least two noun phrases (the subject and the object of the verb), whereas if it is an intransitive verb such as \exword{swim}, it can contain only one (the subject).  The main verb is also important for understanding the event that the sentence describes, which all other elements of the sentence are related to.   The syntactic relations between a verb and its dependent nouns (for example, the \exword{nsubj} relation between a verb and its noun subject)  correspond roughly to semantic relations between this event and its  participants.  For example, the syntactic \exword{nsubj} relation  often corresponds to the semantic relation of agency: in the sentence \exsent{Alice swam}, Alice is not just the subject but also the agent, the doer, of the swimming event.  And in languages such as English, the main verb is also marked for tense, placing this event in time.   So by building the sentence around the verb, dependency grammar aims to highlight information relevant to the sentence's meaning.

In comparing phrase structure grammar to dependency grammar, it is important to understand that each framework has different goals.  Phrase structure is aimed at capturing how speakers of a particular language can generate infinite sentences from a finite set of words and rules; dependency grammar tries to capture the syntactic and semantic relations between words in a sentence in any language.  If you go on to study syntax within linguistics, you should know that there is no single true theory of sentence structure; different theories capture different pieces of the puzzle.

In practice, for all the reasons described above, dependency grammar is widely used in NLP.  Tools are available to automatically generate a structural representation for a new sentence, a task known as \keyword{parsing}.  To parse a sentence, the words are first tagged with their parts of speech (noun, verb, adjective, determiner); then the parser reads the sentence one word at a time, and at each word has to decide whether to assign a dependency relation (choosing the direction, the head, the dependent, and the type of the relation) between that word and a previous word.  Parsers are built using huge datasets of sentences already annotated for their dependency structure, and are trained using machine learning to generalize this information to assign the right parse to new sentences.  Two of the most widely used dependency parsers are the Stanford  Parser (available in Java and Python; \citealt{deMarneffe-etal:2006}) and SpaCy (Python; \citealt{HonnibalJohnson:2015}).  Both of these also offer web demos to visualize what is going on.

% why would you use this.
% verbs are super important actually.
% can also label these relations with semantic labels as well, like agent of patient, that is useful too for linking syntax to semantics
% crosslinguistically important
% dependency length is a worthwhile idea too for psycholinguistics and stuff.
% don't make reference to supra-word categories like VP, instead stick a bit more with what you see is what you get, which can be useful for NLP.
% the focus is a bit more on capturing actual sentences rather than capturing a set of rules to generate sentences
% have to decide how many dependencies you will allow.
% most parsers that are used are dependency parsers.


% words are more important than phrases
% how many relation labels will you allow? this is not  easy, have to make a decision here

% how do you decide what's the head, what's the dependent??
% dependent: may be optional, its position is determined by the position of the head, 

% not sensitive to word order really, this makes them much more universal.
% dependency length - this is a useful idea also, in psycholinguistics and stuff. threw the trash bin out, threw out the trash bin. right.
% transparent relation between predicates and arguements, which helps with semantics.

% it is also worth noting if you continue with linguistics that there's more than one way to do syntax and no one theory is like, the ultimate truth about language, it just depends what you are interested in.

% Universal Dependencies - annotates a bunch of languages with the same rules.
% linguistics, also NLP, build tools that work across languages.


% put this in further reading: https://www.annualreviews.org/doi/full/10.1146/annurev-linguistics-011718-011842

% stanford parser as well as SpaCy, these work super well and you should try them out, they also have web demos.

% https://nlp.stanford.edu/software/nndep.html more widely used....


\subsection{Fun with ambiguity}

After this somewhat technical introduction to phrase structure grammar and dependency grammar, we can pause to appreciate one of the insights that we gain from a syntactic theory: it helps us to understand \keyword{syntactic ambiguity}.  Syntactic ambiguity arises when the same string of words could be assigned to multiple distinct syntactic representations, each with a different meaning.  Syntactic ambiguity is possible because, as we have seen, sentences have structure above and beyond the linear order of words. 

In \REF{ex:telescope}, for instance, we might understand the sentence to mean that the person whom I saw had a telescope (in which case \exword{with the telescope} modifies the noun phrase \exword{the person}); or we might understand it to mean that I used the telescope to see this person (in which case \exword{with the telescope} modifies the verb phrase \exword{saw the person}).  This ambiguity is known as a \keyword{prepositional phrase attachment ambiguity} because it involves the position of the prepositional phrase \exword{with the telescope}. Using phrase structure or dependency grammar, we could draw two different structures to reflect these two different meanings.
%\mad{Maybe a simple exercise to draw some different structurally ambiguous DG \& PSG trees?} - yes, added - LG.

\ea \label{ex:telescope} I saw the person with the telescope.
\z 

Sometimes the effects of ambiguity are amusing. In \REF{ex:baby}, a \textit{Time} headline from 2013, the author intends the headline as a noun phrase: the article describes the incremental steps (plural noun) that the Pope has taken on LGBT issues.  The amusing alternative interpretation takes \exword{steps} as a present-tense verb, meaning that the non-existent \exword{Pope's baby} has stepped on some gay people.  The ambiguity begins with the part-of-speech of the word \exword{steps} (plural noun or present-tense verb), with consequences for the structure of the whole sentence.


\ea \label{ex:baby} Pope's baby steps on gays.
\z 

Using the same thought process, you should be able to identify the source of the ambiguity in  \REF{ex:mushroom}--\REF{ex:foot}:

\ea \ea \label{ex:mushroom} MBA studies mushroom.
   \ex \label{ex:ugly} Complaints about referees growing ugly.
    \ex \label{ex:foot} Hospitals are sued by 7 foot doctors.
\z 
\z 

It is worth noting, though, that not all ambiguity comes from syntax.  There are also cases where the structure of the sentence is clear, but it is still compatible with multiple distinct meanings.  In  \REF{ex:head}, the intended meaning is presumably that the leader of Iraq is seeking military equipment; the amusing alternative is that there is a disembodied Iraqi body part in search of additional body parts.  Here, the ambiguity stems not from the structure, but a \keyword{lexical ambiguity}: the multiple (distantly related) meanings available to the words \exword{head} and \exword{arms}.


\ea \label{ex:head} Iraqi head seeks arms.
\z 

Another non-syntactic source of ambiguity is illustrated in \REF{ex:pillars}--\REF{ex:cookies}.  Here, the structure of the sentence is clear, but what's unclear is exactly what role is played by the participants in the event.  One way to \exword{replace} a pillar is to purchase a new one; another way is to physically stand in its place.  A pleasant way to \exword{include} children in baking is to let them participate; a macabre way is to use them as ingredients.   Here, the ambiguity arises from multiple ways that individuals can participate in the real-world event described by the sentence. 


\ea \ea \label{ex:pillars} Pillars replaced by alumni.
     \ex \label{ex:cookies} Include your children when baking cookies.
\z 
\z 

We hope that this exploration of syntax has given you the tools to explain not just syntactic ambiguity, but also its cause.  We also hope that you see the importance of structure, above and beyond linear word order, in interpreting sentences. 

% there is no single God's truth about syntax. linguists should remember this too... 


% I saw the person with the telescope
% pope's baby steps on gays
% MBA studies mushroom
% complaints about referees growing ugly
% pillars replaced by alumni 
% ok that's enough.

% important because: constituency, lexical categories, ambiguity itself is important, structural, lexical, part of speech.
% how could you represent the ambiguity with a phrase structure tree or dependency parse?
% a single stricng of words has multiple representaitons, this is also important for the idea of hierarchical structure
% is ambiguity bad
% people sometimes distinguish ambiguity from other ways that language is unclear, such as vagueness or underspecification....



%(These concepts are discussed more fully in chapter~\ref{sec:pos-tagging}, in the context of learner language, where such evidence can diverge.)

% part of speech tags
% phrase tags
% phrase structure rules
% also, dependency grammar, too....

% structural ambiguity.... is fun. talk about this. this is what our grammar allows us to handle, among other thigns.... 




%Traditionally, linguists have focused on \subkeyword{descriptive grammar}{grammar}, which refers to a set of rules for how speakers of a language form well-formed sentences.  For our purposes, this is a good start, and we will return to different definitions of grammar in section~\ref{sec:style}.

%Basing our discussion in this section on \citet{LanguageFiles11}, we define grammar in terms of rules, but when we talk about the \emph{rules} of a language, what do we mean by that?  To explain this, we need to introduce \keyword{syntax}, the study of how grammatical sentences are formed from smaller units, typically words.  Some of these concepts will also resurface when analyzing learner language in chapter~\ref{sec:beyond-words}.

% Sentences in human language are organized on two axes.  These are linear order and hierarchical structure.  The idea of \keyword{linear order} is that languages can vary word order so as to to convey different meanings.  Thus, in English \exsent{John loves Mary} is not the same as \exsent{Mary loves John}.  Not all languages use linear order in exactly the same way: for example, in Czech, Russian and German the presence of richer word endings makes it possible to vary word order slightly more freely than we can in English.  In linguistics, the study of word endings and other changes in word forms is called \keyword{morphology}.  Nevertheless, linear order is a guiding principle for organizing words into meaningful sentences, and every human language uses it to some extent.

%The other guiding principle for sentence organization is hierarchical structure, or what is called \keyword{constituency}.  Words are organized into groupings, and intuitively, these groupings are the \emph{meaningful units} of a sentence. 

%When we break it down further, we find that these meaningful groupings, or phrases, can be nested inside larger phrases of the same or different type: for example, \exsent{extremely fun} is a part of \exsent{extremely fun games}.  When we put all those phrases together, it is useful to view them as a \keyword{syntactic tree} as we see in Figure~\ref{fig:tree-basic}.

%\begin{figure}
%\begin{center}
%\begin{tikzpicture}[parent anchor=south, child anchor=north,
%  t/.style={font=\it\strut}, n/.style={font=\rm\strut}]
%\tikzstyle{level 1}=[level distance=12mm, sibling distance=20mm]
%\node[n] {a} 
%child [sibling distance=15mm] {node[n] {b}
%       child {node[t] {Most}}
%       child {node[n] {c}
%              child {node[t] {of}}
%                    child {node[n] {d}
%                          child {node[t] {the}}
%                          child {node[t] {ducks}}}}}
%child [xshift=35mm] {node[n] {e} 
%       child {node[t] {play}}
%       child  {node[n] {f}
%              child {node[n] {g}
%                    child {node[t]  {extremely}}
%                    child {node[t]  {fun}}}
%              child [xshift=5mm] {node[t] {games}}}}
%; 
%\end{tikzpicture} 
%\caption{A labeled tree}
%\label{fig:tree-basic}
%\end{center}\unskip
%\end{figure}




% Most linguists believe that recursion is an important defining property of human language. For this reason, experiments that aim to tease out the similarities and differences between human and animal communication often focus on recursion.

% \subsubsection{Parsing}

% Using these phrase structure, or context-free, rules, we want to get a computer to \keyword{parse} a sentence, that is to assign a structure to a given sentence.   Some parsers are \keyword{bottom-up}, which means that they build a tree by starting with the words at the bottom and working up to the top.  Other parsing techniques are \keyword{top-down}, i.e., they build a tree by starting at the top (e.g., S $\to$ NP VP) and working down the tree.  


% To give a rough idea of how a parser works, let's start with the rules found in Figure~\ref{fig:dragon-rules} and attempt to parse \exsent{the young boy saw a dragon}.  

% A top-down parse starts with the S category -- since the goal is to have a sentence (S) when the parse is done -- and expands that to NP VP.  The next step is to try expanding the NP, in this case into Det N, as that is the only possibility in this grammar.  A trace of the order of rules can be seen in Figure~\ref{fig:top-down}.  If there are multiple possible NP rules, the parser can consider them one at a time and then \keyword{backtrack} if a particular NP expansion does not work out.

%\begin{figure}[htb!]
%\begin{center}
  %\Tree [.\cat{S}{1} 
    %      [.\cat{NP}{2} 
   %         [.\cat{Det}{3} \cat{$the$}{4} ]
  %          [.\cat{N}{5} 
 %             [.\cat{Adj}{6} \cat{$young$}{7} ] 
%              [.\cat{N}{8} \cat{$boy$}{9} ]               
      %      ]
     %     ]
    %      [.\cat{VP}{10} 
       %     [.\cat{Vt}{11} \cat{$saw$}{12} ]
      %      [.\cat{NP}{13} 
     %         [.\cat{Det}{14} \cat{$a$}{15} ]
    %          [.\cat{N}{16} \cat{$dragon$}{17} ]
   %         ]
  %        ]
 %       ]
%\end{center}
%\caption{Trace of a top-down parse}
%\label{fig:top-down}
%\end{figure}

%Instead of starting with the goal of S, the bottom-up way of parsing starts with the very first word, in this case \exsent{the}, and builds the tree upwards.  Based on the lexicon, Det is the only category which fits for \exsent{the}, so this is added to the tree next.  On its own, Det cannot form any higher category, so the next word (\exword{young}) is considered, which is an adjective (Adj).  This continues upwards, as seen in figure~\ref{fig:bottom-up}.

% This is only a brief sketch of how parsing proceeds to analyze a sentence given a grammar.  In general, there are many parsing techniques out there; pointers are provided in the \emph{Further  reading} section at the end of the chapter.



\begin{tblsfilledsymbol}{\underthehoodsubsection{Complexity of languages}}{glass}
\label{sec:complexity-of-languages}
\begin{underthehood}

    
We discussed context-free grammars earlier, which can be used to
process natural languages and to assist in error detection.  But what
we did not justify was that context-free grammars are: a) all we need
(we call this being \keyword{sufficient}) and b) nothing more than we
need (\keyword{necessary}).  In other words, maybe there are simpler ways
of analyzing language?  On the other hand, maybe context-free grammars
are incapable of capturing all natural language phenomena?

What we are discussing is linguistic \keyword{complexity}, or the
limits of what a particular type of grammar can do.  Intuitively, the
idea is that the phenomenon under discussion might be too complex to
fit into the framework that we have for characterizing it.  But what
exactly do we mean by being too complex?  To answer this, we need to
sketch some \keyword{formal language theory}.  In formal language
theory, we \emph{define} a language as a set of acceptable sentences
(albeit, an infinite set). This is a technical use of the word
\emph{language}, not the everyday one, but it turns out that there is
a close parallel between the mathematical reasoning that we can do
about formal languages and the scientific reasoning that we want to do
about natural, human languages.  In the same way, there is a formal
definition of what a grammar is, and a mathematical description of
exactly what it means for a language to be characterized by a grammar.

Formal grammars, such as context-free grammars, differ in terms of
which formal languages they can and cannot describe.  Loosely, what we
are going to say is that a formal grammar is more powerful than
another if it can draw finer distinctions among formal
languages. Instead of relying on informal intuitions about power and
complexity, we can use mathematics.  The point of the move to
mathematics is to gain clarity by learning to view our scientific
questions in an abstract way.  For many of us, this is a scary thing
to do.  Fortunately, in the case of formal language theory, the
results of doing the abstract thinking are so valuable, and so
obviously relevant to language, that overcoming the initial discomfort
is worthwhile.

Formal grammars and languages are therefore useful tools in coming to
grips with the issues around the complexity of natural languages.  You
should think of formal language theory -- with its precise definitions
for what a grammar is, what a language is and what mathematical
operations are allowed -- as a working model of aspects of the way
that natural language behaves.  Real languages are obviously messier
than this, but the models are still useful.

For example, we can write a context-free grammar to capture the
following kinds of \keywordAs{center-embedded structures}{center-embedded structure} in English:

\ea \ea The frog is happy.
    \ex \label{ex:frog2} The frog \emph{that the princess saw} is happy.
    \ex \label{ex:frog3} The frog \emph{that the princess that the prince liked}    saw is happy.
\z 
\z 


At some point, these sentences become difficult to understand, but in
theory we can have an arbitrary number of sentences embedded within
other sentences in English.  (As a side point, we can also remove \exsent{that} and get a valid sentence.)  The relevant context-free rules might
look as in \REF{ex:cfg-embed}.

\ea \label{ex:cfg-embed}
    \ea S $\to$ NP VP
    \ex NP $\to$ NP S
\z 
\z 

It turns out that context-free grammars are more powerful than regular
grammars, such as those used to write regular expressions, as in
\chapref{ch:searching}. (Note that in regular grammar and regular
expression we are using \emph{regular} in a technical sense.) 
There is no way to write a regular expression which can capture an
arbitrary number of center embeddings.  Thus, it seems like we need to
at least use context-free grammars for natural language syntax.

We have said that context-free grammars are more powerful than regular grammars.
This is not the end of the matter: there are even more powerful grammar formalisms,
capable of describing yet more complex formal languages.
namely \keywordAs{context-sensitive grammars}{context-sensitive grammar}.  It turns out that
context-free grammars cannot capture what are called
\keywordAs{cross-serial dependencies}{cross-serial dependency}.  With center embeddings, as shown
above, we have a noun phrase subject which is associated with a verb
phrase; in all three sentences, \exsent{the frog} and \exsent{is happy} go
together.  When we look at it this way, we see that the last sentence
has a structure akin to \exword{abccba}, as shown in \REF{ex:frog}, a repeat of \REF{ex:frog3}.
(This tells us that we can use context-free grammars to write
palindromes, too!)  


\ea \label{ex:frog} The frog$_a$ that the princess$_b$ that the
  prince$_c$ liked$_c$ saw$_b$ is happy$_a$.
\z 

Cross-serial dependencies, on the other hand, are of the form
\exword{abcabc}.  They are called \emph{cross-serial} because, if we draw a dependency between the two  \exword{a}'s, it has to cross over the
dependency between the two \exword{b}'s. If we imagined English to
have cross-serial dependencies, we would expect \REF{ex:frog2} to be
rewritten as in \REF{ex:frog2-prime}.

  \ea \label{ex:frog2-prime} The frog$_a$ the princess$_b$ is happy$_b$
  saw$_a$.
  \z 

This clearly does not happen in English, but does it happen in other
languages?  Perhaps surprisingly, the answer is yes.  In Swiss German,
for example, verbs and their direct objects cross.  We can see this in
 \REF{ex:hans}, from \citet{Shieber:1985}, where \exsent{em Hans} (`Hans') is the object
of \exsent{h\"{a}lfed} (`helped'), and \exsent{huus} (`house') is the
object of \exsent{aastriiche} (`paint').  At least some languages, then,
need more powerful grammars than context-free grammars.

\ea \label{ex:hans}\gll ... mer em Hans es Huus h\"{a}lfed
  aastriiche.\\
  ... we \textsc{dative} Hans the house-\textsc{acc} helped paint \\
  \glt `... we helped Hans paint the house'
\z 

\end{underthehood}
\end{tblsfilledsymbol}





\subsection{From grammar to grammar checkers}
\label{sec:grammar-checking}

After this long detour into syntax and the nature of grammar, we are finally ready to explore how these ideas can be applied to grammar checking. And it turns out that we've already done the heavy lifting by figuring out how to assign structures to sentences!

To begin, a grammar checker can first try to assign a dependency parse to a sentence.  If the grammar checker cannot parse the sentence at all, it should probably be flagged as erroneous.  If the grammar checker can assign a dependency parse, it can check this parse against a set of hand-written grammatical rules (a \keyword{rule-based} technique) corresponding to common errors: for example, it could check that if a verb's \exword{nsubj} dependent is part-of-speech-tagged as a singular noun (with the tag NN), the verb should also be part-of-speech-tagged as the singular third-person form (with the tag VBZ). Such a grammar checker would flag \exsent{The dog swim} and perhaps suggest that it should be corrected to \exsent{The dog swims}. 
% \mad{Do we want to say more about detecting vs. correcting an error? I think that might help some of the confusion set discussion below}

% \mad{Exercise for a CFG parse? e.g., explain how a CFG-based grammar checker would analyze "The dog swim"?}though similar steps can be taken with a context-free parse - ADDED-LG.

As an aside, it's important to note that there are some varieties of English (including Norfolk English and African American English) where \exsent{The dog swim} is perfectly acceptable.  So by treating it as an error to be corrected, our grammar checker already blends together prescriptive grammar with descriptive grammar.

 A grammar checker could also combine error detection and error correction.
 %\faye{added a sentence here that may help clarify - THANKS, good -LG}.
 For example, a grammar checker could automatically consider all alternative spellings for common confusion sets (\exword{there, their,} and \exword{they're}) and then suggest the one that has the highest $n$-gram probability or yields the most probable dependency parse.  To build such a grammar checker, you would need a fast and accurate dependency parser plus a (potentially hand-written) set of rules and confusion sets corresponding to common errors.  A grammar checker could also be crafted not just to flag errors, but also to explain what grammatical rule is violated, along with suggested corrections.

%We could also use a \keyword{relaxation-based technique} whereby the parser is trained to parse erroneous input (``relaxing'' its usual behavior) while flagging such errors. 


% With a syntactic model of language, what we have is a model of grammatical language.  Therefore, we can use a parser to find where the grammar breaks down.  When a parser fails to parse a sentence, we have an indication that something is grammatically wrong with the sentence. We here are more concerned with identifying that there is a grammatical error, such that a native speaker could figure out exactly what the problem is. In the context of Language Tutoring Systems (LTSs) discussed in chapter~\ref{ch:call}, additionally the nature of the error needs to be specifically pointed out to the learner.

%There are a variety of general approaches for dealing with ill-formed input, and here we will discuss relaxation-based techniques. \keywordAs{Relaxation-based techniques}{relaxation-based  technique} for detecting grammar errors relax the parsing processto allow errors.  The parser is altered in some way to accept input that it would normally reject as erroneous. This might involve relaxing the parsing procedure so that what would normally result in a parsing failure is allowed to continue building a parse tree, noting where in the parsing the relaxation  took place.  Alternatively, one might write grammar rules which predict types of failures (i.e., \keywordAs{mal-rules}{mal-rule}); for example, a rule might be expressly written to allow a first person singular subject to combine with a third person singular verb, indicating the construction as erroneous.

% Although this way of writing grammars can be highly effective, it is not without problems.  Firstly, grammars have to be written by hand by trained experts.  Both the training and the grammar writing can be time-consuming and intellectually demanding.  Even the simple context-free formalisms developed so far are subtle enough to call for sustained  thought and careful reasoning, since the consequences of writing a particular rule may be wide-ranging and subtle.  This is even more challenging when we consider that the rules we saw in section~\ref{sec:what-is-grammar} were a simplification of what grammar writers usually do.  In practice, grammars are usually augmented with \keywordAs{linguistic features}{linguistic feature} to encode properties such as agreement.  S $\to$ NP VP, for example, becomes S[PER $x$, NUM $y$] $\to$ NP[PER $x$, NUM $y$] VP[PER $x$, NUM $y$], where PER and NUM stand for `person' and `number,' $x$ and $y$ are variables, where every occurrence of the same variable is assumed to represent the same value.  This is good because it reduces the number of very similar rules that have to be written, but potentially bad because the extra power increases the risk that the grammar writer will get confused and make a mistake.  On the other hand, if you are a particularly patient, orderly and careful thinker, you may find that you are good at writing grammars, in which case you might have a future doing so.

%Secondly, there are grammatical constructions which are rarely used, and we have to decide whether to include these rules in our grammar. For instance, should we have rules to cover \exsent{the more, the   merrier}, where no verb is used?  Finally, as with dictionaries, grammars are likely to be incomplete.  Not only are there new words to deal with, but people also continue to create novel expressions and turns of phrases. So, if you do find work as a grammar writer, you will probably find that you have taken on a long-term obligation to maintain the grammar as new words and phrases appear.

%\subsubsection{Error pattern-based word correction} \label{sec:rule-based-techniques}

%Grammar-based techniques attempt to detect all errors in a text. However, since grammars are a fair amount of work and since people are more concerned about eradicating the most common errors, we can save time by focusing on only a subset of errors without using a full grammar.

%A very simple technique for detecting and correcting grammatical errors is simply to write rules which spot an error pattern and alter it. For example, a rule could match the pattern of \exsent{some} or \exsent{certain} followed by \exsent{extend}, and change \exsent{extend} into \exsent{extent}. Even a small set of such error pattern rules can be used to build a grammar corrector which can work as well as some commercial software.  The disadvantage, again, is that the rules have to be hand-written, and they will be necessarily incomplete, as people make new errors (and new kinds of errors) all the time.

%Note that these rules are typically \keywordAs{regular  expressions}{regular expression} (see chapter~\ref{sec:semi-structured}), which means that we are no longer capturing all of language completely.  To learn more about this, see the Under the Hood section on \emph{complexity of languages}.  The next technique also does not attempt to model language completely.


\section{Style checkers}
\label{sec:style}

Moving further into prescriptive ideas about language, most modern grammar checkers also incorporate suggestions for rephrasing a sentence to align with prescriptive rules of writing style.  Functionally, a style checker works just like the grammar checker described above, checking a (dependency) parse against a set of potentially hand-written rules; the only difference is the nature of these rules.

For example, a style checker might suggest avoiding split infinitives, where an adverb falls between \exword{to} and a verb (\exsent{to go boldly} might be suggested as a correction for \exsent{to boldly go}).  A style checker might propose more socially inclusive language, for example  suggesting \exword{server} as a replacement for \exword{waitress}.  A style checker could also suggest that very big noun phrases (\exsent{the refusal of the advisors to consider the downsides}) should be replaced by sentences (\exsent{the advisors refused to consider the downsides}), thought to be easier for readers to understand.

Sometimes, a style checker enforces subjective rules that a writer might  disagree with, such as the idea that the passive voice (\exsent{The law was passed}) should be replaced by the active form (\exsent{Congress passed the law}).  The popular Grammarly tool even flags \keyword{hedges} such as \exword{I think that} and \exword{somewhat}, suggesting that ``strong, confident'' writing is preferable.  But there may be situations where it is actually advisable to hedge one's claims.

Some stylistic norms  would be easy to automate and yet are not commonly flagged by modern grammar checkers:  a writer should vary the length and structure of their sentences; the same adjective should not be used too often within the same paragraph (unless it is a technical term).

Other stylistic norms would be very difficult to check automatically, at least without a much richer representation of textual meaning: the idea that the syntactic subject of a sentence should be a previously established discourse topic (a preference which sometimes conflicts with the avoidance of the passive voice!);
%\mad{Does the conflict here help illustrate why style checking could go too far? Also, should we discuss an overreliance on such technology?}; - YES, added to "consequences" section, LG.
that parallel ideas should be discussed using parallel sentence/paragraph structure; that the end of a sentence is a place of structural emphasis and should be used to  introduce new information or to advance one's point.  Modern grammar checkers do not have a way of knowing what a writer's point is, much less whether it is a convincing one, and are therefore limited in their capacity to effectively style-check writing.
%\faye{is this addition helpful in connecting these ideas more?} - sure -LG. 
Grammarly's advertisements have suggested that it can help you to write a concise, heart-warming, and hilarious speech for your best friend's wedding, but in reality it will not help you with the content.


% and of course the ideas you're writing about have to make sense and currently grammar checkers do not really check that.... Grammarly ads suggest that their service can help you give a hilarious and heart-warming speech at your best friend's wedding, but in reality it is not clear that it will really do that.....




% Earlier, when we discussed the question of what grammar is, we said that we were interested in \emph{descriptive grammar}: how is the language actually used?  For many writing purposes, however, we are also interested in \keyword{prescriptivism}: how is one supposed to use a language?  For example, many of us were instructed never to end a sentence with a preposition.  When writing a paper or any formal document, many people want to have these rules in place.  When writing a less formal document, these rules may not be followed.

%Thus, we distinguish \keyword{style checkers} from grammar checkers. Style checkers can check for violations of a particular style, such as that prescribed by MLA standards.  The constructions may still be English, but are simply unacceptable for the purposes of the current document.  For example, one might want to have a style checker find singular uses of \exsent{data}, such as \exsent{the data is} if the style dictates it should be plural.

%In terms of technology, style checkers work as grammar checkers do; they just identify different constructions.  For instance, style checkers might flag the use of too many passive constructions, which are usually dispreferred in ``clear'' writing, or they might disallow split infinitives (e.g., \exword{to boldly go}).  As with grammar checkers, style checkers might only examine a small window of context; often, a passive construction with many words between the \exsent{be} verb and the passive participle will go undetected as a passive (e.g., \exword{was very readily and quickly detected}).
  
 
 \section{Auto-complete and beyond}
 
So far, we have focused on writers' aids that correct already-written text.  But some newer writers' aids also help to generate text, suggesting completions for words or sentences that the writer has begun to type.

\keywordAs{Auto-complete}{auto-complete} suggests ways to type the current or subsequent word.  A simple auto-complete program could be written using an $n$-gram language model, for example suggesting the word \exword{the} after the word \exword{of} because \exword{of the} is a common bigram.  In an email client, a fancier version of auto-complete can identify the name of the email recipient and  auto-complete  their name in the salutation, to help avoid embarrassing mis-spellings of the addressee's name.

In Google's email client, the \keyword{SmartReply} feature ``reads'' an incoming email (\exsent{You're invited to a birthday party}) and suggests a variety of short replies (\exsent{See you there!}, \exsent{Sorry, we won't be able to make it}).  \keyword{SmartCompose} suggests ways of completing an entire sentence: when you type \exsent{I ho\ldots}, it will suggest \exsent{pe you are doing well}.  SmartCompose also suggests subject lines for emails that you've drafted, such as \exsent{Happy Birthday} in an email containing birthday greetings.  These tools are built by using various machine learning techniques (discussed further in \chapref{ch:text-classification}--\chapref{ch:mt})
%\mad{Point to document classification chapter for ML?} - yes, thanks, did it. -LG
to generalize from all of the email data that Google has available.  To suggest ways of replying to an email, SmartReply looks for common replies to similar emails in a humongous email corpus; to suggest a subject line, SmartCompose looks for common subject lines of similar emails.  Of course, the architects of such tools had to take care not to violate the privacy of the people who use Gmail, so 
%\mad{This seems like a potentially confusing - but really important - distinction}\faye{added a paper in further reading on masking data and privacy techniques used here. Also reworded a bit here to avoid saying the "computers saw" the data bc usually masking of addresses, names, etc. will take place before training, so the model will never be exposed to this data (ideally), so "See" could be misleading here}
% all this is fine, thank you - LG.
the researchers did not read or have access to any of the email data themselves, and made sure that people's private information such as names and addresses would not be suggested.  
%Therefore, they trained the machine learning models on data without looking at it themselves.  They also had to make sure that people's private information -- names, addresses, and so on -- would not be suggested.  
These modern writers' aids blend together error correction with \keyword{language generation}: automatically generating text, so that the computer itself is in some sense taking advantage of the productive power of language. 

% they allowed the computer to see all the data without looking at it themselves

More recent  \keyword{generative language models} do not just assist a human writer, but to some extent replace them by generating full paragraphs of text. \keyword{ChatGPT}, released in 2022 by the company OpenAI \citep{Ouyang-etal:2022}, is a tool trained to guess the next word in a string of text -- checking whether it was correct, and updating its representations to get closer to the right answer next time, so that it has learned to distill a massive amount of information about language.  ChatGPT works like a much fancier version of predictive text on your phone: given a string of words (\exsent{How are \ldots}), it  guesses possible next words along with the probability of each one (the most probable one is \exword{you}, then \exword{your}, then \exword{we}, and so on).  Just as previewed in our generative $n$-gram language model above (Section \ref{sec:ngramgen}), ChatGPT is designed to sometimes randomly choose slightly less probable next words rather than always choosing the top-ranked one, because this strategy leads to more contentful text.  In your own text messages, you probably sometimes choose a word other than the top-ranked one, for example when you write the slightly less probable, information-richer string \exsent{How are your kids?} rather than the highly probable, emptier \exsent{How are you doing?}.  ChatGPT does the same thing to generate more realistic prose. 

ChatGPT is trained on over 300 billion words of text.  For comparison, a typical novel is about 100,000 words long, so if you read a novel a day for 80 years, you would read 2.92 billion words (100,000 * 365 * 80) in your lifetime -- less than one percent of what ChatGPT has read!  Moreover, the creators of ChatGPT further trained the model to write text evaluated as ``better'' according to human gig workers.  Just like a human writer, ChatGPT learns from both practice (writing probable sentences word by word) and feedback (human evaluations of its output).

Using sophisticated techniques for distilling such information, when a human prompts ChatGPT with a few words to get it started, this tool is able to generate extremely lucid text: grammatical sentences of English that flow into a cohesive narrative, far more sophisticated than the $n$-gram examples above.  

Can such \keyword{large language models (LLMs)} replace a human writer?  The consequences of generative LLMs are still unfolding, but it probably depends on what the human writer wants to say.  If they want to generate a few comprehensible paragraphs about \textit{Romeo and Juliet} for a school assignment, or some pleasant holiday wishes for a mass email, then a generative LLM may do the trick 
more efficiently than a human -- raising concerns about cheating in school!  But if they want to write a legal brief describing the facts of a particular case, or a news story reporting a detailed investigation of a particular scandal, the LLM is unlikely to be much help, because its vast general knowledge about text does not include specific fact patterns from the outside world.  
For some tasks requiring a mix of rote formality and situation-specific details, such as writing a cover letter for a job, perhaps the LLM could generate a usable first draft to be tailored by a human, leveraging the complementary abilities of humans and computers.  As such tools improve, digital writers' aids may take over more and more pieces of the writing process.


\section{Consequences}

Stepping back, this chapter has foreshadowed some larger themes of the book.  

Edit distance quantifies the distance between two words, but because our writing system reflects sound rather than meaning as discussed in \chapref{ch:encodings}, two words that are close by edit distance are unlikely to be close in meaning.  When we explore text as data in \chapref{ch:textasdata}, we come back to the idea of quantifying distance between words, but in a way that  captures meaning rather than sound.  In spelling correction, candidate correct spellings are generated and then ranked;  in \chapref{ch:searching},  the same process of generating and ranking candidates is applied to relevant  results for a given search term.  Spelling correction can be seen as one instantiation of the noisy channel model, whereby we reason about the most likely intended message as well as the likelihood of the observed noisy signal given that message; the same idea  can also be applied to speech recognition in \chapref{ch:encodings} and to machine translation in \chapref{ch:mt}.  We have used $n$-grams to create a simple language model, a representation of likely sentences learned in a bottom-up manner from data, which comes back in \chapref{ch:text-classification}. 

Our discussion of grammar emphasizes that a sentence is not just a list of undifferentiated words, but a structured grouping thereof.  In future chapters such as \chapref{ch:text-classification} on text classification, we will sometimes use the simpler unstructured view as a starting point, but it is important to realize what information is lost when we ignore structure.

Moreover, recurring concerns about ethics and representation are
raised when we consider that spell-checkers may favor majority
cultural groups or that gram\-mar-checkers may privilege the language
variety spoken by people with power.

Writers' aids also have consequences for education. (\chapref{ch:call} comes back to education in the context of second-language learning). Would you consider it a waste of time to memorize correct spellings when a computer can fix misspellings for you? (If you didn't spend time learning to spell, what would you study instead?)  Would you spend less time studying grammar if you knew that you would always have access to a grammar checker? Would you learn to write and type more easily with the assistance of writers' aids?   Should children should spend more time learning to type than learning to write by hand? Would you  worry that they might over-rely on digital tools in such a way that they fail to learn important skills?

 

On a more philosophical note, if writers' aids can to some extent produce  language, what does this mean for the idea that language usage indicates  intelligence?  To what extent do writers' aids automate tedious tasks versus threaten to make skilled human labor redundant?  We will keep coming back to the question of whether humans versus computers compete or complement one another.





\begin{tblsfilledsymbol}{Checklist}{test}

\begin{itemize}
\item Give two reasons why standardized spelling is useful.
\item Give examples of various types of spelling errors and explain why they occur.
\item Calculate the edit distance between a misspelling and a correct spelling.
\item Explain some methods for generating and ranking spelling
  corrections.
 \item Give examples of common unigrams, bigrams, and trigrams in English.
 \item Explain how $n$-grams can be used in a simple generative language model.
 \item Give an example where a simple $n$-gram language model can help to correct spelling errors in context.
  \item Compare and contrast phrase structure grammar with dependency grammar.
  \item Give an example of a syntactic ambiguity -- a string of words that can be assigned two different grammatical structures, each with a different meaning -- and use phrase structure grammar or dependency grammar to distinguish these two structures.
  \item Give an example of how a dependency parse could be used to automatically correct grammar.
\item State the difference between descriptive and prescriptive grammar, and explain why grammar checkers use both.
\item Sketch some affordances and limitations of language-generation tools such as ChatGPT.
\item Explain why dynamic programming methods are efficient.
\item Provide an example of how context-free grammars are insufficient as mathematical descriptions of human languages.
\end{itemize}
\end{tblsfilledsymbol}


\begin{tblsfilledsymbol}{Exercises}{pencil}


\begin{enumerate}

\item Phrase structure grammar is built on the idea that the grammatical behavior of a word is not unique to that word, but can be generalized across all members of a lexical category.  For example, the noun phrase \exword{the reindeer} manifests the more general rule that any determiner can be combined with any noun to make a Noun Phrase, a rule which applies equally to \exword{some games}.  You can try out this exercise with a partner, inspired by the children's game of Mad Libs.  One partner begins with a sentence, which they keep secret from the other partner.  For each word in the sentence, the first partner asks the second partner for a word of the same lexical category (determiner, adjective, noun, transitive verb, intransitive verb, and so on), replacing the sentence word by word until it's a totally new sentence.  Is this new sentence a grammatical sentence of English?  Does it make any sense?

\item  Please draw a phrase structure tree and a dependency graph to represent multiple interpretations of some  humorously ambiguous strings such as \exsent{Pope's Baby Steps On Gays}.


\item  Explore a web demo for a dependency parser, such as the Stanford Parser or SpaCy.  Try out some sentences and see how it visualizes their dependencies.  How does it handle \exsent{Pope's Baby Steps On Gays}?   (Does it matter how you capitalize that string?) What happens if you give it a sentence containing a grammatical error, such as \exsent{The key to the cabinets are on the counter}?

\item Please write some pseudo-code sketching how you would identify a subject-verb agreement error in sentences such as \exsent{The dog swim} using a dependency parse such as \REF{dogswim1}.  Next, please write some pseudo-code sketching how you could do the same thing using a phrase-structure parse \REF{dogswim2}.  Please assume that in both cases, \exword{dog} is part-of-speech-tagged as \exword{NN} (for singular nouns) and \exword{swim} is part-of-speech-tagged as \exword{VB} (the bare form of a verb) when the correct subject-verb agreement would use \exword{swims}, tagged as \exword{VBZ} (3rd-person present singular).

\ea \label{dogswim1}\raisebox{-2\baselineskip}{\begin{dependency}[theme = simple, label style={scale=1.5}]
   \begin{deptext}[column sep=1em]
    The \& dog \& swim \\
    DT \& NN \& VB \\
   \end{deptext}
   % MD: the \sc is a bit of a hack here
    \depedge{2}{1}{\textsc{det}}
    \depedge{3}{2}{\textsc{nsubj}}
   \end{dependency}}

   
\ex   \label{dogswim2}  \Tree[ .S [ .NP [  .DT  the ] [ .NN dog  ] ]   [ .VP [ .VB swim ]  ] ]

\z 


\item  Explore the Google N-grams tool.\footnote{\url{https://books.google.com/ngrams}, accessed 2024-07-01.}  For example, search for \exword{cheese and *}, which gives  the most common trigrams beginning with \exword{cheese and}.  Are you able to correctly guess the most common next word?  How does the frequency of this trigram change over time (in data from historical books)?

\item  To get a firmer grasp on how $n$-grams work, write
  a program which takes a large text file as input and stores all its unigrams
  and bigrams as well as their counts.
  \begin{enumerate}
  \item Write a program that takes in a starting word $w_{0}$ and then generates the next 19 words $w_{1}$ \ldots $w_{19}$, in each case choosing the word $w_{n+1}$ with the highest bigram probability given $w_{n}$.
  \item How sensible is the text generated by your word-prediction program?
  \end{enumerate}
 
 \item  In our exploration of $n$-gram models, we discussed how these models rely on probability.  To gain a more thorough understanding of $n$-grams, try out the below problems. 
 \begin{enumerate}
    \item Can you estimate the unigram probability for the sentence \exword{You put the \{cat/cart\} before the horse} using the Maximum Likelihood Estimate equation below?
    \begin{gather*}
        P(w_i|w_{i-1}) = \frac{\text{count}(w_{i-1},w_i)}{\text{count}(w_{i-1})}
    \end{gather*}
    \item In actual implementation, the probabilities are usually log-transformed and then added rather than multiplied; these operations are equivalent because the log of a product is the sum of the logs.  Replicate the above calculation using logs to see how these operations are equivalent.  
    \item We discussed using $n$-grams to correct real-word errors, as in the example where \exsent{You put the cart before the horse} would result in a higher $n$-gram probability than \exsent{You put the cat before the horse}.  As a thought exercise, how much higher do you think the probability should be to make the correction?
 \end{enumerate}

\item  Select your favorite spell checker and evaluate its effectiveness by designing a \keyword{test suite} of sentences with spelling errors.  A test suite is simply a list of cases which   are used to test the effectiveness of a system.  Include 10-20   errors and include some non-errors, too.
  \begin{enumerate}
  \item What is the purpose of each error?  In other words, what is    each type of error attempting to test about the spell checker's  capabilities?
  \item Of the number of words flagged by the checker as errors, how    many (what percentage) are actually errors?  (This measures    \keyword{error detection precision}.)
  \item Of the errors you introduced, how many (what percentage) does the  spell checker flag?  (This measures \keyword{error detection  recall}; precision and recall are discussed more in Chapter~\ref{sec:evaluating-search-results}.)
  \item Of the suggestions the checker makes, how often (what
    percentage of the time) is the first correction the correct,
    intended word?  (This measures \keyword{error correction precision}.)
  \end{enumerate}

\item  Select your favorite spell checker and type in a number of misspellings.
  \begin{enumerate}
  \item In terms of edit distance, how far off can your misspellings
    be and still have the correct spelling suggested?
  \item Select your second favorite spell checker and perform a
    comparison of their performances.  For each of your misspellings,    which spell checker has a better ranking?  Can the misspellings be  more or less farther off? 
    \item For each system, does it provide helpful feedback?  Does it make a correction for you automatically or just suggest it as an option? Compare and contrast the user experience of both systems.
  \end{enumerate}

\item Compare and contrast several different writers' aids (grammar and style checkers), including Grammarly, LanguageTool, and Ludwig Guru (search online).  Which one would be most useful for a person who is not fully confident in English?  Which one would be best for a person with dyslexia?  What about an adolescent learning to write on the computer?

\item Imagine that you're interviewing for a job at Grammarly.  They ask you what new feature you'd propose.  What is your response?

\item Examine the predictive text suggestions on your mobile device.  To what extent are these suggestions customized to your own specific language usage?  How can you tell?



% \begin{enumerate}
% \setcounter{enumi}{2}
\item  A user types in \exsent{folg} when they meant to  type \exsent{frog}.  Draw the directed graph and describe how  minimum edit distance is calculated, using only deletions and insertions.
\item  Describe how such a graph could be redrawn to  allow for transpositions and substitutions.  

%\begin{enumerate} \setcounter{enumi}{4}
%\item \textbf{ALL:} Consider again the $n$-gram grammar correction  techniques from section~\ref{sec:ngram}.  Would these techniques be appropriate for web spell checking?  For learner language?  Why or  why not?



\item  We discussed edit distance techniques for generating correction candidates (Section~\ref{sec:generating-candidates}) and also discussed using confusion sets (Section~\ref{correction-with-ngrams}). But these aren't the only methods for finding possible corrections. For example, the Soundex algorithm -- developed over 100 years ago for census data by Robert C. Russell and Margaret King Odell \citep{russell:18} -- is another way to generate candidates, based on phonetic properties of words. To take one example, the word \exsent{nub} would be assigned the code N2: an N for the first letter, a 0 (zero) (which is later eliminated) for the vowel, and a 2 for a bilabial (\exword{b}, \exword{p}, \exword{f}, \exword{v}).  The words \exsent{nob}, \exsent{nap}, and so on would thus be put into the same bucket.
  \begin{enumerate}
  \item Read more about Soundex by searching online.
  \item What is the point of Soundex and why is it different from edit distance? 
  \item Why would Soundex be useful for analyzing (early twentieth-century) census data in particular?
  \item With computing techniques becoming faster, why not compare a misspelling to every single word in the dictionary?
  \item We would argue that it's better to have a handful of possible corrections than just a single candidate: why is that? And how many candidate corrections do you think is ideal?
  \item Soundex doesn't take into account anything like edit distance, while edit distance is based on orthography and not phonetics: can you think of ways to combine these techniques?
  \end{enumerate}
  %\end{enumerate}

\item Let's further explore the Soundex system for generating candidates for correction.
 \begin{enumerate}
  \item Develop your own Soundex-like system for
    grouping similar words into buckets by working out what rules you
    need to map a word to the appropriate bucket.  Can your rules
    handle transposition errors?
  \item  If you're a programmer, implement your Soundex  rules.  Make sure that each of the following sets of words all get     mapped to the same bucket:
    \begin{enumerate}
    \item Catherine, Kathryn, Katherine
    \item principal, principle
    \item eye, I, aye
    \end{enumerate}
  \end{enumerate}
 \end{enumerate}

\end{tblsfilledsymbol}


\begin{tblsfilledsymbol}{Further reading}{book}


 Peter Norvig's blog post \exsent{How to write a spelling corrector} \citep{Norvig:2007} is a classic. 
 
 Universal Dependencies, dependency representations for over 100 languages, is presented in \citet{Nivre-etal:2016}.

For background on the types of spelling errors and their causes, \citet{damerau:64,kukich:92} and \citet{mitton:96} are classic references; \citet{Flor-etal:2015} discusses errors made by English learners. As for grammar correction, classic references include \citet{Wilcox-etal:2008}, \citet{hirst:budanitsky:05}, and \citet{leacock-et-al:10}. For rule-based grammar checking, \citet{naber:03} originally used 56 rules to achieve good performance.  This later developed into the open-source software Language Tool.\footnote{\url{https://www.languagetool.org/}, accessed 2024-04-17.}

For more on formal language theory and complexity, see Chapter 16 of \citet{Jurafsky.Martin-09}.  A comprehensive formal treatment of the topic can be found in  \citet{Hopcroft.Motwani.Ullman-07}. For a deeper understanding of dynamic programming, see Chapter 6 of \citet{kleinberg-tardos}.

To delve further into the privacy measures Google took for SmartReply and SmartCompose, see  \citet{Carlini-etal:2019}  on machine learning and security measures.

\citet{Lingard:2023} evaluates the opportunities and limitations of using large language models such as ChatGPT as writers' aids.

\end{tblsfilledsymbol}


%For a good (if slightly dated) overview of spell checking techniques, see \citet{kukich:92}, which has a similar structure to this chapter and presents many of the facts which we relied on (e.g., human agreement rates for isolated-word spelling correction). \citet{mitton:96} is a somewhat more recent book that provides a nice overview of both the types of errors people make and how to correct them. For those who are more computationally-inclined, the LingPipe software suite contains a nice tutorial of spelling correction \citep{lingpipe:tutorial}, including relating spell checking to the noisy channel model and walking through some programming code which corrects web queries (\url{http://purl.org/lang-and-comp/lingpipe}).

%We relied on many references for this chapter.  To see more about storing probabilities, see \citet{kernighan-et-al:90}; for more on types of errors, \citet{damerau:64} provides some early analysis. \citet{mangu:brill:97} discuss automatically deriving spelling rules. The SOUNDEX system was developed in \citet{odell:russell:18} (note that this was back in 1918!).  \citet{wing:baddeley:80} provide evidence of the percentage of real-word spelling errors.

%For more information on what grammar is, one can consult any introductory linguistics textbook, such as \citet{LanguageFiles11}. If you are interested in parsing techniques, you can consult chapters 12-14 of \citet{jurafsky:martin:08}, which also contains pointers to more advanced works.  


%
%While many of the papers on techniques for correcting words in context are somewhat challenging to read, given the amount of computer science background they assume, 

%If you want to explore word sense disambiguation, there is a wealth of papers and resources, such as those associated with the SemEval workshops (\url{http://www.cs.york.ac.uk/semeval/}).  A survey paper aimed at a general audience is \citet{mccarthy:09}.  WordNet is more fully described in \citet{fellbaum:98} and can be freely found at \url{http://wordnet.princeton.edu/}.

%More on machine learning techniques can be found in \citet{golding:roth:99, mangu:brill:97, jones:martin:97}; note, however, that these papers require some formal or computer science background to read.

