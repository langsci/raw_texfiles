\chapter{Data summary (ICE)}\label{appendix:A}

\section{Corpus design}\label{appendix:A.1}\label{bkm:Ref36971356}

Traditional \is{International Corpus of English}ICE \is{International Corpus of English!components}components consist of approximately 1 million words (60\% spoken, 40\% written) in 500 texts of approx. 2,000 words each, sampled according to a standardised scheme as described in \sectref{sec:6.1} (see also \url{https://www.ice-corpora.uzh.ch/en.html}).

The first column in \tabref{tab:A.1} states the alphanumerical handle that is conventionally used to identify \is{International Corpus of English!macro genres}macro genres in \is{International Corpus of English}ICE, along with an explanation in parentheses. The \is{International Corpus of English!sections}section (spoken or written) that a text belongs to is indicated by the first capital letter (“S” or “W”) in those handles. \is{International Corpus of English!subsections}Subsections, like \is{International Corpus of English!sections}sections, are not indicated as an extra level in \tabref{tab:A.1} but become clear from the structure: \is{International Corpus of English!macro genres}Macro genres S1A and S1B together make up the spoken \is{International Corpus of English!subsections}subsection of dialogues (S1); similarly, \is{International Corpus of English!macro genres}macro genres S2A and S2B constitute the \is{International Corpus of English!subsections}subsection of monologues (S2); \is{International Corpus of English!macro genres}macro genres W1A and W1B comprise non-printed writing (W1); and \is{International Corpus of English!macro genres}macro genres W2A–F constitute printed writing (W2). The number of texts conventionally sampled for each \isi{genre} within the \is{International Corpus of English!macro genres}macro genres is given in the second column. Note that in the published \is{International Corpus of English!components}components of \is{International Corpus of English}ICE, texts are counted consecutively within \is{International Corpus of English!macro genres}macro genres, so that “ICE-GB:S1B-017” is a classroom lesson, “ICE-GB:S1B-024” is a broadcast discussion, and “ICE-GB:S1B-049” is a broadcast interview, for instance. Further note that ICE-Nigeria contains considerably more than 500 texts, because the compilers did not follow the practice of combining several shorter texts of the same kind into larger units of 2,000 word. However, ICE-Nigeria still adheres to the total number of 1 million words.

The specific \is{International Corpus of English!genres}\isi{genre} labels are given in the third column of \tabref{tab:A.1}, and the final column states their abbreviations as used to code the data for the variable \textsc{genre} for the statistical analyses in the present study (cf. \sectref{sec:6.3.6}). These particular abbreviations were introduced for ICE-Nigeria (and were also adopted by the compilers of ICE-Scotland), because they are more explicit than the purely alphanumerical labels traditionally used in \is{International Corpus of English}ICE.

\begin{table}[p]
\caption{The structure of the \textit{International Corpus of English}\label{tab:A.1}}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcll}
\lsptoprule
\is{International Corpus of English!macro genres}Macro genre              &  $n$ (texts) &  \is{International Corpus of English!genres}Genre                      & Short label \\\midrule
S1A (private dialogues)      &  90        &  face-to-face conversations    & con         \\
                             &  10        &  phonecalls                    & ph          \\
S1B (public dialogues)       &  20        &  classroom lessons             & les         \\
                             &  20        &  broadcast discussions         & bdis        \\
                             &  10        &  broadcast interviews          & bint        \\
                             &  10        &  parliamentary debates         & parl        \\
                             &  10        &  legal cross-examinations      & cr          \\
                             &  10        &  business transactions         & btrans      \\
S2A (unscripted monologues)  &  20        &  spontaneous commentaries      & com         \\
                             &  30        &  unscripted speeches           & unsp        \\
                             &  10        &  demonstrations                & dem         \\
                             &  10        &  legal presentations           & leg         \\
S2B (scripted monologues)    &  20        &   broadcast news (20)           & bnew        \\
                             &  20        &  broadcast talks (20)          & btal        \\
                             &  10        &  non-broadcast talks (10)      & nbtal       \\
W1A (student writing)        &  10        &  student essays (10)           & ess         \\
                             &  10        &  exam scripts (10)             & ex          \\
W1B (letters)                &  15        &  social letters (15)           & sl          \\
                             &  15        &  business letters (15)         & bl          \\
W2A (academic writing)       &  10        &  humanities (10)               & AHum        \\
                             &  10        &  social sciences (10)          & ASoc        \\
                             &  10        &  natural sciences (10)         & ANat        \\
                             &  10        &  technology (10)               & ATec        \\
W2B (popular writing)        &  10        &  humanities (10)               & PHum        \\
                             &  10        &  social sciences (10)          & PSoc        \\
                             &  10        &  natural sciences (10)         & PNat        \\
                             &  10        &  technology (10)               & PTec        \\
W2C (reportage)              &  20        &  press news reports (20)       & rep         \\
W2D (instructional writing)  &  10        &  administrative writing (10)   & adm         \\
                             &  10        &  skills/hobbies (10)           & SkHo        \\
W2E (persuasive writing)     &  10        &  press editorials (10)         & ed          \\
W2F (creative writing)       &  20        &  novels \& short stories (20)   & nov         \\
\lspbottomrule
\end{tabular}}
\end{table}

\section{\label{bkm:Ref497474782}Word counts}\label{appendix:A.2}

Word counts (and corpus queries) are based on files in which content captured by the following tags was ignored: \texttt{<X>…</X>}, \texttt{<O>…</O>}, \texttt{<\&>…</\&>} (see comments in \sectref{sec:6.1}). For ICE-NIG, word counts as stated in the corpus manual are reported.\footnote{Available at \url{https://sourceforge.net/projects/ice-nigeria/}; accessed 14 October 2023.}

\begin{table}[H]
\caption{Word counts in components of ICE\label{tab:A2}}

\begin{tabular}{lrrr}
\lsptoprule
 & \multicolumn{3}{c}{Number of words}\\\cmidrule(lr){2-4}
Variety & \multicolumn{1}{c}{spoken} & \multicolumn{1}{c}{written} & \multicolumn{1}{c}{Total}\\\midrule
BrE & 647,837 & 432,769 & 1,080,606\\
IrE & 627,627 & 423,757 & 1,051,384\\
CanE & 612,060 & 393,099 & 1,005,159\\
AusE & 655,117 & 423,868 & 1,078,985\\
JamE & 614,860 & 403,665 & 1,018,525\\
NigE & 609,586 & 400,796 & 1,010,382\\
IndE & 674,678 & 412,523 & 1,087,201\\
HKE & 709,365 & 471,734 & 1,181,099\\
SingE & 600,635 & 402,506 & 1,003,141\\\midrule
Total & 5,751,765 & 3,764,717 & 9,516,482\\
\lspbottomrule
\end{tabular}
\end{table}

\section{Token numbers}\label{appendix:A.3}\label{bkm:Ref41473918}\label{bkm:Ref51055053}\largerpage

\tabref{tab:A3} reports the number of tokens (per conjunction) that were semantically \is{disambiguation}disambiguated, using the original four semantic types (anticausal, dialogic, epistemic and narrow-scope dialogic), and not counting tokens that were excluded. Frequencies are reported for both speech and writing. The total number of tokens is \textit{n}~=~3,502. This forms the basis of Models A \& B in Chapters \ref{sec:7} \& \ref{sec:8}~– see the files “concessives\_1” and “concessives\_2” in the published data \citep{Schützler2021}.

\begin{table}
\caption{\label{tab:A3}Token numbers of conjunctions in ICE (Models A \& B); W~= written; S~= spoken}
\begin{tabular}{l *3{rrr}}
\lsptoprule
% % & \multicolumn{2}{c}{Marker}  &    &    &    &   & \\
& \multicolumn{3}{c}{{\itshape although}} & \multicolumn{3}{c}{{\itshape though}} & \multicolumn{3}{c}{{\itshape even though}}\\\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
Variety & W & S & Total  & W & S & Total  & W & S & Total\\\midrule
GB & 219 & 131 & 350  &  118 & 61 & 179  &  29 & 16 & 45\\
IRE & 140 & 64 & 204  &  57 & 22 & 79  &  30 & 32 & 62\\
CAN & 166 & 59 & 225  &  54 & 17 & 71  &  33 & 35 & 68\\
AUS & 172 & 70 & 242  &  65 & 20 & 85  &  23 & 33 & 56\\
JAM & 93 & 74 & 167  &  60 & 25 & 85  &  24 & 35 & 59\\
NIG & 73 & 40 & 113  &  56 & 51 & 107  &  26 & 46 & 72\\
IND & 69 & 29 & 98  &  129 & 106 & 235  &  25 & 26 & 51\\
SING & 131 & 74 & 205  &  82 & 30 & 112  &  27 & 25 & 52\\
HK & 212 & 77 & 289  &  81 & 18 & 99  &  51 & 41 & 92\\\midrule
Total & 1,275 & 619 & 1,893  &  702 & 350 & 1,052  &  268 & 289 & 557\\
\lspbottomrule
\end{tabular}
\end{table}


\tabref{tab:A.4} shows the reduced number of tokens used for the remaining analyses (Models C–E in Chapters \ref{sec:9}–\ref{sec:11}), in which epistemic and narrow-scope concessives were excluded. The remaining total number of tokens was \textit{n}~=~3,275~– see the file “concessives\_3” in the published data \citep{Schützler2021}.

\begin{table}
\caption{\label{tab:A.4}Token numbers of conjunctions in ICE (Models C–E); W~= written; S~= spoken}
\begin{tabular}{l *3{rrr}}
\lsptoprule
% % %  & \multicolumn{2}{c}{Marker}  &    &    &    &   & \\
& \multicolumn{3}{c}{{\itshape although}} & \multicolumn{3}{c}{{\itshape though}} & \multicolumn{3}{c}{{\itshape even though}}\\\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
Variety & W & S & Total  &  W & S & Total  &  W & S & Total\\\midrule
GB & 210 & 123 & 333  &  102 & 56 & 158  &  28 & 14 & 42\\
IRE & 136 & 62 & 198  &  49 & 21 & 70  &  30 & 31 & 61\\
CAN & 162 & 57 & 219  &  47 & 13 & 60  &  31 & 35 & 66\\
AUS & 168 & 67 & 235  &  56 & 17 & 73  &  21 & 32 & 53\\
JAM & 88 & 71 & 159  &  49 & 19 & 68  &  24 & 29 & 53\\
NIG & 69 & 36 & 105  &  50 & 49 & 99  &  26 & 44 & 70\\
IND & 65 & 27 & 92  &  110 & 95 & 205  &  24 & 26 & 50\\
SING & 125 & 69 & 194  &  73 & 27 & 100  &  27 & 23 & 50\\
HK & 202 & 74 & 276  &  78 & 18 & 96  &  50 & 40 & 90\\\midrule
Total & 1,225 & 586 & 1,811  &  614 & 315 & 929  &  261 & 274 & 535\\
\lspbottomrule
\end{tabular}
\end{table}


Finally, \tabref{tab:A5} documents the precise number of tokens, sorted by the categories relevant in the most complex scenario analysed in \chapref{sec:11}: (i)~variety, (ii)~mode of production, (iii)~semantic type, (iv)~subordinate clause position, (v)~conjunction and (vi)~clause structure. The total number of tokens is \textit{n~=~}3,275.

\begin{table}
\caption{\label{tab:A5}Detailed list of tokens by all categories; S~= spoken, W~= written, a~= anticausal, d~= dialogic, fn~= final, nf~= nonfinal, A~= \textit{although}, T~= \textit{though}, E~= \textit{even though}, f~= finite, n~= nonfinite}
\footnotesize
\begin{tabular}{lll *{12}{r}}
\lsptoprule
&  &  & \multicolumn{2}{c}{S} &  \multicolumn{2}{c}{W}  & \multicolumn{2}{c}{S} & \multicolumn{2}{c}{W} & \multicolumn{2}{c}{S} & \multicolumn{2}{c}{W}\\\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}\cmidrule(lr){14-15}
&  &  & a & d & a & d & a & d & a & d & a & d & a & d\\\cmidrule(lr){4-15}
&  &  & \multicolumn{4}{c}{BrE} & \multicolumn{4}{c}{IrE} &  \multicolumn{4}{c}{CanE}\\\cmidrule(lr){4-7}\cmidrule(lr){8-11}\cmidrule(lr){12-15}
fn & A & f & 2  & 58 & 11 & 9  & 3  & 28  & 7   & 52 & 3  & 36 & 6  & 45\\
    &              & n & 1  & 3  &    & 2  &    &     & 1   & 3  &    &    &    & \\
    & T & f & 1  & 39 & 5  & 4  &    & 15  & 1   & 33 & 1  & 8  & 5  & 19\\
    &              & n &    & 3  &    &    &    &     &     & 1  &    &    &    & 2\\
    & E & f & 6  & 3  & 13 & 6  & 14 & 9   & 9   & 10 & 17 & 6  & 14 & 8\\
    &              & n &    &    &    &    &    & 1   &     &    &    &    &    & \\
nf & A  & f & 12 & 45 & 88 & 96 & 7  & 18  & 7  & 56 & 3  & 15 & 18  & 82\\
   &               & n &    & 2  &    & 4  & 1  & 5   & 3  & 7  &    &    & 2   & 9\\
   & T  & f & 3  & 9  & 56 & 27 & 1  & 3   & 2  & 9  &    & 3  & 3   & 12\\
   &               & n &    & 1  & 2  & 8  &    & 2   &    & 3  &    & 1  & 1   & 5\\
   & E  & f & 2  & 3  & 7  & 2  & 3  & 4   & 3  & 8  & 4  & 8  & 3   & 6\\
   &               & n &    &    &    &    &    &     &    &    &    &    &     & \\
\cmidrule(lr){4-7}\cmidrule(lr){8-11}\cmidrule(lr){12-15}
& & & \multicolumn{4}{c}{AusE} & \multicolumn{4}{c}{JamE} & \multicolumn{4}{c}{NigE}\\
\cmidrule(lr){4-7}\cmidrule(lr){8-11}\cmidrule(lr){12-15}
fn & A & f & 3  & 38 & 4  & 80 & 3 & 27 & 7 & 16 &   & 17 &   & 15\\
    &              & n &    & 1  &    & 1  &   & 1  &   &    &   &    &   & \\
    & T & f & 1  & 10 & 4  & 28 &   & 10 & 1 & 14 & 1 & 25 & 1 & 16\\
    &              & n &    &    & 1  & 3  &   & 1  &   &    &   & 3  &   & 1\\
    & E & f & 10 & 8  & 9  & 6  & 7 & 5  & 6 & 7  & 7 & 15 & 9 & 11\\
    &              & n &    &    &    &    &   &    & 1 &    &   &    &   & 1\\
nf  & A & f & 3  & 19 & 14 & 66 & 6 & 32 & 9 & 47 & 5 & 14 & 4 & 48\\
    &              & n & 2  & 1  &    & 3  & 1 & 1  & 3 & 6  &   &    &   & 2\\
    & T & f & 3  & 3  & 2  & 13 & 1 & 6  & 2 & 19 & 3 & 15 & 4 & 24\\
    &              & n &    &    & 3  & 2  &   & 1  & 4 & 9  &   & 2  & 1 & 3\\
    & E & f & 5  & 8  & 3  & 3  & 9 & 8  & 2 & 8  & 3 & 19 & 1 & 4\\
    &              & n &    & 1  &    &    &   &    &   &    &   &    &   & \\
\cmidrule(lr){4-7}\cmidrule(lr){8-11}\cmidrule(lr){12-15}
&  & & \multicolumn{4}{c}{IndE}  & \multicolumn{4}{c}{SingE}  & \multicolumn{4}{c}{HKE}\\
\cmidrule(lr){4-7}\cmidrule(lr){8-11}\cmidrule(lr){12-15}
fn & A & f & 4  & 9  & 3  & 14 & 2 & 21 & 6  & 32 & 4  & 19 & 7 & 53\\
    &              & n &    &    &    &    &   &    &    &    &    & 1  &  & 1\\
    & T & f & 14 & 35 & 10 & 32 & 1 & 9  & 3  & 26 & 2  & 6  & 9 & 22\\
    &              & n &    & 3  &    & 3  &   & 1  &    & 1  &    & 2  &  & 3\\
    & E & f & 6  &    & 5  & 7  & 8 & 2  & 12 & 8  & 13 & 8  & 21 & 12\\
    &              & n &    &    &    &    &   &    &    &    &    &    &  & \\
nf  & A & f &  5 & 9  & 8  & 35 & 6 & 36 & 12 & 69 & 9  & 41 & 25 & 111\\
    &              & n &    &    & 3  & 2  & 1 & 3  &    & 6  &    &    & 1 & 4\\
    & T & f & 15 & 27 & 13 & 39 & 1 & 10 & 7  & 26 & 2  & 5  & 15 & 21\\
    &              & n & 1  &    & 4  & 9  & 3 & 2  & 1  & 9  & 1  &    & 5 & 3\\
    & E & f & 9  & 11 & 5  & 6  & 3 & 10 & 1  & 6  & 5  & 14 & 7 & 10\\
    &              & n &    &    & 1  &    &   &    &    &    &    &    &  & \\
\lspbottomrule
\end{tabular}
\end{table}

\chapter{Statistical models}\label{appendix:B}

\section{Model A: Frequencies of conjunctions (\chapref{sec:7})}\label{appendix:B.1}\label{bkm:Ref41307048}

The analysis is based on a total number of \textit{n}~= 14,706 individual observations, which represent the frequency values of the three markers (\textit{although}, \textit{though} and \textit{even though}) in all \textit{n~}=~4,902 texts contained in the nine corpora~– see the file “concessives\_1” in the published data \citep{Schützler2021}. \tabref{tab:B.1} shows the overall token numbers as well as the number of levels of the random factor \textsc{genre}. The model was run with four chains, each with $n = 3{,}500$ iterations and a \isi{warmup} phase of \textit{n}~= 1,000 iterations. The number of data points in the \isi{posterior sample} was thus \textit{n}~= 10,000. The R-hat diagnostic equalled 1.00 for all parameters, indicating the \is{convergence (of statistical model)}convergence of the four chains. The full model output (i.e. tables of coefficients) can be found in the online materials (cf. \sectref{sec:1.4}). Priors are shown in \tabref{tab:B.2}. The \is{priors}prior for the intercept was fixed at the \isi{geometric mean} of the frequencies of the most frequent item in ICE-GB (\textit{the}, with a normalised frequency of \textit{f} ${\approx}$ 64,000 pmw) and an assumed rare (lexical) item with \textit{f}~= 1 pmw. The standard deviation of the \is{priors}prior was then set to the value of 1.8, so that the difference between the mean and the two extreme values mentioned above equalled roughly three standard deviations. As seen in \tabref{tab:B.1}, there are no missing genres (nor, of course, texts) in any of the nine varieties, since zero counts and positive counts were all entered into the underlying data frame.

\begin{table}
\caption{Number of observations for all models; \textit{n}~= total number of observations, \textsc{genre}/\textsc{text}~= number of levels of the respective random factor\label{tab:B.1}}
\begin{tabular}{l *7{c}}
\lsptoprule
        & \multicolumn{7}{c}{Model} \\\cmidrule(lr){2-8}
        & \multicolumn{2}{c}{A}& \multicolumn{2}{c}{B} & \multicolumn{3}{c}{C–E}\\\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-8}
Variety & $n$ & \textsc{genre} & $n$ & \textsc{genre}  & $n$ & \textsc{genre} & \textsc{text}\\\midrule
%\textsc{genre} & \textsc{text}\\\midrule
GB  	& 1,500	& 32  & 2,000  & 32 & 533  & 32 & 262\\
IRE 	& 1,500 & 32  & 2,000  & 32 & 329  & 31 & 189\\ 
CAN 	& 1,500 & 32  & 2,000  & 32 & 345  & 32 & 207\\
AUS 	& 1,500	& 32  & 2,000  & 32 & 361  & 32 & 194\\
JAM 	& 1,500 & 32  & 2,000  & 32 & 280  & 30 & 177\\ 
NIG 	& 2,706 & 32  & 3,608  & 32 & 274  & 31 & 189\\ 
IND     & 1,500 & 32  & 2,000  & 32 & 347  & 31 & 207\\
SING    & 1,500 & 32  & 2,000  & 32 & 344  & 31 & 190\\
HK      & 1,500 & 32  & 2,000  & 32 & 462  & 31 & 251\\
\lspbottomrule
\end{tabular}
\end{table}

\section{\label{bkm:Ref41307102}Model B: Frequencies of semantic types (\chapref{sec:8})}\label{appendix:B.2}

This analysis is based on a total number of $n$~= 19,608 individual observations, which represent the frequencies of all four semantic types (anticausal, epistemic, wide-scope dialogic and narrow-scope dialogic) in the \textit{n~}=~4,902 texts that make up the nine components of \is{International Corpus of English}ICE used in this study. The complete data are accessible in the file “concessives\_2” in the published data \citep{Schützler2021}. All nine regression models were run with four chains, each with \textit{n~}=~3,500 iterations and a \isi{warmup} phase of $n$~= 1,000 iterations. The number of data points in the \isi{posterior sample} was therefore $n$~= 10,000. The R-hat diagnostic equalled 1.00 for all parameters, which indicates that the four chains reached \is{convergence (of statistical model)}convergence. The respective parts of Tables \ref{tab:B.1} \& \ref{tab:B.2} document the total number of observations, the number of levels of the random factor \textsc{genre}, as well as the \isi{priors} that were specified. More comprehensive documentation in the form of regression tables can be found online (cf. \sectref{sec:1.4}). The selection of \isi{priors} followed the same principles as for Model A (cf. Appendix~\ref{appendix:B.1} above). Like for Model A, there are no missing genres (or missing texts) in any of the nine varieties, since zero counts were entered into the data frame along with positive counts.

\section{Model C: Clause position (\chapref{sec:9})}\label{appendix:B.3}\label{bkm:Ref41473777}

These analyses are based on a total of \textit{n}~= 3,275 individual observations, distributed across the nine varieties under investigation. The data can be found in the file “concessives\_3” in what has been published as \citep{Schützler2021}. The total number of observations as well as the number of levels of the two random factors \textsc{genre} and \textsc{text} are given in \tabref{tab:B.1}, the \isi{priors} are documented in \tabref{tab:B.2}, and full regression tables for all individual models can be retrieved from the online repository (cf. \sectref{sec:1.4}). A separate model with four~chains of \textit{n~}=~4,750 iterations each and a \isi{warmup} phase of \textit{n}~=~1,000 iterations was run for each variety. The resulting number of posterior samples was thus \textit{n~}=~15,000 per model. The R-hat diagnostic equalled 1.00 for all parameters, which is an indicator of the \is{convergence (of statistical model)}convergence of the four chains. There are missing genres in six out of the nine varieties, i.e. genres which do not appear as a level of the \is{cluster variables}cluster variable \textsc{genre}, because they produced no hits: IrE (missing: business transactions), JamE (administrative writing and cross examinations), NigE (parliamentary debates), IndE (cross examinations), SingE and HKE (phone calls).

\section{Model D: Choice of conjunction (\chapref{sec:10})}\label{appendix:B.4}\label{bkm:Ref41509070}

The analysis is based on a total number of \textit{n}~= 3,275 individual observations made in all nine varieties under investigation. The data are contained in the file “concessives\_3” in \citet{Schützler2021}. This model was run with four~chains, each with \textit{n~}=~5,500 iterations and a \isi{warmup} phase of \textit{n}~=~1,000 iterations, which resulted in \textit{n~}=~18,000 posterior samples. The R-hat diagnostic took the value of 1.00 for all parameters, confirming the \is{convergence (of statistical model)}convergence of chains. Basic information on token numbers and \isi{priors} are provided in Tables \ref{tab:B.1} \& \ref{tab:B.2}, and~– as for the other models~– much more detailed model summaries are documented online (cf. \sectref{sec:1.4}). Missing genres (and missing texts) are the same as in Model C (see Appendix~\ref{appendix:B.3} above).

\section{\label{bkm:Ref52610381}\label{bkm:Ref52610458}Model E: Clause structure (\chapref{sec:11})}\label{appendix:B.5}

Like Models C \& D, this analysis is based on \textit{n}~= 3,275 individual observations in the nine varieties that were included. The data can be accessed in the file “concessives\_3” in the published data \citep{Schützler2021}. All of the nine models were run with four chains, each with \textit{n}~= 4,000 iterations and a \isi{warmup} of \textit{n}~= 1,000 iterations, which yielded a total of \textit{n}~= 12,000 \is{Bayesian statistics!posterior distribution}posterior samples. R-hat equalled 1.00 for all parameters. Again, see Tables \ref{tab:B.1} \& \ref{tab:B.2} for basic information on overall token numbers and the number of levels of the two random factors \textsc{genre} and \textsc{text}, as well as for information regarding the \isi{priors}. Full documentation of model output is provided in the online appendix (cf. \sectref{sec:1.4}). Once again, missing genres (and missing texts) are the same as in Models C \& D (see Appendices~\ref{appendix:B.3} \&~\ref{appendix:B.4} above).

\begin{table}
\caption{Specification of \isi{priors} for all models\label{tab:B.2}}
\begin{tabular}{lll}
\lsptoprule
Model & Parameter                & Prior               \\\midrule
A     & Intercept                & normal (−8.28, 1.8) \\
      & \textit{b}                        & normal (0, 1.5)     \\
      & \textit{SD}                       & normal (0, 1.5)     \\
      & \textit{cor}                      & lkj(2)              \\\addlinespace
B     & Intercept                & normal (−8.28, 1.8) \\
      & \textit{b}                        & normal (0, 1.5)     \\
      & \textit{SD}                       & normal (0, 1.5)     \\
      & \textit{cor}                      & lkj(2)              \\\addlinespace
C     & Intercept                & normal (0, 1)       \\
      & \textit{b}                        & normal (0, 1.5)     \\
      & \textit{SD}                       & normal (0, 1)       \\
      & \textit{cor}                     & lkj(2)              \\\addlinespace
D     & Intercept (\textit{though})       & normal (−0.5, 2)    \\
      & Intercept (\textit{even though})  & normal (−0.5, 2)    \\
      & \textit{b}                        & normal (0, 1.5)     \\
      & \textit{SD} (\textit{though})              & normal (0, 1.5)     \\
      & \textit{SD} (\textit{even though})         & normal (0, 1.5)     \\
      & \textit{cor}                      & lkj(2)              \\\addlinespace
E     & Intercept                & normal (−3, 2)      \\
      & \textit{b}                        & normal (0, 1.5)     \\
      & \textit{SD}                       & normal (0, 1)       \\
      & \textit{cor}                      & lkj(2)              \\
\lspbottomrule
\end{tabular}
\end{table}
