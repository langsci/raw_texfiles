\chapter{\label{bkm:Ref475086195}Methodologies}\label{sec:6}

This chapter sets out by describing the corpus used for the main analyses in the present study, the \textit{International Corpus of English} (\sectref{sec:6.1}), followed by an account of procedures adopted in data retrieval, processing and annotation (\sectref{sec:6.2}). A fairly detailed but general discussion of the statistical approaches central to the analytical chapters is provided in \sectref{sec:6.3}. The specific formulations of regression models used in Chapters \ref{sec:7}–\ref{sec:11} are discussed at the beginnings of the respective chapters (in sections entitled “Statistical model”).

\section{\label{bkm:Ref466296362}\label{bkm:Ref52284624}\label{bkm:Ref52284673}\label{bkm:Ref58229970}\label{bkm:Ref73115312}\label{bkm:Ref82554550}\label{bkm:Ref89925109}\label{bkm:Ref118499987}\label{bkm:Ref118500361}\label{bkm:Ref118501487}The \textit{International Corpus of English} (ICE)}\label{sec:6.1}


The present study draws on data from the \textit{International Corpus of English} (\is{International Corpus of English}ICE, \citealt{KirkNelson2018}), although additional corpus examples presented in \chapref{sec:3} were taken from xBrown as well as from COHA, as explained there, and an example from ARCHER was also cited in Chapter~\ref{sec:2}. The \textit{International Corpus of English} (\is{International Corpus of English}ICE) was initiated in 1988 by Sidney Greenbaum at the \textit{Survey of English Usage} (\is{Survey of English Usage}SEU), University College London (\citealt{NelsonWallisAarts2002}: 2). According to \citet{Greenbaum1988}, the project’s main objectives were
(i)~to compile corpora representative of \is{varieties of English!L1}L1 varieties other than \il{American English}AmE and \il{British English}BrE;
(ii)~to sample \is{varieties of English!L2}L2 \isi{varieties of English}; and
(iii)~to build corpora that include spoken and written language (also cf. \citealt{Greenbaum1991}).\footnote{\citet[4]{Greenbaum1991} also points to a possible use of \is{International Corpus of English}ICE for the monitoring of diversification processes, one function of the corpus being “to preserve the international character of at least written English”.} However, the potential of \is{International Corpus of English}ICE for comparative studies of different \isi{varieties of English} was also a consideration \parencites[4]{Greenbaum1991}[10]{Greenbaum1996}. Components of \is{International Corpus of English}ICE are restricted to \is{varieties of English!L1}L1 and \is{varieties of English!L2}L2 \isi{varieties of English}; see \citet[4]{Greenbaum1996}, for example, who excludes from \is{International Corpus of English}ICE “English used in countries where it is not a medium for communication between natives of the country”, i.e. English in the \is{varieties of English!Expanding Circle}Expanding Circle (\is{varieties of English!EFL}EFL, \citealt{Kachru1985,Kachru1988}; cf. \sectref{sec:4.1}).\footnote{Increasingly, varieties in the \is{varieties of English!Expanding Circle}Expanding Circle are explored using corpora inspired by \is{International Corpus of English}ICE (e.g. \citealt{Edwards2016,Laitinen2010}). There is also the \textit{International Corpus of Learner English} (ICLE, \citealt{GrangerEtAl2009}; cf. \citealt{Greenbaum1996} and \citealt{Granger1996}).} Today, there is an abundance of research that uses the various components of \is{International Corpus of English}ICE in the \isi{World Englishes} paradigm (see \sectref{sec:4.1}).\footnote{See, for example, papers in \citet{HundtGut2012,SeoaneSuárez-Gómez2016,HoffmannSiebers2009}, and in volume 34 of the \textit{ICAME Journal}. See also \citegen{Aarts2011} \textit{Oxford Modern English Grammar}, which uses authentic examples from ICE-GB, very much in the spirit of \citet[33]{QuirkEtAl1985}, whose reference grammar is partly informed by data from the \is{Survey of English Usage}SEU database and other corpora.}

The design of \is{International Corpus of English}ICE is documented in several publications (e.g. \citealt{NelsonWallisAarts2002}: 307–308, \citealt{Aarts2011}: 347–348). Greenbaum (\citeyear{Greenbaum1992}: 171, \citeyear{Greenbaum1996}: 5) refers to the different national versions of \is{International Corpus of English}ICE as \is{International Corpus of English!components}\textit{components}, a convention I will follow. National compo\-nents of \is{International Corpus of English}ICE are constructed “along parallel lines” \citep[171]{Greenbaum1992}, and each comprises about 1,000,000 words~– 600,000 spoken and 400,000 written (\citealt{NelsonWallisAarts2002}: 5); this rather limited size of the individual components can be a drawback in the investigation of certain linguistic phenomena. The structure of \is{International Corpus of English}ICE is often repre\-sented as four hierarchical levels: At the highest level, the corpus breaks down into two \is{International Corpus of English!sections}\textit{sections} (spoken/written), each of which contains two \is{International Corpus of English!subsections}\textit{subsections}. These are dialogues and monologues in the spoken section and printed and non-printed texts in the written section. \is{International Corpus of English!subsections}Subsections are further subdivided into a total of 12 \is{International Corpus of English!macro genres}\textit{macro genres}, and at the lowest level there are the \is{International Corpus of English!genres}32 specific \textit{genres}. According to the original scheme, speakers and writers sampled for \is{International Corpus of English}ICE should be at least 18 years old and should have undergone formal schooling in English at least until the end of secondary school, complemented perhaps by a first university degree in \is{varieties of English!L2}L2 countries. A broader selection criterion was to sample “professionals in the widest sense”, i.e. “academics, lawyers, politicians, authors, broadcasters, journalists, and business professionals (e.g. managers, accountants)” including students \citep[177]{Greenbaum1992}.

As \citet[5]{Greenbaum1996} points out, there are certain limitations to the representativeness of individual \is{International Corpus of English}ICE components as well as to their \is{corpora!comparability of}comparability. While the design is the same for all components, the subject matter of texts will naturally not be the same, which may have linguistic effects that are impossible to monitor. Furthermore, speakers and writers are not rigorously balanced according to extralinguistic parameters like sex, age, education or occupation. Finally, certain text categories may be difficult or impossible to obtain in some countries and have to be substituted by related kinds of text.

\citet[173]{Greenbaum1992} states that components should date from the same period, namely 1990–1993. This principle was clearly not upheld, and there is a diachronic dimension both within individual components and between them (cf. \citealt{MukherjeeSchilkBernaisch2010}: 64, \citealt{Bernaisch2015}: 64). Many components were compiled much later (e.g. ICE-Nigeria, \citealt{WunderVoormannGut2010}), over a considerably longer period, or are still under compilation (e.g. ICE-Scotland, \citealt{SchützlerGutFuchs2017}). Early corpora~– most notably ICE-GB~– might therefore benefit from a second edition that would then be \is{corpora!comparability of}comparable to the newer components.\footnote{At the time of writing, more than 30 years have elapsed since the compilation of ICE-GB, which is roughly the time gap between corpora of the (diachronic) \is{corpora!xBrown}xBrown family (see beginning of \chapref{sec:3}).}

\begin{sloppypar}
Markup conventions used for \is{International Corpus of English}ICE (\citealt{Nelson2002a,Nelson2002b}) play no role in the present study, since the corpus was searched lexically. Non-corpus material \parencites[8–9]{Nelson2002a}[12–13]{Nelson2002b} was excluded from the files before they were searched. This comprises extra-corpus text, i.e. text that simply exceeds the envisaged 2,000 words per text, untranscribed text (e.g. references to tables, formulae or figures in a text), and editorial comments. Non-corpus material was deleted from the files using regular expressions targeting the respective tags in the original files, as documented in some more detail in Appendix~\ref{appendix:A.2}.\footnote{Thanks are due to Fabian Vetter and Thomas Brunner for their help in preparing the corpus material.} Deleted passages were neither searched, nor were they included in the word counts when measuring the sizes of individual texts (cf. Appendix~\ref{appendix:A.2}).
\end{sloppypar}

Nine national components of \is{International Corpus of English}ICE were used in this study, corresponding to the nine varieties detailed in \sectref{sec:4.3}: Great Britain (ICE-GB), Ireland (ICE-IRE), Canada (ICE-CAN), Australia (ICE-AUS), Jamaica (ICE-JAM), Nigeria (ICE-NIG), India (ICE-IND), Hong Kong (ICE-HK), and Singapore (ICE-SING). They are shown in \figref{fig:6.1}.\footnote{\figref{fig:6.1} is based on a file retrieved from \url{https://commons.wikimedia.org/wiki/File:BlankMap-World_gray.svg}; original by user Vardion, transformed into svg-file by Simon Eugster; published under a CC BY-SA 3.0 licence: \url{https://creativecommons.org/licenses/by-sa/3.0/}.} The perspective that is taken is broad, since these nine varieties are scattered across the different \isi{world regions} or continents. In the study, we can therefore not expect (nor attempt) to reveal patterns that are characteristic of certain kinds of \is{varieties of English!L1}L1 or \is{varieties of English!L2}L2 English, since these are often represented by only one variety. For example, \il{Canadian English}CanE is the only North \ili{American English}, \il{Jamaican English}JamE is the only \ili{Caribbean English}, \il{Nigerian English}NigE is the only \ili{African English}, and so forth. Further, strong generalisations regarding differences between \is{varieties of English!L1}L1 and \is{varieties of English!L2}L2 varieties are not possible.

\begin{figure}
\includegraphics[width=\textwidth]{figures/CCs.Fig.6.1.pdf}
 \caption{\label{bkm:Ref36942266}\label{fig:6.1}Varieties investigated in this study. L1: BrE (1), IrE (2), CanE (3), AusE (4); L2: JamE (5), NigE (6), IndE (7), SingE (8) and HKE (9).}
\end{figure}

Finally, US-\ili{American English} is not part of the study, since there is no spoken section of ICE-USA. The \textit{Santa Barbara Corpus of Spoken American English} (\citealt{DuBoisEtAl2000}) was designed to contain spontaneous conversations of various descriptions, but not to be a true substitute for the complete set of spoken genres of ICE-USA, as discussed on the project website.\footnote{\url{http://www.linguistics.ucsb.edu/research/santa-barbara-corpus}; accessed 5 {October 2023}.} \citet[64–68]{ChafeEtAl1991} point out that the corpus was originally conceived as an \il{American English}AmE counterpart to the \textit{London-Lund Corpus of Spoken English} (\is{corpora!LLC}LLC, \citealt{SvartvikQuirk1980,GreenbaumSvartvik1990}) and was therefore not sampled from a balanced mix of (representative) \is{genre}genres (\citealt{ChafeEtAl1991}: 69). The main \is{text types}text type contained in the corpus is \isi{face-to-face conversation}; other \is{genre}genres comprise phone conversations, on-the-job exchanges, lectures, sermons, story-telling and public meetings or conventions, among others. Direct comparisons with the other \is{International Corpus of English}ICE components in this study was therefore considered infeasible, and several results in \citet{Schützler2018c} support this assessment.

\section{\label{bkm:Ref466296377}Data retrieval and annotation}\label{sec:6.2}


Components of \is{International Corpus of English}ICE were used in the form of individual plain text files, with \textit{n~}= 500 for the traditional ICE-structure and \textit{n~}= 902 for ICE-Nigeria. These were searched using \isi{AntConc} \citep{Anthony2018}. Non-embedded tags (“<…>”) were retained in the searches (but not in the word counts; cf. Appendix~\ref{appendix:A.2}). The search terms were entered as a list comprising the expressions \textit{although}, \textit{though}, \textit{even though} and \textit{as though}. Adding \textit{as though} to the query made it possible to exclude most instances of this item right from the start. Queries were executed corpus by corpus~– that is, nine individual queries and retrievals were run, one for each ICE component. The resulting concordance lists were exported as text files and compiled into a single spreadsheet in Microsoft Excel, which was then taken to the annotation stage. During retrieval, the context window in \isi{AntConc} had been set to 300 symbols to the left and right of the search terms; in cases for which this was insufficient for semantic analysis, the full context was reinspected with the “file view” option in \isi{AntConc}, after a renewed search for the respective occurrence.

\begin{sloppypar}
Basic coding included labels for variety, text-ID and for the conjunction that was used. Variety labels were added manually when combining the individual output files, text-IDs were part of the output, and the connective (\SchuetzlerIndexExpression{although}, \SchuetzlerIndexExpression{though}, \SchuetzlerIndexExpression{even though}) was found in the KWIC slot (“keyword in context”) of each concordance line. Text-IDs were somewhat problematic, since they do not differentiate for variety and may be characterised by diverse formats. That is, filenames like “S1A-027” exist in several corpora, and they may also appear as “s1a-027”, i.e. variably with or without capital letters. Since for the \is{regression!mixed-effects}mixed-effects regression models (see \sectref{sec:6.3.3}) it was essential to have one unique text-ID per file, across varieties, I used regular expressions in \isi{R} to
(i)~regularise the capitalisation of letters and use of hyphens, and to
(ii)~add the variety label to each text-ID. This yielded labels such as “GB-S1A-027” which then described unique texts in the set of national components of ICE that was queried.\footnote{In ICE-NIG, labels took a different form (e.g. “NIG-PHum-001”) since this corpus uses explicit \is{International Corpus of English!genres}\isi{genre} labels instead of the alphanumerical IDs found in other ICEs (e.g. “W2B”) and files are numbered consecutively within each \isi{genre} category~– a practice also followed in the compilation of ICE-Scotland (\citealt{SchützlerGutFuchs2017}).}
\end{sloppypar}

In the next step, labels for \isi{mode of production} (spoken/written), \is{International Corpus of English!genres}genre (e.g. “con”~= conversations) and the total number of words in each individual text file were added to each line. The spoken-written distinction was one of the predictor variables, \is{International Corpus of English!genres}genre was required as a random effect, and the number of words per text was required for the \is{regression!negative binomial}negative binomial model (cf. \sectref{sec:6.3.3}, \sectref{sec:7.2} and \sectref{sec:8.1}). These pieces of information were added by referring each line in the concordance file to a documentation file that had previously been prepared. This contained the complete information for all individual text files in the components of ICE that were involved: the unique text-ID, the \isi{mode of production}, the \is{International Corpus of English!genres}genre and the number of words. Based on the homogenised IDs, there was a match between each of the lines in the concordance file and exactly one line in the reference file, and the relevant information was extracted from the latter and added to the former.\footnote{Fabian Vetter was a tremendous help to me in this regard, as he had documented the number of words per individual text file in several components of \is{International Corpus of English}ICE for his own work (\citealt{Vetter2021,Vetter2022}).}

  While the parameters described thus far concern the provenance of corpus findings (variety, \is{International Corpus of English!genres}genre category, text file) and the obvious structural parameter of the concessive marker itself, the semantic type of each concessive (cf. \sectref{sec:2.2}), the position of the subordinate clause (cf. \sectref{sec:2.3.1}) and the syntactic structure of the subordinate clause (cf. \sectref{sec:2.3.2}) had to be coded manually. Although this was technically less involved than the process described in the previous paragraph, it was naturally much more time-consuming. During this process of semantic and syntactic annotation, there was some additional \is{disambiguation}disambiguation and exclusion of \isi{false positives} and items that could not be used for other reasons. For example,
(i)~instances of \textit{though} were manually inspected and classified as either conjunctions or \isi{conjuncts} (cf. \citealt{Schützler2020a}) and were excluded if they were found to be \isi{conjuncts};
(ii)~items were discarded if the context was too fragmented for meaningful analysis, which was considerably more often the case in speech than in writing; finally,
(iii)~\isi{false positives} like the ones illustrated in (\ref{ex:76}–\ref{ex:79}) were sorted out (all emphases in these examples are my own; OS). In \REF{ex:76}, \textit{although} is not used as a grammatical marker but quoted as a linguistic object; in \REF{ex:77} and \REF{ex:78}, there are obvious transcription errors (<though> should read <through> and <thought>, respectively); and in \REF{ex:79}, there is a false start resulting in a duplicate marker (leading to the exclusion of the first instance of \textit{though}). Other cases not shown here included instances in which it was simply not possible to classify \textit{though} as a subordinating conjunction or a \is{though@\textit{though}!as conjunct}\is{conjuncts}conjunct.

\ea\label{ex:76}\label{bkm:Ref466970222}New topics should begin in a new paragraph and there should be a proper link between the two paragraphs and to do this proper words like \textbf{Although}, However, etc. (ICE-IND:W1A-013)\\
    \ex\label{ex:77}\label{bkm:Ref466970225}Every bric-a-brac hurtled \textbf{though} the living room. (ICE-PHI:W2F-005)\\
    \ex\label{ex:78}\label{bkm:Ref466970277}They gave little \textbf{though} to what he was called in the macrocosm outside. (ICE-HK:W2F-020)\\
    \ex\label{ex:79}\label{bkm:Ref466970288}\textbf{Though} I \textbf{though} \textit{I have worked as both teacher and an administrator} I had more satisfaction as a [sic] administrator than as a teacher […]. (ICE-IND:S1A-026)\\
\z

After the data had been checked in this way and all valid cases had been annotated semantically and syntactically, the dataset was ready for statistical analysis in \isi{R} and was exported as a comma-separated file.

\section{\label{bkm:Ref37060331}Mixed-effects Bayesian regression models}\label{sec:6.3}\largerpage


This section introduces \is{regression!mixed-effects}mixed-effects \is{Bayesian statistics}Bayesian regression modelling, the main tool for statistical analysis in this book. After a general introduction to \isi{hierarchical data} structures and the consequent need for \is{regression!mixed-effects}mixed-effects models (\sectref{sec:6.3.1}), the rationale behind and the advantages of \is{Bayesian statistics}Bayesian models are discussed (\sectref{sec:6.3.2}). Further, the three relevant model types are described (\sectref{sec:6.3.3}), followed by a few words on random effects and centred predictors (\sectref{sec:6.3.4}), a description of the estimation and visualisation process (\sectref{sec:6.3.5}) and a definition of all variables used in the models (\sectref{sec:6.3.6}). All analyses were conducted with \isi{R} (\citealt{RDevelopmentCoreTeam2021}), working within the \isi{RStudio} environment (\citealt{RStudioTeam2009}).

\subsection{\label{bkm:Ref38459565}Hierarchical data structures}\label{sec:6.3.1}\largerpage

Naturally occurring, \is{observational data}observational language data almost invariably have a \is{hierarchical data}hierarchical structure (see \citealt{Johnson2014,SpeelmanHeylenGeeraerts2018}: 2–3, \citealt{Winter2020}: 232–233, \citealt{WinterGrice2021}), which means that data points are not independent but \is{hierarchical data}\is{hierarchical data}clustered or \is{hierarchical data}grouped. In this study, the grouping of observations happens at three levels:
(i)~the variety,
(ii)~the text category (or \isi{genre}), and
(iii)~the author (or speaker). All three categories constitute \is{language-external factors}language-external, contextual grouping factors: Data points produced in certain contexts or by certain individuals are assumed to belong together and form a group. Although this is not relevant in the present study, language-internal groupings are also common: If, for instance, we are interested in the dative alternation in English, there will be several observations involving the same verb (e.g. \textit{give} or \textit{show}), which introduces a grouping structure on linguistic grounds (cf. \citealt{SpeelmanHeylenGeeraerts2018}: 2).

Hierarchical structures can be conceptualised as resulting from drawing a “multistage \isi{sample}” (\citealt{Hox2010}: 4; cf. \citealt{GelmanHill2007}: 7), where lower-level units are sampled from higher-level units, potentially at several levels (see also the discussions in Section~3.4.1 of \citealt{Sönning2020}, and \citealt{SönningSchlüter2022}). It is essential that such \is{hierarchical data}clustered structures are taken into account when statistically modelling language variation. In this study, the clustering of data by variety is addressed by fitting a separate model for each of the nine varieties. The three grouping levels are thus reduced to two, and, accordingly, there are only two \is{random effects}random-effects components in the statistical models, \textsc{genre} and \textsc{text}. Each variety, then, contains a \isi{sample} of \is{genre}genres, each of which in turn contains a \isi{sample} of texts, in both cases according to the design of \is{International Corpus of English}ICE. Finally, each text contains a number of individual observations. The structure is fully \is{nested data}nested, not crossed (or partly non-\is{nested data}nested; cf. \citealt{Baayen2008}: 260–261), since an individual observation is attributed to one (and \textit{only} one) specific text, and each text belongs to one (and \textit{only} one) \isi{genre} within the respective variety of English. \figref{fig:6.2} shows the \isi{hierarchical data} structure in each of the nine varieties in a schematic form.{\interfootnotelinepenalty=10000\footnote{Note that the concrete design of a statistical model is based on active decisions taken by the researcher, which, in turn, are based on their assumptions about the data, as well as their aims concerning generalisability.}}

\begin{figure}
\includegraphics[width=9cm]{figures/CCs.Fig.6.2.pdf}
\caption{\label{bkm:Ref37231385}\label{fig:6.2}Grouped (or hierarchical) data structure in the present study}
\end{figure}

Two issues may arise if \is{hierarchical data}grouped data structures are not taken into account:
(i)~\is{point estimate}Point estimates (e.g. percentages of variant constructions or frequencies of markers\slash semantic types) may be less precise (\citealt{SönningSchlüter2022}) and
(ii) our assessment of statistical \isi{uncertainty} may be overly optimistic or pessimistic. Typically, \isi{uncertainty} tends to be underestimated for between-cluster effect estimates and overestimated for within-cluster effect estimates \citep[489]{Agresti2013}. Since data points in non-hierarchical models are treated as independent, we effectively inflate our \isi{sample size}. In the schematic structure of \figref{fig:6.2}, let us assume that the first ten data points stem from spoken texts, while the remaining six stem from written texts. In a non-hierarchical analysis, all of these would be independent data points, while in actual fact there are only \textit{n}~=~3 spoken texts and \textit{n}~=~2 written texts, and the hierarchical analysis that takes this fact into account will be more cautious in estimating the contrast between speech and writing. For further discussions of the potential conse\-quences of applying an ordinary least-squares analysis to \isi{nested data}, see \citet[3]{Hox2010},    \citet[15–16]{SnijdersBosker1999} and \citet[6–7]{Luke2004}. Hierarchical models, then, relax the assumption of independent observations by making dependencies and hierarchies part of the model structure \citep[6]{Hox2010}. Further, \isi{missing data} (or, in this case, unequal numbers of observations within groups) are unproblematic (\citealt{SnijdersBosker1999}: 52, \citealt{SpeelmanHeylenGeeraerts2018}: 1).


\citet[3]{SpeelmanHeylenGeeraerts2018} suggest that, with regard to random effects, we should consider the levels found in the data to be merely a \isi{sample} from possible levels. For instance, there may be other \is{genre}genres similar to the ones in \is{International Corpus of English}ICE, and there are of course more speakers/writers producing language in those \is{genre}genres than the ones that happen to be sampled for our corpora. By specifying a random effect for \textsc{genre}, we aim to \is{extrapolation}extrapolate from the \is{genre}genres included in the corpus to the population of \is{genre}genres represented by our \isi{sample} (cf. \citealt{SpeelmanHeylenGeeraerts2018}: 3). In contrast, we know the levels or values of our fixed effects. For example, if we use \isi{mode of production} as a variable, its levels have to be either “spoken” or “written”. Combining both random and fixed effects results in a \is{regression!mixed-effects}mixed-effects model. In this study, only \textsc{genre} and \textsc{text} are defined as random effects. Thus, results are taken to hold for each of the nine varieties, and varieties are compared to each other, but the study is not designed to make claims that hold for the entire English language complex.\footnote{The inclusion of random effects in a model~– be it \is{Bayesian statistics}Bayesian or \is{frequentist statistics}frequentist~– results in what is called \textit{\isi{shrinkage}} (or \textit{\isi{pooling}}), as discussed by \citet[245–249]{Kruschke2015}, \citet[252–259]{GelmanHill2007} and \citet[275–278]{Baayen2008}. This means that individual estimates in a \is{random effects}random-effects structure are partially adjusted towards the general trend (the regression line). However, since it is not discussed or interpreted in this book, I do not discuss this concept in detail here.}

\subsection{\label{bkm:Ref38459573}Bayesian statistics}\label{sec:6.3.2}

\is{Bayesian statistics}Bayesian data analysis is gaining ground in different empirical disciplines, including linguistics. While it contrasts with classical “\is{frequentist statistics}\is{frequentist statistics}frequentist” methods in several important respects, the two approaches will often produce similar results. \is{Bayesian statistics}Bayesian \isi{inference} is computationally more expensive, both in running the models and in generating meaningful output, e.g. predicted probabilities and the respective visualisations. Its advantages may thus not be immediately obvious, and a few obstacles need to be overcome when adopting it. I will therefore briefly outline some of the properties of \is{Bayesian statistics}Bayesian models and motivate their preference over \is{frequentist statistics}frequentist ones.\footnote{See, for example, \citet[chapter 11]{Kruschke2015} for a strong argument against classical \is{frequentist statistics}Null Hypothesis Significance Testing (NHST).} Decisions were based both on statistical and practical considerations (cf. \citealt{Sönning2020}: Section~3.4.2).

\begin{enumerate}
\item \textit{Incorporating prior information:} In the \is{Bayesian statistics}Bayesian paradigm, the researcher must incorporate information that is external to the data at hand. Such information is referred to as prior information, or \is{priors}\textit{priors}. A prior is a distribution of likely values established without having seen the data. Another distribution of likely values~– the \is{Bayesian statistics!likelihood}\textit{likelihood}~– is generated from the data. Finally, prior distribution and \is{Bayesian statistics!likelihood}likelihood are fused into a \is{Bayesian statistics!posterior distribution}\textit{posterior distribution} of values, the outcome of the analysis. This is \is{Bayesian statistics!Bayes' rule}Bayes’ rule: “the mathematical relation between the prior allocation of credibility and the \is{Bayesian statistics!posterior distribution}posterior reallocation of credibility conditional on data” (\citealt{Kruschke2015}: 99–100; cf. \citealt{Shikano2015}: 36). Priors can reflect
(i)~results from previous research,
(ii)~common-sense assumptions (or “consensual experience”; cf. \citealt{Kruschke2015}: 27) concerning reasonable outcome values, and
(iii)~\is{subjectivity}subjective beliefs held by the researcher. Priors can (and should) be criticised if their motivation is not transparent or if they seem to be designed to support a particular hypothesis. If they are carefully motivated and implemented, however, they have considerable advantages. Epistemologically, we should neither blindly trust previous research or common-sense assumptions, nor should we put complete faith in our own data. There is good reason to believe that both components have a contribution to make and should therefore find entry into our analysis. Furthermore, using prior information in regression models also has practical computational advantages, as will be explained below.

\item\sloppy \textit{Interpretability and statistical thinking:} Compared to \is{frequentist statistics}frequentist approaches, \is{Bayesian statistics}Bayesian \isi{estimation} is based on a different notion of probability. Credible intervals in \is{Bayesian statistics}Bayesian analysis (which will be called \textit{\isi{uncertainty intervals}} in this study) can be directly interpreted as having a certain probability (e.g. 50\% or 90\%) of containing the model parameter, or the predicted \isi{point estimate} of the outcome variable. We thus arrive at statements like the following (cf. \figref{fig:7.3} in \sectref{sec:7.3}): “Based on our statistical model, our prior assumptions and our data, we take the most likely frequency of \textit{though} in HKE to be 72 pmw; there is a probability of 50\% that it is found somewhere in the interval of 59–86 pmw, and a 90\% probability that it is found within the interval of 44–108 pmw”.\footnote{We also implicitly assume that we have not only considered all relevant predictor variables but also did not make any systematic measurement errors. These conditions are never met completely, and all models will therefore be overoptimistic.} Direct statements of this kind cannot be made on the basis of \is{frequentist statistics}frequentist regression models (cf. \citealt{BolstadCurran2017}: 7). However, while this is often adduced as a main advantage of \is{Bayesian statistics}Bayesian statistics, its practical consequences for empirical researchers are in fact rather limited in many cases: Formulating parallel \is{frequentist statistics}frequentist and \is{Bayesian statistics}Bayesian models for the same dataset will often produce virtually identical results. In this study, considerations listed under the fourth point below played a much more important role.

\item \textit{Analytical efficiency and robustness:} Under certain circumstances, \is{frequentist statistics}frequentist models may be unable to compute model parameters and fail to \is{convergence (of statistical model)}converge. This can be due to the complexity of a model, or to a particular data structure (cf. \citealt{GelmanHill2007}: 344). Concerning the first point, a \is{frequentist statistics}frequentist model has no prior knowledge concerning model parameters. If, therefore, a very large number of parameters (e.g. due to many interaction terms) has to be estimated, all values are considered possible, and the model has to iterate through the entire, multi-dimensional parameter space. Carefully constraining the range of possible parameter values by defining \isi{priors}~– even if they are quite weak~– can considerably reduce the strain put on such models. Secondly, if there is a lack of variation concerning some parameter (i.e. if certain predictor values correlate perfectly with certain outcomes), \is{frequentist statistics}frequentist models may fail to \is{convergence (of statistical model)}converge or will compute unreasonable parameter values (\citealt{Agresti2013}: 233–235). For instance, if all spoken \is{genre}genres use variant A of a binary variable and all written varieties use variant B, this is likely to result in the non-\is{convergence (of statistical model)}convergence of a conventional model. While a \is{Bayesian statistics}Bayesian model will still predict very high proportions of variant A for speech and very high proportions of variant B for writing, some (minimal) probability of the alternative variant is generated from the \is{priors}prior, and the model will \is{convergence (of statistical model)}converge.

\item \textit{Flexible estimates of means, differences and ratios:} Results of \is{Bayesian statistics}Bayesian regression analyses can be summarised and visualised very flexibly, using the \isi{R} packages selected for this study. The output can be used to calculate \is{point estimate}point estimates and directly associate them with their statistical \isi{uncertainty}. For instance, we can not only estimate \is{central tendency}central tendencies and their \is{dispersion}dispersions for two conditions, but we can also estimate the difference~– be it an absolute difference or a ratio~– between the two conditions and its \isi{uncertainty}, all based on a single model object. This degree of flexibility is beyond current non-\is{Bayesian statistics}Bayesian approaches as implemented using the \isi{R} package \is{R packages!lme4}\textit{lme4} \citep{BatesEtAl2020}, for instance. Together with the previous point, I regard this as the main practical advantage of \is{Bayesian statistics}Bayesian regression.
\end{enumerate}

In this study, the data were modelled with the utilities in the R package \is{R packages!brms}\textit{brms} \citep{Bürkner2020}, which in turn uses \is{Stan (software)}Stan (\citealt{StanDevelopmentTeam2011}). The syntax for constructing \is{Bayesian statistics}Bayesian regression models on this basis is rather similar to conventions in non-\is{Bayesian statistics}Bayesian \is{regression!mixed-effects}mixed-effects regression modelling with \isi{R}, e.g. using \is{R packages!lme4}\textit{lme4} \citep{BatesEtAl2020}. With \is{R packages!brms}\textit{brms}, it is possible to implement a vast range of model types, including the ones used in this study: \is{regression!negative binomial}negative binomial, binary \is{regression!logistic}logistic and \is{regression!multinomial}multinomial \is{regression!logistic}logistic models. The major difference compared to conventional \is{frequentist statistics}frequentist models is the necessity to specify \isi{priors}. If the researcher does not do so, \is{R packages!brms}\textit{brms} will automatically specify vague \isi{priors}. However, as explained above, the opportunity to specify \isi{priors} based on background knowledge and expectations should be actively exploited.

Priors are expected distributions of model parameters, comprised of a \isi{central tendency} and a \isi{dispersion} measure. In this study, \isi{priors} were defined as normal distributions with mean and standard deviation. Priors can be uninformative or \is{priors!informative}informative. Uninformative \isi{priors} may be entirely “\is{priors!flat}flat”, i.e. they make no assumptions about more or less likely parameter values at all, or they may assume a \is{priors}prior distribution centred at zero and attached to a large standard deviation. Results from \is{Bayesian statistics}Bayesian analyses using such \isi{priors} will (nearly) coincide with \is{frequentist statistics}frequentist analyses. Informative \isi{priors} make assumptions about the most likely parameter value. For example, they might assume that the frequency of a phenomenon is larger (or smaller) in L2 varieties, compared to L1 varieties. Weakly \is{priors!informative}informative \isi{priors} attach high degrees of \isi{uncertainty} (i.e. large standard deviations) to such assumptions; strongly \is{priors!informative}informative \isi{priors} are characterised by a high degree of certainty (i.e. smaller standard deviations). Finally, \is{priors!regularising}regularising \isi{priors} are centred on zero and can again be characterised by smaller or larger standard deviations.\footnote{Regularising \isi{priors} are sometimes used as a precaution against overfitting and can be seen as a special case of \is{priors!informative}informative \isi{priors}. If strongly regularising, they assume a parameter value of zero (i.e. “no effect”) in combination with a small standard deviation. It is then relatively difficult for the data to prevail over such \isi{priors}.}

\figref{fig:6.3} illustrates the mechanism whereby a prior distribution of parameter values is confronted with a distribution of values suggested by data (the \is{Bayesian statistics!likelihood}\textit{likelihood}), and the two are merged into the \is{Bayesian statistics!posterior distribution}posterior distribution of the parameter values (the \textit{posterior}). The dotted line in each panel of the figure represents the \is{priors}prior, which is in this case always centred at zero but comes with decreasing standard deviations, from left to right. That is, in panel (d), the researcher is least prepared to accept parameter values that differ from zero, prior to seeing the data. The dashed line in each panel represents the distribution of parameter values that would be considered most likely if only the data were consulted. This distribution is the same in each panel, since our intention is to illustrate the effects of different \isi{priors} on an analysis. Finally, the solid line represents the \is{Bayesian statistics!posterior distribution}posterior distribution of the parameter, which results from combining the \is{priors}prior and the \is{Bayesian statistics!likelihood}likelihood.

\begin{figure}
\includegraphics{figures/CCs.Fig.6.3.pdf}
\caption{\label{bkm:Ref38902853}\label{fig:6.3}Parameter distributions in Bayesian models: priors, data and posteriors}
\end{figure}

We can see that in \figref{fig:6.3}a, the \is{priors}prior essentially lets the data speak for themselves; in panel (b), the \is{Bayesian statistics!posterior distribution}posterior is pulled slightly towards zero; in panel (c), this effect is more pronounced; and in panel (d), \is{priors}prior and \is{Bayesian statistics!likelihood}likelihood are equally \is{priors!informative}informative, so that the position of the \is{Bayesian statistics!posterior distribution}posterior is intermediate between them. Many of the \isi{priors} used in this study resemble the one used in \figref{fig:6.3}b, with a \isi{central tendency} at zero and a relatively generous standard deviation. Such \isi{priors}, while they will easily yield to strong data, will nevertheless provide computational support to complex models by placing gentle constraints on the possible parameter space. In some cases, \isi{priors} are selected so as to reflect the direction of an effect in previous publications, but they, too, come with a large standard deviation.

Running a \is{Bayesian statistics}Bayesian model based on \isi{priors} and data returns sampled combinations of parameter values (the \isi{posterior sample}), and each parameter can then be described concerning its \isi{central tendency} and \isi{dispersion}. Sampling happens via a method called \isi{Markov Chain Monte Carlo}, or MCMC (\citealt{Agresti2013}: 23, \citealt{Kruschke2015}: 115–116, 144, \citealt{Shikano2015}: 37). \citet[263–298]{McElreath2020} provides an accessible discussion of the procedure. Essentially, a particular, randomly selected combination of parameter values is likely to make it into the \isi{posterior sample} if it describes the data relatively well. Therefore, a large number of likely combinations of parameter values (positioned in regions of relatively high probability densities) and a correspondingly lower number of \textit{un}likely combinations are returned by the sampling process.

The MCMC routine is partitioned into several parallel processes (or chains) that should be generally aligned, or correlated, and each chain contains a specified number of \isi{warmup} samples~– used to calibrate the sampler~– that are excluded from the final sample (\citealt{GelmanHill2007}: 356). For example, if there are $n=3$ chains with $n=4{,}000$ iterations each, and if there is a \isi{warmup} phase of $n=500$ iterations, the total number of data points in the \isi{posterior sample} will be $3 \times (4{,}000 - 500) = 10{,}500$ samples, each of which contains a unique combination of model parameter values.

\subsection{\label{bkm:Ref52284724}\label{bkm:Ref38459589}\label{bkm:Ref37061243}Types of regression used in this study}\label{sec:6.3.3}\largerpage

The models used in this book all qualify as \is{regression!generalised linear}generalised linear models, since the outcome quantity is modelled on a transformed, nonlinear scale (\citealt{RaudenbushBryk2002}: 291). This transformation of original quantities (here: rates of occurrence and proportions) happens via \textit{link functions}, as will be explained below.{\interfootnotelinepenalty=10000\footnote{Strictly speaking, \is{regression!linear}linear models are a special case of generalised models (see \citealt{GelmanHill2007}: 109) that use the \is{link function!identity}\textit{identity function} as a link between the outcome and the statistical model, i.e. the outcome is left untransformed during the estimation process.}}

\subsubsection{Negative binomial count regression}\label{sec:6.3.3.1}

Since rates of occurrence (i.e. how often something happens) have a lower bound at zero, they are usually analysed using \is{regression!count}count regression models; ordinary regression models are not recommended for such outcomes (\citealt{Long1997}: 217–218). Several types of count models are available, e.g. the \is{regression!Poisson}Poisson model and the \is{regression!negative binomial}negative binomial model. For reasons discussed by \citet[7, 127]{Agresti2013}, \citet[230]{Long1997} and \citet[115–116]{GelmanHill2007}, I will use the \is{regression!negative binomial}negative binomial model in this book (cf. \citealt{CameronTrivedi2013}: 1). Binomial models address certain issues in \is{regression!Poisson}Poisson regression models, for instance the problem of \isi{overdispersion}: \is{regression!Poisson}Poisson regression assumes that predicted means have a constant variance which equals the mean, when in reality means become increasingly overdispersed as they grow larger, i.e. the variance exceeds the mean, which has undesired effects on the performance of the model. The four equations in (\ref{eq:6.1}) show the steps involved in modelling frequency of occurrence using \is{regression!count}count regression models.

\ea\label{bkm:Ref40195674}\label{eq:6.1}%
Count regression: outcome (a), \is{link function!log}log link (b), model (c), exponential (d)\smallskip\\
{\multicolsep=0pt\begin{multicols}{2}
\ea ${\displaystyle y = \frac{n}{N} = R} $
\ex ${\displaystyle \eta = \ln(R)}$
\ex ${\displaystyle \eta = a+xb}  $
\ex ${\displaystyle y = \text{exp}(\eta)} $
\z
\end{multicols}}
\z\largerpage[-2]

Ultimately, we are interested in the outcome \textit{y}, which is the rate of occurrence \textit{R} of a phenomenon, calculated by dividing the raw number of occurrences \textit{n} by some baseline \textit{N}, e.g. the number of words in a corpus. For count models, the \is{link function!log}log link function shown in part (b) of the equation is used, i.e. rates are transformed using the natural logarithm (\citealt{CameronTrivedi2013}: 36, \citealt{Kruschke2015}: 705–706, \citealt{MolenberghsVerbeke2005}: 32, \citealt{Agresti2013}: 115). This results in values of $-\infty < \eta \leq 0$. Logged rates are then modelled as shown in (\ref{eq:6.1}c): The average value (or intercept) \textit{a} and the predictor coefficient \textit{b} are estimated in such a way that, in combination with different predictor values (like \textit{x}), they provide a good approximation of the outcome \textit{η}.\footnote{Note that, unlike the analyses in this book, the schematic example used here is very simple and non-hierarchical.} Estimated values directly based on the values of coefficients in the posterior are on this logged scale and need to be re-transformed as shown in (\ref{eq:6.1}d), obtaining values of $0< R\leq 1$ \citep[125]{Agresti2013}. To arrive at the typical representation of rate per million words, we finally need to multiply by 1,000,000, a step that is not shown in (\ref{eq:6.1}).

\subsubsection{Binary logistic regression}\label{sec:6.3.3.2}

If an ordinary regression model is applied to binary outcomes, nonsensical estimates of $p > 1$ or $p < 0$ may result (\citealt{SnijdersBosker1999}: 211, \citealt{Luke2004}: 53; cf. \citealt{BestWolf2015}). Partly for this reason, regression models with binary response variables, like count models, depend on a nonlinear link function. In analogy to (\ref{eq:6.1}) above, the equations in (\ref{eq:6.2}) show the steps involved in a binary \is{regression!logistic}logistic regression analysis.


\ea
\label{bkm:Ref40389840}\label{eq:6.2}
Binary \is{regression!logistic}logistic regression: outcome (a), \is{link function!logit}logit link (b), model (c), \is{logistic function}logistic (d)\smallskip\\
{\multicolsep=0pt
\begin{multicols}{2}
  \ea $ {\displaystyle y=\frac{n}{N} = p}$
  \ex $ {\displaystyle \eta = \ln\left(\frac{p}{1-p}\right)}$
  \ex $ {\displaystyle \eta = a+xb}$
  \ex $ {\displaystyle y = \frac{\text{exp}(\eta)}{1+\text{exp}(\eta)}}$
  \z
\end{multicols}
}
\z

The outcome \textit{y} is a proportion \textit{p}, which is the fraction of the total number of times a variant occurs over the total number of contexts in which it \textit{could} have occurred; it takes values of $0<p<1$. In binary \is{regression!logistic}logistic regression, the \is{link function!logit}logit link function shown in (\ref{eq:6.2}b) is used to transform an expected proportion to \isi{log odds} (or \isi{logits}), i.e. the natural logarithm of the odds (\citealt{SnijdersBosker1999}: 212, \citealt{Luke2004}: 53, \citealt{Agresti2013}: 115, \citealt{GelmanHill2007}: 80, \citealt{Kruschke2015}: 622). For instance, the odds for a proportion of 0.8 are 4:1 (or simply 4), and the corresponding \is{log odds}log-odds (or \is{logits}logit) value is $\ln(4)=1.39$. For the inverse case with a proportion of 0.2, the odds are 1:4 (or 0.25), yielding a \is{logits}logit of $\ln(0.25)=-1.39$. Complementary \isi{logits} (e.g. for pairs of proportions like 0.8 and 0.2, or 0.45 and 0.55) are symmetrically arranged around zero, with the same unsigned values, and they lie in the range of $-\infty < \eta < +\infty$. Logits can then be modelled linearly, as shown in (\ref{eq:6.2}c). For the communication of results, it is desirable (and perhaps necessary) to transform estimated values back into proportions, using the \isi{logistic function} illustrated in formula (d) above \citep[56]{Luke2004}. To obtain percentages instead of proportions, values thus obtained simply need to be multiplied by 100. In a binary regression model, predictor coefficients refer to a difference in the probability (expressed as \isi{log odds}) of observing one of the two outcome variants relative to the other. In such models, it is usually enough to show results for the “target” category, because we can infer the relative frequency of the \isi{reference category}: $p_1=0.8$ corresponds to $p_0=0.2$, for instance, and vice versa. However, in order to estimate values for the baseline category, we simply need to insert the value 1 for the numerator in formula (d).

\subsubsection{Multinomial logistic regression}\label{sec:6.3.3.3}

If there are more than two possible outcome categories, \is{regression!multinomial}multinomial regression may be used. To this end, one level of the categorical response serves as a baseline category. Multinomial regression can be thought of as a series of binary models, since the odds of each of the remaining levels are compared to the odds of the baseline category (\citealt{Agresti2013}: 293–294). The link function is once again the \is{link function!logit}logit link, but in this case, there are \isi{logits} for all categories except the baseline. In essence, however, \is{regression!multinomial}multinomial regression can be regarded as an extension of binary \is{regression!logistic}logistic regression (\citealt{GelmanHill2007}: 119), or as \citet[173]{Long2015} puts it, they are “sets of binary regressions that are estimated simultaneously”. In practice, this means that the specification of \isi{priors}~– if a \is{Bayesian statistics}Bayesian approach is used~– can be more complex, as different specifications can be made for different outcome categories. More importantly, the re-transformation of \isi{logits} into predicted probabilities is also more involved (cf. \citealt{Agresti2013}: 296–297). If we assume three outcome categories (e.g. in the selection of one of the three concessive conjunctions in this book; cf. \chapref{sec:10}), the equations in (\ref{eq:6.2}a--c) above can be directly applied to \is{regression!multinomial}multinomial regression, except that the number of parameters is multiplied by the factor \textit{c}$-$1, \textit{c} being the total number of possible outcomes. With three outcome categories, that is, \textit{y}, \textit{η}, \textit{a} and \textit{b} remain unspecified for the \isi{reference category}, but for the other two categories \textit{y}\textsubscript{1}, \textit{y}\textsubscript{2}, \textit{η}\textsubscript{1}, \textit{η}\textsubscript{2}, \textit{a}\textsubscript{1}, \textit{a}\textsubscript{2}, \textit{b}\textsubscript{1} and \textit{b}\textsubscript{2} are required. Accordingly, the overall number of model parameters will be higher, compared to binary \is{regression!logistic}logistic regression. In (\ref{eq:6.3}), the \isi{logistic function} for transforming summed \isi{logits} back into proportions is shown for \is{regression!multinomial}multinomial models with three outcome categories (see \citealt{Long2015}: 176). In this scenario, the index \textit{i} takes two values, representing the alternative (non-baseline) categories.

\ea\label{bkm:Ref40425386}\label{eq:6.3}%
Logistic function for tricategorical \is{regression!multinomial}multinomial response models
\[y_i = \frac{\text{exp}(\eta_i)}{1+\text{exp}(\eta_1)+\text{exp}(\eta_2)}\]
\z

Summed \isi{logits} for different outcome categories are inserted into the equation, where they are exponentiated to obtain odds, then transformed to obtain proportions. The denominator is fixed, i.e. it always consists of the summed odds of all categories, including the \isi{reference category}, which is represented by the value 1. So it is only the numerator that varies, depending on which category is being estimated. If the baseline category is of interest, the numerator will take the value 1 (see the discussion of binary \is{regression!logistic}logistic regression above).

\subsection{Comments on random effects and centred predictors}\label{sec:6.3.4}

One property of \isi{logits} is that they take increasingly extreme values for underlying proportions close to \textit{p~}=~0 and \textit{p~}=~1, respectively. In \is{random effects}random-effects models with a nonlinear link function, this may have effects on the predicted probabilities that run counter to our intuitions, and to what we see in the raw data. In \tabref{tab:6.1}, let us assume that we compare proportions of some categorical outcome category (e.g. \textit{even though}) in five texts.  Proportions 1, 2 and 3 are very close to zero, proportion 4 is substantially higher (corresponding to 4.3\%), and proportion 5 is higher still (corresponding to 25.1\%). While the mean proportion is 0.063, a much lower mean results if we first average the \isi{logits} corresponding to proportions and then reconvert that average into a proportion. Instead of 0.063 (i.e. 6.3\%) we obtain only 0.019 (i.e. 1.9\%).

\begin{table}\tabcolsep=.75\tabcolsep
\caption{\label{bkm:Ref40709736}\label{tab:6.1}The distorting effect of averaging logits}
\begin{tabularx}{.95\textwidth}{l *{6}{S[table-format=-1.3]} Y}
\lsptoprule
    & \multicolumn{5}{c}{Text} & \\\cmidrule(lr){2-6}
    & {1} & {2} & {3} & {4} & {5} &   {\textit{M}}  \\\midrule
$p$ & 0.003 & 0.005 & 0.012 & 0.043 & 0.251  & 0.063 \\
\textsc{logit} & -5.81 & -5.29 & -4.41 & -3.10 & -1.09 & -3.94 & → $ p = 0.019$\\
\lspbottomrule
\end{tabularx}
\end{table}

That is, predicted probabilities in \is{random effects}random-effects models will be biased towards the extremes (0 and 1) if there is a nonlinear link function, and if the variance between groups is large~– usually because some groups (e.g. texts/speakers) behave near-categorically (see \citealt{MolenberghsVerbeke2005}: 299, \citealt{Agresti2013}: 495–498). Even if there are groups with considerably higher proportions of a variant, their effect on the overall outcome will be overruled by groups with very low proportions, because random intercepts (and slopes) are centred around the mean \is{logits}logit. Thus, there may be a marked contrast between predicted probabilities and what we see in the raw data.\footnote{Of course, model-based estimates will always differ more or less dramatically from what the raw data tell us, but one’s suspicions should be raised if there is a fundamental difference, not one of degree.}

As a strategy to cope with this phenomenon, \citet[301]{MolenberghsVerbeke2005} suggest that \is{conditional mean}conditional means (i.e. predicted percentages or proportions) should be established for different levels of a \is{random effects}random variable, and that these should then be averaged. As a practical solution, they further suggest that for each random coefficient, a large number of values should be randomly sampled from the distribution returned by the model (which is always a standard deviation attached to a mean of zero). For each randomly sampled value, the \isi{conditional mean} is calculated (as a proportion, or percentage), then the average of these values is established. In effect, we estimate the outcome (proportions\slash percentages) for a large number of (fictive) units and then show the average unit; by contrast, in the easier, default approach, we average values whilst still operating within the nonlinear link function. Since in a \is{Bayesian statistics}Bayesian approach the estimated standard deviation of a random effect will be different in each line of the \isi{posterior sample}, we need to conduct this \isi{random-sampling} approach for each line. The procedure is unproblematic, but computationally expensive.

A similar issue arises in the fixed part of binary \is{regression!logistic}logistic or \is{regression!multinomial}multinomial models with centred predictor variables. Take, for instance, the predictor \textsc{spoken.ct}, which in this study takes values of +1 (“spoken”) and $-1$ (“written”). If the average proportion of an outcome is 0.05 in writing and 0.35 in speech, the estimated average proportion that should result if we constrain the predictor to take its centred (or neutral) value of zero is \textit{p}~=~0.20, as shown in \tabref{tab:6.2}.

\begin{table}
\caption{\label{bkm:Ref74562473}\label{tab:6.2}Skew introduced by centred predictors}
\begin{tabular}{l *3{S[table-format=-1.2]} c}
\lsptoprule
                    & {written} & {spoken} &  {\textit{M}}  & \\\midrule
$p$            & 0.05 & 0.35  & 0.20 &   \\
\textsc{logit} & -2.94 & -0.62  & -1.78 & → $p=0.14$\\
\lspbottomrule
\end{tabular}
\end{table}

However, since the model operates on the \is{logits}logit scale and since we use a centred predictor, the mean value that we obtain on the basis of \textsc{spoken.ct} will introduce a skew. The value in the example is $-1.78$ and yields a proportion of \textit{p}~= 0.14 when reconverted. That is, average values in regression models with nonlinear link functions tend to be biased towards the extremes (\textit{p}~=~0; \textit{p}~= 1) if we rely on the centred (zero) value of a predictor. The strategy adopted in this book to counteract this effect was to make concrete posterior predictions on the percentage scale for all specific conditions, and then to average across them. The technical details can be traced in the published analysis scripts (see \sectref{sec:1.4}).

\subsection{\label{bkm:Ref74482113}Estimation and visualisation}\label{sec:6.3.5}

Each item in the \isi{posterior sample}, then, consists of one complete set of values for all model parameters. In the \isi{estimation} process, we calculate one outcome percentage for each line of the \isi{posterior sample}, based on the predictor values for the condition of interest. It is also possible to calculate values for two conditions and directly subtract one from the other~– again this happens line by line, yielding a distribution of possible values. We can summarise these values in various ways, e.g. by reporting their means and standard deviations, or their medians along with certain quantiles. These values can then be plotted and interpreted: We can make a statement about the most likely, \is{central tendency}central outcome value, and we frame intervals that contain the central 50\% or 90\% of values, for example. In this study, these are called \textit{\isi{uncertainty} intervals} and can be regarded as analogues to \is{frequentist statistics}frequentist \is{confidence intervals}\textit{confidence intervals} \citep[23]{Agresti2013}, but see \citet{GelmanGreenland2019} for a terminological discussion. Uncertainty intervals can also be calculated for individual parameters (coefficients) if we are interested in their precision. In the text, a 90\% \is{uncertainty intervals}uncertainty interval will sometimes be stated along with a \isi{point estimate} (i.e. the “best guess” for a value of interest). Thus, in \sectref{sec:10.2.2}, the proportion of \textit{even though} in \is{concessives (types of)!anticausal}anticausal CCs is given as “54.5\% [43.9; 64.9]” for CanE, which indicates that in this variety the median estimate is 54.5\%, with a 90\% \is{uncertainty intervals}uncertainty interval extending from 43.9\% to 64.9\%.

\figref{fig:6.4} illustrates what \isi{conditional effects plots} look like in this book. The \isi{point estimate} shown as a filled black circle at the centre of the distribution represents the median \is{Bayesian statistics!posterior distribution}posterior predicted probability, i.e. the median of all values sampled from the \is{Bayesian statistics!posterior distribution}posterior for a given condition~– this would be the most typical outcome value. The two intervals extending upwards and downwards from the median represent 50\% and 90\% \isi{uncertainty intervals}: The thicker bar covers values between the 25\textsuperscript{th} and 75\textsuperscript{th} \isi{percentiles} and the thinner bar extends to the 5\textsuperscript{th} and 95\textsuperscript{th} \isi{percentiles} of the posterior predicted probability of the outcome.

\begin{figure}
\includegraphics{figures/CCs.Fig.6.4.pdf}
\caption{\label{bkm:Ref74567287}\label{fig:6.4}Typical plotting symbols used in the visualisation of results}
\end{figure}

In all analyses in this book, outcomes and effects will be expressed in readily understood quantities, i.e. rates of occurrence and percentages of variant outcomes, the latter being based on predicted probabilities (e.g. \citealt{Long2015}: 173). In the comparison of conditions, differences and ratios will be used. I follow \citet{BestWolf2015}, who advise against using \isi{logits} or \isi{odds ratio}s when interpreting \is{regression!logistic}logistic regression models, since, in isolation, they show little more than the direction (and general strength) of an effect. Odds ratios may be easier to interpret than \isi{logits}, but they, too, do not make the magnitude of an actual change in probabilities between conditions transparent \citep[188]{Long2015}. Presentations of results in this study will thus consist of back-transformed outcomes that are appropriately visualised (cf. \citealt{BolstadCurran2017}: 31), using the \is{R} packages \is{R packages!lattice}\textit{lattice} (\citealt{Sarkar2008,Sarkar2021}) and \is{R packages!latticeExtra}\textit{latticeExtra} (\citealt{SarkarAndrews2019}). Regression tables are relegated to the online appendix as they contribute little to our understanding of actual patterns (cf. \citealt{GelmanHill2007}: 457). Rather than \textit{p}{}-value significance testing of individual coefficients, more informative \isi{estimation} approaches are used (\citealt{GelmanHill2007}: 22–23, \citealt{CummingCalin-Jageman2017}).

\subsection{\label{bkm:Ref35244287}\label{bkm:Ref41305990}\label{bkm:Ref41473424}Specification of variables and model selection}\label{sec:6.3.6}

According to \citet[241–242]{Baayen2008}, the distinction between fixed and random effects is not primarily about grouping structures, but about “repeatable levels” (i.e. predictor levels/values that are known) and factors “with levels randomly sampled from a much larger population”. Mixed-effects models include both types of variables. \tabref{tab:6.3} lists the variables used in this book, ordered according to the three types “outcome”, “predictor” (i.e. fixed-effect), “control” and “random”. It also specifies which variables are relevant in which chapter(s), as well as the values each of them can take.

\begin{table}
\caption{\label{tab:6.3}Specification of variables; for uncentred categorical variables, baseline categories are stated first, predicted categories and non-baseline predictor levels appear in bold print.}
\tabcolsep=0.7\tabcolsep\is{finite clauses}\is{concessives (types of)!dialogic}\is{concessives (types of)!dialogic}\is{concessives (types of)!anticausal}\is{final position}\is{final position}\is{length (of clauses)}
\begin{tabularx}{\textwidth}{llcccccQ}
\lsptoprule
                   &                        & \multicolumn{5}{c}{Chapters} &\\\cmidrule(lr){3-7}
                   & Variable               & 7 & 8 & 9 & 10 & 11 & Values/levels\\\midrule
\multicolumn{2}{l}{Outcome}\\
                   & \textsc{count}         & • & • &   &    &    & logged frequency (pmw): ln(\textit{f})\\
                   & \textsc{final}         &   &   & • &    &    & nonfinal, \textbf{final}\\
                   & \textsc{marker}        &   &   &   & •  &    & \textit{although}, \textbf{\textit{though}}, \textbf{\textit{even though}}\\
                   & \textsc{nonfin}        &   &   &   &    & •  & finite, \textbf{nonfinite}\\\midrule
\multicolumn{2}{l}{Predictor}\\
                   & \textsc{spoken.ct}     & • & • & • & •  & •  & written = $-$1; spoken = +1\\
                   & \textsc{marker}        & • &   &   &    & •  & \textit{although}, \textbf{\textit{though}}, \textbf{\textit{even though}}\\
                   & \textsc{type}          &   & • &   &    &    & dialogic, \textbf{anticausal}, \textbf{narrow-scope}, \textbf{epistemic}\\
                   & \textsc{anti.ct}       &   &   & • & •  & •  & dialogic = $-$1; anticausal = +1\\
                   & \textsc{final.ct}      &   &   &   & •  & •  & nonfinal~=~$-$1; final~=~+1\\\midrule
\multicolumn{2}{l}{Control}\\
                   & \textsc{length.ct} &   &   & • &    &    & continuous; range = [$-$2.17; 1.41]; \textit{M}~= 0\\\midrule
\multicolumn{2}{l}{Random}\\
                   & \textsc{genre}         & • & • & • & •  & •  & 30–32 levels per variety\\
                   & \textsc{text}          &   &   & • & •  & •  & 177–262 levels per variety\\
\lspbottomrule
\end{tabularx}
\end{table}

The variable \textsc{length.ct} is labelled as a control variable, since no research interest is attached to it and it is not interpreted in detail. Actual values for this variable were in the range of 1–36 words. They were logged and centred round the \isi{geometric mean} of all tokens at 8.8 words. Thus, the shortest subordinate clause (\is{length (of clauses)}length~= 1 word) and the longest subordinate clause (\is{length (of clauses)}length~= 36 words) translate into values of $\textsc{\is{length (of clauses)}length.ct} = \ln(1) - \ln(8.8)=-2.17$ and $\textsc{\is{length (of clauses)}length.ct} = \ln(36) - \ln(8.8)=1.41$, respectively. Possible values for all other variables are more straightforward, as documented in \tabref{tab:6.3}. For the five models that were used, \tabref{tab:6.4} provides a concise definition of the outcome variables, states the labels given to the models, and makes reference to the chapters and appendices relevant with regard to each model.

\begin{table}
\caption{\label{bkm:Ref74568422}\label{tab:6.4}Model designations}
\begin{tabular}{llrl}
\lsptoprule
Outcome & Label & Ch. & Details\\\midrule
Rate of occurrence: Markers        & Model A & \ref{ch:7}  & Appendix~\ref{appendix:B.1}\\
Rate of occurrence: Semantic types & Model B & \ref{ch:8}  & Appendix~\ref{appendix:B.2}\\
Percentages: Clause positions      & Model C & \ref{ch:9}  & Appendix~\ref{appendix:B.3}\\
Percentages: Markers               & Model D & \ref{ch:10} & Appendix~\ref{appendix:B.4}\\
Percentages: Clause types          & Model E & \ref{ch:11} & Appendix~\ref{appendix:B.5}\\
\lspbottomrule
\end{tabular}
\end{table}

\citet[210]{Agresti2013} discusses \isi{model selection} strategies that build more complex models up from more basic ones (“\is{regression!forward}forward”) and strategies that start from maximally complex models and reduce them in a stepwise fashion (“\is{regression!backward}backward”). He concludes that both approaches can be problematic and may yield suboptimal models, and that selection procedures for variables should be applied with due caution:

\begin{quote}
[S]tatistical significance should not be the sole criterion for inclusion of a term in a model […]. It is sensible to include a variable that is central to the purposes of the study and report its estimated effect even if it is not statistically significant. Keeping it in the model may help reduce bias in estimated effects of other predictors and may make it possible to compare results with other studies where the effect is significant, perhaps because of a larger \isi{sample size}. Algorithmic selection procedures are no substitute for careful thought in guiding the formulation of models.
\end{quote}

\citet[276–279]{Winter2020} provides excellent arguments along similar lines, and the present study follows this kind of thinking: A carefully selected set of variables is included and kept in the model even if one or several of them have only a small effect and/or come with a high degree of uncertainty and would therefore be non-significant in a frequentist model (see also \citealt{Tizón-CoutoLorenz2021}). Thus it is ensured that the link between the theoretical background of the study and the results remains unbroken at all times, even if this means that the presented models are usually not the most parsimonious ones. In order to enable comparison in the present study, it is crucial that models should have the same structure across varieties.

