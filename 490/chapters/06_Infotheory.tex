%*****************************************
\chapter{Information-theoretic account of topic drop usage}
\label{ch:infotheory}
%*****************************************

To explain the usage of topic drop, I propose an account that builds on concepts from information theory.%
%% Footnote
\footnote{Parts of this chapter appeared in a similar form in \citet{schafer2021}, such as the presentation of the \textit{UID hypothesis} with the \textit{avoid troughs} and \textit{avoid peaks} principles, its application to topic drop, and the additionally assumed concept of the \textit{facilitate recovery} principle.
However, they were specified and extended.}
%
According to this account, the choice between topic drop and the corresponding full form is guided by the predictability \is{Predictability} and recoverability of the preverbal constituent.
If the constituent is predictable from the preceding context or if it can be easily recovered, \is{Recoverability} e.g., through the following verb, speakers are more likely to omit it because this reduces the processing effort for the hearer. \is{Processing effort}

In order to be able to expound my account, I first provide an overview of information theory and its most important concepts.
In the second step, I turn to the influential \textit{uniform information density hypothesis} (\textit{UID}) proposed by \citet{levy.jaeger2007}, which is the basis for my information-theoretic account.
I discuss two central predictions that follow from this hypothesis and their implications for topic drop: the principle of avoiding troughs, i.e., regions of low information, and the principle of avoiding peaks, i.e., regions of high information.
In addition to the \textit{UID hypothesis}, my information-theoretic account considers recoverability \is{Recoverability} as a further factor that impacts the usage of topic drop.
More specifically, I build upon the definition of recoverability developed in Chapter \ref{ch:recover} to further expand the link to processing effort. \is{Processing effort} \is{Recoverability}

\section{Information theory: basic concepts}\label{sec:infotheory.basic}
Information theory is a mathematical theory of communication based on Claude Shannon's seminal 1948 paper \textit{A mathematical theory of communication} \citep{shannon1948}.
Shannon addresses the question of how signals of information can be accurately and efficiently transmitted from a technical, engineering perspective \citep[see also][]{weaver1949}.
He explicitly states that ``semantic aspects of communication are irrelevant to the engineering problem'' \citep[379]{shannon1948}.
At first glance, this statement seems to call into question whether information theory can be applied at all to linguistics in general and semantics, pragmatics, and psycholinguistics in particular, where the focus is on the meaning of messages exchanged in communication.
While the concept of information that underlies Shannon's information theory is indeed distinct from that used in linguistics (see below), already \citet[24--28]{weaver1949} notes that the technical problem of communication overlaps with semantic and pragmatic issues and that information theory could also be applied to them.
For example, he proposes the application of Markov processes, a special kind of stochastic process \citep[385]{shannon1948}, to handle the impact of context on meaning \citep[28]{weaver1949}, and suggests understanding the channel capacity,\is{Channel capacity} the amount of information that can be successfully sent over a channel (see below), not only as a technical limitation but also as a cognitive limitation on the part of the audience \citep[26--27]{weaver1949}.%
%% Footnote
\footnote{This anticipates the reinterpretation of the channel capacity \is{Channel capacity} as a limit to the hearer's cognitive resources, which I assume in this book.
I come back to this below.} 
Consequently, there has been a  fruitful application of information-theoretic concepts in linguistic and psycholinguistic research, with beginnings going back even before \citet{shannon1948} and his formalization, such as \citeg{zipf1935} famous law about the inverse relationship between word length and word frequency \citep[see also][]{mandelbrot1965}.
Before discussing the application of information theory to topic drop, I first introduce the most central concepts of information theory.

\subsection{Information} \is{Information|(}
In linguistics or everyday language use, the concept of \textit{information} is usually closely related to meaning.
For example, the dictionary Merriam-Webster defines information as ``knowledge that you get about someone or something; facts or details about a subject'' \citep{information.webster}.
In linguistic terms, ``[t]o inform a person of something'' can be considered ``to induce a change in that person's knowledge state by adding one or more propositions'' \citep[44]{lambrecht1994} (this view is related to the concept of common ground \is{Common ground} by \citet{stalnaker1974,stalnaker1978, stalnaker2002}, see Section \ref{sec:topicality.lit} for a discussion).
\citet[43]{lambrecht1994} points out that whether the meaning expressed by an utterance is informative depends on the communicative situation, i.e., on whether it changes the hearer's mental representation of the world or not.
In this sense, then, information is related to meaning because for a proposition to change the hearer's knowledge, it needs to be in one way or another ``about'' the world.
In information theory, however, \textit{information} is a purely probabilistic concept, independent of facts about the world.
An event $x$, in particular an occurrence of an expression, is more informative the less probable it is.

Following \citet{shannon1948}, the information content ($I$), also termed Shannon information or surprisal ($S$) \citep{samson1953,attneave1959,hale2001},%
%%Footnote
\footnote{The term \textit{surprisal} was reintroduced into linguistic research by \citet[159]{hale2001}.
While Hale attributes it to \citet[6]{attneave1959}, Attneave himself points to \citet[293]{samson1953}, who seems to be the creator of the term in this usage.
In this book, I use the terms (Shannon) information and surprisal interchangeably.}
%
of an event $x$ is mathematically defined as the negative logarithm to the base $2$ of the probability of $x$:

%\vspace{-1\baselineskip}
\begin{equation}
\label{eq:information}
I(x) = S(x) = \log_{2}\ \frac{1}{p(x)} = \mathbin{-}\log_{2}\ p(x) 
\end{equation}

\noindent
Information or surprisal is measured in the unit \textit{bits}.
$1\ bit$ corresponds to the amount of information that is contained in a choice between two equally possible options \citep[380]{shannon1948}, like, e.g., a coin flip with the two options heads and tails, as shown in equation \ref{eq:information.coin}.

%\vspace{-1\baselineskip}
\begin{equation}
\label{eq:information.coin}
I(\text{coin flip}) = \mathbin{-}\log_{2}\ \frac{1}{2} = 1\ bit
\end{equation}

\noindent
In particular in linguistics, the formula is often modified in the following way to express that the probability of $x$ usually depends on its context \citep[see also][1130]{levy2008}:

%\vspace{-1\baselineskip}
\begin{equation}
\label{eq:information.context}
I(x) = S(x) = \mathbin{-}\log_{2}\ p(x\mathbin{|}\textit{context})
\end{equation}

\noindent
According to \citet[379--380]{shannon1948}, using the logarithmic measure instead of bare probabilities is ``practically more useful'', more intuitive, and simplifies mathematical operations. 
The inverted polarity in \eqref{eq:information} and \eqref{eq:information.context} has two logical implications as \citet[156]{lemke2021} points out.
First, this way, the surprisal never becomes negative and, second, it ensures that the amount of surprisal decreases with the likelihood of the event $x$.

This event $x$ can be any linguistic unit, from very basic units like a phoneme, a morpheme, or a syllable, to larger units like a word or a phrase, to complete utterances or messages, which I use as examples below.
By message, I mean an abstract preverbal, nonlinear, and propositional representation of what a speaker wants to communicate \citep[8]{konopka.brown-schmidt2014}.
For example, \citet[101]{levelt1989} expresses the message underlying the question \ref{ex:message.q} formally as \ref{ex:message.f}, using a bracket structure to indicate thematic roles, tense, and sentence mood (see also \cite[9]{konopka.brown-schmidt2014}, for a review on further approaches to how messages are structured).

\ex.
\a.\label{ex:message.q} Did John fall?
\b.\label{ex:message.f} ?(PAST(FALL(JOHN))) \citep[101]{levelt1989}

The concrete lexicalization of a message is termed a \textit{signal} in information-theore- tic terms and corresponds to an utterance in practice, e.g., in the experimental studies of the following chapters.
In the following, I surround utterances or signals with quotation marks and set messages in capitals.

I illustrate the concept of surprisal with the following toy example:
Imagine after several years of relationship, person A decides to propose to person B and asks, ``B, will you marry me?''.
Person B now has several options to answer, of which YES and NO are probably the most prominent messages in this highly ritualized conversational situation.%
%% Footnote
\footnote{As \citet[90]{levelt1989} points out, it is not straightforward how the messages underlying the utterances ``yes'' and ``no'' would actually look like in a formal representation such as the one proposed by him.
\citet{reich2003} formalizes them by means of two rhetorical relations \texttt{agree} and \texttt{disagree}, which function as binary operators and include a context variable $\Gamma$, which anaphorically takes up the meaning of the preceding question.
An answer ``yes'' to the question ``B, will you marry me?'' would then be formalized as \texttt{agree}($\Gamma$, that B will marry A),  additionally presupposing that $p$ = \textit{that B will marry A} is already given.
See \citet[189--195]{reich2003} for more details.
However, for the sake of simplicity, I refrain from using this formalization here and represent the answer particles just like the corresponding utterances.}
%
Normally, one can assume that A will propose to B only if A is relatively sure that B will say ``yes''.
Therefore, YES is the more likely and less informative message, we could say:

\begin{equation}
\begin{gathered}
\phantom{\Rightarrow\ } p(\text{YES}\mathbin{|}\text{proposal}) >  p(\text{NO}\mathbin{|}\text{proposal})\\
\Rightarrow \mathbin{-}\log_{2}\ p(\text{YES}\mathbin{|}\text{proposal}) < \mathbin{-}\log_{2}\ p(\text{NO}\mathbin{|}\text{proposal})
\\
\Rightarrow I(\text{YES}) < I(\text{NO})
\end{gathered}
\end{equation}

\noindent
By contrast, the message NO would be the less likely answer in this situation, having greater Shannon information or higher surprisal.

Compare the proposal situation to a situation where the couple wants to have dinner at a diner.
A waiter comes to the table and asks: ``What can I get you?''
Assume for the sake of simplicity that it is the couple's favorite diner, where they like all 20 dishes on the menu equally, and that the waiter knows this.
Then all messages related to dishes on the menu are about equally likely (\sfrac{1}{20} in this case) and thus equally informative.
Suppose that person A is not very hungry and therefore orders only one serving of fries with the message SPEAKER(WANT (FRIES)).
The information content of this message would be:
\begin{equation}
\begin{gathered}
\mathbin{-}\log_{2}\ p(\text{SPEAKER(WANT(FRIES))}\mathbin{|}\text{main dish order}) \\
= \mathbin{-}\log_{2}\ \frac{1}{20} \approx 4.3\ bits
\end{gathered}
\end{equation}

\noindent
Assume further that person B orders a cheeseburger and is asked which side dish should be served with it, fries or coleslaw.
Suppose that B takes fries half the time and coleslaw the other half.
Then the messages SPEAKER(WANT(FRIES)) and SPEAKER (WANT(COLESLAW)) would both have a probability of about \sfrac{1}{2} in the context of ordering the side dish.
If B responds with basically the same message as A, i.e., SPEAKER(WANT(FRIES)), the surprisal of this message would be:
\begin{equation}
\mathbin{-}\log_{2}\ p(\text{SPEAKER(WANT(FRIES)}\mathbin{|}\text{side dish order}) = \mathbin{-}\log_{2}\ \frac{1}{2} = 1\ bit
\end{equation}
\noindent
Thus, the same message is less informative in the side dish situation than in the main dish situation because the number of (equally likely) competing messages is smaller, and the probability that this message will be selected is higher.

In summary, the surprisal of a message is higher the less likely the message is.
This means that in the special case where all possible messages are equally likely, the surprisal increases with the number of competing messages.
If there is only one possible message, its probability is $1$, thus, the surprisal in this case is 0 -- the message conveys no information.
\is{Information|)}

\subsection{Communication and channel}
According to Shannon, ``[t]he fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point'' \citep[379]{shannon1948}.
He illustrates this process schematically by presenting the general communication system in Figure \ref{fig:communication.system} \citep[381]{shannon1948}.%
%% Footnote
\footnote{I recreated the figure in Microsoft Word.}

In communication, an information source first selects a message to be communicated from a set of possible messages.
This message is then converted into a signal in some way by the transmitter and sent over a channel with a limited capacity. \is{Channel capacity|(} 
The signal is received and decoded back into a message by the receiver.
Finally, the corresponding message gets to its destination, i.e., the intended addressee.

Referring to \citet[7]{weaver1949}, I illustrate \citeg{shannon1948} model using (spoken) communication.
Suppose we are in the restaurant situation described above, and the speaker wants to tell the waiter their order.
The information source is the speaker's brain, which chooses a message encoding the desired dish from a set of possible messages, e.g., (SPEAKER(WANT(FRIES)):

\begin{equation}
\begin{gathered}
m = \{\text{(SPEAKER (WANT(FRIES)), (SPEAKER(WANT(BURGER)),}\\ \text{(SPEAKER(WANT(PIZZA)), ...}\}
\end{gathered}
\end{equation}

This message is then transferred by the articulatory system, which functions as the transmitter, into a concrete signal consisting of sound waves, such as ``Fries, please''.%
%% Fußnote
\footnote{This is of course an extremely simplified description of the complex process of speech production.
It disregards important aspects of speech production like how a message is conceptualized, how the message is actually transferred into linguistic units, or how these units are arranged relative to each other.
This is, however, not the place to discuss them in detail.
See, e.g., \citet{goldrick.etal2014} for detailed overviews of these topics.}
%
The sound waves are sent through the air, the channel, and get to the waiter's ear, the receiver.
Here the sound signals are decoded by the auditory system and then reach the waiter's brain, the destination.

\begin{figure}
\begin{center}
\includegraphics[scale=0.75]{figures/Communication_Model.PNG}
\caption{Scheme of a general communication system according to \citet{shannon1948}, recreated from \citet[381]{shannon1948}}
\label{fig:communication.system}
\end{center}
\end{figure}
%\vspace{-1\baselineskip}

There are two properties of the communication channel that need to be discussed as they are important for the successful and efficient transmission of information.
First, a communication channel can be noisy. \is{Noisy channel|(}
This means that during transmission, the signal may get corrupted by noise so that the received signal is distorted or erroneous compared to the original signal \citep[406]{shannon1948}.
In the ordering example, the restaurant may be crowded with many people chattering and clattering dishes and cutlery.
This noise can distort the signal as it is transmitted from the speaker to the hearer, so that instead of ``Fries, please'', the waiter might understand the signal as something else, such as ``Rice, please''. 
To avoid such a misunderstanding, speakers can mitigate the effect of a noisy channel by adding redundancy to the signal \citep[410]{shannon1948}, e.g., they could say ``I would like to have a serving of your golden, crispy French fries please''.
The additional adjectives increase the chance that the waiter will understand the intended message correctly.

The second relevant property of a communication channel is that it has a certain capacity, \is{Channel capacity} i.e., ``the maximum possible rate of transmission'' \citep[410]{shannon1948} at which signals can be sent over the channel.
This capacity is crucial for \citeauthor{shannon1948}'s (\citeyear{shannon1948}) theorem of communication over a noisy channel. \is{Channel capacity} 
If the source sends bits per second at a rate that is equal to or lower than the channel capacity, \is{Channel capacity} the error rate in the signal can become arbitrarily small with suitable coding systems (\cite[411]{shannon1948}; see also \cite[21]{weaver1949}).
This, however, is not possible if the bits per second sent by the source exceed the channel capacity.
Any attempt to transmit at a rate higher than the channel capacity \is{Channel capacity} will result in an even greater increase in the error rate \citep[410]{shannon1948}.
It follows that communication over a noisy channel is guided by a trade-off between communicating efficiently, i.e., sending as much information \is{Information} as possible, and being understood, i.e., preventing as many errors as possible.
Therefore, the optimal transmission rate should be close to the channel capacity \is{Channel capacity} so that as many bits per second as possible can be transmitted, but it should not exceed it to prevent errors in the signal. \is{Noisy channel|)}

Above, I already briefly mentioned \citeg{weaver1949} idea of a reinterpretation of this technical concept of channel capacity. \is{Channel capacity} 
Weaver assumes that also the audience, i.e., the hearers or addressees in communication, have a certain capacity and that ``if you overcrowd the capacity of the audience you force a general and inescapable error and confusion'' \citep[27]{weaver1949}.
This sensible transfer, which Weaver proposed just one year after the publication of \citeg{shannon1948} seminal paper, can be summarized as follows in light of the information-theoretic concepts already discussed:
Given that surprisal is known to index processing effort  \is{Processing effort} \citep{hale2001,levy2008, demberg.keller2008}, the channel capacity \is{Channel capacity} can be interpreted as an upper bound to the processing resources \is{Processing effort} that are available to the hearer for language understanding (\cite[254]{lemke2021}; \cite[see also][]{fenk.fenk1980}).
That is, there is some sort of hearer- and probably also situation-dependent threshold that determines the amount of surprisal a hearer can process with the cognitive resources available to them. \is{Channel capacity}
If the amount of surprisal contained in an utterance is too high, this capacity is exceeded, i.e., the hearer's cognitive resources are no longer capable of processing all of the input, which results in, e.g., longer processing times and additional comprehension difficulties. \is{Processing effort}
While an utterance such as ``Fries, please'' is not likely to exceed the hearer's channel capacity \is{Channel capacity} in the ordering situation, the utterance ``Can I also have sauce for home?'' might.
In the context of the utterance situation, this question is less predictable \is{Predictability} than an utterance that contains an order.
It could be that it leads to an overload of the waiter's processing capacities \is{Processing effort} resulting in a delayed response or a clarification question such as ``So, you are asking whether you can buy or get some of our burger sauce to use at home?''.
Speakers can minimize such processing difficulties if they construct their utterances to conform to the so-called \textit{uniform information density hypothesis}. \is{Processing effort} \is{Uniform information density}

\is{Uniform information density|(}
\section{\textit{Uniform information density} (\textit{UID})}\label{sec:info.theory.uid} \is{Processing effort|(}
The central idea of the \textit{uniform information density hypothesis} (\textit{UID}) proposed by \citet{levy.jaeger2007} and \citet{jaeger2010} is that rational speakers strive to optimize their production and transmit surprisal as uniformly as possible to their hearers.
This is made concrete in \citeauthor{jaeger2010}'s (\citeyear[25]{jaeger2010}) definition of \textit{UID}:
\begin{quote}
Within the bounds defined by grammar, speakers prefer utterances that distribute information \is{Information} uniformly across the signal (information density). Where speakers have a choice between several variants to encode their message, they prefer the variant with more uniform information density (ceteris paribus)
\end{quote}
Thereby, information density (ID) corresponds to Shannon information or surprisal per linguistic \is{Information} unit \citep[849]{levy.jaeger2007} or per time \citep[25]{jaeger2010}, as illustrated in Figure \ref{fig:UID}, which shows three hypothetical ID profiles.%
%% Footnote
\footnote{All figures with ID profiles in this book were created using the package ggplot2 \citep{wickham2016} in R.}
%

\begin{figure}
\centering
\includegraphics[scale=1]{UID3_wider.pdf}
\caption{Hypothetical information density profiles showing (A) a uniform distribution of surprisal close to channel capacity,   (B) a uniform but underchallenging distribution of surprisal, and (C) a non-uniform distribution of surprisal}
\label{fig:UID}\is{Channel capacity}
\end{figure}

The strategy described by \textit{UID} of distributing surprisal uniformly, or more precisely efficiently \citep{jaeger2010}, across an utterance is a direct consequence of the trade-off described above between sending as much surprisal as possible over the available channel and minimizing the error rate.
Both the profile in (A) and the profile in (B) are uniform in that they avoid an alternation between areas of low surprisal, so-called \textit{troughs}, and areas of high surprisal exceeding the channel capacity, \is{Channel capacity} so-called \textit{peaks} \citep[849]{levy.jaeger2007}.
This way, they contrast with the profile in (C), which is much more variable and has visible troughs and peaks, some of which even exceed the hypothetical channel capacity, \is{Channel capacity} i.e., the hearer's processing capacities (see above).
But there is also a clear difference between profile (A) and profile (B):
(A) is more efficient because the transmission rate is much closer to the channel capacity \is{Channel capacity} than in (B).
The profile in (A) follows the principle of exploiting the channel capacity \is{Channel capacity} by sending as much surprisal as possible, whereas in (B) available resources are wasted and the hearer is underchallenged.

With respect to \citeg{jaeger2010} \textit{UID} definition, another point must be stressed.
\textit{UID} is said to operate only ``within the bounds defined by grammar''.
This means that utterances are constrained not only by the distribution of surprisal but also by grammatical rules that a priori exclude variants from the set of possible utterances that are more optimal concerning the distribution of surprisal but are ungrammatical.
For topic drop, this means that topic drop is only a valid alternative if it is grammatically licensed, i.e., if it fulfills the licensing conditions sketched in the first part of this book.

It is important to emphasize that while the work by \citet{levy.jaeger2007} was highly influential during the rise of information-theoretic approaches in linguistics in the last 20 years, they were not the first ones who developed the idea behind \textit{UID}.
Already in 1980, \citet{fenk.fenk1980} proposed what was later discussed as the \textit{principle of constant information flow} \citep[536]{fenk-oczlon1989}.
According to this principle, the lossless transmission of messages requires first an average redundancy level that does not exceed the hearer's short-term memory capacity and, second, that the information \is{Information} is distributed as uniformly as possible over short time intervals \citep[402]{fenk.fenk1980}.
\citet[402]{fenk.fenk1980} illustrate the latter point by stating that areas with maximal information exceed the hearer's memory capacity while shortfalls of information \is{Information} would not use the available capacity \citep[see also][38]{fenk-oczlon1990}.
This is very similar to \textit{UID}.
Both principles say that speakers should communicate efficiently by distributing information at a constant rate without peaks and troughs.
That is, such a uniform distribution is ``an optimal solution to the problem of \textit{low-effort comprehension}'' \citep[851, original emphasis]{levy.jaeger2007} because it minimizes the comprehension difficulty for the hearer.
There is, however, a difference between \textit{UID} and the \textit{principle of constant information flow} for peaks and the capacity limit.
According to \textit{UID}, speakers should by all means avoid exceeding the channel capacity \is{Channel capacity} and smooth surprisal peaks by adding redundancy before the expression that causes the peak.
For \citet[403]{fenk.fenk1980}, in contrast, short-term peaks of surprisal are basically acceptable, as long as they are followed by redundant passages, which allow the hearer to catch up on processing the information excess.
This means that according to \textit{UID}, speakers should communicate \textit{below} channel capacity \is{Channel capacity} because any exceedance would overuse the processing capacities of the hearer.
In contrast, according to the \textit{principle of constant information flow}, speakers can communicate \textit{around} the capacity because short-term peaks can be compensated for by catch-up processing.%
%% Footnote
\footnote{This also leads to different predictions with respect to topic drop:
While, according to \citet{fenk.fenk1980}, topic drop before a verb with a high surprisal should be easier to process if it is followed by a predictable \is{Predictability} expression that causes a trough, \textit{UID} does not predict such an effect.
In this book, I do not test such predictions motivated by \citeg{fenk.fenk1980} approach, but it may be beneficial to do so in future research.}
%

While \textit{UID} is explicitly defined to operate on all levels of linguistic representation \citep[24]{jaeger2010}, there are related hypotheses that have been applied to certain linguistic units only.
\citet{genzel.charniak2002, genzel.charniak2003} propose the \textit{entropy rate constancy principle} for whole texts and their constituting sentences, according to which it is optimal to communicate at a constant rate that is equal to the channel capacity. \is{Channel capacity} 
They evidence the validity of this principle for written texts in several languages.
In the area of phonetics, Aylett and Turk suggest the \textit{smooth signal redundancy hypothesis}, according to which redundancy is distributed uniformly across speech \citep{aylett.turk2004, turk2010}.
They argue that what they term language redundancy and acoustic redundancy are inversely related to each other.
Predictable \is{Predictability} linguistic units such as syllables are articulated less strongly than less predictable syllables in terms of phonetic features such as duration and prosodic prominence. \is{Prosody}

What these presented hypotheses have in common is that they are based on the information-theoretic principle that information, surprisal, or redundancy \is{Information} is best distributed uniformly across utterances, avoiding too much variation in the information density (ID) profile, i.e., the overall distribution of surprisal across an utterance.
In what follows, I refer to the \textit{UID hypothesis} for this idea since it has been applied decidedly to other phenomena involving optional omissions in various languages, such as the omission of relativizers \citep{levy.jaeger2007}, complementizers \is{Complementizer omission} \citep{jaeger2010, kaatari2016}, discourse connectives \citep{asr.demberg2015,yung.etal2016}, and verb phrases \is{Verb phrase} \citep{schafer.etal2021} in \ili{English}, the use of contractions \citep{frank.jaeger2008} and short forms of nouns \citep{mahowald.etal2013} in \ili{English}, null subjects \is{Null subject} in Russian \il{Russian}\is{Argument omission} \citep{kravtchenko2014}, optional object case-marking in \ili{Japanese} \citep{kurumada.jaeger2015}, complementizer omission \is{Complementizer omission} in Quebec \ili{French} \citep{liang.etal2021}, the use of article omissions \is{Article omission} \citep{lemke.etal2017}, fragments \is{Fragment} \citep{lemke2021}, and sluicing \is{Sluicing} \citep{lemke.etal2022} in German, as well as the reduction of compounds in German \citep{zarcone.demberg2021}.
Since I am concerned with topic drop in this book, i.e., the omission of constituents, mostly of one-word proforms, I explain the predictions of \textit{UID} at the word level in the following, but they can be easily applied to other levels as well.

There are two general principles concerning the ID profile that can be derived from \textit{UID} and that speakers should follow to achieve efficient communication: \textit{avoid troughs} and \textit{avoid peaks}.
To avoid troughs on the utterance level, speakers can omit predictable \is{Predictability} words, i.e., words with low surprisal.
Such an omission is beneficial because redundant words cause undesirable minima of surprisal, i.e., regions way below channel capacity. \is{Channel capacity} 
Therefore, the omission increases efficiency because it avoids that processing resources available to the hearer are wasted.
Peaks in the ID profile, i.e., regions exceeding channel capacity, \is{Channel capacity} can be avoided by inserting additional redundancy before the peak.
A speaker can place a word or several words before the highly informative word that causes the peak to make that word more predictable. \is{Predictability}
This lowers the information maximum, i.e., it reduces the processing cost, ideally to the point where the peak falls below the channel capacity \is{Channel capacity} and the hearer's processing capacity \is{Processing effort} is not overloaded.
In what follows, I look more closely at both principles and discuss them for topic drop.

\subsection{\textit{Avoid troughs} principle}\label{sec:avoid.troughs}
Troughs in the ID profile are caused by predictable \is{Predictability} linguistic units, e.g., by words that are likely given the previous context and, thus, have low surprisal.
If speakers include a lot of such predictable words in their utterances, they communicate inefficiently because in these regions of low surprisal, the channel capacity, \is{Channel capacity} i.e., the hearer's processing capacity, is heavily underused.
Speakers can solve this issue and increase the efficiency of their utterances by omitting the predictable \is{Predictability} expressions where grammar permits it.
One way to do this is to use topic drop, if licensed, because it allows speakers to omit prefield constituents with low surprisal.
In the optimal case, an utterance with topic drop distributes surprisal more uniformly than the corresponding full form.
This is illustrated in Figure \ref{fig:tdtrough}, which shows the hypothetical ID profiles for an utterance with and without topic drop.%
%% Footnote
\footnote{Since the plots here serve to illustrate the difference in processing effort between predictable and unpredictable words, hypothetical values suffice.
Therefore, I also refrain from giving concrete numeric values, which would furthermore depend on the corpus and model used.
}
%

\begin{figure}
\begin{center}
\includegraphics[scale=1]{TDTrough_wider.pdf}
\caption{Hypothetical ID profile: Since \textit{ich} creates a surprisal trough, the utterance with topic drop is more uniform.}
\label{fig:tdtrough}
\end{center}
\end{figure}

On the x-axis, the words of the utterance \textit{(Ich) backe gerade Muffins} (`(I) am baking muffins right now') produced as an answer to the question \textit{Was machst du gerade?} (`What are you.\textsc{2sg} doing?') are plotted.
The y-axis shows hypothetical surprisal values for these words which together form the ID profile of the utterance.
The 1st person singular pronoun \textit{ich}, which refers to the speaker, is very predictable \is{Predictability} because the speaker is prominent both linguistically (through the question `What are you doing?') and extralinguistically (through the communication situation with speaker and hearer as default roles).
Consequently, \textit{ich} creates a trough in the ID profile of the full form, as indicated by the reddish curve.
In other words, the surprisal of \textit{ich} is far below the (hypothetical) channel capacity. \is{Channel capacity} 
To avoid this suboptimal trough in the ID profile, speakers can use topic drop and omit \textit{ich}, as shown in the blue curve.
In this way, surprisal is distributed more uniformly across the utterance, so that the hearer's processing capacity is used at a constant level.

The \textit{avoid troughs} principle can be considered a greedy strategy because it prevents the waste of cognitive capacities and increases efficiency. \is{Processing effort} \is{Channel capacity}
That is, a speaker should tend to avoid not only severe troughs but every possible trough, as long as (i) grammar permits it and (ii) the omission does not cause a surprisal peak on the subsequent material.
For topic drop, this predicts that the prefield constituent is omitted whenever (i) topic drop is licensed and (ii) this does not lead to a peak on the following verb (see Section \ref{sec:avoid.peaks}).
As I discuss in the next chapters, the tendency to avoid troughs in the ID profile can explain the impact of several grammatical factors on topic drop observed in the literature.

\subsection{\textit{Avoid peaks} principle}\label{sec:avoid.peaks}
In addition to the \textit{avoid trough} principle, \textit{UID} also predicts that speakers should avoid peaks of surprisal when communicating.
A surprisal peak is caused by an ``overinformative'' expression whose surprisal exceeds the channel capacity. \is{Channel capacity} 
The information content of this particular expression is so high that the hearer has difficulties processing the expression.
Such a peak can be smoothed by adding additional redundancy before the expression that causes the peak.
This additional redundancy serves the purpose of making this expression more predictable. \is{Predictability}
For topic drop, the \textit{avoid peaks} principle has the consequence that the omission of the prefield constituent should be depreciated if a verb with high surprisal would move to the sentence-initial position.
This is shown in Figure \ref{fig:tdpeak}.

\begin{figure}
\centering
\includegraphics[scale=1]{TDPeak3_wider.pdf}
\caption{Hypothetical ID profile: Since \textit{karamellisiert} creates a surprisal peak, the full form is more uniform.}
\label{fig:tdpeak}
\end{figure}

Suppose that A asks \textit{Was macht ihr gerade?} (`What are you.\textsc{2pl} doing?') and that instead of writing \textit{Backe gerade Muffins}, B would use an utterance with topic drop of a 3rd person singular subject such as \textit{Karamellisiert gerade Nüsse} (`Is caramelizing nuts').
First, the less frequent verb \textit{karamellisieren} should generally have a higher surprisal than \textit{backen}.
Second, we can assume that the surprisal of \textit{karamellisiert} is even high enough in the context of A's very general question to cause a peak in the ID profile that exceeds the channel capacity, \is{Channel capacity} as shown in the reddish curve.
This region of high information is expected to challenge A's processing resources.

Assume that although A asks the general question \textit{What are you doing?}, they do know that B and their flatmates C and D are just preparing a birthday cake for a common friend.
Assume further that C is known to top every cake they bake with nuts caramelized by themselves.
In this situation, it would be more beneficial for B not to use topic drop but to overtly realize C's name \textit{Kim} as the prefield constituent, as indicated in the blue curve.
This increases the likelihood of \textit{karamellisiert} for (i) structural reasons and (ii) because of A's knowledge about C's behavior.
(i) The proper name \textit{Kim} in the prefield position makes it almost certain that it will be followed by a congruent finite verb.%
%% Footnote
\footnote{Of course, this is also true for the utterance \textit{Backe gerade}, but it is of more relevance here because the surprisal of \textit{karamellisiert} is higher and likewise the need to lower it.
This means that indicating that a congruent verb will follow by inserting the subject into the prefield is more beneficial if processing this verb requires more effort. \is{Processing effort}}
%
(ii) The fact that C is passionate about caramelizing nuts increases the probability of \textit{karamellisiert} following C's name.
Therefore, inserting \textit{Kim} lowers the surprisal of \textit{karamellisiert} and the effort required to process the verb.
This effort is even reduced in a third way.
Using the full form instead of topic drop avoids the processing effort that would result from having to resolve the ellipsis.
In the case of an omitted 3rd person singular subject, this effort would even be higher than for the 1st or the 2nd person because the ellipsis could potentially refer to any 3rd person referent known or present in the discourse or the situation, in our example, at least to both C and D (see also Section \ref{sec:resolving}).

Finally, using the full form has another advantage for processing the verb.
If the speaker does not use an ellipsis, the hearer does not have to resolve it.
Thus, the effort associated with this process of ellipsis resolution can be saved.
Since under an assumption of an incremental parser \is{Parser|(} (see Section \ref{sec:resolving}), this effort would presumably also be incurred directly on the verb, the overt realization of the prefield constituent should in any case facilitate the processing of this verb.

Ideally, by inserting an overt prefield constituent, the surprisal associated with the verb falls below channel capacity. \is{Channel capacity|)} 
However, a reduction of the peak and the processing effort is desirable in any case.
From this reasoning, it follows that the surprisal of the following verb is a predictor of topic drop and that an effect of this predictor would provide genuine support for the information-theoretic account of topic drop usage.
Such support comes from my corpus study of text messages in Section \ref{sec:corpus.regression.rep} but could not be obtained in experiments \ref*{exp:surprisal} and \ref*{exp:surprisal.vt}, which are discussed in Sections \ref{sec:exp.surprisal} and \ref{sec:exp.surprisal.vt}.
\is{Uniform information density|)}

\is{Recoverability|(}
\section{\textit{Facilitate recovery} principle}\label{sec:resolving}
The \textit{avoid peaks} and the \textit{avoid troughs} principles are determined by predictability, \is{Predictability} as I illustrated for topic drop.
The prefield constituent is predictable \is{Predictability} from the linguistic or extralinguistic precontext.
The constituent itself, in turn, impacts the likelihood of the following verb.
However, the processing effort on this verb is not only associated with its predictability \is{Predictability} but also with the process of ellipsis resolution, which can only take place after the ellipsis is identified.
Assuming an incremental parser, \is{Parser} which uses any incoming data immediately to make a parsing decision \citep{marslen-wilson1973, marslen-wilson1975,altmann.kamide1999}, it is reasonable that topic drop is at least initially resolved directly on the subsequent verb \ref{ex:parser.decl}.
I say at least initially because in written discourse, due to the lack of prosodic cues,%
%% Footnote
\footnote{\citet[415]{zifonun.etal1997} state that in spoken discourse an empty prefield in declarative mood is indicated prosodically \is{Prosody} by a falling tone, among other things.
}
%
\is{Prosody} subsequent material may require a reanalysis of the apparent topic drop structure, e.g., as a polar question, when the parser \is{Parser|)} finds a postverbal subject \ref{ex:parser.quest}.%
%% Footnote
\footnote{The likelihood that such a reanalysis is necessary depends heavily on the context.
Also, it may generally be more likely for a verb inflected for the 2nd person to introduce a question than for a verb inflected for the 1st person.
In the case of a suitable syncretic \is{Syncretism} verb form, a reanalysis as imperative \is{Imperative} may also be possible \ref{ex:parser.syn} (see Section \ref{sec:usage.ambiguity.theory}).
%\vspace{-0.5\baselineskip}
\exg.\label{ex:parser.syn}Bring Muffins mit!\\
bring.\textsc{1sg.ind.prs}/\textsc{imp.sg} muffins with\\
`(I) bring muffins!' or `Bring muffins!'

%\vspace{-2em}
}%
\ex.\label{ex:parser}
\a. A: `What are you doing?'
\bg.\label{ex:parser.decl}B: $\Delta$ Backe gerade Muffins.\\
{} I bake now muffins\\
B: `(I) am baking muffins right now.'
\cg.\label{ex:parser.quest}B: Backe ich Muffins oder nicht? Rate mal!\\
{} bake I muffins or not guess.\textsc{imp.sg} \textsc{part}\\
B: `Do I bake muffins or not? Guess!'

On the verb, the processing load \is{Processing effort} increases with the difficulty of resolving topic drop at this point.
This resolution process can be facilitated by cues that make it easier to identify the omitted element: the \textit{facilitate recovery} principle.
In German, for instance, a distinct inflectional marking on the verb \is{Verbal inflection} provides information about the person and number of the congruent subject.
\citet[70--71]{sigurdsson.maling2010} argue that ``by reducing ambiguity, \is{Ambiguity} agreement morphology both facilitates and constrains interpretation or identification.''
A distinct inflectional\is{Verbal inflection}  marking can thus facilitate recovering an omitted subject, prevent an information peak, and reduce the overall processing costs for the verb.
This idea is illustrated graphically in Figure \ref{fig:uid.peak.verb}.

%\vspace{-0.5\baselineskip}
\begin{figure}
\centering
\includegraphics[scale=1]{TDPeak1Sg3Sg_wider.pdf}
\caption{Hypothetical ID profiles indicating the additional processing effort on the verb for resolving topic drop in yellow.
Resolving subject topic drop is usually easier from an inflectional ending for the 1st person singular (A) than for the 3rd person singular (B).}
\label{fig:uid.peak.verb}
\end{figure}
%\vspace{-0.5\baselineskip}

The yellow curve indicates the hypothetical additional processing effort associated with ellipsis resolution on the verb.
For the verb form that is inflected for the 1st person singular present tense, this effort is lower (A) than for the 3rd person singular verb form (B).
This is because upon finding a verb with an inflectional \is{Verbal inflection} ending indicating the 1st person singular, the hearer can be certain that if they are just processing a topic drop structure with an omitted subject,%
%% Footnote
\footnote{It is also possible that an object is omitted from the prefield.
However, in text messages, for instance, object topic drop is less frequent than subject drop.
Thus, it is reasonable for the hearer to first assume that the subject is omitted and only consider an alternative analysis as object topic drop  when they encounter a postverbal subject.
}
%
the omitted subject must be a 1st person singular pronoun referring to the speaker.

In contrast, if they encounter a verb form inflected for the 3rd person singular, they usually cannot equally easily determine the referent of topic drop.
Unlike for the speaker, who is uniquely determinable in an utterance situation, there are usually several potential 3rd person singular antecedents. \is{Antecedent}
This means that the set of potential referents for topic drop is also reduced by a 3rd person singular inflectional ending \is{Verbal inflection} but to a lesser extent than by a 1st person singular inflectional ending.
Therefore, the 1st person singular ending lowers the processing effort on the corresponding verb compared to the same verb with a 3rd person singular ending.

To sum up, the idea is that ellipsis resolution causes processing effort on the subsequent verb and that this processing effort is not just impacted by the predictability \is{Predictability} of the verb but can also be modulated by its inflectional ending. \is{Verbal inflection}
From this, it follows that an information-theoretic account of topic drop usage must also take recoverability-driven processing costs into account.
This fits naturally with my interpretation of \textit{UID} as a hypothesis of distributing processing effort uniformly and efficiently.
Recoverability and the \textit{facilitate recovery} principle are further factors that impact this processing effort besides the predictability, which is central to the original \textit{UID hypothesis}.
Note that this concept of recoverability is underlyingly identical to the concept of recoverability as a felicity or usage condition for topic drop that I presented in Chapter \ref{ch:recover}.
Here, I simply integrate it explicitly into my information-theoretic approach.
\is{Recoverability|)}

\section{Summary: information-theoretic approach to topic drop usage}\label{sec:info.theory.summary}
I presented an information-theoretic account of the usage of topic drop according to which, in environments where it is licensed, topic drop is used or not used to distribute processing effort uniformly and efficiently across utterances.
The use of topic drop is subject to a trade-off between two tendencies.
The first tendency is to omit a prefield constituent whenever it creates a trough to avoid underchallenging the hearer.
The second tendency is to realize the prefield constituent even if it creates a trough, provided that it smooths the peak on the subsequent verb because this prevents an overload of the hearer's processing capacities. \is{Processing effort}
Besides the processing effort caused by the predictability of the omitted constituent in context, there is an additional source of effort, namely the recoverability \is{Recoverability} of the omitted constituent given the context and the following verb.
Cues on the verb, in particular the inflectional marking, \is{Verbal inflection} can lower the overall processing effort on the verb and, thus, increase the likelihood or acceptability of topic drop.

The information-theoretic approach that I propose here is based on the \textit{UID hypothesis}, which I interpret as a hypothesis about the efficient distribution of surprisal.
In principle, \textit{UID} is a theory about production, \is{Production} but it decidedly also considers perception because the efficiency of communication is increased by facilitating processing for the hearer.
This means that to accomplish this facilitation the speaker is taken to perform audience design \is{Audience design} \citep{bell1984}, i.e., they adapt their utterances to the hearer and the situation.
Consequently, insights into whether and how \textit{UID} and its underlying principles shape language production can be gained from two sides. \is{Production}
First, corpus or production \is{Production} studies can be used to directly investigate how speakers structure their utterances to increase their communicative efficiency.
Second, perception experiments like acceptability rating studies can indirectly test whether utterances in line with \textit{UID} are perceived as more well-formed by hearers.
In the next chapter, I outline the relationship between production, perception, and processing effort in more detail.
 \is{Processing effort|)}
