
\chapter{Linguistic models: Between symbolic discreteness and dynamic continuity}\label{sec:lingMod}

The study of the sound system of human languages has been one of the longest-standing intersections of symbol-based categorical analyses on the one hand, and signal-based continuous descriptions on the other. These two different types of analysis stand at the core of the distinction many researchers make between \emph{phonetics} and \emph{phonology}, where the former addresses continuous and measurable aspects of the speech signal (namely, sensorimotor aspects of articulation, acoustic signals and neurological patterns in perception), while the latter addresses categorical aspects of the speech signal using discrete and symbolic units like \emph{consonants}, \emph{vowels} and \emph{phonemes} (see overviews in \citealt{harris2007representation} and \citealt{ladd2011phonetics}).

\section{Thesis and antithesis: Problems with symbol-based models}\label{sec:risenfall}

The incompatibility between the continuous and the discrete types of description did not escape early studies.
\citet{menzerath1933koartikulationsk, wickelgren1969context} and \citet{fowler1980coarticulation}
noted how the reality of co-articulation of segments defies the idealized conception of speech signals as consisting 
%of a sequence of non-overlapping 
in a non-overlapping sequence of
discrete phonemes.
\citet[172--187]{warren1982auditory} also provides an overview of this problem given the limitations of auditory perception.

Morris Halle acknowledged the problem of discrete descriptions already in \citet{halle1954strategy}, which was written in defense of phonemes (p.~198): \enquote{It is now necessary for us to show why the discrete picture of language is preferable. Our answer is that it enables us to account for many facts which on the assumption of continuity would be extremely difficult, if not impossible, to explain}.
Halle wrote this a decade before he published \citetitle{chomsky1968spesk} with Noam Chomsky \citep{chomsky1968spesk}, perhaps the most influential work in phonology from the second half of the twentieth century, in which speech sounds are modeled as discrete symbolic units and phonological processes are modeled as rules that manipulate symbols.

Beyond phonology and linguistics, many successful enterprises in the cognitive sciences of the second half of the twentieth century likened cognitive capacities to symbol-processing machines.
In that context, John Searle provided some well-known attacks on common notions of \textit{artificial intelligence} \citep{searle1980minds} and the computer metaphor of the mind \citep{searle1990cognitive}, responding, among others, to prominent voices like \citet{fodor1983modularity, pylyshyn1985computation} and \citet{cummins1983nature}.

\begin{sloppypar}
The \emph{connectionist} program in cognitive psychology (e.g.~\citealt{rumelhart1986parallel1, rumelhart1986parallel2, bates1993counectionism}) was set to change that classic view with the introduction of connectionist models to phonology (e.g.~\citealt{goldsmith1992local, joanisse2000connectionist, laks1995connectionistsk, smolensky2006harmonic, tupper2012sonoritysk}). These models replaced classic symbolic models with \emph{neuromimetic} models \citep[52]{laks1995connectionistsk} that attempt to improve the cognitive plausibility of language models with architectures that resemble neurobiological systems.
They were faced with fierce opposition from voices like \citet{fodor1988connectionism} and \citet{pinker1988onlanguage}, who criticized the connectionist models of the time for lacking a symbolic level of representation.
\end{sloppypar}

It should be noted, however, that the main focus of connectionist models is not so much on the symbols in the system as on the classic processes of symbol manipulation. Connectionist models present alternatives to the notion of \emph{rules} of symbol manipulation that directly transform symbols (see \citealt{harnad1990symbol}).
Indeed, as \citet{smolensky2006harmonic} point out, (some versions of) connectionist models in phonology are largely compatible with \emph{Harmonic Grammar}, which describes the learning processes in a constraint-based system like \emph{Optimality Theory} \citep{prince1993optimality}. In that sense, connectionism is like a low-level description for which Optimality Theory provides the high-level description.

The problem with all the models that \citet{smolensky2006harmonic} mention is that they still take discrete symbols in their input in order to generate discrete symbols in their output. As \citet[57]{gafos2006dynamics} points out in the context of modeling variation, these models deal with \enquote{variation among discrete alternatives}, without accounting directly for the continuous aspects of the system.
For different reasons, related to the architecture of neural networks in connectionism, \citet[337]{harnad1990symbol} even suggested that it may be reasonable to consider connectionism as \enquote{a special family of symbolic algorithms}.

In the context of the present work, connectionist models can be effective in modeling the phonology of speech perception in a top-down model, which represents processes that start and end with discrete symbols. However, I assume here that there is a functional source for linguistic distinctions in perception, which has to be accounted for via the bottom-up route, originating from continuous events in real time.

\section{Synthesis: Integrating dynamic and symbolic notions}\label{sec:synthesis}

\citet{port1995mind, kelso1997dynamic} and \citet{spivey2007continuity} have presented a comprehensive refutation of the computer metaphor of the mind at the turn of the century, relying primarily on the many advancements achieved with dynamical systems models of cognition. Dynamical systems have been also successfully implemented in phonology, underlying the enterprise known as \emph{Articulatory Phonology} (see, e.g., \citealt{browman1992articulatorysk, goldstein2009coupled, nam2009self}). This approach models phonology with continuous motor gestures, whereby
coordinative structures
can be understood in terms of the coupling and decoupling of oscillations with respect to syllabic organization (see \citealt{haken1985theoretical} for early incarnations of these models).

Using models of 
%Within 
dynamical systems, 
%models 
it is possible to integrate continuous aspects of the speech system (articulatory trajectories and the related output on the acoustic surface)
with the discrete symbolic categories that linguists postulate. One way to achieve this is via attractor landscape models, where discrete categorical units can be modeled as
stable states in a continuous phase space in terms of
attractor basins (see \citealt{haken1990synergeticssk}). In this type of model, various continuous events can contribute more or less to the (partial) activation of different, often competing, attractors.
Convincing examples for the application of such attractor landscapes can be found in \citet{tuller1994nonlinear, case1995evaluation, raczaszek1999categorization, gafosbenus2006dynamics, roessig2019modeling} and \citet{roessig2019dynamics}.

In fact, using attractor landscape models shows not only that discrete alternatives can be selected by continuous events, but that attractor landscapes can also be advantageous in modeling categories. This is especially true for models that embody a more nuanced understanding of the nature of discrete categories in responding to multiple -- often redundant -- cues and exhibiting \emph{fuzzy} category boundaries that can be readily accounted for in terms of \emph{noise} in the system (see \cite[8--9]{roessig2019dynamics}).

\begin{sloppypar}
Attractor landscapes are therefore an essential component of
models concerned with the interaction between continuous and discrete entities in a language system.
However, much like the connectionist models in phonology, attractor landscapes explicate the process by which a discrete alternative can be (partially) activated, but they have little to say about the components of the system otherwise.
For example, attractor models cannot explain or predict the shape and behavior of the attractor landscape itself (e.g.~universal and idiosyncratic language categories), they cannot address the limitations on dynamic events that the system can reliably detect (e.g.~selecting the relevant effects in auditory perception), and they pose no restrictions on what a valid symbol is in a natural language system (what \citealt{harnad1990symbol} called ``the symbol grounding problem'').
In other words, attractor-based models are good at integrating continuous variables with discrete alternatives, but they are not designed to reveal what drives and limits the dynamic and symbolic modes of the system.
\end{sloppypar}

\section{Dynamics in perceptual phonology}\label{dynamics-in-perceptual-phonology}

Dynamic descriptions have played an increasingly important role in phonological theory
with the growing body of work from the school of \emph{Articulatory Phonology}, which suggests a framework for modeling phonological systems with
continuous articulatory gestures as the basic units of speech production.
Applying similar concepts to perception has been thus far a much less productive avenue in phonology, perhaps because it is much less clear what the relevant continuous entities are that need to be modeled in perception.

Dynamic accounts of phonological perception have been presented in works like \citet{tuller1994nonlinear, case1995evaluation, hock2003dynamical, tuller2004categorization, tuller2008dynamical}, and \citet{lancia2013interaction}.
However, they mostly deal with categorical perception of systematically varying speech stimuli without breaking down the speech input into subcomponents, as is the case in Articulatory Phonology, whereby different moving parts within the vocal tract (e.g.~tongue tip and lips) are the continuous subcomponents of the speech signal.\footnote{Note the work in \citet{liberman1985motor}, where perception is conceived of as continuous, albeit in a manner that is contingent on production (\enquote{the motor theory of speech perception}).}

In that sense, the vast majority of dynamic perception accounts that I am aware of cover the same aspects as the attractor landscape models mentioned above (and indeed, attractor landscape models are common in dynamic perception studies): providing a unified model for the integration of continuous and discrete entities in cognition.

\section{Making sense: Symbols and dynamics in Howard Pattee's work}\label{sec:pattee}

\citet{cariani2001symbols} and \citet{raczaszek2008reconciling} note the writings of physicist and theoretical biologist, Howard Pattee, as a potential source of novelty in cognitive modeling (see \citealt{pattee2012lawssk} for a collection of Pattee's classic papers with contemporary commentary).
For Pattee, the symbolic and dynamic modes of biological systems are two crucial components with specific roles to play:
symbols are the stable forms that harness dynamic events. Symbols, according to \citet[337]{pattee1987instabilities}, cannot exist outside of a dynamic system that they constrain.

Bear in mind that these descriptions were initially laid out to investigate biological systems in which DNA appears to be the symbolic mode that constrains the dynamics of the cell.
However, as Pattee and his followers have been arguing in recent decades, this description can be extended to any \emph{language} system. In that context, it is perhaps useful to elucidate the difference in Pattee's thought between a \emph{language} and a \emph{code}.

Joanna Rączaszek-Leonardi clarifies the difference between language and code in Pattee's work \citep[307--310]{pattee2012lawssk} and emphasizes the centrality of the idea that language systems are characterized by symbols that \emph{harness} or \emph{constrain} dynamics.
The effects of \emph{constraining}, rather than \emph{mapping}, are, in her words, \enquote{naturally context-dependent (crucially relying on the dynamics being constrained), thus are predictable only to some degree}.
In contrast, coding is a relatively noise-free process in which we \enquote{map one symbolic structure onto another symbolic structure} (p.~309).
\enquote{In natural language}, Rączaszek-Leonardi suggests as an example, \enquote{writing is a code for spoken expressions, but it is the spoken expressions that are the level at which meaning relation should be sought}.
In Pattee's original analysis, this meant that the DNA bases \emph{code} for the amino acids, while it is \enquote{the folded amino acid sequence (the protein enzyme) where the first informational constraint on dynamics occurs} (p.~310).

What makes symbols meaningful in a language system is therefore \enquote{a relation in which a symbolic structure acts to harness dynamics} (p.~309).
Symbols in language systems acquire their meanings from the co-occurrence with dynamic events, i.e.~via \emph{grounding} (and later \emph{ungrounding}, see \citealt{Raczaszek2018language}).
Symbols can be \enquote{coded in another set of symbols, perhaps for a better adaptation to a given transmission medium (e.g., the Morse code is better adapted to a telegraph than the alphabet) but it does not make them more, or less meaningful. A code is not a language} \citep[309]{pattee2012lawssk}.

\section{The complementarity of mind}\label{sec:compofmind}

Pattee's specific conception of language systems entails a few interesting outcomes.
One of the most important ones in the context of the current work is the idea that the two modes of language -- the discrete/symbolic on the one hand, and the continuous/dynamic on the other hand -- require two separate complementary models. Pattee summarizes this in \citet[18--21]{pattee2012lawssk}:

\begin{quote}
Complementary models as I define them are models of a system that may be \emph{formally} incompatible in the sense that no one model is logically or mathematically derivable from, or reducible to the others, and all such models are necessary for a complete understanding of the system. (pp. 18–19)
\end{quote}

Pattee is careful not to imply ontological dualism, as he means that complementarity is an \enquote{epistemic necessity}, although he still finds it difficult to assume conceptual compatibility given that \enquote{conceptual categories such as \enquote{discrete} and \enquote{continuous} derive from different pattern recognizing regions of the brain} (p.~19).
Why it is so hard to see this picture more clearly is suggested to be related to inherent limitations on what \enquote{our classical brains can model}:

\begin{quote}
The complementarity of discrete and continuous models is a fundamental aspect of the symbol-matter problem. Evolution prepared the simplest brains to distinguish discrete objects from the continuous motion of objects, thereby allowing effective sensorimotor control. Our everyday experience as well as classical physics is based on a clear and objective distinction between discrete particles and continuous motion. In modern physics, however, this clear distinction is no longer possible. Discrete particles and continuous fields, matter and energy, space and time are no longer objectively separable but depend on how we observe nature. It appears that our artificial instruments have extended our senses beyond what our classical brains can model without cognitive dissonance. It is not clear how far we can reduce this dissonance by learning new concepts. (pp. 20–21)
\end{quote}

It is therefore pertinent to understand symbols in language systems with respect to the continuous events that they relate to. In the present work, this means that to fully understand sonority we need to address its potential functions in auditory perception and cognition, and their effects in linguistic communication. Although tightly related to top-down symbol-based generalizations, this bottom-up route is a separate process that is based on different driving forces (e.g.~laws of physics rather than statistics).

\section{Missing links: Anticipating current contributions}\label{sec:missinglinks}

In an attempt to break new ground, the current project proposes two theoretical novelties that are still missing from the picture described above in Sections~\ref{sec:risenfall}--\ref{sec:pattee}:
(i) decomposing speech into continuous subcomponents in the acoustic signal that allow us to extend our dynamic vocabulary with perception-based entities; and
(ii) suggesting a principled way to separately model continuous and symbolic aspects of speech.
To elaborate on (i), I present the PRiORS framework in \chapref{sec:priors}. PRiORS stands for \emph{Perceptual Regimes of Repetitive Sound}, essentially targeting a single primitive -- \emph{repetition} -- at different timescales that activate two distinct \emph{regimes} in perception.
To elaborate on (ii), I present two complementary models for a single phonological principle in \chapref{sec:modelimp}. These models are built around the distinction between \emph{bottom-up} and \emph{top-down} processes in cognition of speech sounds (see, e.g., \citealt{klatt1979speech, fowler1986event}).
The bottom-up route expects dynamic and continuous inputs, while the top-down route in perception arrives at inferences via the learned categorical and symbolic constructs of the system.
The top-down route requires symbolic models that are based on the distributional history of categorizable speech sounds, while the bottom-up route requires a model that can deal with continuous entities that must obey the laws of physics and fit with the capabilities of human perception and cognition.
Crucially, these two models are irreducible into one and they explicitly attempt to model two complementary aspects of cognition, considering both \emph{bottom-up} and \emph{top-down} inferences in perception of speech.
