\chapter{Machine translation}\label{sec:2}

\is{machine translation|(}This chapter will introduce the main concepts of machine translation. The term \textit{machine translation} (MT) can simply be defined as “[a]utomatic translation from one human language to another using computers” %\label{ref:ZOTEROITEMCSLCITATIONcitationID4xboC4cXpropertiesformattedCitationAlOnaizanetal1999plainCitationAlOnaizanetal1999citationItemsid179urishttpzoteroorgusers1255332itemsGWX8U2UHurihttpzoteroorgusers1255332itemsGWX8U2UHitemDataid179typepaperconferencetitleStatisticalmachinetranslationcontainertitleFinalReportJHUSummerWorkshopvolume30authorfamilyAlOnaizangivenYaserfamilyCuringivenJanfamilyJahrgivenMichaelfamilyKnightgivenKevinfamilyLaffertygivenJohnfamilyMelamedgivenDanfamilyOchgivenFranzJoseffamilyPurdygivenDavidfamilySmithgivenNoahAfamilyYarowskygivenDavidissueddateparts1999locatorschemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRND3VqMjCm82O}
(\citealt{Al-OnaizanEtAl1999}: 1). The idea behind MT goes back to cryptography as discussed by \citet{Weaver1955}. %\label{ref:ZOTEROITEMCSLCITATIONcitationIDYdZx0osupropertiesformattedCitationWeaver1955plainCitationWeaver1955citationItemsid8urishttpzoteroorgusers1255332itemsSFJXDCHKurihttpzoteroorgusers1255332itemsSFJXDCHKitemDataid8typearticlejournaltitleTranslationcontainertitleMachinetranslationoflanguagespage1523volume14authorfamilyWeavergivenWarrenissueddateparts1955schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDqvXeGLJORw} 
The basic idea is that information is encrypted in one language and therefore cannot be understood if the encryption is unknown. However, if the code used to encrypt language A  is known and it can be transferred into language B, the information will be available in language B, too.


\begin{quote}
All languages – at least all the ones under consideration here – were invented and developed by men; and all men […] have essentially the same equipment to bring to bear on this problem. They have vocal organs capable of producing about the same set of sound […]. Their brains are of the same general order of potential complexity. (ibid.: 16)
\end{quote}


Even then, Weaver was aware that it would not be that simple to automatically translate human language. In a letter to Norbert Wiener, he suggested that one could take scientific texts into consideration for MT as they are semantically not as complex and that the result may then not be perfect but intelligible (cf. ibid.:18).



In addition, MT has always been one of the main focuses and challenges of research in artificial intelligence %\label{ref:ZOTEROITEMCSLCITATIONcitationIDVmqhdK4kpropertiesformattedCitationMylonakis2012plainCitationMylonakis2012citationItemsid45urishttpzoteroorgusers1255332items2XS9TSMNurihttpzoteroorgusers1255332items2XS9TSMNitemDataid45typebooktitleLearningthelatentstructureoftranslationpublisherILLCUniversiteitvanAmsterdamHostpublisherplaceAmsterdamAmsterdamsourceOpenWorldCateventplaceAmsterdamAmsterdamURLhttpstaffscienceuvanlmmylonakpublicationsthesismylonakisthesisISBN9789057762352languageEnglishauthorfamilyMylonakisgivenMarkosissueddateparts2012schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDVVYQicUNFV}
(cf. \citealt{Mylonakis2012}). However, many problems and challenges of MT have not yet been solved, or as %\label{ref:ZOTEROITEMCSLCITATIONcitationID1druGDEtpropertiesformattedCitationWarwick2012plainCitationWarwick2012citationItemsid73urishttpzoteroorgusers1255332itemsP7EJCF3Aurihttpzoteroorgusers1255332itemsP7EJCF3AitemDataid73typebooktitleArtificialintelligencethebasicspublisherRoutledgepublisherplaceNewYorknumberofpages183sourceLibraryofCongressISBNeventplaceNewYorkISBN9780415564823callnumberQ335W3652012shortTitleArtificialintelligenceauthorfamilyWarwickgivenKissueddateparts2012schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDrdTZiqszzp}
\citet{Warwick2012} puts it:


\begin{quote}
Machine translation is a field that includes the research areas of translation science, computational linguistics and artificial intelligence. Although there are some real-world applications of machine translation, the development is not as great as in 'the finance, manufacturing and military sectors' where applications 'are performing in ways with which the human brain simply cannot compete'.
\end{quote}


This chapter will introduce the development of MT, the different approaches to MT, as well as the application and the state of the art of MT. It is not meant to be an exhaustive description of the whole field, but instead to provide a short overview.


\section{Machine translation development}
\label{sec:2:1}

Research on MT started more or less simultaneously with the invention of the electronic computer in the 1940s. However, the idea for MT goes back even further: Some origins can be traced back to 17\textsuperscript{th} century philosophical thought on universal and logical languages as well as mechanical dictionaries. Early technological development did not facilitate working mechanical systems. In 1933, two patents were granted for MT-resembling ideas in France and Russia, which are considered the first real precursors of MT systems (%\label{ref:ZOTEROITEMCSLCITATIONcitationIDqfQFJ2bmpropertiesformattedCitationJHutchins2004plainCitationJHutchins2004citationItemsid12urishttpzoteroorgusers1255332itemsZBKCQCPCurihttpzoteroorgusers1255332itemsZBKCQCPCitemDataid12typearticlejournaltitleTwoprecursorsofmachinetranslationArtsrouniandTrojanskijcontainertitleInternationalJournalofTranslationpage1131volume16issue1authorfamilyHutchinsgivenJohnissueddateparts2004schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDtocqGgAOrt}
\citealt{Hutchins2004} – who also provides a detailed description of the two forerunner systems). These ideas, however, did not receive much attention and only Warren Weaver's memorandum “brought the idea of MT to general notice” (%\label{ref:ZOTEROITEMCSLCITATIONcitationIDCPsSBUdYpropertiesformattedCitationWJHutchinsandSomers1992plainCitationWJHutchinsandSomers1992citationItemsid48urishttpzoteroorgusers1255332itemsAX2JS6JRurihttpzoteroorgusers1255332itemsAX2JS6JRitemDataid48typebooktitleAnintroductiontomachinetranslationpublisherAcademicPressIncpublisherplaceLondonnumberofpages320eventplaceLondonURLhttpwwwhutchinswebmeukIntroMTTOChtmauthorfamilyHutchinsgivenWJohnfamilySomersgivenHaroldissueddateparts1992schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRND6wMMwNbBGy}
\citealt{HutchinsSomers1992}) and research on MT was launched during the next years.



Initially, the idea was received with great enthusiasm: In 1954, the \isi{Georgetown Experiment} was presented – the first public presentation of an MT system, developed by Georgetown University in cooperation with IBM. It raised many expectations of MT development, although the presented text was well-selected and vocabulary entries and grammar rules were very restricted. This led to more funding in the US and to new MT projects all over the world, especially in Russia. Although research at this time had a significant influence not only on MT research but also on computational linguistics, artificial intelligence, and theoretical linguistics, a proficient system was not developed and the high expectations were not met (cf. ibid.: 6). Therefore, the US government assigned the Automatic Language Processing Advisory Committee (ALPAC), which was formed in 1964, to determine how well MT was actually working in 1966. The resulting report was devastating and stopped funding for MT almost entirely for the next decades in the United States. According to the \isi{ALPAC report}, MT was not worth funding, because \isi{post-editing} of MT was as expensive as human translation. The committee recommended funding other research areas such as computational linguistics and investing in the development of methods to improve human translation.



Despite this regress, MT was not fully abandoned and the first commercial systems were launched on the market after 1966 – mostly outside the US. Two examples are \isi{\textit{Météo}} (1976) – a system developed at the university of Montreal to translate weather forecasts – and \isi{\textit{Systran}} – a company founded in 1968, which is still one of the most famous MT companies on the market; Systran's system was installed by the US Air Force in 1970 for \ili{Russian}-\ili{English} and by the European Union in 1976 (cf. %\label{ref:ZOTEROITEMCSLCITATIONcitationIDHFUt9Cj8propertiesformattedCitationWJHutchins1995plainCitationWJHutchins1995citationItemsid117urishttpzoteroorgusers1255332itemsIAG2DI8Burihttpzoteroorgusers1255332itemsIAG2DI8BitemDataid117typechaptertitleMachinetranslationAbriefhistorycontainertitleConcisehistoryofthelanguagesciencesfromtheSumerianstothecognitivistspublisherCambrigeUniversityPresspublisherplaceCambridgepage431445eventplaceCambridgeauthorfamilyHutchinsgivenWJohnissueddateparts1995schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDZGouQluOo3}
 \citealt{Hutchins1995}: 139-142).


\newpage 
In the meantime, the development of MT had reached Europe as well. Some bigger projects were the \isi{\textit{Ariane} system} developed by the \isi{GETA-group} (CETA in earlier days) in Grenoble, France, and the \textit{SUSY} system of the Saarland University in Saarbrücken, Germany. Both research facilities prevailed in the huge \isi{\textit{EUROTRA} project} of the European Union. The European Union naturally has a huge demand for translations. Therefore, they became very interested in MT at a very early stage. The EUROTRA project spanned 150 scientists from 18 institutes and ten member states at the end of 1989. It was intended to cover all 72 language pairs that were required in that the respective state of the Union (today even more language pairs need to be covered). Although the project never produced a working system, the research had a major influence on computational and linguistic research (cf. %\label{ref:ZOTEROITEMCSLCITATIONcitationIDflgwaLA3propertiesformattedCitationWJHutchinsandSomers1992plainCitationWJHutchinsandSomers1992dontUpdatetruecitationItemsid48urishttpzoteroorgusers1255332itemsAX2JS6JRurihttpzoteroorgusers1255332itemsAX2JS6JRitemDataid48typebooktitleAnintroductiontomachinetranslationpublisherAcademicPressIncpublisherplaceLondonnumberofpages320eventplaceLondonURLhttpwwwhutchinswebmeukIntroMTTOChtmauthorfamilyHutchinsgivenWJohnfamilySomersgivenHaroldissueddateparts1992schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDad9pFFxRVO}
\citealt{HutchinsSomers1992}: 239-241).\footnote{More details on MT systems in the EU, especially in the European Commission, are provided in \sectref{sec:4:3:2}}



It was only in the 1990s that the first tools were developed for computer assisted translation (CAT tools) which are intended to support the human translation process (cf. %\label{ref:ZOTEROITEMCSLCITATIONcitationIDBfns73KqpropertiesformattedCitationGarcia2009plainCitationGarcia2009citationItemsid173urishttpzoteroorgusers1255332itemsXKNUWNKNurihttpzoteroorgusers1255332itemsXKNUWNKNitemDataid173typearticlejournaltitleBeyondtranslationmemoryComputersandtheprofessionaltranslatorcontainertitleTheJournalofSpecialisedTranslationpage199214volume12issue12authorfamilyGarciagivenIgnacioissueddateparts2009schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDpMPYcJTEoj}
\citealt{Garcia2009}: 199). The most beneficial tools are \isi{translation memory systems} (TMSs) which essentially save completed translations and provide translation suggestions of former translations to the translator when a similar (\isi{\textit{fuzzy matches}}) or identical (\isi{\textit{100\% matches}}) segment occurs. The source text is usually segmented on a sentence basis and matches are presented accordingly, but the translator can also search the \isi{translation memory} to find single words or phrases (\isi{\textit{concordance search}}). TMSs simply store translations and recall what they have stored when matches occur, but they do not produce translations automatically. Most \isi{TMS} also incorporate a \isi{\textit{terminology management} system}. \isi{TMS} have become indispensable in translation practice, especially for translators who have to deal with domain-specific texts like texts related to technology, law, medicine, etc.


\largerpage
With the spread of the Internet, it was only a matter of time until MT went online. Systran provided the first online MT for users of \isi{\textit{Minitel}} in France in 1988. The users could send Minitel the texts requiring translation. The service was provided for \ili{English} and \ili{French} (both directions) as well as from \ili{German} into \ili{English} and the systems were capable of translating 22 pages per minute. In 1992, \isi{\textit{CompuServe}} introduced MT for their users. In addition to the MT service itself, CompuServe offered PE services for an extra fee. Most customers requested MT rather than PE services, though: In 1997, 85\% of all requests were for MT only. However, the PE tasks were generally conducted for longer texts – therefore, the percentage was 60\% MT and 40\% PE on a word-basis %\label{ref:ZOTEROITEMCSLCITATIONcitationIDqwX6qmO9propertiesformattedCitationGaspariandHutchins2007plainCitationGaspariandHutchins2007dontUpdatetruecitationItemsid49urishttpzoteroorgusers1255332itemsDN8576VZurihttpzoteroorgusers1255332itemsDN8576VZitemDataid49typepaperconferencetitleOnlineandFreeTenYearsofOnlineMachineTranslationOriginsDevelopmentsCurrentUseandFutureProspectscontainertitleProceedingsofMTSummitXIpublisherplaceCopenhagenDenmarkpage199206eventMTSummitXIeventplaceCopenhagenDenmarkauthorfamilyGasparigivenFredericofamilyHutchinsgivenWJohnissueddateparts2007schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRND3gZvdja8R7}
(cf. \citealt{GaspariHutchins2007}: 199-200).



\isi{\textit{Bable Fish}} was developed by \isi{Systran} and \isi{AltaVista} and went online on 9 December 1997. It was the first live MT service that was available for all Internet users and was free of charge. This launched a new era of \isi{free online MT services}. In 2007, over 30 similar services were online (cf. ibid.: 200). One of the most famous online MT systems nowadays is \isi{\textit{Google Translate}}, which covers 103 languages and also recently integrated neural MT.\footnote{More information is provided in \sectref{sec:2:2}.} Google Translate can be used on a desktop, mobile device, offline, and even in connection with other apps. The user can contribute to the MT development by rating or providing translations.\footnote{cf. \url{http://translate.google.de/about/intl/en_ALL/index.html}, last accessed 15 March 2017.} Further tools like the \isi{\textit{Translator Toolkit}}\footnote{cf. \url{https://translate.google.com/toolkit/list?hl=de\#translations/active}, last accessed 15 March 2017.} – an environment resembling a \isi{translation memory}, where the source text is segmented and automatically translated and that can be used to improve the MT suggestions within this tool or to assign the job to a language service – are also provided by Google. Although MT systems – especially popular online MT systems like \textit{Google Translate} – are often not taken seriously by some Internet users, because the mistakes amuse native speakers of the target languages\footnote{e.~g. \url{http://ackuna.com/badtranslator} (last accessed 4 April 2016) – a website that translates back and forth from \ili{English} into different languages to show that the mistakes of MT add up after many translations to a misleading/funny text – or \url{http://www.boredpanda.com/funny-chinese-translation-fails/?afterlogin=savevote & post=73070 & score=-1} (last accessed 4 April 2016) – a website showing funny \ili{Chinese} to \ili{English} translations.}, \isi{\textit{Gisting}} (raw MT for information retrieval, see \sectref{sec:2:3} for more details) has become a common phenomenon on many websites. Furthermore, many websites work in cooperation with online MT services and offer an automatic translation of their contents by a simple mouse-click (see examples in \sectref{sec:2:3}).



In the meantime, the projects \isi{\textit{EuroMatrix}} and its successor \isi{\textit{EuroMatrixPlus}} had also been generating ground-breaking results in the field of statistical and hybrid MT\footnote{More information on the different approaches of MT is provided in the subsequent chapter.} in Europe. They impacted the development of the open source MT system \isi{\textit{Moses}}, which enables users to train a statistical system with their own corpus data or other freely available corpus data. \textit{Moses} is one of the most frequently used MT systems in academia and the translation industry. The projects aimed at generating an exemplary MT system for every EU language, providing the necessary corpora to build an MT system (the “Euromatrix” with monolingual resources, parallel corpora and MT systems, can be accessed freely via the Internet\footnote{\url{http://www.euromatrixplus.net/matrix/}, last accessed 16 March 2017.}), and bringing MT systems closer to the end user (cf. %\label{ref:ZOTEROITEMCSLCITATIONcitationIDj2ypwcB8propertiesformattedCitationBusemannetal2012plainCitationBusemannetal2012citationItemsid177urishttpzoteroorgusers1255332itemsRVMEPGSEurihttpzoteroorgusers1255332itemsRVMEPGSEitemDataid177typereporttitleEuroMatrixPlusFinalReportURLhttpwwweuromatrixplusorgresources86authorfamilyBusemanngivenSfamilyBojargivenOfamilyCallisonBurchgivenCfamilyCettologivenMfamilyFedericogivenMfamilyGarabikgivenRfamilyGenabithgivenJnondroppingparticlevanfamilyKoehngivenPfamilyMatuskagivenDfamilySchwenkgivenHfamilySimovgivenKfamilyUszkoreitgivenHfamilyWolfgivenPissueddateparts2012accesseddateparts2017316schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDhdbatevTuA}
\citealt{BusemannEtAl2012}). The \isi{Europarl Corpus} (cf. %\label{ref:ZOTEROITEMCSLCITATIONcitationIDwhrCV7QupropertiesformattedCitationKoehn2005plainCitationKoehn2005citationItemsid178urishttpzoteroorgusers1255332items8JGT3F9Purihttpzoteroorgusers1255332items8JGT3F9PitemDataid178typepaperconferencetitleEuroparlAparallelcorpusforstatisticalmachinetranslationcontainertitleMTsummitpage7986volume5authorfamilyKoehngivenPhilippissueddateparts2005schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDTuZA1BOKfo}
\citealt{Koehn2005}), for instance, gathers data of parallel corpora in 21 European languages taken from the proceedings of the European parliament.\footnote{\url{http://www.statmt.org/europarl/}, last accessed 16 March 2017.} The latter is not only important for developing MT systems, but it also enables professional translators to access valuable reference material for free.



Although full high-quality MT is still not possible and probably will not be any time soon – although hope and expectations are rising again with the newly developed neural MT systems\footnote{See next chapter.} – MT is a thriving research area. This persistence was already explained in detail by %\label{ref:ZOTEROITEMCSLCITATIONcitationIDwMJjHteNpropertiesformattedCitationKaiserCooke1993plainCitationKaiserCooke1993citationItemsid222urishttpzoteroorgusers1255332itemsN458885Kurihttpzoteroorgusers1255332itemsN458885KitemDataid222typearticlejournaltitleMachineTranslationandthehumanfactorKnowledgeanddecisionmakinginthetranslationprocesscontainertitleUnpublishedPhDdissertationUniversityofViennaauthorfamilyKaiserCookegivenMichleissueddateparts1993schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRND5Y4YBNxlcl}
\citet{Kaiser-cooke1993}:


\begin{quote}
Despite the many set-backs it has experienced, MT has proved extremely resilient. This can be explained partly by the external fascination of language in general and translation in particular, and the ambitions of the AI community to prove the practical applicability of their theories, as well as the unshakeable conviction of many that MT has enormous commercial potential.
\end{quote}

\section{Machine translation approaches}
\label{sec:2:2}

In general, MT was historically divided into two different types: \textit{rule-based} and \textit{data}{}-\textit{based}. \textit{Hybrid systems} combine both approaches and have only been developed in recent years. The latest approach is called \textit{neural MT}, which is also based on data, but is based on neural networks. In the following, the different systems will be briefly introduced and their advantages and disadvantages will be highlighted. The following sources were used to create this overview – if not specified otherwise – and can be used to find more detailed descriptions: %\label{ref:ZOTEROITEMCSLCITATIONcitationIDWpmbJd8wpropertiesformattedCitationGoutteetal2009plainCitationGoutteetal2009citationItemsid32urishttpzoteroorgusers1255332itemsMTZX4ZW3urihttpzoteroorgusers1255332itemsMTZX4ZW3itemDataid32typebooktitleLearningmachinetranslationcollectiontitleNeuralinformationprocessingseriespublisherMITPresspublisherplaceCambridgeMassnumberofpages316sourceLibraryofCongressISBNeventplaceCambridgeMassISBN9780262072977callnumberP309L432009editorfamilyGouttegivenCyrilfamilyCanceddagivenNicolafamilyDymetmangivenMarcfamilyFostergivenGeorgeissueddateparts2009schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDkZEYaTGrs8}
\citet{GoutteEtAl2009}, %\label{ref:ZOTEROITEMCSLCITATIONcitationIDyiM0iW19propertiesformattedCitationHutchinsandSomers1992plainCitationHutchinsandSomers1992dontUpdatetruecitationItemsid48urishttpzoteroorgusers1255332itemsAX2JS6JRurihttpzoteroorgusers1255332itemsAX2JS6JRitemDataid48typebooktitleAnintroductiontomachinetranslationpublisherAcademicPressIncpublisherplaceLondonnumberofpages320eventplaceLondonURLhttpwwwhutchinswebmeukIntroMTTOChtmauthorfamilyHutchinsgivenWJohnfamilySomersgivenHaroldissueddateparts1992schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDvBqu22VJqU}
\citet{HutchinsSomers1992}, %\label{ref:ZOTEROITEMCSLCITATIONcitationID8GhtX42lpropertiesformattedCitationKoehn2010plainCitationKoehn2010citationItemsid50urishttpzoteroorgusers1255332itemsK8X93W3Uurihttpzoteroorgusers1255332itemsK8X93W3UitemDataid50typebooktitleStatisticalmachinetranslationpublisherCambridgeUniversityPresspublisherplaceCambridgeNewYorknumberofpages433sourceLibraryofCongressISBNeventplaceCambridgeNewYorkISBN9780521874151callnumberP308K642010authorfamilyKoehngivenPhilippissueddateparts2010schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDXQsjX2pLdO}
\citet{Koehn2010}, and %\label{ref:ZOTEROITEMCSLCITATIONcitationIDaV4GdC78propertiesformattedCitationWilks2008plainCitationWilks2008citationItemsid13urishttpzoteroorgusers1255332itemsZR7NI3RWurihttpzoteroorgusers1255332itemsZR7NI3RWitemDataid13typebooktitleMachinetranslationitsscopeandlimitspublisherSpringerpublisherplaceNewYorkedition1stedsourceLibraryofCongressISBNeventplaceNewYorkISBN9780387727738shortTitleMachinetranslationauthorfamilyWilksgivenYorickissueddateparts2008schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDBipQIgp5pE}
\citet{Wilks2008}.



\is{Rule-based approaches|(}\textit{Rule-based approaches} launched the development of MT. Generally, these systems attempt to define the single characteristics of the source language and how these need to be converted into the target languages. Different rule-based approaches to realise MT have been developed over the years: \textit{direct MT}, \textit{transfer-based MT}, and \textit{interlingual MT}. %\label{ref:ZOTEROITEMCSLCITATIONcitationIDXMnv8goOpropertiesformattedCitationChesterman2016plainCitationChesterman2016citationItemsid199urishttpzoteroorgusers1255332items4J97GUJFurihttpzoteroorgusers1255332items4J97GUJFitemDataid199typebooktitleMemesoftranslationthespreadofideasintranslationtheorycollectiontitleBenjaminstranslationlibrarycollectionnumberv123publisherJohnBenjaminsPublishingCompanypublisherplaceAmsterdamPhiladelphiaeditionRevisedEditionsourceLibraryofCongressISBNeventplaceAmsterdamPhiladelphiaISBN9789027258687callnumberP306C5352016shortTitleMemesoftranslationauthorfamilyChestermangivenAndrewissueddateparts2016schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDCjWPgES8h0}
\citet[28--29]{Chesterman2016} mentions that he sees this early form of MT as “the Linguistic meme of translation theory” (ibid.: 29), because it assumes that languages can solely be expressed through rules, which, accordingly, must also be representable in algorithms.



\isi{\textit{Direct translation}} is the oldest approach. This type of MT is constructed specifically for one language pair and usually one translation direction. Essentially, the words of the source text are morphologically analysed and then looked up in a dictionary, which means that ideally all morphology rules are defined, so that the dictionary only has to contain the stems of the words. In the next steps, the words of the source language are replaced by the words in the source language and all morphological changes required by the \isi{target language} are applied. The main disadvantage of this approach is that it takes a lot of effort to develop such a system, because the better the intended system, the more rules have to be defined. If morphology, grammar, and syntax are only defined superficially, the source text might be interpreted incorrectly which may lead to (severe) mistakes in the \isi{target language}. Further, the rules have to be defined from scratch for every language and every language direction.



The \isi{\textit{transfer-based approach}} constructs a syntactic representation of the source text (often in a tree structure) that is free of ambiguities, etc. Next, this representation is generated for the \isi{target language} with the help of a grammar that contains the bilingual transfer rules. Now, the target text can be produced. Theoretically, it is possible to use these systems in both language directions, but this is rarely done in practice, because the transfer rules often do not apply in both directions.



The last rule-based approach that should be introduced is \isi{\textit{interlingual MT}}, which experienced its peak in the 1980s and 1990s. For this approach, an Interlingua needs to be created that represents meaning in an abstract form, which can theoretically be achieved by either a natural or an artificial language or a language-independent representation. The basic principle of this approach is that the source text is translated into the Interlingua and then the Interlingua into the \isi{target language}. Due to the abstract Interlingua, it would be easier to add a new language. However, the task of presenting content and meaning in a formal and neutral manner so that it can be applied to various languages is one of the biggest challenges in the field of Artificial Intelligence and is still an unsolved issue.
\is{Rule-based approaches|)}


\is{data-based approaches|(}At the end of the 20\textsuperscript{th} century, a new concept of MT became popular in MT research: \textit{data-based translatio}n. The explosion of the world wide web made many mono- and bilingual corpora available that enabled MT researchers to construct systems that are independent of linguistic rules: \textit{example-based MT} and \textit{statistical MT}.



\is{example-based approaches|(}The \textit{example-based approach} was mainly developed in Japan starting in the mid-1980s. Essentially, the systems search in bilingual corpora for the sentence that is closest to the source sentence and combine it with (an)other sentence(s) from the corpus. These fragments then generate the new sentence in the target language. The basic functionality is similar to a translation memory system. However, a TM system only searches for similar sentences that have been translated before, but it does not automatically combine these with other sentences to present a full target sentence. In their study, %\label{ref:ZOTEROITEMCSLCITATIONcitationIDVkkuxWQqpropertiesformattedCitationCarlandHansen1999plainCitationCarlandHansen1999citationItemsid176urishttpzoteroorgusers1255332itemsNEK5S43Qurihttpzoteroorgusers1255332itemsNEK5S43QitemDataid176typepaperconferencetitleLinkingtranslationmemorieswithexamplebasedmachinetranslationcontainertitleMachineTranslationSummitVIIpage617624authorfamilyCarlgivenMichaelfamilyHansengivenSilviaissueddateparts1999schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDA4ecMFEyNy}
\citet{CarlHansen1999} compare example-based MT with two different \isi{TMS} approaches. Both TM systems translate the reference sentences better with 90.7\% and 89.4\% for entirely correct sentences, respectively, than the example-based MT system (85.5\%). However, when the translation score was decreased, the example-based MT system outperformed the TM systems, i.e. 96.6\% of the sentences from MT output were translated correctly at least 66\% of the time, while the TM systems only delivered 93.7\% and 95.0\%, respectively. This means that the TM systems generate good translations if the training corpus contains (almost) identical sentences, while the MT system can treat sentences that are not literally contained in the training data.\is{example-based approaches|)}



The most extensive research was conducted in \isi{\textit{statistical MT}} in the last decades. Statistical MT emerged in the late 1980s as a result of IBM's first successes in speech recognition. The basic idea is to generate a translation from a parallel training corpus by calculating the most likely equivalent of a source word\slash phrase\slash sentence in the \isi{target language}. Statistical translation models are generated and trained on the corpus\slash the corpora with the help of machine learning. Both mono- and bilingual corpora are used to capture the typical linguistic structure of the languages – the monolingual corpora generate the language model, the bilingual parallel corpora generate the translation model. These models are constructed during the training phase. Further, additional information can be extracted during the training phase from all corpora, e.g. models of relative sentence length or information about word order. All these models and information receive a value in the tuning phases. These values represent the weight of the models and information, when the most probable translation of the source text is translated. During the decoding phase the target text is produced. Statistical MT uses word-aligned n-grams – sequences of words (usually $n\leq7$) – that are assigned probabilities representing how probable the word (or the sequence) is in the training corpus and combined those with the additional information, for example information from the monolingual corpus (see %\label{ref:ZOTEROITEMCSLCITATIONcitationIDI877y1EbpropertiesformattedCitationHearneandWay2011plainCitationHearneandWay2011citationItemsid205urishttpzoteroorgusers1255332itemsEDCH5N56urihttpzoteroorgusers1255332itemsEDCH5N56itemDataid205typearticlejournaltitleStatisticalmachinetranslationaguideforlinguistsandtranslatorscontainertitleLanguageandLinguisticsCompasspage205226volume5issue5authorfamilyHearnegivenMaryfamilyWaygivenAndyissueddateparts2011schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRND63eAMOvgjx}
\citet{HearneWay2011} for detailed information).


\largerpage
Recent developments have attempted to unite different approaches (usually rule-based and statistical) in ]\isi{hybrid systems} so that the advantages of the respective approach can be combined. While systems with deep integration construct a whole new system that combines the advantages of two approaches, shallow integration systems unite two or more existing systems in one new system (cf. e.g. %\label{ref:ZOTEROITEMCSLCITATIONcitationIDBNDpdfRTpropertiesformattedCitationEisele2007plainCitationEisele2007citationItemsid46urishttpzoteroorgusers1255332items77R5V5TEurihttpzoteroorgusers1255332items77R5V5TEitemDataid46typespeechtitleHybridmachinetranslationcombiningrulebasedandstatisticalMTsystemspublisherplaceEdinburgheventFirstMachineTranslationMarathoneventplaceEdinburghURLhttpmtarchiveinfoMTMarathon2007EiselepdfauthorfamilyEiselegivenAndreasissueddateparts2007416accesseddateparts2013415schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDRz8LNRk2yr}
\citealt{Eisele2007}).



The latest approach to MT is the use of neural networks, which is also a data-driven approach and uses parallel training corpora. However, \isi{neural MT systems} try to build one large neural network for translation, while statistical MT systems are composed of many small sub-components. This MT approach usually uses an encoder-decoder system, with one encoder and one decoder for each language. The sentence is read and encoded by the encoder for a vector with a fixed length. This vector is then processed by the decoder which is also responsible for constructing the target sentence. In these systems, the decoder is the hidden layer between input and output. It is called hidden layer, because it can only assess what is put into the system and what comes out of the system, but not what happens in between. The whole encoder-decoder system is trained simultaneously for one language pair to increase the probability of a correct translation. Pressing all the necessary information of a source language into a vector with a fixed length becomes problematic when long sentences need to be translated. This issue can be addressed when the translation is aligned and translated simultaneously. The source sentence is not necessarily encoded into one vector, but into a sequence of vectors, or an alignment model is added. Further, NMT systems can consider the context through a word embedding layer, which appears between the input layer and hidden layer. It  represents words that occur in similar contexts and hence helps predicting the next word. Although the MT approach with neural networks is very new, the results are already promising and the combination with existing MT systems has already superseded the previous state-of-the-art of MT performance (cf. %\label{ref:ZOTEROITEMCSLCITATIONcitationIDqocWieUzpropertiesformattedCitationBahdanauChoandBengio2014plainCitationBahdanauChoandBengio2014citationItemsid155urishttpzoteroorgusers1255332itemsK5MTHV8Vurihttpzoteroorgusers1255332itemsK5MTHV8VitemDataid155typepaperconferencetitleNeuralmachinetranslationbyjointlylearningtoalignandtranslatecontainertitlearXivpreprintarXiv14090473eventICLR2015authorfamilyBahdanaugivenDzmitryfamilyChogivenKyunghyunfamilyBengiogivenYoshuaissueddateparts2014schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDEO26e9N5as}
\citealt{BahdanauEtAl2014}: 1-2 and %\label{ref:ZOTEROITEMCSLCITATIONcitationID6ZMTB0g4propertiesformattedCitationKoehn2017plainCitationKoehn2017citationItemsid4055urishttpzoteroorgusers1255332itemsFCA94KC4urihttpzoteroorgusers1255332itemsFCA94KC4itemDataid4055typechaptertitleChapter13NeuralMachineTranslationDraftcontainertitleStatisticalMachineTranslationURLhttpspdfssemanticscholarorg247328a082d86199ed5a98e1d726aa205c1da9dfpdfauthorfamilyKoehngivenPhilippissueddateparts2017schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDA3GgV7NWCZ}
\citealt{Koehn2017}).
\is{data-based approaches|)}

\section{Machine translation applications}
\label{sec:2:3}

MT is used in many professional and private contexts (cf. %\label{ref:ZOTEROITEMCSLCITATIONcitationID8K0wlocrpropertiesformattedCitationKoehn2010plainCitationKoehn2010citationItemsid50urishttpzoteroorgusers1255332itemsK8X93W3Uurihttpzoteroorgusers1255332itemsK8X93W3UitemDataid50typebooktitleStatisticalmachinetranslationpublisherCambridgeUniversityPresspublisherplaceCambridgeNewYorknumberofpages433sourceLibraryofCongressISBNeventplaceCambridgeNewYorkISBN9780521874151callnumberP308K642010authorfamilyKoehngivenPhilippissueddateparts2010schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDYHHJMJ43Vn}
\citealt{Koehn2010}: 20). There are roughly three different areas of application for MT that can be realised: \textit{assimilation} – the (final) target text is used for information gathering – \textit{dissemination} – the (final) target text is used for publication – and \textit{communication –} the (final) target text is used for communication purposes.



The most desirable application for MT is \isi{\textit{Full-Automatic High-Quality Machine Translation}} (\isi{FAHQMT}), which means that the MT output does not need any editing by a human. This is the highest goal of MT research. However, this aim has only been achieved for very restricted domains and text types so far like weather forecasts, summaries of sport news, train and plane information, due to the general complexity of language and unresolved problems in MT development,  (cf. ibid.: 20-21).



One of the most wide-spread uses of MT is \isi{\textit{gisting}}. Gisting describes the concept of using MT solely for information gathering. Therefore, the output does not need to be perfect in terms of language. Many websites use online MT systems to make their content accessible to a wide range of users\slash customers. One popular example is \textit{Facebook}\footnote{\url{https://www.facebook.com/}; last accessed 2 February 2016.} which offers online MT to its users so that posts by other users in foreign languages can be translated automatically. The MT is powered by Microsoft Bing; the platform's interface, however, is localised into many languages by humans. \textit{TripAdvisor}\footnote{\url{http://www.tripadvisor.de/}; last accessed 2 April 2016.} uses MT to translate recommendations\slash reports on holiday destinations, hotels, restaurants, sights, etc. into the user's mother tongue (MT provided by Google). Therefore, the users can write their reports in their native language and can still be certain that their recommendations will be read all over the world. On the other hand, users can read reports (or at least get an idea about the reports depending on the MT quality) they might otherwise not have understood at all. The homepage of the \ili{English} town of \textit{Lincolnshire}\footnote{\href{http://www.lincolnshire.gov.uk/}{www.lincolnshire.gov.uk}; last accessed 2 April 2016.} provides a Google Translate implementation that can translate the page's content. These examples present a vital aspect of gisting: It would not be economical to translate all the information of the aforementioned websites into several languages. For town communities, for example, translations of their homepages are relevant in order to support tourism. However, the costs may be too high for small communities especially considering the running expenses of translating news, etc. Moreover, gisting is of great interest for home security departments in order to observe local news and the communication of other countries, because there is no need (and time) to get perfect translations. Finally, gisting is very helpful to judge information and to estimate whether the document should be translated or post-edited by a human (cf. ibid.: 121).



In a broader sense, MT is further used in combination with \isi{speech recognition} to facilitate real time translations of spoken conversations, audio data or telephone calls. This special form of MT is particularly difficult, because it includes two extra steps. First, speech must be recognised and transcribed so that an MT system can translate the text and finally, the translated text must be rendered as speech (although written output is possible, too). Further, all components must work in both participating languages or language directions, respectively. Two main areas of use can be categorised: consecutive interpretation of dialogue for mobile appliances as well as static simultaneous interpreting of longer monologues or speeches. Consecutive mobile translation applications were initially extremely restricted and could be used only in specific situations, such as medical emergencies or for police operations. In 2009, the first speech-to-speech system was published that was not restricted to any domain and could also be used offline, called JIBBIGO\footnote{\url{http://jibbigo-translator-2-0.soft112.com/}, last accessed 28\textsuperscript{th} October 2016.}. Simultaneous automatic interpreting is advancing tremendously, too, although many problems are still unsolved. The Karlsruhe Institute of Technology (KIT), Germany, for example uses an automatic interpretation system that translates lectures from \ili{German} into \ili{English} for those students who are not able to understand \ili{German} perfectly. The quality of the automatic interpretation is not perfect, but it is a help for the students, nonetheless. Further, the students can access the \ili{German} transcription – which can also be useful for hearing impaired students – and the \ili{English} translation via a website (cf. %\label{ref:ZOTEROITEMCSLCITATIONcitationID2gqTQ4bopropertiesformattedCitationWaibel2015plainCitationWaibel2015citationItemsid204urishttpzoteroorgusers1255332itemsNRCXCHDKurihttpzoteroorgusers1255332itemsNRCXCHDKitemDataid204typearticlejournaltitleSprachbarrierendurchbrechenTraumoderWirklichkeitcontainertitleNovaActaLeopoldinaNFpage101123volume122issue410authorfamilyWaibelgivenAlexanderissueddateparts2015schemahttpsgithubcomcitationstylelanguageschemarawmastercslcitationjsonRNDJT9IEhKsaV}
\citealt{Waibel2015}).



The development of MT for offline applications is also of great interest. Early experimental systems were used by aid workers in developing countries or soldiers in the line of duty (more information on projects of DARPA is provided in \sectref{sec:4:3:4}). Last but not least, the use of MT for PE is becoming increasingly frequent (cf. \citealt{Koehn2010}: 121f-123). As PE is one of the main topics of this thesis, it will be explained in detail in the following chapters.
\is{machine translation|)}

