% This file was converted to LaTeX by Writer2LaTeX ver. 1.4
% see http://writer2latex.sourceforge.net for more info
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb,amsfonts,textcomp}
\usepackage{array}
\usepackage{supertabular}
\usepackage{hhline}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, filecolor=blue, urlcolor=blue}
\usepackage{graphicx}
\makeatletter
\newcommand\arraybslash{\let\\\@arraycr}
\makeatother
\raggedbottom
% Paragraph styles
\renewcommand\familydefault{\rmdefault}
\newenvironment{styleStandard}{\setlength\leftskip{0cm}\setlength\rightskip{0cm plus 1fil}\setlength\parindent{0cm}\setlength\parfillskip{0pt plus 1fil}\setlength\parskip{0cm plus 1pt}\writerlistparindent\writerlistleftskip\leavevmode\normalfont\normalsize\writerlistlabel\ignorespaces}{\unskip\vspace{0cm plus 1pt}\par}
% List styles
\newcommand\writerlistleftskip{}
\newcommand\writerlistparindent{}
\newcommand\writerlistlabel{}
\newcommand\writerlistremovelabel{\aftergroup\let\aftergroup\writerlistparindent\aftergroup\relax\aftergroup\let\aftergroup\writerlistlabel\aftergroup\relax}
\setlength\tabcolsep{1mm}
\renewcommand\arraystretch{1.3}
\title{}
\author{Richard}
\date{2020-10-09}
\begin{document}
\clearpage\setcounter{page}{1}\begin{styleStandard}
\textbf{Cascading Collocations: Collocades as Correlates of Formulaic Language}
\end{styleStandard}

\begin{styleStandard}
\textbf{Abstract}
\end{styleStandard}

\begin{styleStandard}
This chapter focuses on a technique for detecting, measuring and displaying traces of formulaic language. For this purpose, a suite of computational procedures has been developed in order to quantify the degree to which individual texts and text types incorporate inflexible sequences of words. This development is predicated on the assumption that, even if we have no precise definition of formulaic language, it is widely accepted that it is characterized by repetition of fixed sequences. The method involves compiling a \textit{formulexicon} from a corpus of two or more text types and then using coverage by elements of that\textit{ formulexicon} as an index of the degree to which a text, possibly absent from the training corpus, is pervaded by formulaic sequences. The problem of deciding what lengths of n-grams are warranted by the data is dealt with by the simple expedient of binarizing coverage counts by n-grams of various lengths. Trials on a variety of text types show that this allows \textit{collocades} -{}- cascades of collocations, whose lengths are not pre-determined -{}- to emerge from the data. Here the term \textit{collocation} is used in its broader sense, as in {\textquotedbl}collocations are co-occurrences of words{\textquotedbl} (Gries, 2009: 14). Software in Python3 that implements this approach is available online under a Creative Commons licence. Examples of applying these procedures to a number of corpora illustrate some of the uses of this approach.
\end{styleStandard}

\begin{styleStandard}
\textbf{Keywords}: collocades, corpus-driven research, formulaic language, linguistic statistics, multi-word units, phraseology
\end{styleStandard}

\begin{styleStandard}
\textbf{1. Introduction}
\end{styleStandard}

\begin{styleStandard}
Many linguists have celebrated the {\textquotedbl}unlimited creative potential{\textquotedbl} of human language (Eggins 1994: 117). Ron Carter (2004) has argued that creativity is an all-pervasive feature of everyday language, a point also emphasized by Chomsky.
\end{styleStandard}

\begin{styleStandard}
{\textquotedbl}The normal use of language relies in an essential way on this unboundedness, on the fact that language contains devices for generating sentences of arbitrary complexity. Repetition of sentences is a rarity; innovation, in accordance with the grammar of the language, is the rule in ordinary day-by-day performance.{\textquotedbl} (Chomsky 1972: 118).
\end{styleStandard}

\begin{styleStandard}
On the other hand, others have noted the {\textquotedbl}deadly repetitiousness of language{\textquotedbl} (Bolinger 1965: 570). This refers to the fact that speakers and writers tend to reuse chunks of language, perhaps with slight variation, a phenomenon dubbed by Sinclair (1991) {\textquotedbl}the idiom principle{\textquotedbl}. Presumably this reflects a natural tendency to save mental effort. As Halliday puts it (2014), {\textquotedbl}repeated patterns require less brain power both to produce and to understand.{\textquotedbl}
\end{styleStandard}

\begin{styleStandard}
This apparent contradiction points to a dimension on which examples of language use can vary widely, from creative to routine. Research into \textit{formulaic language} is at least in part an attempt to explore this polarity. However, the term refers to a wide variety of linguistic phenomena. Essentially, formulaic language is a negative concept: we recognize it when the creative potential of ordinary language, celebrated by Carter, Chomsky, Eggins and others, appears to be partially or completely restricted. Such restriction can happen for diverse reasons, which helps to explain why no precise, agreed definition exists of what exactly constitutes formulaic language, although many researchers are actively engaged in studying its manifestations, such as idioms, clichés, legal boiler-plate and apparently prefabricated lexical bundles.
\end{styleStandard}

\begin{styleStandard}
It should perhaps be noted that, in contexts such as second-language learning, the use of multi-word units is sometimes viewed in a positive manner, as a sign that the learner is gaining phraseological competence. For instance, Granger \& Bestgen (2014) and Leńko-Szymańska (2016) explore the use of statistics relating to multi-word units as potential indices of learner competence. In this case the ability to deploy word groupings is evidence that the learner can operate with higher-level chunks.
\end{styleStandard}

\begin{styleStandard}
In any case, whether or not formulaic language is viewed pejoratively or as valuable, we do not have a widely accepted method of assessing just where on the polarity from creative to formulaic a given text or corpus lies. A major objective of the present chapter is therefore to describe a computable index of linguistic flexibility/inflexibility which could serve to indicate of the degree to which a text or text type exhibits formulaic language. This problem, namely to what degree a particular kind of language is formulaic, is one of the key questions in the field (Wray 2002: 4), one that has not been answered in a comprehensive manner.
\end{styleStandard}

\begin{styleStandard}
Strictly speaking, without a definition of our key term, it should not be possible to measure the extent to which a given text or speech is formulaic. Nevertheless, the present chapter sets out to describe, and apply, procedures that are designed to provide researchers with a quantitative index to associate with more impressionistic judgements; and to help identify sections within texts that can be further scrutinized as embodying mainly prefabricated segments. This development is predicated on the assumption that, even if we have no precise definition of formulaic language, it is widely accepted that it is characterized by repetition of fixed sequences.
\end{styleStandard}

\begin{styleStandard}
In short, we don't know exactly what formulaic language is, and suspect that it has multiple causes, but we will attempt to measure it anyway. This attitude isn't quite as unscientific as it might seem. One can compare the situation in biology, for example, regarding the crucial concept \textit{biodiversity}. Assessing biodiversity at various locations and trying to estimate the biodiversity of planet earth is something that scientists and many lay people agree is a matter of grave importance, although there is no single method for measuring it. Nevertheless, different researchers have proposed, and refined, a number of ways of quantifying the diversity of life forms in various habitats, which, between them, have helped to advance knowledge in this area (Magurran 2004); consequently we can do better than mere guesswork when assessing whether and where biodiversity is increasing or decreasing.
\end{styleStandard}

\begin{styleStandard}
In the sphere of linguistics, many studies have explored the phenomenon of collocation in a general sense, and several techniques have been developed to seek examples in corpora, using a variety of terms such as multi-word units, lexical bundles and others (e.g., Shimohata et al., 1999; Zhang et al., 2009; Kilgarriff et al., 2012). Moreover, programs exist that are generally available, for example kfNgram (Fletcher, 2012) and Wordsmith Tools (Scott, 2020), which automate some aspects of the search for co-occurring linguistic units. These tools and techniques are primarily aimed, however, at throwing light on the linguistic behaviour of the speakers or writers of the texts in question. In other words, the multi-item units discovered are regarded as results in themselves. Some researchers also limit themselves to pre-specified grammatical functions, such as noun phrases (e.g. Daille, 2003; Zhang et al., 2009) and thus presuppose reliance on ancillary parsing or tagging software. For this reason they are difficult or impossible to adapt to the main purpose of the present approach, which is to quantify the pervasiveness of such multi-item units in various texts and/or text types.
\end{styleStandard}

\begin{styleStandard}
The method described in this chapter involves compiling a \textit{formulexicon} from a corpus of two or more text types and then using coverage by elements of that\textit{ formulexicon} as an index of the degree to which a text, possibly absent from the training corpus, is pervaded by formulaic sequences. The problem of deciding what lengths of n-grams are warranted by the data is dealt with by the simple expedient of binarizing coverage counts by n-grams of various lengths. Trials on a variety of text types show that this allows \textit{collocades} -{}- cascades of collocations, whose lengths are not pre-determined -{}- to emerge from the data. The extent to which a text is covered by such collocades can be quantified as an index of the degree to which that text is formulaic. Software in Python3 that implements this approach is available online under a Creative Commons licence. (See Appendix.)
\end{styleStandard}

\begin{styleStandard}
\textbf{2. The formulib suite}
\end{styleStandard}

\begin{styleStandard}
To illustrate this approach, a small test corpus, consisting of seven subcorpora, has been compiled, briefly described in Table 1.
\end{styleStandard}

\begin{styleStandard}
Table 1: Test corpora
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.97885984in}|m{3.5386598in}|}
\hline
\textbf{Short name} &
\textbf{Description}\\\hline
BEER &
Texts from the back labels of beer bottles\\\hline
EW &
Short stories by Edith Wharton (1862-1937)\\\hline
FEWREPS &
Postings on Hong Kong Tripadvisor travel forum (2016) with fewer than 2 replies\\\hline
LEAFLET &
Information leaflets of medicines\\\hline
MANYREPS &
Postings on Hong Kong Tripadvisor travel forum (2016) with more than 10 replies\\\hline
SRES &
United Nations Security Council resolutions, 1999-2004\\\hline
WINE &
Texts from the back labels of wine bottles\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Basic details of the numbers and sizes of these text collections are described in Table 2.
\end{styleStandard}

\begin{styleStandard}
Table 2: Sizes of text corpora
\end{styleStandard}

\begin{flushright}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.97885984in}|m{0.58585984in}|m{0.68935984in}|m{0.6726598in}|m{0.6775598in}|m{0.67885983in}|}
\hline
\textbf{Short name} &
\textbf{Texts} &
\textbf{Tokens} &
\textbf{Smallest} &
\textbf{Median length} &
\textbf{Longest}\\\hline
BEER &
\raggedleft 118 &
\raggedleft 15781 &
\raggedleft 56 &
\raggedleft 129 &
\raggedleft\arraybslash 314\\\hline
EW &
\raggedleft 44 &
\raggedleft 365158 &
\raggedleft 2271 &
\raggedleft 8228 &
\raggedleft\arraybslash 15682\\\hline
FEWREPS &
\raggedleft 213 &
\raggedleft 14898 &
\raggedleft 17 &
\raggedleft 56 &
\raggedleft\arraybslash 359\\\hline
LEAFLET &
\raggedleft 461 &
\raggedleft 482373 &
\raggedleft 180 &
\raggedleft 946 &
\raggedleft\arraybslash 5251\\\hline
MANYREPS &
\raggedleft 610 &
\raggedleft 65494 &
\raggedleft 17 &
\raggedleft 80 &
\raggedleft\arraybslash 1468\\\hline
SRES &
\raggedleft 275 &
\raggedleft 248676 &
\raggedleft 102 &
\raggedleft 635 &
\raggedleft\arraybslash 5452\\\hline
WINE &
\raggedleft 86 &
\raggedleft 11474 &
\raggedleft 57 &
\raggedleft 125 &
\raggedleft\arraybslash 296\\\hline
\end{supertabular}
\end{flushright}
\begin{styleStandard}
In Table 2 text lengths are given in tokens, which are normally words, although numbers, i.e. strings of numerals, also count as tokens. It will be seen that many of these documents are individually very short. The Hong Kong TripAdvisor postings can be as small as 17 tokens in length. The above texts are all in English. The system has been used with other languages, including Chinese, and can be applied to any language than can be encoded in Unicode (utf-8).
\end{styleStandard}

\begin{styleStandard}
The suite of programs in Python3 that constitute the formulib package and their functions are summarized in Table 3.
\end{styleStandard}

\begin{styleStandard}
Table 3: Programs of formulib and their functions
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.9018598in}|m{3.5393598in}|}
\hline
\textbf{Program} &
\textbf{Function}\\\hline
outgrams &
The main input of this program is a collection of text files, normally divided into more than one text category. It compiles the components of a \textit{formulexicon} by finding the most frequent n1-grams up to n2-grams in two or more categories of text files where n1 is 2 and n2 is 5 by default.\\\hline
formulex &
This program takes a collection of text files of more than one category, typically the same collection as input to outgrams, and applies the \textit{formulexicon} already generated to compute coverage by collocades for each file and thereby indicate the extent of formulaic language in it. The program also produces a list of collocades, multi-word units whose lengths are derived from the data rather than being specified in advance (as will be explained below).\\\hline
taverns &
This program (Textual Affinity Values Employing Repeated N-gram Sequences) computes coverage of two or more categories of document not only by the collocades generated from their own category but by those of the other categories as well, thus identifying highly typical and highly atypical texts in each class. It also functions as a classifier by using coverage by collocades from all categories, to indicate likely category membership. Usually its input will be a holdout sample of text files which were not used in generating the n-gram lists.\\\hline
flicshow &
This program produces a colour-coded FLiC list (Formulaic Language in Context) designed to show how the collocades are distributed in various texts. (See section 5.)\\\hline
postflab &
This program takes a secondary output of formulex, the collocade list, and applies a 1-dimensional similarity scaling procedure to them, so that related sequences can be plotted in a way that reveals their inter-relationships. (See section 7.)\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
\textbf{3. The trouble with n-grams}
\end{styleStandard}

\begin{styleStandard}
The initial program of this suite, outgrams, is relatively conventional. Its main output, referred to here as a \textit{formulexicon}, is a list of the N most frequent n-grams for each category of the input text files, with N=80 by default. A segment of its output for the SRES category, when the minimum and maximum n-grams lengths were specified as 3 to 6 (tokens), follows. The software allows punctuation to be ignored or preserved, and, independently, for upper case to be preserved or folded to lower case. In this and subsequent examples the option of removing punctuation was chosen, and lower case was enforced. Among other things, this illustrates why n-gram lists, in and of themselves, are not particularly enlightening.
\end{styleStandard}

\begin{styleStandard}
\# sres 154799 978607
\end{styleStandard}

\begin{styleStandard}
1 (6, 175, 34, ('adopted', 'by', 'the', 'security', 'council', 'at'))
\end{styleStandard}

\begin{styleStandard}
2 (6, 175, 30, ('by', 'the', 'security', 'council', 'at', 'its'))
\end{styleStandard}

\begin{styleStandard}
3 (6, 153, 36, ('the', 'democratic', 'republic', 'of', 'the', 'congo'))
\end{styleStandard}

\begin{styleStandard}
4 (6, 130, 33, ('the', 'charter', 'of', 'the', 'united', 'nations'))
\end{styleStandard}

\begin{styleStandard}
5 (6, 121, 35, ('the', 'report', 'of', 'the', 'secretary', 'general'))
\end{styleStandard}

\begin{styleStandard}
6 (6, 97, 28, ('of', 'the', 'charter', 'of', 'the', 'united'))
\end{styleStandard}

\begin{styleStandard}
7 (6, 95, 36, ('decides', 'to', 'remain', 'actively', 'seized', 'of'))
\end{styleStandard}

\begin{styleStandard}
8 (6, 93, 36, ('remain', 'actively', 'seized', 'of', 'the', 'matter'))
\end{styleStandard}

\begin{styleStandard}
9 (6, 92, 32, ('to', 'remain', 'actively', 'seized', 'of', 'the'))
\end{styleStandard}

\begin{styleStandard}
10 (6, 87, 34, ('report', 'of', 'the', 'secretary', 'general', 'of'))
\end{styleStandard}

\begin{styleStandard}
[.... many lines omitted ....]
\end{styleStandard}

\begin{styleStandard}
1 (3, 624, 21, ('the', 'secretary', 'general'))
\end{styleStandard}

\begin{styleStandard}
2 (3, 579, 18, ('the', 'united', 'nations'))
\end{styleStandard}

\begin{styleStandard}
3 (3, 520, 20, ('the', 'security', 'council'))
\end{styleStandard}

\begin{styleStandard}
4 (3, 319, 17, ('the', 'government', 'of'))
\end{styleStandard}

\begin{styleStandard}
5 (3, 312, 13, ('of', 'the', 'united'))
\end{styleStandard}

\begin{styleStandard}
6 (3, 245, 16, ('of', 'the', 'secretary'))
\end{styleStandard}

\begin{styleStandard}
7 (3, 243, 20, ('secretary', 'general', 'to'))
\end{styleStandard}

\begin{styleStandard}
8 (3, 237, 18, ('in', 'accordance', 'with'))
\end{styleStandard}

\begin{styleStandard}
9 (3, 207, 21, ('the', 'implementation', 'of'))
\end{styleStandard}

\begin{styleStandard}
10 (3, 202, 22, ('requests', 'the', 'secretary'))
\end{styleStandard}

\begin{styleStandard}
Here we have the ten most frequent 6-grams and the ten most frequent 3-grams derived from the SRES subcorpus. The top line indicates that these n-grams are based on 154,799 tokens which amount to 978,607 characters. This is less than the size given in Table 3 because the full dataset has been split randomly into training and test sets, of 1117 and 690 text files respectively.
\end{styleStandard}

\begin{styleStandard}
The first two 6-grams both occur 175 times in this corpus. The first is 34 characters in length, including blanks between tokens, and the second is 30 characters long. It seems a fair inference that they arise from a 7-gram, namely {\textquotedbl}adopted by the security council at its{\textquotedbl}, but that is hardly obvious from the listing. Likewise, if we take items 7, 8 and 9 together, which occur 95, 93 and 92 times respectively, with a bit of background knowledge, we might arrive at the phrase {\textquotedbl}decides to remain actively seized of the matter{\textquotedbl}, an 8-gram with which many of these Security Council resolutions sign off. However, this also is not immediately obvious from the listing (which is in fact designed more to be read by other programs than by people).
\end{styleStandard}

\begin{styleStandard}
The point applies also with the shorter n-grams. The first three 3-grams (with the benefit of background knowledge) would seem to be natural units in their own right. But along with them, we find {\textquotedbl}secretary general to{\textquotedbl} and {\textquotedbl}requests the secretary{\textquotedbl}, which are fragments of longer phrases.
\end{styleStandard}

\begin{styleStandard}
What this illustrates is that in a standard frequency list of fixed-size n-grams, such as that above, longer n-grams tend to appear in the form of multiple fragments. It requires tedious inspection, along with background knowledge, to identify appropriate lengths for the fragmented pieces of repetitive phrasings. It is desirable for the computer to provide more help in that identification process.
\end{styleStandard}

\begin{styleStandard}
\textbf{4. From n-grams to collocades}
\end{styleStandard}

\begin{styleStandard}
The formulex program is designed for this purpose. The basic idea behind this program is very simple. The problem with n-gram lists is that they tend to contain multiple fragments of longer sequences, losing track of what might be considered the natural length of the sequences from which they are derived. So formulex tries to put them back together by going back over the original texts to find out exactly which passages are covered by the items in the frequent n-gram list. The key concept here is \textbf{coverage}. The important point is that a text sequence is either covered or not: the number of n-grams that match a particular sequence of tokens doesn't matter, just whether any do or none.
\end{styleStandard}

\begin{styleStandard}
To give an illustration of the covering process, suppose that you have gathered a corpus of political propaganda in which the phrase
\end{styleStandard}

\begin{styleStandard}
{\textquotedbl}securing a better future for hardworking families{\textquotedbl}
\end{styleStandard}

\begin{styleStandard}
is repeated ad nauseam.
\end{styleStandard}

\begin{styleStandard}
This could be regarded as a frequent 7-gram, but with the system's default settings, the longest n-grams to be saved will be 5-grams. Thus the n-gram list would probably include
\end{styleStandard}

\begin{styleStandard}
{\textquotedbl}securing a better future for{\textquotedbl}
\end{styleStandard}

\begin{styleStandard}
{\textquotedbl}a better future for hardworking{\textquotedbl}
\end{styleStandard}

\begin{styleStandard}
{\textquotedbl}better future for hardworking families{\textquotedbl}
\end{styleStandard}

\begin{styleStandard}
as well as shorter subsequences, probably going down to 2-grams such as {\textquotedbl}a better{\textquotedbl}, {\textquotedbl}better future{\textquotedbl} and {\textquotedbl}hardworking families{\textquotedbl}. Suppose further that the program is processing the sentence shown in Table 4 (tabulated vertically, for convenience).
\end{styleStandard}

\begin{styleStandard}
Table 4: Computing coverage in formulex
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.93095976in}|m{0.59275985in}|m{0.8011598in}|m{1.0337598in}|m{1.0038599in}|}
\hline
\textbf{(word) token} &
\textbf{match count} &
\textbf{covering n-gram(s):} &
 &
\\\hline
we &
0 &
 &
 &
\\\hline
are &
0 &
 &
 &
\\\hline
committed &
0 &
 &
 &
\\\hline
to &
0 &
 &
 &
\\\hline
securing &
1 &
securing &
 &
\\\hline
a &
2 &
a &
a &
\\\hline
better &
3 &
better  &
better &
better\\\hline
future &
3 &
future &
future &
future\\\hline
for &
3 &
for &
for &
for\\\hline
hardworking &
2 &
 &
hardworking &
hardworking\\\hline
families &
1 &
 &
 &
families\\\hline
throughout &
0 &
 &
 &
\\\hline
britain &
0 &
 &
 &
\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
To keep things manageable, 4-, 3- and 2-grams have been ignored. This might well increase the totals in the column labelled {\textquotedbl}match count{\textquotedbl}, but the main point is that coverage will be determined by whether this figure is greater than zero or not. The total number of matches isn't taken into account for this purpose. (It can be used in flicshow, as described in section 5.)
\end{styleStandard}

\begin{styleStandard}
Sticking just to these 13 words (87 characters, including single spaces between words) and three 5-grams, the coverage would be 48/87 characters and 7/13 tokens, i.e. just the words that have a nonzero entry next to them under {\textquotedbl}match count{\textquotedbl}. This would appear as 55.17\% and 53.85\% in the output. These two percentages tend to be highly correlated, meaning that similar conclusions are likely to be drawn from either. Character-coverage is placed first because I believe it is likely to be a slightly more sensitive indicator.
\end{styleStandard}

\begin{styleStandard}
To summarize, the program works out coverage of tokens in this manner for each file separately using the n-grams from the same category as the text concerned (or the largest category if the text has an unseen class label) and also aggregates the coverage for each category. The texts are listed in descending order of character coverage.
\end{styleStandard}

\begin{styleStandard}
The beginning of the main output file resulting from processing by formulex of a training sample of 1117 texts from the seven text categories described in Table 1 is shown below. In this case the formulexicon generated by outgrams contained the most frequent 3- to 6-grams from each category.
\end{styleStandard}

\begin{styleStandard}
Wed Oct 16 15:28:36 2019
\end{styleStandard}

\begin{styleStandard}
parafile: C:{\textbackslash}keywork{\textbackslash}parapath{\textbackslash}grabchap.txt
\end{styleStandard}

\begin{styleStandard}
metafile: c:{\textbackslash}keywork{\textbackslash}mets{\textbackslash}grabchap\_met1.dat
\end{styleStandard}

\begin{styleStandard}
miniglen: 3
\end{styleStandard}

\begin{styleStandard}
maxiglen: 6
\end{styleStandard}

\begin{styleStandard}
topgrams: 80
\end{styleStandard}

\begin{styleStandard}
1117 7
\end{styleStandard}

\begin{styleStandard}
Category coverage\% (characters, tokens) by frequent n-grams :
\end{styleStandard}

\begin{styleStandard}
0 EW \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 3.1528 \ \ \ 4.0587
\end{styleStandard}

\begin{styleStandard}
1 beer \ \ \ \ \ \ \ \ \ \ \ \ \ 23.6432 \ \ 23.4533
\end{styleStandard}

\begin{styleStandard}
2 fewreps \ \ \ \ \ \ \ \ \ \ \ 9.1539 \ \ 10.3654
\end{styleStandard}

\begin{styleStandard}
3 leaflet \ \ \ \ \ \ \ \ \ \ 13.3395 \ \ 14.9340
\end{styleStandard}

\begin{styleStandard}
4 manyreps \ \ \ \ \ \ \ \ \ \ 8.2614 \ \ \ 9.6554
\end{styleStandard}

\begin{styleStandard}
5 sres \ \ \ \ \ \ \ \ \ \ \ \ \ 17.6441 \ \ 18.8619
\end{styleStandard}

\begin{styleStandard}
6 wine \ \ \ \ \ \ \ \ \ \ \ \ \ 20.0487 \ \ 20.2816
\end{styleStandard}

\begin{styleStandard}
Document coverage\% (characters, tokens) by frequent n-grams :
\end{styleStandard}

\begin{styleStandard}
\ \ \ 1 \ \ \ 526 \ \ \ 85 \ \ 57.50 \ 58.82 \ beer \ \ \ low\_alcohol\_czech\_lager.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 2 \ \ \ 607 \ \ 103 \ \ 49.34 \ 49.51 \ wine \ \ \ fabcab.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 3 \ \ \ 789 \ \ 136 \ \ 48.86 \ 47.79 \ beer \ \ \ island\_hopper\_pale\_ale.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 4 \ \ \ 596 \ \ 102 \ \ 48.07 \ 47.06 \ sres \ \ \ S\_RES\_13212000-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 5 \ \ \ 613 \ \ 102 \ \ 47.07 \ 46.08 \ beer \ \ \ ruddles\_best.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 6 \ \ \ 188 \ \ \ 39 \ \ 46.03 \ 43.59 \ fewreps poor72995652\_9302772\_Getting\_to\_Kai\_Tak\_Cruise\_Termin.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 7 \ \ \ 940 \ \ 154 \ \ 45.70 \ 42.86 \ sres \ \ \ S\_RES\_15042003-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 8 \ \ 1761 \ \ 290 \ \ 43.25 \ 45.17 \ sres \ \ \ S\_RES\_15552004-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 9 \ \ 1030 \ \ 165 \ \ 42.97 \ 46.06 \ sres \ \ \ S\_RES\_14892003-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 10 \ \ \ 237 \ \ \ 45 \ \ 42.86 \ 46.67 \ manyreps good75047818\_9504892\_First\_visit\_to\_hong\_Kongwhere\_t.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 11 \ \ \ 525 \ \ \ 96 \ \ 42.78 \ 40.62 \ wine \ \ \ long\_slim\_chile.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 12 \ \ \ 645 \ \ 112 \ \ 42.72 \ 41.07 \ beer \ \ \ youngs\_bitter.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 13 \ \ \ 543 \ \ \ 90 \ \ 42.10 \ 42.22 \ beer \ \ \ corona\_extra.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 14 \ \ \ 618 \ \ 104 \ \ 42.00 \ 42.31 \ wine \ \ \ coop\_chilean\_merlot.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 15 \ \ \ 718 \ \ 118 \ \ 41.72 \ 40.68 \ sres \ \ \ S\_RES\_15052003-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 16 \ \ 1259 \ \ 193 \ \ 41.67 \ 44.04 \ wine \ \ \ lime\_tree\_cabernet\_sauvignon.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 17 \ \ \ 782 \ \ 131 \ \ 41.51 \ 43.51 \ sres \ \ \ S\_RES\_14852003-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 18 \ \ \ 780 \ \ 139 \ \ 40.85 \ 40.29 \ beer \ \ \ battersea\_rye.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 19 \ \ \ 889 \ \ 149 \ \ 40.67 \ 40.27 \ beer \ \ \ salts\_burton\_ale.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 20 \ \ \ 717 \ \ 111 \ \ 40.67 \ 42.34 \ wine \ \ \ coop\_chianti\_2013.txt
\end{styleStandard}

\begin{styleStandard}
[.... many lines omitted ....]
\end{styleStandard}

\begin{styleStandard}
According to either character-coverage or token coverage, the categories can be ranked from most to least as follows: BEER, WINE, SRES, LEAFLET, FEWREPS, MANYREPS, EW.
\end{styleStandard}

\begin{styleStandard}
If we accept n-gram coverage as illustrated in Table 4 as an index of formulaicity, the most formulaic individual text is the beer back label for low alcohol Czech lager. The numbers on the line preceding that item indicate that this text consisted of 526 character containing 85 tokens. The 3- to 6-grams of the beer back label formulexicon covered 57.50\% of the whole text in terms of characters, and 58.82\% of its tokens.
\end{styleStandard}

\begin{styleStandard}
Only texts from five of the seven categories appear in this top 20. The patient-information leaflet with the highest coverage was ranked 44th and the tale by Edith Wharton with the highest coverage came in at position 918 with scores of 4.65\% and 5.59\% -{}- in the last 100 of 1117 items. Clearly literary fiction does not contain long slabs of prefabricated language.
\end{styleStandard}

\begin{styleStandard}
\textbf{5. Collocades in context, and in colour}
\end{styleStandard}

\begin{styleStandard}
The formulex program identifies which text types are most pervaded by repetitive sequences, and thus most likely to contain a high level of formulaic language. It also identifies individual texts that are high or low in coverage by repetitive sequences, but does not identify which sequences occur where.
\end{styleStandard}

\begin{styleStandard}
After examining the output and particularly after noting texts that are particularly high or low in collocade coverage, an analyst will naturally want to know more about which collocades are responsible for such differences. This is the purpose of the flicshow program, which is intended to show formulaic language in context.
\end{styleStandard}

\begin{styleStandard}
It takes as input the formulexicon file produced by outgrams and applies it to a list of specified files. As output it writes a tokenized version of each input file in html. These outputs can be viewed in a browser such as Edge, Chrome or Mozilla Firefox.
\end{styleStandard}

\begin{styleStandard}
In these output files the portions covered by the n-grams of the relevant category (i.e. the category of the text being processed, or the largest category if it has an unknown category label) are highlighted in colour, while the rest of the text is printed in black. Each token is written in a colour that depends on the length of the longest n-gram by which it is covered (by default) or the number of, potentially overlapping, n-grams that cover it (as an option).
\end{styleStandard}

\begin{styleStandard}
Table 5: Colour-coding by longest n-gram that covers a particular token (default mode)
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{1.0011599in}|m{2.7747598in}|}
\hline
Score &
Colour\\\hline
6+ &
purple\\\hline
5 &
red\\\hline
4 &
orange\\\hline
3 &
green\\\hline
2 &
blue\\\hline
1 &
cyan\\\hline
0 &
black\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
An example, which is the text of UN Security Council resolution 1321, is shown in in Figure 1, to illustrate the kind of output generated. The colour scheme employed is detailed in Table 5.
\end{styleStandard}

\begin{styleStandard}
  [Warning: Image ignored] % Unhandled or unsupported graphics:
%\includegraphics[width=4.6063in,height=2.6728in,width=\textwidth]{forsyth-img001.jpg}
 
\end{styleStandard}

\begin{styleStandard}
Figure 1: Text of UN Security Council resolution 1321, colour-coded by flicshow
\end{styleStandard}

\begin{styleStandard}
The format here is that each change from covered to uncovered text starts a new line, so the original layout is lost; moreover, punctuation has been ignored, as in previous examples. The last line, {\textquotedbl}decides to remain actively seized of the matter{\textquotedbl}, is an 8-gram. As already mentioned, the system was using up to 6-grams, so this indicates how fragmented portions of a sequence can overlap to give a better idea of the natural length of a repeated sequence.
\end{styleStandard}

\begin{styleStandard}
More interesting is the third line from the bottom, {\textquotedbl}of the united nations mission in sierra leone{\textquotedbl}. This begins with a single token {\textquotedbl}of{\textquotedbl} in orange. Orange is the colour of 4-grams, but each token gets the colour of the longest n-gram that covers it; so this implies that the 4-gram {\textquotedbl}of the united nations{\textquotedbl} was, in a sense, trumped, by the 5-gram, {\textquotedbl}the united nations mission in{\textquotedbl}. Similarly, the last two words, {\textquotedbl}sierra leone{\textquotedbl} are in green, indicating a 3-gram, but presumably the 3-gram {\textquotedbl}in sierra leone{\textquotedbl} is also trumped by the same 5-gram, so that its leading token, {\textquotedbl}in{\textquotedbl}, receives the colour of the longer sequence, in which it is the final token.
\end{styleStandard}

\begin{styleStandard}
The intent of this colour scheme is to assist investigation of phraseological patterns by highlighting what might be termed a quasi-syntax, showing how longer collocades are built up from shorter segments. Figure 2 shows another example, from the Hong Kong Tripadvisor postings, one that received many replies.
\end{styleStandard}

\begin{styleStandard}
  [Warning: Image ignored] % Unhandled or unsupported graphics:
%\includegraphics[width=4.6063in,height=2.0492in,width=\textwidth]{forsyth-img002.jpg}
 
\end{styleStandard}

\begin{styleStandard}
Figure 2: Colour-coded collocades in HK Tripadvisor forum posting
\end{styleStandard}

\begin{styleStandard}
  [Warning: Image ignored] % Unhandled or unsupported graphics:
%\includegraphics[width=4.6063in,height=4.5146in,width=\textwidth]{forsyth-img003.jpg}
 
\end{styleStandard}

\begin{styleStandard}
Figure 3: Extract from a tale by Edith Wharton
\end{styleStandard}

\begin{styleStandard}
By contrast, Figure 3 shows output from flicshow when applied to a story by Edith Wharton called Venetian Nights. This extract exhibits long blocks of running text in black, with only a scattering of highlighted n-grams, most of which do not connect or overlap.
\end{styleStandard}

\begin{styleStandard}
\textbf{6. \ Classification via collocade coverage}
\end{styleStandard}

\begin{styleStandard}
The taverns program (Textual Affinity Values Employing Repeated N-gram Sequences) uses the formulexicon in a slightly different way, intended to indicate which texts are typical and atypical of their category and indicate how distinctive the categories are among themselves.
\end{styleStandard}

\begin{styleStandard}
Inspecting individual texts can be a valuable opportunity to get close to the data, but in a typical corpus there is a huge amount of data to be inspected. The program taverns works in bulk mode and thereby gives an indication of which particular files might deserve the kind of close attention given to the output of flicshow. It goes a step further than formulex, using the same method, by computing coverage of each text specified not only by the n-grams of its own category, but by those of all the categories in the formulexicon file. Thus, in effect, it ranks each text file according to how typical it is of each category, including its own. Normally these texts are an unseen holdout sample, not used by outgrams to create the formulexicon.
\end{styleStandard}

\begin{styleStandard}
In addition, having done this, it performs text classification by assigning each text to the category which gives it the highest coverage score. It is not intended primarily as a text classifier, but the results in classification mode often shed light on the relationships between the text types involved, as well as identifying typical and anomalous texts.
\end{styleStandard}

\begin{styleStandard}
The listing below shows the first 15 and last 15 lines of the taverns output for coverage by the BEER formulexicon.
\end{styleStandard}

\begin{styleStandard}
Ranking by coverage of sequences from beer
\end{styleStandard}

\begin{styleStandard}
\ \ \ 1 \ \ \ 587 \ \ \ 97 \ \ 47.53 \ 47.42 \ beer \ \ \ \ youngs\_hummingbird.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 2 \ \ \ 586 \ \ 100 \ \ 44.37 \ 43.00 \ beer \ \ \ \ sol\_cerveza.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 3 \ \ \ 438 \ \ \ 79 \ \ 42.01 \ 40.51 \ beer \ \ \ \ budweiser.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 4 \ \ \ 651 \ \ 111 \ \ 41.47 \ 40.54 \ beer \ \ \ \ tolly\_english\_ale.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 5 \ \ \ 595 \ \ 100 \ \ 41.01 \ 41.00 \ beer \ \ \ \ wells\_bombardier.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 6 \ \ \ 636 \ \ 105 \ \ 35.69 \ 36.19 \ beer \ \ \ \ lancaster\_blonde.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 7 \ \ \ 709 \ \ 123 \ \ 34.41 \ 33.33 \ beer \ \ \ \ mcewans\_amber.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 8 \ \ \ 568 \ \ \ 90 \ \ 33.98 \ 34.44 \ beer \ \ \ \ marstons\_burton\_bitter.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 9 \ \ \ 808 \ \ 135 \ \ 32.18 \ 28.89 \ beer \ \ \ \ blacksheep\_venusmars.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 10 \ \ 1101 \ \ 186 \ \ 31.06 \ 29.57 \ beer \ \ \ \ spitfire.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 11 \ \ \ 683 \ \ 120 \ \ 29.28 \ 28.33 \ beer \ \ \ \ brains\_sa.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 12 \ \ \ 592 \ \ 102 \ \ 28.89 \ 29.41 \ beer \ \ \ \ wadworth\_ipa.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 13 \ \ \ 561 \ \ \ 96 \ \ 26.92 \ 27.08 \ beer \ \ \ \ yorkshire\_gold.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 14 \ \ \ 931 \ \ 153 \ \ 26.85 \ 26.80 \ beer \ \ \ \ weetwood\_southern\_cross.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 15 \ \ \ 571 \ \ \ 92 \ \ 26.27 \ 27.17 \ wine \ \ \ \ domaine\_mandeville.txt
\end{styleStandard}

\begin{styleStandard}
[.... many lines omitted ....]
\end{styleStandard}

\begin{styleStandard}
\ 676 \ \ 7397 \ 1287 \ \ \ 0.00 \ \ 0.00 \ leaflet \ Angitil\_SR.txt
\end{styleStandard}

\begin{styleStandard}
\ 677 \ \ 5045 \ \ 909 \ \ \ 0.00 \ \ 0.00 \ leaflet \ Amoxil\_Syrup.txt
\end{styleStandard}

\begin{styleStandard}
\ 678 \ \ 4384 \ \ 789 \ \ \ 0.00 \ \ 0.00 \ leaflet \ Amoxil\_Capsules.txt
\end{styleStandard}

\begin{styleStandard}
\ 679 \ \ 6619 \ 1144 \ \ \ 0.00 \ \ 0.00 \ leaflet \ Algitec\_Chewtab\_Tablets.txt
\end{styleStandard}

\begin{styleStandard}
\ 680 \ \ 6198 \ \ 946 \ \ \ 0.00 \ \ 0.00 \ leaflet \ Adenocor.txt
\end{styleStandard}

\begin{styleStandard}
\ 681 \ 12823 \ 2238 \ \ \ 0.00 \ \ 0.00 \ leaflet \ Actrapid\_Pen.txt
\end{styleStandard}

\begin{styleStandard}
\ 682 \ \ \ 762 \ \ 123 \ \ \ 0.00 \ \ 0.00 \ wine \ \ \ \ two\_oceans\_chardonnay.txt
\end{styleStandard}

\begin{styleStandard}
\ 683 \ \ 1072 \ \ 180 \ \ \ 0.00 \ \ 0.00 \ wine \ \ \ \ rina\_ianca.txt
\end{styleStandard}

\begin{styleStandard}
\ 684 \ \ \ 483 \ \ \ 75 \ \ \ 0.00 \ \ 0.00 \ wine \ \ \ \ perlage\_pinot\_grigio.txt
\end{styleStandard}

\begin{styleStandard}
\ 685 \ \ \ 638 \ \ 111 \ \ \ 0.00 \ \ 0.00 \ wine \ \ \ \ paulmas\_vinus.txt
\end{styleStandard}

\begin{styleStandard}
\ 686 \ \ \ 812 \ \ 139 \ \ \ 0.00 \ \ 0.00 \ wine \ \ \ \ la\_chiave\_2013.txt
\end{styleStandard}

\begin{styleStandard}
\ 687 \ \ \ 569 \ \ \ 92 \ \ \ 0.00 \ \ 0.00 \ wine \ \ \ \ finca\_fabian.txt
\end{styleStandard}

\begin{styleStandard}
\ 688 \ \ \ 972 \ \ 153 \ \ \ 0.00 \ \ 0.00 \ wine \ \ \ \ era\_puglia\_falanghina.txt
\end{styleStandard}

\begin{styleStandard}
\ 689 \ \ \ 649 \ \ 105 \ \ \ 0.00 \ \ 0.00 \ wine \ \ \ \ domaine\_begude.txt
\end{styleStandard}

\begin{styleStandard}
\ 690 \ \ \ 532 \ \ \ 92 \ \ \ 0.00 \ \ 0.00 \ wine \ \ \ \ doblez\_garnacha.txt
\end{styleStandard}

\begin{styleStandard}
The first item refers to the text of the back label of Young's Hummingbird ale. The first number is its rank, 1. The next two numbers give its size in characters and tokens, 587 and 97. The next two numbers show that 47.53\% of its characters and 47.42\% of its tokens were covered by 3- to 6-grams from the formulexicon of the BEER category. The last two columns give the actual category of the text and its file name. Note that this is a genuine holdout test, on 690 files that were not used by outgrams to create the formulexicon.
\end{styleStandard}

\begin{styleStandard}
It will be seen that one WINE text creeps into the top 15 items as measured by typicality to the BEER category. In the bottom 15 items there are nine WINE texts. However, only 117 texts have more than zero coverage by the BEER formulexicon, so the order of the last 573 texts, with no coverage at all, is essentially arbitrary.
\end{styleStandard}

\begin{styleStandard}
After listing each text as covered by n-grams from the formulexicon of each category (7 in this case) the program classifies each text according to how much of it is covered by each category's n-grams, taking maximum coverage to decide the assigned category. For the present example, the most confident 15 entries are listed below.
\end{styleStandard}

\begin{styleStandard}
Results in classification mode :
\end{styleStandard}

\begin{styleStandard}
\ \ \ \ \ \ relative \ \ actual \ \ \ \ \ \ categories
\end{styleStandard}

\begin{styleStandard}
rank \ coverage\% coverage\% \ \ \ \ pred : true \ \ \ \ \ docname
\end{styleStandard}

\begin{styleStandard}
\ \ \ 1 \ \ \ 100.00 \ \ \ 35.29 \ \ \ \ \ \ sres + sres \ S\_RES\_12942000-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 2 \ \ \ 100.00 \ \ \ 30.10 \ \ \ \ \ \ sres + sres \ S\_RES\_13362001-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 3 \ \ \ 100.00 \ \ \ 30.09 \ \ \ \ \ \ sres + sres \ S\_RES\_15002003-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 4 \ \ \ 100.00 \ \ \ 29.86 \ \ \ \ \ \ sres + sres \ S\_RES\_13162000-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 5 \ \ \ 100.00 \ \ \ 29.85 \ \ \ \ \ \ sres + sres \ S\_RES\_14762003-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 6 \ \ \ 100.00 \ \ \ 29.63 \ \ \ \ \ \ sres + sres \ S\_RES\_14432002-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 7 \ \ \ 100.00 \ \ \ 28.53 \ \ \ \ \ \ sres + sres \ S\_RES\_13482001-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 8 \ \ \ 100.00 \ \ \ 27.94 \ \ \ \ \ \ sres + sres \ S\_RES\_13882002-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ \ 9 \ \ \ 100.00 \ \ \ 24.90 \ \ \ \ \ \ sres + sres \ S\_RES\_14582003-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 10 \ \ \ 100.00 \ \ \ 24.86 \ \ \ \ \ \ sres + sres \ S\_RES\_15182003-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 11 \ \ \ 100.00 \ \ \ 24.73 \ \ \ \ \ \ sres + sres \ S\_RES\_14652003-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 12 \ \ \ 100.00 \ \ \ 24.10 \ \ \ \ \ \ sres + sres \ S\_RES\_15482004-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 13 \ \ \ 100.00 \ \ \ 24.03 \ \ \ \ \ \ sres + sres \ S\_RES\_13872002-en.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 14 \ \ \ 100.00 \ \ \ 23.73 \ \ \ leaflet + leaflet \ SlowFe.txt
\end{styleStandard}

\begin{styleStandard}
\ \ 15 \ \ \ 100.00 \ \ \ 23.62 \ \ \ \ \ \ sres + sres \ S\_RES\_15302004-en.txt
\end{styleStandard}

\begin{styleStandard}
The top line of this output
\end{styleStandard}

\begin{styleStandard}
\ \ \ 1 \ \ \ 100.00 \ \ \ 35.29 \ \ \ \ \ \ sres + sres \ S\_RES\_12942000-en.txt
\end{styleStandard}

\begin{styleStandard}
signifies that Security Council resolution 1294 (from year 2000) has 35.29\% coverage by SRES n-grams. The number in the second column, 100.00, indicates that this 35.29\% represents 100\% of the total coverage by all 7 formulexicons, i.e. that n-grams from no category apart from SRES covered any of this text. These are the most confident classifications, 14 of the 15 being Security Council resolutions and one a medicine information leaflet.
\end{styleStandard}

\begin{styleStandard}
The column labels {\textquotedbl}pred{\textquotedbl} and {\textquotedbl}true{\textquotedbl} stand for predicted and true category. The segment {\textquotedbl}sres + sres{\textquotedbl} means that this file was predicted to belong to the SRES class and it was indeed from that class. The plus sign marks a correct decision: a minus sign would appear if it were incorrect and a question mark if the text's category were unknown.
\end{styleStandard}

\begin{styleStandard}
Overall classification performance is summarized at the foot of the output listing by a confusion matrix, such as that for the present example, listed below.
\end{styleStandard}

\begin{styleStandard}
Confusion matrix :
\end{styleStandard}

\begin{styleStandard}
Truecat = \ \ \ \ \ \ \ \ \ EW \ \ \ \ beer \ fewreps \ leaflet manyreps \ sres \ wine
\end{styleStandard}

\begin{styleStandard}
Predcat : EW \ \ \ \ \ \ 23 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 3 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 6 \ \ \ 0 \ \ \ \ \ 0
\end{styleStandard}

\begin{styleStandard}
Predcat : beer \ \ \ \ \ 0 \ \ \ \ \ \ 37 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ 0 \ \ \ \ \ 4
\end{styleStandard}

\begin{styleStandard}
Predcat : fewreps \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ 10 \ \ \ \ \ \ \ 1 \ \ \ \ \ \ 42 \ \ \ 0 \ \ \ \ \ 0
\end{styleStandard}

\begin{styleStandard}
Predcat : leaflet \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ 172 \ \ \ \ \ \ \ 1 \ \ \ 0 \ \ \ \ \ 0
\end{styleStandard}

\begin{styleStandard}
Predcat : manyreps \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ 65 \ \ \ \ \ \ \ 0 \ \ \ \ \ 189 \ \ \ 0 \ \ \ \ \ 2
\end{styleStandard}

\begin{styleStandard}
Predcat : sres \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 1 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ 100 \ \ \ \ \ 0
\end{styleStandard}

\begin{styleStandard}
Predcat : wine \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ 0 \ \ \ \ 34
\end{styleStandard}

\begin{styleStandard}
[As a monospaced text Listing.]
\end{styleStandard}

\begin{styleStandard}
  [Warning: Image ignored] % Unhandled or unsupported graphics:
%\includegraphics[width=4.6063in,height=1.3209in,width=\textwidth]{forsyth-img004.jpg}
 
\end{styleStandard}

\begin{styleStandard}
[As a Figure.]
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{4.52056in}|}
\hline
Confusion matrix :

Truecat = \ \ \ \ \ \ \ \ \ EW \ \ \ \ beer \ fewreps \ leaflet manyreps \ sres \ wine

Predcat : EW \ \ \ \ \ \ 23 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 3 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 6 \ \ \ 0 \ \ \ \ \ 0

Predcat : beer \ \ \ \ \ 0 \ \ \ \ \ \ 37 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ 0 \ \ \ \ \ 4

Predcat : fewreps \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ 10 \ \ \ \ \ \ \ 1 \ \ \ \ \ \ 42 \ \ \ 0 \ \ \ \ \ 0

Predcat : leaflet \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ 172 \ \ \ \ \ \ \ 1 \ \ \ 0 \ \ \ \ \ 0

Predcat : manyreps \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ 65 \ \ \ \ \ \ \ 0 \ \ \ \ \ 189 \ \ \ 0 \ \ \ \ \ 2

Predcat : sres \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 1 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ 100 \ \ \ \ \ 0

Predcat : wine \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ 0 \ \ \ 0 \ \ \ \ 34\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
[As a Table.]
\end{styleStandard}

\begin{styleStandard}
Here the procedure makes 125 errors out of 690 decisions, about 18 percent, but only 18 of these mistakes, 2.6 percent of all 690 cases, arise from categories other than FEWREPS and MANYREPS. Essentially, this means that the system cannot distinguish between Hong Kong forum posts that receive few replies and those that receive many. Given how short these texts are (median sizes of 56 and 80 tokens) it would have been surprising, though interesting, if the two classes had been readily distinguishable by such a process. On the other hand, the other categories, even BEER and WINE, are well distinguished on this basis.
\end{styleStandard}

\begin{styleStandard}
To give a point of comparison, the method described in Wright (2017) was implemented and applied to the same dataset. Wright obtained good results with this method in classifying messages from the Enron email corpus (Cohen, 2009) according to author. Each text was assigned the category with the highest similarity score based on the Jaccard coefficient (J) using sets of n-grams. The Jaccard coefficient divides the size of the set intersection by the size of the set union as in the formula
\end{styleStandard}

\begin{styleStandard}
\ \ J = {\textbar}A ${\cap}$ B{\textbar} / {\textbar}A ${\cup}$ B{\textbar}
\end{styleStandard}

\begin{styleStandard}
were A is the set of n-grams in a single test text and B is the set of n-grams in the group of texts belonging to a particular category.
\end{styleStandard}

\begin{styleStandard}
Wright's best results were found with tetragrams (4-grams) so that length was used on the same data as processed by the taverns program, above. Where taverns achieved a classification success rate of 81.88\% (565/690), the Jaccard-similarity technique achieved 79.71\% (550/690). When ignoring cases with zero similarity to any category, the rates were 83.77\% (542/647) with taverns and 81.39\% (503/618) with Jaccard similarity. This is only a single data point, but it does suggest that the present approach of using n-gram coverage gives results that are competitive with an established technique for this sort of application.
\end{styleStandard}

\begin{styleStandard}
\textbf{7. Clues from clusters of collocades}
\end{styleStandard}

\begin{styleStandard}
The previous sections have concentrated on analyses of individual texts or text categories; but a researcher in this field will typically not only be interested in how formulaic particular texts are, how typical or atypical they are of their class, and how similar or different a group of text types are amongst themselves, but also on how the repetitive sequences identified in this process relate to each other. In other words, it would be desirable for researchers into formulaic language to have a tool that helps to shed light on the patterns of phraseology that are responsible for high or low scores in terms of collocade coverage.
\end{styleStandard}

\begin{styleStandard}
The program postflab is designed with this aim in mind. It uses a secondary output file of formulex, the flab listing (Frequently Assembled Lexical Bundles) as input and attempts to organize these frequent collocades in a manner that brings out their interrelationships.
\end{styleStandard}

\begin{styleStandard}
An extract from a flab output file follows to illustrate the kind of data in question. This specimen consists of the first twelve lines from the patient information leaflet subcorpus.
\end{styleStandard}

\begin{styleStandard}
3 leaflet 288 294415 1689519
\end{styleStandard}

\begin{styleStandard}
\ 0.2868 \ \ 285 \ \ 16 \ \ 3 \ tell your doctor
\end{styleStandard}

\begin{styleStandard}
\ 0.2294 \ \ 323 \ \ 11 \ \ 3 \ if you have
\end{styleStandard}

\begin{styleStandard}
\ 0.2162 \ \ 332 \ \ 10 \ \ 3 \ if you are
\end{styleStandard}

\begin{styleStandard}
\ 0.1951 \ \ 206 \ \ 15 \ \ 3 \ your doctor may
\end{styleStandard}

\begin{styleStandard}
\ 0.1932 \ \ 192 \ \ 16 \ \ 3 \ your doctor will
\end{styleStandard}

\begin{styleStandard}
\ 0.1888 \ \ 145 \ \ 21 \ \ 3 \ the active ingredient
\end{styleStandard}

\begin{styleStandard}
\ 0.1749 \ \ 197 \ \ 14 \ \ 3 \ you are taking
\end{styleStandard}

\begin{styleStandard}
\ 0.1715 \ \ 138 \ \ 20 \ \ 3 \ taking your medicine
\end{styleStandard}

\begin{styleStandard}
\ 0.1545 \ \ \ 30 \ \ 86 \ 16 \ if you have any questions or are not sure about anything ask your doctor or pharmacist
\end{styleStandard}

\begin{styleStandard}
\ 0.1509 \ \ \ 85 \ \ 29 \ \ 5 \ ask your doctor or pharmacist
\end{styleStandard}

\begin{styleStandard}
\ 0.1442 \ \ \ 84 \ \ 28 \ \ 6 \ out of the reach of children
\end{styleStandard}

\begin{styleStandard}
\ 0.1406 \ \ 198 \ \ 11 \ \ 3 \ do not take
\end{styleStandard}

\begin{styleStandard}
The first line merely identifies the text category, and adds the information that it contains 288 files, comprising 294,415 tokens and 1,689,519 characters.
\end{styleStandard}

\begin{styleStandard}
The next line shows that the collocade which covers the largest proportion of the subcorpus overall is {\textquotedbl}tell your doctor{\textquotedbl}. This contains three tokens, is 16 characters in length and occurs 285 times altogether in that text category. The figure 0.2868 is a percentage, the percentage of the entire text of the subcorpus that is covered by this 3-token sequence. Further down the list can be seen the longest of these collocades {\textquotedbl}if you have any questions or are not sure about anything ask your doctor or pharmacist{\textquotedbl}, a 16-element collocade that contains 86 characters and covers 0.1545 percent of the whole subcorpus, occurring 30 times.
\end{styleStandard}

\begin{styleStandard}
A proportion of 0.1545 percent may seem tiny, but given that the commonplace triple {\textquotedbl}one of the{\textquotedbl} is the most frequent collocade in the tales by Edith Wharton, covering a mere 0.0692 percent of the EW subcorpus, a 16-token sequence that accounts for even 0.1545 percent of a text corpus is worthy of attention.
\end{styleStandard}

\begin{styleStandard}
A point to realize about such listings is that each item coverage is computed separately. For example, the eleventh item, {\textquotedbl}ask your doctor or pharmacist{\textquotedbl}, occurring 85 times, is a substring of the item immediately preceding it. What this implies is that the shorter string occurs 30+85 = 115 times altogether. It occurs 30 times preceded by {\textquotedbl}if you have any questions or are not sure about anything{\textquotedbl} and 85 times without that preceding context. This avoids double counting.
\end{styleStandard}

\begin{styleStandard}
The principle behind this mode of reckoning coverage can best be explained with reference to the digram {\textquotedbl}your doctor{\textquotedbl}. This pair of tokens forms part of the items {\textquotedbl}tell your doctor{\textquotedbl}, {\textquotedbl}your doctor may{\textquotedbl} and {\textquotedbl}your doctor will{\textquotedbl}, as well as the two longer items just discussed. As it happens, that particular word-pair occurs 2193 times in this training sample. What the figure of 206 next to {\textquotedbl}your doctor may{\textquotedbl} tells us is that of these 2193 occurrences, 206 are followed immediately by {\textquotedbl}may{\textquotedbl}; and likewise with the other collocades containing {\textquotedbl}your doctor{\textquotedbl}.
\end{styleStandard}

\begin{styleStandard}
The finding that shorter sequences often occur within longer collocades hints at a kind of network of phraseological possibilities surrounding a core component. However, because the flab output is listed in frequency order, it is very tedious to extract such information from the data as given. The program postflab is intended to alleviate this problem.
\end{styleStandard}

\begin{styleStandard}
This program reads in the flab output produced by formulex and performs a 1-dimensional scaling on the collocades concerned, using string similarity as the value to be optimized. Multidimensional scaling (Upton and Cook, 2006) \ is a statistical optimization procedure which aims to reproduce as closely as possible a matrix of distances between items by assigning to each item coordinate values on a small number of dimensions. In this example, rather unusually, the algorithm is applied with just a single dimension. In postflab the process is taken to its minimal form, which means that the items are arranged in a single linear order that tries as far as possible to ensure that distances along the line correlate with distances derived from the entire matrix of inter-item similarities. In effect, the procedure adds, for strings, the concept of similarity order to the well-known concepts of alphabetic order and frequency order.
\end{styleStandard}

\begin{styleStandard}
The derived ordering is written to a text file for inspection and also, more usefully, to a data file to be processed in the R package so that it can be displayed visually. Figure 4 shows the results of this procedure for the 36 most frequent collocades from the medical leaflet category.
\end{styleStandard}

\begin{styleStandard}
  [Warning: Image ignored] % Unhandled or unsupported graphics:
%\includegraphics[width=4.6063in,height=2.5992in,width=\textwidth]{forsyth-img005.jpg}
 
\end{styleStandard}

\begin{styleStandard}
Figure 4: One-dimensional scaling of collocades from medical leaflet category
\end{styleStandard}

\begin{styleStandard}
In this diagram the vertical axis merely separates the items so that they do not overwrite each other. The horizontal axis represents the closeness of the items along a single dimension. The width of the blue lines is proportional to the aggregate coverage of all the collocades with the same score on the x-axis. These lines are intended to reveal the presence of certain groupings along the x-axis, including the main grouping which consists of a number of collocades containing the digram {\textquotedbl}your doctor{\textquotedbl}. Hence the program has performed a clustering as a side-effect.
\end{styleStandard}

\begin{styleStandard}
Although this graphic representation is based only on superficial string similarity, and has no semantic underpinning, it nevertheless makes it much easier for a researcher to find clusters of related phrasings than the text-based listing.
\end{styleStandard}

\begin{styleStandard}
\textbf{8. Concluding remarks}
\end{styleStandard}

\begin{styleStandard}
The formulib package implements one particular way of operationalizing the concept of formulaic language by using a traditional resource, the frequent n-gram list, in a slightly novel manner. It constitutes an innovative tool based on a simple idea, which offers the researcher informative ways of viewing repetitive phrasings in a corpus or collection of corpora. It takes further the work of Forsyth \& Grabowski (2015) by providing estimates of how much formulaic language is found in individual texts as well as how formulaic certain text types are. Although the examples quoted in the present chapter are in English, formulib can be applied to any language, and it does not need pre-processing resources such as lexicons, parsers of taggers.
\end{styleStandard}

\begin{styleStandard}
We cannot expect a single approach to cover all the different aspects of such a complex phenomenon. However, the argument of this chapter is that the concept that underlies the methods employed by the formulib software, namely collocade coverage, offers a straightforward but relatively effective way of investigating some of the more important aspects of formulaic language.
\end{styleStandard}

\begin{styleStandard}
Formulib cannot, of course, be regarded as an endpoint in the continuing attempt to explore patterns of formulaic sequences. Even staying with collocade coverage as a key indicator, there is room for further development. For instance, it would be highly desirable to find a more precise notation for integrating clearly related elements of the formulexicon such as
\end{styleStandard}

\begin{styleStandard}
{\textquotedbl}please read this leaflet carefully before taking your tablets{\textquotedbl}
\end{styleStandard}

\begin{styleStandard}
and
\end{styleStandard}

\begin{styleStandard}
{\textquotedbl}please read this leaflet carefully before you take your medicine{\textquotedbl}
\end{styleStandard}

\begin{styleStandard}
into a form that reveals their relatedness (a kind of micro-grammar). The two items above could be unified with the aid of a pattern-description language, such as the following.
\end{styleStandard}

\begin{styleStandard}
please read this leaflet carefully before [taking {\textbar} you take] your [medicine {\textbar} tablets]
\end{styleStandard}

\begin{styleStandard}
However, to do this efficiently and reliably would require leading-edge artificial intelligence applied to the induction of a small-scale grammar, and the results would doubtless be hard to interpret without advanced data visualization techniques. Perhaps a reader may take up that challenge. At any rate it shows this approach opens up plenty of avenues for further research.
\end{styleStandard}

\begin{styleStandard}
\textbf{Appendix}
\end{styleStandard}

\begin{styleStandard}
The suite of programs in Python3 that constitute the formulib package can be found at the address below.
\end{styleStandard}

\begin{styleStandard}
\url{http://www.richardsandesforsyth.net/software.html}
\end{styleStandard}

\begin{styleStandard}
Software and sample text files are freely available under a Creative Commons licence.
\end{styleStandard}

\begin{styleStandard}
\textbf{References}
\end{styleStandard}

\begin{styleStandard}
Bolinger, Dwight. 1965. The atomization of meaning. \textit{Language}, 41 (4), 555-573.
\end{styleStandard}

\begin{styleStandard}
Carter, Ronald. 2004. \textit{Language and Creativity: the Art of Everyday Talk}. Routledge.
\end{styleStandard}

\begin{styleStandard}
Chomsky, Avram Noam. 1972. \textit{Language and Mind} [enlarged edition]. New York: Harcourt Brace Jovanovich.
\end{styleStandard}

\begin{styleStandard}
Cohen, William W. 2009. \textit{Enron Email Dataset }[online]. Available from https://www.cs.cmu.edu/\~{}./enron/ (last accessed May 2015).
\end{styleStandard}

\begin{styleStandard}
Daille, Béatrice. 2003. Terminology Mining. In Maria Teresa Pazienza, editor, \textit{Information Extraction in the Web Era}, pages 29–44. Springer.
\end{styleStandard}

\begin{styleStandard}
Eggins. Suzanne. 1994. \textit{An Introduction to Systemic Functional Linguistics}. London: Pinter.
\end{styleStandard}

\begin{styleStandard}
Fletcher, William H. 2012. kfNgram Information \& Help. http://www.kwicfinder.com/kfNgram/kfNgramHelp.html
\end{styleStandard}

\begin{styleStandard}
Forsyth, Richard and Grabowski, Łukasz. 2015. Is there a formula for formulaic language? \textit{Poznań Studies in Contemporary Linguistics}, 51(4), 511-549. doi: 10.1515/psicl-2015-0019
\end{styleStandard}

\begin{styleStandard}
Granger, S., \& Bestgen, Y. 2014. The use of collocations by intermediate vs. advanced non-native writers: a bigram-based study. \textit{International Review of Applied Linguistics in Language Teaching}, 52(3): 229–252. http://doi. org/10.1515/iral-2014-0011
\end{styleStandard}

\begin{styleStandard}
Gries, Stephan Th. 2009. \textit{Quantitative Corpus Linguistics with R}. New York \& London: Routledge.
\end{styleStandard}

\begin{styleStandard}
Halliday, Michael. 2014. That “certain cut”: towards a characterology of Mandarin Chinese. \textit{Functional Linguistics}, 1(2), doi:10.1186/2196-419X-1-2 .
\end{styleStandard}

\begin{styleStandard}
Kilgarriff, A., Rychlý, P., Kovár, V., \& Baisa, V. 2012. Finding multiwords of more than two words. Paper presented at the \textit{15th EURALEX International Congress,} University of Oslo, Norway.
\end{styleStandard}

\begin{styleStandard}
Leńko-Szymańska, A. 2016. CollGram pro[FB01?]les and n-gram frequencies as gauges of phraseological competence in EFL learners at di[FB00?]erent pro[FB01?]ciency levels. Paper presented at the \textit{Teaching and Language Corpora Conference}, Giessen. 20-23 July 2016.
\end{styleStandard}

\begin{styleStandard}
Magurran, Anne Elizabeth. 2004. \textit{Measuring Biodiversity}. Oxford: Blackwell.
\end{styleStandard}

\begin{styleStandard}
Scott, M. 2020. \textit{WordSmith Tools version 8}. Stroud: Lexical Analysis Software.
\end{styleStandard}

\begin{styleStandard}
Shimohata, S., Sugio, T., \& Nagata, J. 1999. Retrieving domain-specific collocations by co-occurrences and word order constraints. \textit{Computational Intelligence}, 15(2), 92.
\end{styleStandard}

\begin{styleStandard}
Sinclair, John. 1991. \textit{Corpus, Concordance, Collocation}. Oxford: Oxford University Press.
\end{styleStandard}

\begin{styleStandard}
Upton, Graham \& Cook, Ian (eds.). 2006. \textit{The Oxford dictionary of statistics}. Oxford: Oxford University Press.
\end{styleStandard}

\begin{styleStandard}
Wray, Alison. 2002. \textit{Formulaic language and the lexicon}. Cambridge: Cambridge University Press.
\end{styleStandard}

\begin{styleStandard}
Wright, David. 2017. Using word n-grams to identify authors and idiolects: a corpus approach to a forensic linguistic problem. I\textit{nternational Journal of Corpus Linguistics }22(2), 212-241.
\end{styleStandard}

\begin{styleStandard}
Zhang, W., Yoshida, T., Tang, X., \& Ho, T.-B. 2009. Improving effectiveness of mutual information for substantival multiword expression extraction. \textit{Expert Systems with Applications}, 36(8), 10919-10930. doi: 10.1016/j.eswa.2009.02.026
\end{styleStandard}

\end{document}
