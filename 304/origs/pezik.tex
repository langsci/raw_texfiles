% This file was converted to LaTeX by Writer2LaTeX ver. 1.4
% see http://writer2latex.sourceforge.net for more info
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb,amsfonts,textcomp}
\usepackage{array}
\usepackage{supertabular}
\usepackage{hhline}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, filecolor=blue, urlcolor=blue}
\usepackage{graphicx}
% footnotes configuration
\makeatletter
\renewcommand\thefootnote{\arabic{footnote}}
\makeatother
% Text styles
\newcommand\textstyleListLabelxix[1]{\textrm{#1}}
\newcommand\textstyleListLabelxx[1]{#1}
\makeatletter
\newcommand\arraybslash{\let\\\@arraycr}
\makeatother
\raggedbottom
% Paragraph styles
\renewcommand\familydefault{\rmdefault}
\newenvironment{styleStandard}{\setlength\leftskip{0cm}\setlength\rightskip{0cm plus 1fil}\setlength\parindent{0cm}\setlength\parfillskip{0pt plus 1fil}\setlength\parskip{0in plus 1pt}\writerlistparindent\writerlistleftskip\leavevmode\normalfont\normalsize\writerlistlabel\ignorespaces}{\unskip\vspace{0.111in plus 0.0111in}\par}
\newenvironment{stylelsSectioni}{\setlength\leftskip{0.25in}\setlength\rightskip{0in plus 1fil}\setlength\parindent{0in}\setlength\parfillskip{0pt plus 1fil}\setlength\parskip{0.1665in plus 0.016649999in}\writerlistparindent\writerlistleftskip\leavevmode\normalfont\normalsize\fontsize{18pt}{21.6pt}\selectfont\bfseries\writerlistlabel\ignorespaces}{\unskip\vspace{0.0835in plus 0.00835in}\par}
\newenvironment{styleListParagraph}{\renewcommand\baselinestretch{1.0}\setlength\leftskip{0.5in}\setlength\rightskip{0in plus 1fil}\setlength\parindent{0in}\setlength\parfillskip{0pt plus 1fil}\setlength\parskip{0in plus 1pt}\writerlistparindent\writerlistleftskip\leavevmode\normalfont\normalsize\writerlistlabel\ignorespaces}{\unskip\vspace{0in plus 1pt}\par}
\newenvironment{stylelsSectionii}{\setlength\leftskip{0.25in}\setlength\rightskip{0in plus 1fil}\setlength\parindent{0in}\setlength\parfillskip{0pt plus 1fil}\setlength\parskip{0.222in plus 0.0222in}\writerlistparindent\writerlistleftskip\leavevmode\normalfont\normalsize\fontsize{16pt}{19.2pt}\selectfont\bfseries\writerlistlabel\ignorespaces}{\unskip\vspace{0.0835in plus 0.00835in}\par}
% List styles
\newcommand\writerlistleftskip{}
\newcommand\writerlistparindent{}
\newcommand\writerlistlabel{}
\newcommand\writerlistremovelabel{\aftergroup\let\aftergroup\writerlistparindent\aftergroup\relax\aftergroup\let\aftergroup\writerlistlabel\aftergroup\relax}
\newcounter{listWWNumxxiileveli}
\newcounter{listWWNumxxiilevelii}[listWWNumxxiileveli]
\newcounter{listWWNumxxiileveliii}[listWWNumxxiilevelii]
\newcounter{listWWNumxxiileveliv}[listWWNumxxiileveliii]
\renewcommand\thelistWWNumxxiileveli{\arabic{listWWNumxxiileveli}}
\renewcommand\thelistWWNumxxiilevelii{\arabic{listWWNumxxiileveli}.\arabic{listWWNumxxiilevelii}}
\renewcommand\thelistWWNumxxiileveliii{\arabic{listWWNumxxiileveli}.\arabic{listWWNumxxiilevelii}.\arabic{listWWNumxxiileveliii}}
\renewcommand\thelistWWNumxxiileveliv{\arabic{listWWNumxxiileveli}.\arabic{listWWNumxxiilevelii}.\arabic{listWWNumxxiileveliii}.\arabic{listWWNumxxiileveliv}}
\newcommand\labellistWWNumxxiileveli{\thelistWWNumxxiileveli.}
\newcommand\labellistWWNumxxiilevelii{\thelistWWNumxxiilevelii.}
\newcommand\labellistWWNumxxiileveliii{\thelistWWNumxxiileveliii.}
\newcommand\labellistWWNumxxiileveliv{\thelistWWNumxxiileveliv.}
\newenvironment{listWWNumxxiileveli}{\def\writerlistleftskip{\setlength\leftskip{0.5in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\stepcounter{listWWNumxxiileveli}\makebox[0cm][l]{\labellistWWNumxxiileveli}\hspace{-0.635cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxiilevelii}{\def\writerlistleftskip{\setlength\leftskip{1.5299in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\stepcounter{listWWNumxxiilevelii}\makebox[0cm][l]{\labellistWWNumxxiilevelii}\hspace{-3.2509458cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxiileveliii}{\def\writerlistleftskip{\setlength\leftskip{1.5in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.1252in}}\def\writerlistlabel{\stepcounter{listWWNumxxiileveliii}\makebox[0cm][r]{\labellistWWNumxxiileveliii}\hspace{-3.4919918cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxiileveliv}{\def\writerlistleftskip{\setlength\leftskip{2in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\stepcounter{listWWNumxxiileveliv}\makebox[0cm][l]{\labellistWWNumxxiileveliv}\hspace{-4.4449997cm}\writerlistremovelabel}}}{}
\newcounter{listWWNumxxviileveli}
\newcounter{listWWNumxxviilevelii}[listWWNumxxviileveli]
\newcounter{listWWNumxxviileveliii}[listWWNumxxviilevelii]
\newcounter{listWWNumxxviileveliv}[listWWNumxxviileveliii]
\renewcommand\thelistWWNumxxviileveli{\arabic{listWWNumxxviileveli}}
\renewcommand\thelistWWNumxxviilevelii{\alph{listWWNumxxviilevelii}}
\renewcommand\thelistWWNumxxviileveliii{\roman{listWWNumxxviileveliii}}
\renewcommand\thelistWWNumxxviileveliv{\arabic{listWWNumxxviileveliv}}
\newcommand\labellistWWNumxxviileveli{\thelistWWNumxxviileveli)}
\newcommand\labellistWWNumxxviilevelii{\thelistWWNumxxviilevelii.}
\newcommand\labellistWWNumxxviileveliii{\thelistWWNumxxviileveliii.}
\newcommand\labellistWWNumxxviileveliv{\thelistWWNumxxviileveliv.}
\newenvironment{listWWNumxxviileveli}{\def\writerlistleftskip{\setlength\leftskip{0.6965in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\stepcounter{listWWNumxxviileveli}\makebox[0cm][l]{\labellistWWNumxxviileveli}\hspace{-1.13411cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxviilevelii}{\def\writerlistleftskip{\setlength\leftskip{1.1965in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\stepcounter{listWWNumxxviilevelii}\makebox[0cm][l]{\labellistWWNumxxviilevelii}\hspace{-2.4041097cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxviileveliii}{\def\writerlistleftskip{\setlength\leftskip{1.6965in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.1252in}}\def\writerlistlabel{\stepcounter{listWWNumxxviileveliii}\makebox[0cm][r]{\labellistWWNumxxviileveliii}\hspace{-3.9911017cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxviileveliv}{\def\writerlistleftskip{\setlength\leftskip{2.1965in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\stepcounter{listWWNumxxviileveliv}\makebox[0cm][l]{\labellistWWNumxxviileveliv}\hspace{-4.94411cm}\writerlistremovelabel}}}{}
\newcommand\labellistWWNumxxivleveli{\textstyleListLabelxix{{}-}}
\newcommand\labellistWWNumxxivlevelii{\textstyleListLabelxx{o}}
\newcommand\labellistWWNumxxivleveliii{[F0A7?]}
\newcommand\labellistWWNumxxivleveliv{[F0B7?]}
\newenvironment{listWWNumxxivleveli}{\def\writerlistleftskip{\setlength\leftskip{0.75in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\makebox[0cm][l]{\labellistWWNumxxivleveli}\hspace{-1.27cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxivlevelii}{\def\writerlistleftskip{\setlength\leftskip{1.25in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\makebox[0cm][l]{\labellistWWNumxxivlevelii}\hspace{-2.54cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxivleveliii}{\def\writerlistleftskip{\setlength\leftskip{1.75in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\makebox[0cm][l]{\labellistWWNumxxivleveliii}\hspace{-3.81cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxivleveliv}{\def\writerlistleftskip{\setlength\leftskip{2.25in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\makebox[0cm][l]{\labellistWWNumxxivleveliv}\hspace{-5.08cm}\writerlistremovelabel}}}{}
\newcounter{listWWNumxxvileveli}
\newcounter{listWWNumxxvilevelii}[listWWNumxxvileveli]
\newcounter{listWWNumxxvileveliii}[listWWNumxxvilevelii]
\newcounter{listWWNumxxvileveliv}[listWWNumxxvileveliii]
\renewcommand\thelistWWNumxxvileveli{\arabic{listWWNumxxvileveli}}
\renewcommand\thelistWWNumxxvilevelii{\alph{listWWNumxxvilevelii}}
\renewcommand\thelistWWNumxxvileveliii{\roman{listWWNumxxvileveliii}}
\renewcommand\thelistWWNumxxvileveliv{\arabic{listWWNumxxvileveliv}}
\newcommand\labellistWWNumxxvileveli{\thelistWWNumxxvileveli)}
\newcommand\labellistWWNumxxvilevelii{\thelistWWNumxxvilevelii.}
\newcommand\labellistWWNumxxvileveliii{\thelistWWNumxxvileveliii.}
\newcommand\labellistWWNumxxvileveliv{\thelistWWNumxxvileveliv.}
\newenvironment{listWWNumxxvileveli}{\def\writerlistleftskip{\setlength\leftskip{0.5in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\stepcounter{listWWNumxxvileveli}\makebox[0cm][l]{\labellistWWNumxxvileveli}\hspace{-0.635cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxvilevelii}{\def\writerlistleftskip{\setlength\leftskip{1in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\stepcounter{listWWNumxxvilevelii}\makebox[0cm][l]{\labellistWWNumxxvilevelii}\hspace{-1.905cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxvileveliii}{\def\writerlistleftskip{\setlength\leftskip{1.5in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.1252in}}\def\writerlistlabel{\stepcounter{listWWNumxxvileveliii}\makebox[0cm][r]{\labellistWWNumxxvileveliii}\hspace{-3.4919918cm}\writerlistremovelabel}}}{}
\newenvironment{listWWNumxxvileveliv}{\def\writerlistleftskip{\setlength\leftskip{2in}}\def\writerlistparindent{}\def\writerlistlabel{}\def\item{\def\writerlistparindent{\setlength\parindent{-0.25in}}\def\writerlistlabel{\stepcounter{listWWNumxxvileveliv}\makebox[0cm][l]{\labellistWWNumxxvileveliv}\hspace{-4.4449997cm}\writerlistremovelabel}}}{}
\setlength\tabcolsep{1mm}
\renewcommand\arraystretch{1.3}
\title{}
\author{dyskobol2 dyskobol2}
\date{2021-02-03}
\begin{document}
\title{\textsuperscript{Exploring the valency of }collocational chains}
\maketitle

\begin{styleStandard}
\textbf{Abstract}
\end{styleStandard}

\begin{styleStandard}
Whereas the prefabricated status of idioms or restricted collocations is relatively self-evident in their context of use, the “underlying rigidity” (Sinclair 1991: 110) of other types of phraseological units may only become evident through large-scale analyses of reference corpora. This chapter focuses on the identification of subtle lexico-grammatical petrification of multiword units in dependency-annotated corpora. More specifically, it investigates restrictions on the valency of binary collocations and their tendency to be regularly subsumed by larger collocational chains. For example, the binary collocation \textit{deep breath} is almost invariably a direct object of a small set of verbs: \textit{take}, \textit{draw}, \textit{let out}. This restriction can be contrasted with collocational chains in which other adjectival collocations of \textit{breath} (e.g. \textit{bad breath}) have a wider range of syntactic roles determined mainly by the potential valency of their head noun (i.e. its propensity to function as subject, object etc.). Apart from discussing examples of such constructions from Polish and English corpus data, the chapter also attempts to show how lexico-syntactic properties of multiword units can be systematically accounted for and explored using a dependency-based approach to phraseology extraction. 
\end{styleStandard}


\setcounter{listWWNumxxiileveli}{0}
\begin{listWWNumxxiileveli}
\item 
\begin{stylelsSectioni}
Introduction
\end{stylelsSectioni}
\end{listWWNumxxiileveli}
\begin{styleStandard}
Since phraseology is a field {\textquotedbl}bedevilled by the proliferation of terms and by the conflicting uses of the same term{\textquotedbl} (Cowie 1998: 210), it is not superfluous to clarify what is meant by the terms ‘collocations’ and ‘collocational chains’ in the context of this paper. Unless otherwise indicated, the term ‘collocation’ is taken to mean a \textit{binary lexical collocation}, i.e. a recurrent combination of just two content words (possibly linked by a grammatical word) which remain in an explicit syntactic relation, e.g. ‘blind date’, ‘turn of phrase’. \ From the perspective of language production, collocations are assumed to be recalled from memory, either associatively or holistically, rather than recomposed in a completely spontaneous and uninspired manner. A review of different definitions of collocations by Pęzik (2018) shows that they usually appeal to three main types of identification criteria: formal, distributional of psycholinguistic. The so-called ‘restricted collocations’ are combinations which consist of an ‘autosemantic’ base and a ‘synsemantic’ collocate (Heid \& Gouws 2006). They can be roughly classified into four major groups (Mel’\v{c}uk 2001), (Pęzik 2018). Open binary collocations are composed of two largely autosemantic words, which makes them less obvious to recognize as phraseological units. One of their subtle characteristics as units of prefabricated language is a degree of ‘stereotyped recurrence’ i.e. the tendency to occur in similar semantic, syntactic and pragmatic contexts (ibid.: 51). While both restricted and open collocations play a key role in the production of fluent, native-like language, restricted collocations may also cause reception problems for non-native learners of a given language. Some restricted collocations are in fact figurative idioms as they instantiate conventionalized metaphors, metonymies and other conceptual blends, e.g. \textit{blind alley}.
\end{styleStandard}

\begin{styleStandard}
There is a wide spectrum of phraseological units which may consist of more than two words, such as pure and figurative idioms, proverbs, commonplaces, catch phrases, slogans etc. (Cowie 1998). Although tens of thousands of idioms and collocations have been identified and recorded in dictionaries and combinatorial databases, there is a need for further research on some of the more subtle types of phraseological prefabrication. Among the less extensively researched phraseological phenomena are collocational chains, which are defined here as overlapping combinations of two or more lexical collocations\footnote{ Some definitions of collocational chains also distinguish between collocational chains and collocational clusters (Hausmann 2004), (Heid \& Gouws 2006). \ \ }. As shown further in this paper, collocational chains can be composed spuriously or largely predetermined to occur in their entirety.
\end{styleStandard}

\begin{styleStandard}
Defining collocations and other types of phraseological units (PUs) as word combinations linked by an explicit syntactic relation may come across as somewhat controversial, partly because syntactic idiosyncrasy of PUs is regularly enumerated as one of their most salient characteristics. In its extreme form it can be described as ‘ill-formedness’ or deviation from grammatical regularity. For example, the fact that it is difficult to assign the constituents of the idiomatic expression \textit{by and large} to modern day English morphosyntactic categories has earned it the name of ‘an ill-formed collocation’ (Moon 1998)\footnote{ Such highly idiosyncratic combinations are difficult to directly integrate in the standard dependency representation used for the proposed method of phraseology extraction. }. More often, phraseological units tend to be ‘petrified’ in that they are mostly used in a limited subset of the morphological variants licensed by their otherwise regular syntactic structure. However, although syntactic idiosyncrasy testifies to the status of some PUs as clearly prefabricated constructions, one should not conclude that all or even most PUs are marked by syntactic irregularity. In reality, most idioms and collocations seem to be lexical realizations of regular syntactic patterns, even if their prototypical forms are petrified. The most obvious proof of this statement is the existence of dictionaries of idioms (Cowie and Mackin 1975), (Cowie et al. 1993) and collocations (Crowther \ et al. 2003) whose macro- and microstructures are organized around a set of productive syntactic patterns of idiomatic expressions. Furthermore, the very fact that most subsentential PUs have to be embedded in the syntactic structure of a sentence means that they also have an ‘external valency’ (Burger 2003). To put it in the parlance of dependency syntax, PUs have typical syntactic roles as governors or dependents of other words and phrases in the sentence. Those two properties of PUs, i.e. their internal structure and external valency are implicitly recognized in combinatorial dictionaries as illustrated in the following entry for the phrasal verb \textit{to back on to} from the Oxford Dictionary of Current Idiomatic English (Cowie and Mackin 1975: 10):
\end{styleStandard}


\setcounter{listWWNumxxviileveli}{0}
\begin{listWWNumxxviileveli}
\item 
\begin{styleListParagraph}
back on to [A3] \textit{have at its back, face at the back}. \textbf{S }house, shop; study, kitchen. \textbf{o}: court-yard; lane, alley
\end{styleListParagraph}
\end{listWWNumxxviileveli}
\begin{styleStandard}
The internal structure of the phrasal expression is indicated by the label A3, which denotes intransitive verbs with a particle and a preposition, while its external valency is implied by the two lists of its typical subjects (S) and objects (o).
\end{styleStandard}

\begin{styleStandard}
The assumption that most phraseological units have both a regular internal syntactic structure and an external valency specification opens up some perspectives of computerized explorations of their distribution as either self-contained or largely embeddable constructions. This point is elucidated at some length in the subsequent sections of this paper, but it can be illustrated right away with a simple example. In reference corpora of English, the seemingly independent binary collocation \textit{profound effect} functions almost exclusively as a direct object of \textit{have} as in \textit{have a profound effect}. The latter construction is in turn subsumed by an even longer collocational chain with \textit{on} as a fixed prepositional dependent and its open-ended prepositional nominal or pronominal object dependent as in ‘\textit{to have a profound effect on + NOUN/PRON’}. The fact that such structures may be recursively recombined in multiple, possibly also prefabricated constructions has some practical implications for the design of phraseological dictionaries and databases. 
\end{styleStandard}

\begin{styleStandard}
This paper first discusses the problem of fragmentation of such collocational chains in dictionaries and automatic combinatorial databases. The phenomenon of syntactically restricting subsumption of shorter word combinations in longer recurrent constructions is then discussed \ in terms of potential valency restrictions. Finally, the paper presents a new software tool named Treelets\footnote{ See http://pelcra.pl/new/treelets.}, which showcases some applications of dependency-based phraseology extraction. The method of generating a combinatorial dictionary implemented in this tool uses special data structures called ‘subsumption graphs’ to facilitate the search and visualization of embedded and overlapping phraseological constructions.
\end{styleStandard}


\setcounter{listWWNumxxiileveli}{0}
\begin{listWWNumxxiileveli}
\item 
\begin{stylelsSectioni}
Relational phraseology extraction
\end{stylelsSectioni}


\setcounter{listWWNumxxiilevelii}{0}
\begin{listWWNumxxiilevelii}
\item 
\begin{stylelsSectionii}
Fragmentation of Phraseological Units
\end{stylelsSectionii}
\end{listWWNumxxiilevelii}
\end{listWWNumxxiileveli}
\begin{styleStandard}
The degree to which various PUs can be expected to adhere to regular syntactic structure is important in the context of phraseology extraction (PE) -{}- an area of corpus research which deals with automated identification of phraseological units in corpora through aggregation of word co-occurrences attested in reference corpora. PE techniques can be broadly categorized into positional and relational (Evert 2005), although this distinction is sometimes blurred by practical considerations. Positional approaches rely on counting and weighing linearly related word co-occurrences in text. Relational PE techniques utilize explicit annotations of syntactic relations between constituents of PUs. As a result, the latter type of methods crucially depend on the syntactic predictability of PUs; word combinations which co-occur in syntactic configurations unpredicted by predefined syntactic patterns are ignored in the process of extraction. Because syntactic patterns used in the process of extraction have to a) conform to the particular treebank formalism used to annotate the working corpus and b) be consistently annotated by automatic syntactic parsers, the results of relational extraction may reveal only “details of language” covered by a particular syntactic theory (Sinclair 1991: 4) rather than the full spectrum of usage.
\end{styleStandard}

\begin{styleStandard}
Another broad distinction can be made between ‘ad hoc’ PE modules and extraction systems which precompute combinatorial databases with a dictionary-like macro structure. Ad hoc PE modules available in various corpus search engines usually perform positional extraction of binary collocations, n-grams or skip-grams for single and multiterm queries defined by users. For example, the collocation extraction of the MoncoEN corpus search engine\footnote{ See http://\textrm{monco.frazeo.com.}} can be used to define a single- or multiword node expression for any corpus query formulated in its query syntax. Table 1 presents a list of adjectival collocates extracted from a sample of almost 70~000 occurrences of the noun \textit{advantage} in data crawled from various English-language news websites. The results of the extraction query can be sorted by frequency or their strength of association, which is a variation of the Dice score in this case. The fourth column contains a frequency list of positions relative to the node word (which occurs at position=0) which is useful in identifying the predominant syntactic roles of the collocates. For example, the adjective \textit{competitive} seems to mainly precede the noun \textit{advantage}, which suggests that it is used as its adjectival premodifiers in this case. The last column lists word N-grams bounded by the node and collocate, which is meant to indicate some of the recurrent forms of each collocation as well as its higher-order constructions such as noun phrases with multiple adjectival modifiers, e.g. \textit{unfair competitive advantage}.
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.18375984in}|m{0.8045598in}|m{0.8309598in}|m{0.5365598in}|m{0.71085984in}|m{2.85526in}|}
\hline
\textbf{\#} &
\textbf{Adjective} &
\textbf{Frequency} &
\textbf{Dice} &
\textbf{Positions} &
\textbf{Example N-grams}\\\hline
1 &
competitive &
\raggedleft 1260 &
\raggedleft 0,0413 &
\{-1=1239, -2=17, 2=4\} &
\{competitive advantage=1078\},\{competitive advantages=161\},\{competitive business advantage=3\}\\\hline
2 &
full &
\raggedleft 1165 &
\raggedleft 0,0382 &
\{-1=1154, -2=9, 2=2\} &
\{full advantage=1134\},\{fullest advantage=12\},\{full advantages=4\},\{fuller advantage=3\},\{full of advantage=3\\\hline
3 &
big &
\raggedleft 900 &
\raggedleft 0,0295 &
\{-2=76, -1=789, 2=34, 1=1\} &
\{big advantage=488\},\{biggest advantage=167\},\{big advantages=75\},\{advantage of big=14\},\{big an advantage=5\},\{big size advantage=5\},\{big fundraising advantage=4\},\{biggest home-field advantage=3\}\\\hline
4 &
unfair &
\raggedleft 643 &
\raggedleft 0,0211 &
\{-1=567, 2=2, -2=74\} &
\{unfair advantage=525\},\{unfair advantages=42\},\{unfair competitive advantage=34\},\{unfair commercial advantage=5\},\{unfair trade advantage=4\},\{unfair competitive advantages=4\}\\\hline
5 &
great &
\raggedleft 571 &
\raggedleft 0,0187 &
\{-1=456, 2=41, -2=74\} &
\{great advantage=320\},\{great advantages=92\},\{greater advantage=37\},\{advantage of great=23\},\{great comparative advantage=9\},\{advantage , great=7\},\{greater advantages=7\},\{great natural advantages=6\},\{great competitive advantage=5\}\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 1 Adjectival collocates of \textit{advantage} retrieved with the MoncoEN search engine.
\end{styleStandard}

\begin{styleStandard}
Table 2 shows the top five adjectival collocates of the noun \textit{advantage} recorded in HASK EN (Pęzik 2014)\footnote{ See also http://pelcra.pl/hask\_en.}, a combinatorial database precomputed from the original edition of the British National Corpus (BNC). The remaining columns of the table show a selection of strength of association and dispersion scores.
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.40595984in}|m{1.0663599in}|m{0.6969598in}|m{0.6518598in}|m{0.6511598in}|m{0.9330599in}|m{0.8004598in}|}
\hline
\textbf{\#} &
\textbf{Collocate} &
\textbf{Frequency} &
\textbf{T-score} &
\textbf{MI}\textbf{\textsuperscript{3}} &
\textbf{G}\textbf{\textsuperscript{2}} &
\textbf{JD}\\\hline
1 &
competitive &
149.0 &
11.57 &
18.7144 &
613.96 &
0.82\\\hline
2 &
full &
166.0 &
8.43 &
16.28461 &
139.453 &
0.90\\\hline
3 &
added &
71.0 &
8.12 &
17.097 &
346.23 &
0.84\\\hline
4 &
comparative &
68.0 &
7.89 &
16.72091 &
308.08 &
0.72\\\hline
5 &
unfair &
67.0 &
7.69 &
16.20767 &
260.583 &
0.85\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 2 Adjectival collocates of advantage retrieved recorded in HASK EN database.
\end{styleStandard}

\begin{styleStandard}
Even though all of the top adjectives from the two lists seem to be genuine collocates of the noun \textit{advantage}, both of the extraction systems illustrated above suffer from the problem of fragmentation: recurrent fragments of larger multiword expressions are represented as unrelated binary collocations. For example, taken at face value, both of the lists above might imply that \textit{full advantage} is a self-contained intensifying binary collocation which could be used freely in a variety of syntactic roles predetermined by its head noun. However, in the first edition of the British National Corpus (BNC) more than 86 per cent of the occurrences of \textit{full advantage} function as part of the longer expression \textit{take full advantage (of)}. In a 440 million word version of the Corpus of Contemporary American English (COCA) the same phrase is used as a direct object of \textit{take }in over 96 per cent of its attested usages. In other words, \textit{full} seems to function as a collocational intensifier of \textit{advantage} mostly when the latter is a direct object of the light verb construction \textit{take + advantage} as illustrated in Ex. 2 below:
\end{styleStandard}


\setcounter{listWWNumxxviileveli}{0}
\begin{listWWNumxxviileveli}
\item 
\begin{styleListParagraph}
Customers \textbf{\textit{take full advantage}} of in-house electropolishing (...). [COCA, Physics Today]
\end{styleListParagraph}
\end{listWWNumxxviileveli}
\begin{styleStandard}
Intended users of such tools and automatically extracted resources are therefore required to inspect the concordances underlying such tabular results to distinguish between mostly subsumed collocations and freely recombinable collocations such as \textit{big advantage}. The latter combination is not restricted to occur in a single syntactic function. It is used as a direct object of verbs such as \textit{have} or \textit{give} in approx. 34 per cent of its occurrences in COCA and as a nominal subject dependent (28 per cent of occurrences) as in ex. 3 below:
\end{styleStandard}

\begin{listWWNumxxviileveli}
\item 
\begin{styleListParagraph}
The \textbf{\textit{big advantage}} for the investor \textbf{\textit{is}} that he can trade all his cryptocurrencies in one place. [MoncoEN, thenextweb.com]
\end{styleListParagraph}
\end{listWWNumxxviileveli}
\begin{styleStandard}
Collocation dictionaries may also be affected by the problem of PU fragmentation. For example the Oxford Dictionary of Collocations (Crowther et al. 2003) defines the noun bearing as a ‘way in which something is related’ and lists three of its adjectival collocates: \textit{direct}, \textit{important} and \textit{significant}. None of those collocations is likely to be used outside of the larger construction \textit{have a} \textit{direct/ important/ significant} \textit{bearing on}. This information is only indirectly implied by the example sentence illustrating the use of the first of those collocations and a separate entry for the direct object lexical collocation of \textit{have + bearing} and the grammatical collocation of \textit{bearing + on}:
\end{styleStandard}

\begin{listWWNumxxviileveli}
\item 
\begin{styleListParagraph}
bearing
\end{styleListParagraph}
\end{listWWNumxxviileveli}
\begin{styleStandard}
1.\textit{ way in which sth is related}
\end{styleStandard}

\begin{styleStandard}
ADJ. \textbf{direct, important, significant} 
\end{styleStandard}

\begin{styleStandard}
\textit{The rise in interest rates had a direct bearing on the company's profits.}
\end{styleStandard}

\begin{listWWNumxxivleveli}
\item 
\begin{styleListParagraph}
VERB + BEARING \ \textbf{have}
\end{styleListParagraph}
\item 
\begin{styleListParagraph}
PREP. \~{} \textbf{on}
\end{styleListParagraph}
\end{listWWNumxxivleveli}
\begin{styleStandard}
Of course, the coverage of this particular dictionary was by design limited to binary collocations and its space limitations preclude detailed usage notes. On the other hand, it could be argued that this collocational chain would probably be better represented as a single unit in this case. In OCD a special section labelled \textit{PHRASES} is occasionally used to enumerate additional set expressions which do not conform to the four basic patterns of binary collocations covered by this dictionary.
\end{styleStandard}

\begin{styleStandard}
Table 3 shows more examples of intensifying adjectival modifier collocations which are rarely, i.e. usually in less than 15 per cent of cases used independently of larger, recurrent constructions. 
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.40185985in}|m{1.1865599in}|m{0.43515983in}|m{0.5615598in}|m{1.4872599in}|m{2.0469599in}|}
\hline
\textbf{\#} &
\textbf{Binary AMOD collocation} &
\multicolumn{2}{m{1.0754598in}|}{\textbf{Frequency}} &
\textbf{Subsumption as DOBJ in BNC; COCA} &
\textbf{Frequent verb governors in BNC; COCA}\\\hline
 &
 &
\textbf{BNC} &
\textbf{COCA} &
 &
\\\hline
1 &
upper hand &
122 &
670 &
122/122=1 608/670=0.91 &
have (36; 223), gain (41; 157), get(19; 100), give (8; 36), hold(8; 25) + the upper hand\\\hline
2 &
little resemblance &
59 &
208 &
57/59=0.966; 181/208=0.93 &
bear (56; 181) + little resemblance\\\hline
3 &
mental note &
78  &
289 &
75/78=0.96; 262/289=0.90 &
make (74; 228), take (1; 20) + a mental note\\\hline
4 &
deep breath &
664 &
4282 &
587/664=0.88; 3872/4282=0.89 &
take (505; 3438), draw (72; 225), let out (7; 30) + a deep breath\\\hline
5 &
important bearing &
30 &
14 &
30/30=1; 14/14=1 &
have (30; 14) + an important bearing\\\hline
6 &
profound effect &
164  &
460 &
156/164=0.95;

420/460=0.91 &
have (150; 392) + profound effect\\\hline
7 &
significant role &
146 &
905 &
129/146=0.88; 

824/905=0.91 &
play (105; 692), have (15; 66) + a significant role\\\hline
8 &
full advantage &
170 &
423 &
146/170=0.86;

406/423=0.96 &
take (142; 402) + full advantage\\\hline
9 &
excellent job &
58 &
326 &
49/58=0.85; 306/326=0.94 &
do (46; 298) + an excellent job\\\hline
10 &
short laugh &
72 &
100 &
72/82=0.88; 74/100=0.74 &
give (67; 43) + a short laugh\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 3 Examples of binary adjectival modifier collocations regularly embedded in larger collocational chains in BNC and COCA. 
\end{styleStandard}

\begin{styleStandard}
The first example ‘collocation’ in the second column of Table 3 is \textit{upper hand}. In BNC it is always used a direct object of a handful of verbs shown in the last column (\textit{have},\textit{ gain},\textit{ give}, hold) while in COCA there are sporadic instances of usage as implicit direct objects in elliptic headlines or in other syntactic roles. A similar level of subsumption is observed for the phrase \textit{mental note} which is rarely used outside of the set expressions \textit{make/ take a mental note}. Non-direct object usage requires a creative context such as the piece of science-fiction writing illustrated in Ex. 5 below:
\end{styleStandard}


\setcounter{listWWNumxxviileveli}{0}
\begin{listWWNumxxviileveli}
\item 
\begin{styleListParagraph}
Amber put it on her mental note pad. [COCA, Analog Science Fiction \& Fact] 
\end{styleListParagraph}
\end{listWWNumxxviileveli}
\begin{styleStandard}
Some of the examples from Table 3, such as \textit{have the upper hand} and \textit{take a mental note} are simply multiword figurative idioms and thus the problem of their identification as phraseological units is purely technical. On the other hand, example phrases 5 (\textit{important bearing}, which is invariably embedded in \textit{have a direct bearing} in the two corpora), 6 (\textit{profound effect}), and 7 (\textit{significant role}) are restricted intensifying collocations. Examples 9 (\textit{excellent job}) and 10 (\textit{short laugh}) are open collocations comprised of two largely autosemantic constituents which simply happen to regularly form a longer structure with an overlapping direct object collocation. The typological dilemma with the latter examples is therefore whether they should be recognized as self-contained phraseological units or as more spurious and open-ended constructions. For the practical purposes of phraseology extraction, we might describe the frequently embedded phrases as \textit{subsumed binary collocations}, depending on how unlikely they are to be used independently of the larger constructions. The subsuming constructions can be multiword idioms or simply recurrent collocational chains. Collocational chains which consist of subsumed collocations should be distinguished from spurious chains of independent collocations such as\textit{ }‘my heart of stone is filled with pride’\footnote{ See \url{https://www.youtube.com/watch?v=wvYRQ-sFMJw} (2:18).}. 
\end{styleStandard}

\begin{styleStandard}
As shown in Table 4 some of the \textit{amod + dobj }constructions from Table 3 are highly likely to recur in larger recurrent structures which are also subtrees of the sentence dependency tree. For instance, more than 87 percent of the occurrences of the recurrent chain \textit{play a significant role} in COCA have a prepositional object introduced by \textit{in} as in \textit{play a significant role in + POBJ}. The subsumption of the other four second-order chains shown in Table 4 in third-order chains is even higher in the two reference corpora.
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.21295986in}|m{1.5941598in}|m{0.5615598in}|m{0.7580598in}|m{1.1025599in}|m{1.6927599in}|}
\hline
\# &
\textbf{amod + dobj collocational chain } &
\multicolumn{2}{m{1.3983598in}|}{\textbf{Frequency}} &
\multicolumn{2}{m{2.87406in}|}{\textbf{Used with a pobj dependent }}\\\hline
 &
 &
\textbf{BNC} &
\textbf{COCA} &
\textbf{BNC} &
\textbf{COCA}\\\hline
1 &
play a significant role &
105 &
692 &
92/105=0.88 &
616/692=0.89 (in)\\\hline
2 &
bear little resemblance &
59 &
181 &
52/59=0.88 &
175/181=0.97 (to)\\\hline
3 &
have a direct bearing &
37 &
54 &
33/37=0.89 &
54/54=1 (on/ upon)\\\hline
4 &
have profound effect &
150 &
392 &
133/150=0.89 &
348/392=0.89 (on/ upon)\\\hline
5 &
take full advantage &
142 &
402 &
132/142=0.93 &
380/402=0.95 (of)\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 4 Subsumption of \textit{amod + dobj} collocational chains in structures with a prepositional attachment.
\end{styleStandard}


\setcounter{listWWNumxxiileveli}{0}
\begin{listWWNumxxiileveli}
\item 

\setcounter{listWWNumxxiilevelii}{0}
\begin{listWWNumxxiilevelii}
\item 
\begin{stylelsSectionii}
Potential vs. Activated Valency of PUs
\end{stylelsSectionii}
\end{listWWNumxxiilevelii}
\end{listWWNumxxiileveli}
\begin{styleStandard}
The subsumption of shorter dependency subtrees (including single word ‘subtrees’) in longer recurrent collocational or idiomatic structures may considerably affect the distribution of their syntactic roles. As an example, the ratio of the noun \textit{fact} as a prepositional object dependent is much higher than the ratio of all nouns used as prepositional objects in reference corpora of English. However, the ratio of \textit{fact} as a prepositional object ‘drops’ approximately to the level observed for all nouns when \textit{fact} is modified by an adjective (see the discussion of Tables 5-7 below). More generally, dependency type ratios vary considerably not only for different content words but also with respect to the higher order constructions in which they occur.
\end{styleStandard}

\begin{styleStandard}
In dependency syntax, vertices representing words in the sentence dependency tree can be said to have a ‘passive valency’ (Mel[2B9?]čuk 1988), cf. (Boguslavsky 2003, 2016). The passive valency of a dependency subtree (including single-word subtrees with their morphosyntactic roles such as nouns, verbs, adjectives etc.) can be defined as its default propensity to function as governors or dependents of a set of types. Since the terms ‘active’ and ‘passive valency’ have also been used to refer to the direction of the dominance relationship between words in a dependency tree (Moroz 2013), to avoid confusion in this paper, the terms ‘potential’ and ‘activated valency’ will be used to describe the default and corpus-attested dependency patterns formed by words and phrases. For example, nouns have the rather obvious default potential of functioning as nominal subjects, objects of verbs or prepositions, nominal modifiers, etc. while verbs are typically sentence roots, auxiliaries, x-complements etc. The approximate activation of such potential valency roles \ (i.e. the activated valency of a word or phrase) can be estimated from manually annotated dependency treebanks or automatically parsed corpora. Neither of these options is ideal as treebanks are limited in size and parsers produce erroneous annotations, but even an approximate estimation of the activated valency of a word or phrase may throw some light on its actual usage.
\end{styleStandard}

\begin{styleStandard}
As shown in Table 5, in both COCA and BNC, nominal dependents are usually prepositional objects (35 – 38 per cent of all noun occurrences), direct objects (16 per cent) and nominal subjects (12-18 per cent).
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.82475984in}|m{0.82405984in}|m{0.47405985in}|m{0.8066598in}|m{0.8205598in}|m{0.49905983in}|m{0.90875983in}|m{0.5080598in}|}
\hline
\multicolumn{3}{|m{2.28036in}|}{\centering \textbf{BNC}} &
\multicolumn{5}{m{3.85806in}|}{\centering \textbf{COCA}}\\\hline
Dep. type &
Freq. &
Ratio &
Dep. type &
Freq. &
Ratio &
Mean per noun &
Std. dev.\\\hline
pobj &
\raggedleft 7~844 932 &
\raggedleft 0.38 &
pobj &
\raggedleft 30~824 682 &
\raggedleft 0.35 &
\raggedleft 0.20 &
\raggedleft\arraybslash 0.34\\\hline
dobj &
\raggedleft 3~246 866 &
\raggedleft 0.16 &
dobj &
\raggedleft 14 922 242 &
\raggedleft 0.16 &
\raggedleft 0.09 &
\raggedleft\arraybslash 0.24\\\hline
nsubj &
\raggedleft 2~416 031 &
\raggedleft 0.18 &
nsubj &
\raggedleft 10 870 549 &
\raggedleft 0.12 &
\raggedleft 0.10 &
\raggedleft\arraybslash 0.25\\\hline
compound &
\raggedleft 2~008 918 &
\raggedleft 0.09 &
compound &
\raggedleft 9 787 999 &
\raggedleft 0.11 &
\raggedleft 0.15 &
\raggedleft\arraybslash 0.31\\\hline
conj &
\raggedleft 1~550 954 &
\raggedleft 0.07 &
conj &
\raggedleft 5 509 718 &
\raggedleft 0.06 &
\raggedleft 0.08 &
\raggedleft\arraybslash 0.22\\\hline
attr &
\raggedleft 853 480 &
\raggedleft 0.04 &
attr &
\raggedleft 3 728 782 &
\raggedleft 0.04 &
\raggedleft 0.02 &
\raggedleft\arraybslash 0.13\\\hline
nsubjpass &
\raggedleft 579 565 &
\raggedleft 0.03 &
ROOT &
\raggedleft 3 068 416 &
\raggedleft 0.03 &
\raggedleft 0.01 &
\raggedleft\arraybslash 0.07\\\hline
ROOT &
\raggedleft 533 112 &
\raggedleft 0.02 &
npadvmod &
\raggedleft 2 537 174 &
\raggedleft 0.02 &
\raggedleft 0.12 &
\raggedleft\arraybslash 0.30\\\hline
npadvmod &
\raggedleft 474 680 &
\raggedleft 0.02 &
appos &
\raggedleft 2 215 588 &
\raggedleft 0.02 &
\raggedleft 0.02 &
\raggedleft\arraybslash 0.12\\\hline
appos &
\raggedleft 305 757 &
\raggedleft 0.01 &
nsubjpass &
\raggedleft 1 474 911 &
\raggedleft 0.01 &
\raggedleft 0.06 &
\raggedleft\arraybslash 0.21\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 5 Ten most frequent types of nominal dependents in BNC and COCA.
\end{styleStandard}

\begin{styleStandard}
The most common dependents of English nouns are determiners (24.6 per cent), followed by adjectival modifiers (16.6) and prepositions (12.2). The proportions of dependency relation types for specific nouns, verbs or adjectives may be very different from such overall distributions. As shown in the last column of Table 5, the standard deviation of the prepositional object dependent type is 0.34 percentage points in COCA. 
\end{styleStandard}

\begin{styleStandard}
In studies of verb valency, it is taken for granted that different verbs can be classified into groups of similar ‘subcategorization frames’. In other words, different verbs require or ‘subcategorize’ different types of configurations of their dependents. From the perspective of phraseology, it is also interesting to consider the activated valency of nouns and other open-class ‘content words’ such as adjectives or adverbs which function as headwords defining the entry structure of lexicographic resources. The activated valency of a dependency subtree (such as a word or phrase) can be defined as the set of dependent and governor types in which it is found in a reference corpus. Although corpus-based valency estimations can only be probabilistic and approximate in nature, they do shed light on the actual usage patterns of words and phrases, and they are especially revealing when such words or phrases tend to be embedded in larger recurrent constructions.
\end{styleStandard}

\begin{styleStandard}
Table 6 shows the distribution of dependent types realized by the nouns \textit{breath} and \textit{fact} estimated from the syntactically annotated version of COCA used in this study. The use of \textit{breath} as a direct object is considerably more frequent than the average value observed for nouns in this corpus (49.95 vs. 16.8 per cent) whereas its frequency as a prepositional object is lower than the average (29.54 vs. 34.12). On the other hand, the noun \textit{fact} is a prepositional object in over 65 per cent of its occurrences, which is considerably higher than the average ratio of 34 per cent observed for all nouns in this corpus. This example shows that the activated valency of those two words tends to differ considerably either from their potential valency as nouns or even from the overall or average rations observed for all nouns in a reference corpus.
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{1.1622599in}|m{0.8177598in}|m{0.5170598in}|m{1.1677599in}|m{0.8177598in}|m{0.45525983in}|}
\hline
\multicolumn{3}{|m{2.6545599in}|}{\centering \textbf{\textit{breath}}\textbf{ in COCA}} &
\multicolumn{3}{m{2.59826in}|}{\centering \textbf{\textit{fact}}\textbf{ in COCA}}\\\hline
Dep. type &
\textbf{Freq.} &
\textbf{Ratio} &
\textbf{Dep. type} &
\textbf{Freq.} &
\textbf{Ratio}\\\hline
dobj &
\raggedleft 12 830 &
\raggedleft 0.50 &
pobj &
\raggedleft 108 026 &
\raggedleft\arraybslash 0.67\\\hline
pobj &
\raggedleft 7 589 &
\raggedleft 0.30 &
nsubj &
\raggedleft 18 964 &
\raggedleft\arraybslash 0.12\\\hline
nsubj &
\raggedleft 2 344 &
\raggedleft 0.09 &
dobj &
\raggedleft 16 150 &
\raggedleft\arraybslash 0.10\\\hline
compound &
\raggedleft 851 &
\raggedleft 0.03 &
attr &
\raggedleft 7 336 &
\raggedleft\arraybslash 0.04\\\hline
conj &
\raggedleft 520 &
\raggedleft 0.02 &
conj &
\raggedleft 3 657 &
\raggedleft\arraybslash 0.02\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 6 The nouns \textit{breath} and \textit{fact} as dependents in COCA.
\end{styleStandard}

\begin{styleStandard}
It is not obvious whether the difference between the typical dependency types of the two nouns can be linked to their general semantic properties. What seems to be the case is that at least \textit{some} of this variation is due to a handful of phraseological restrictions on the valency of those two nouns. For example, almost 72 per cent of all the occurrences of \textit{breath} as a direct object are governed by just four verbs: \textit{take} (5194), \textit{hold} (1896), \textit{catch} (1428), \textit{draw} (708). Taken alone, the support verb restricted collocation \textit{take a breath} accounts for over 40 per cent of the use of \textit{breath} as a direct object. The syntactic distribution of the noun \textit{fact} is even more biased by its formulaic usage: over 64 per cent (69295) of its occurrences as a prepositional object \ \textit{fact} are instances of a single discourse linking phrase: \textit{in fact}. 
\end{styleStandard}

\begin{styleStandard}
The activated valency levels observed for a single word may change considerably once this word in used in a collocation. The previous section shows how the potential valency of binary collocations may also be restricted by the distribution of a small set of prefabricated higher order structures in which they are typically found. Table 7 shows frequencies of dependent types assumed by the nouns \textit{fact} and \textit{breath} when they are modified by adjectives as in \textit{[access] to simple facts} or \textit{have a bad breath}. The proportion of individual dependency types of the two nouns is only partly consistent with their overall dependency type distribution. Adjective-modified occurrences of \textit{breath} are even more likely to be direct objects (68 per cent), whereas instances of \textit{fact} with an adjectival modifier are half as likely to be prepositional objects. Much of the first difference can be explained by the existence of the construction \textit{take a deep breath}, which is used both literally as an established collocational chain and idiomatically as a figurative expression (see Table 3). \ The decrease in the ratio of prepositional object instances of \textit{fact} observed when we consider its use with adjectival modifiers results to a large extent from the absence of \textit{in fact} or a similar phrase in this ranking. There are some formulaic adjective-modified usages of \textit{fact} as a prepositional object such as the sentence initial discourse marker \textit{in actual fact }(67 occs. in COCA), but they do not compensate for the absence of the much more frequent prepositional phrase \textit{in fact}. The propensity of a word or phrase to be used as a dependent or governor of a larger structure may be significantly skewed by its use in a single higher-order phraseological construction such as \textit{take a deep breath} or \textit{in fact}. 
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{1.0011599in}|m{0.8066598in}|m{0.90525985in}|m{1.0045599in}|m{1.0038599in}|m{1.0705599in}|}
\hline
\multicolumn{3}{|m{2.87056in}|}{\centering \textbf{amod(fact, x)}} &
\multicolumn{3}{m{3.2364597in}|}{\centering \textbf{amod(breath, x)}}\\\hline
\centering \textbf{Type} &
\centering \textbf{Freq.} &
\centering \textbf{Ratio} &
\centering \textbf{Type} &
\centering \textbf{Freq.} &
\centering\arraybslash \textbf{Ratio}\\\hline
Pobj &
\raggedleft 3,333 &
\raggedleft 0.32 &
dobj &
\raggedleft 6,033 &
\raggedleft\arraybslash 0.68\\\hline
Dobj &
\raggedleft 2,247 &
\raggedleft 0.21 &
pobj &
\raggedleft 1,824 &
\raggedleft\arraybslash 0.21\\\hline
Nsubj &
\raggedleft 1,870 &
\raggedleft 0.18 &
nsubj &
\raggedleft 314 &
\raggedleft\arraybslash 0.04\\\hline
Attr &
\raggedleft 1,332 &
\raggedleft 0.13 &
ROOT &
\raggedleft 171 &
\raggedleft\arraybslash 0.02\\\hline
Conj &
\raggedleft 401 &
\raggedleft 0.04 &
conj &
\raggedleft 163 &
\raggedleft\arraybslash 0.02\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 7 Top five dependent types \textit{fact} and \textit{breath} with a modifying adjective.
\end{styleStandard}

\begin{styleStandard}
In order for automatic combinatorial databases to account for activated valency patterns of phraseological units, they have to identify and represent such subsumption phenomena. The following section describes a method of storing extracted collocational structures which was designed to address the issue of recursive subsumption of PUs. 
\end{styleStandard}

\begin{listWWNumxxiileveli}
\item 
\begin{listWWNumxxiilevelii}
\item 
\begin{stylelsSectionii}
Subsumption Graphs
\end{stylelsSectionii}
\end{listWWNumxxiilevelii}
\end{listWWNumxxiileveli}
\begin{styleStandard}
Pęzik (2018) describes an experimental method of extracting combinatorial databases from dependency-parsed corpora which keeps track of subsumption relations between overlapping constructions of different sizes. The working assumption of the PE approach used in this study is known as the Continuity Restraint (O’Grady 1998), which predicts that an idiom’s obligatory lexical components form a subtree of the sentence dependency tree. The validity of this assumption depends on the exact dependency formalism used to represent PUs. Also, it seems to fail in the case of some variable idiomatic expressions such as \textit{walk a thin{\textbar}fine line{\textbar}path between}. It is nevertheless a useful assumption in large-scale phraseology extraction. One of its advantages is that it covers collocational subtrees which are neither complete or single phrasal constituents such as \textit{include such factors as}. 
\end{styleStandard}

\begin{styleStandard}
The extraction process starts with a set of headwords, which are simply part-of-speech typed lemmas of content words, and a set of dependency patterns in which those headwords are expected to occur. Next, for each headword, the full set of lexically recurrent subtrees, ‘catenae’ (Osbourne et al. 2012) or ‘treelets’ is extracted from a reference corpus. Extracted subtrees are stored with some distributional and structural properties in a relational database. The headwords define the macrostructure of the resulting Automatic Combinatorial Dictionary (ACD) and the set of patterns used determines the microstructure of each of its entries.
\end{styleStandard}

\begin{styleStandard}
Recurrent subtrees containing a given headword are stored in a data structure called a ‘subsumption graph’. A section of a subsumption graph generated for the noun \textit{effect} in COCA is shown in Fig. 1. Its full version comprises 10 855 vertices representing subtrees containing this noun and occurring at least twice in this corpus. The vertices of the subsumption graph represent recurrent binary collocations and higher-order collocational subtrees whose syntactic structure matches one of the predefined patterns. The patterns can be defined manually as explained in section 3.2 or derived in a weakly supervised manner from the corpus. The weighted directed edges indicate the subsumption relation. The value of the edge weights represents the frequency of subsumption observed in the reference corpus (it is in fact equal to the frequency of the subsumed combination). A loop edge is added to vertices without outgoing edges to indicate the frequency of the combination represented by that vertex. For example, the binary collocations \textit{have + effect} and \textit{profound + effect} have frequencies of 1 026 and 465 respectively as indicated by the weights on their loop edges. The subsumption ratio of a given collocation can be calculated to the extent it can be estimated from the set of patterns used as the sum of the frequency weights of edges incoming from other vertices divided by the total frequency of that node. For example, the subsumption score of the chain \textit{have + profound effect on} in longer structures in this graph is 10 (\textit{have + profound effect on people}) + 19 (\textit{have + profound effect on life}) / 313 = 0.092. The number of the incoming edges (indegree) other than the loop edge reflects the ‘productivity’ of a given subtree. 
\end{styleStandard}

\begin{styleStandard}
  [Warning: Image ignored] % Unhandled or unsupported graphics:
%\includegraphics[width=6.2957in,height=4.0791in,width=\textwidth]{pezik-img001.png}
 
\end{styleStandard}

\begin{styleStandard}
Fig. 1 A simplified subsumption graph generated for recurrent dependency subtrees containing the noun \textit{effect }in COCA. Only a subset of recurrent subtrees containing the noun is shown here.
\end{styleStandard}

\begin{styleStandard}
Complex restrictions on the potential valency of a given lexicalized subtree can be visualized as a syntactic subsumption graph similar to the one shown in Fig. 2. As shown earlier in Tables 3 and 4, the subsumed intensifying adjectival modifier collocation \textit{profound effect} is largely restricted to occur as a direct object (420/ 460 occurrences) of just 5 recurrent verbs, and when this is the case, it is in turn largely restricted to take a prepositional object (348/ 420). Such recurrent subsumption is conveniently represented as a subsumption graph\footnote{ \textrm{The edge label }\textrm{\textit{420, 5}}\textrm{ means that }\textrm{\textit{profound effect }}\textrm{occurs 420 times as a direct object of only 5 different verbs; The label }\textrm{\textit{3,1}}\textrm{ means that it is used only three times as a nominal subject with just one verb (to be), etc.}}.
\end{styleStandard}

\begin{styleStandard}
  [Warning: Image ignored] % Unhandled or unsupported graphics:
%\includegraphics[width=4.9756in,height=2.3035in,width=\textwidth]{pezik-img002.png}
 
\end{styleStandard}

\begin{styleStandard}
Fig. 2 Corpus-attested valency patterns on the subsumed collocation \textit{profound effect}. \ 
\end{styleStandard}

\begin{styleStandard}
The examples discussed in this section show that both lexical and syntactic subsumption graphs provide an intuitive representation of such complex phenomena. As shown in the following sections, subsumption graphs can also be used to define the microstructure of entries in an automatic combinatorial dictionary. \ 
\end{styleStandard}

\begin{listWWNumxxiileveli}
\item 
\begin{stylelsSectioni}
Treelets
\end{stylelsSectioni}
\end{listWWNumxxiileveli}
\begin{styleStandard}
The last section of this chapter presents the first version of \textit{Treelets} – a new application which implements the dependency-based phraseology extraction and visualization methods described above. The application is distributed freely as a Docker image and it can be used to extract one or more ACDs from a dependency-parsed reference corpus using user-defined dependency subtree patterns. The resulting ACDs can be searched through the built-in web application, exported or used directly as relational databases.
\end{styleStandard}

\begin{listWWNumxxiileveli}
\item 
\begin{listWWNumxxiilevelii}
\item 
\begin{stylelsSectionii}
Corpus Formats and Metadata
\end{stylelsSectionii}
\end{listWWNumxxiilevelii}
\end{listWWNumxxiileveli}
\begin{styleStandard}
The input formats currently supported by Treelets are: 1) plain text files with one text per line and 2) JSON Lines\footnote{ See http://jsonlines.org.} format where each line contains a serialized dictionary with text metadata and contents. The metadata types supported include strings, floats, integers, text, dates and arrays of basic types and they can be explicitly imported into the corpus database using the second format. It is therefore possible to preserve the original structure of the imported corpus at the level of bibliographic annotation and use it to create filtering or aggregating queries against the corpus database (see Table 11). It is also possible to provide externally parsed texts in the CoNLL-U format. Plain text files can be dependency-parsed with one of the spaCy\footnote{ See https://spacy.io.} or UDPipe models (Straka \& Straková 2017). The largest corpus indexed so far with Treelets contains 500 million words, but the database backend of the application is fairly scalable and it is possible to index larger corpora.
\end{styleStandard}

\begin{listWWNumxxiileveli}
\item 
\begin{listWWNumxxiilevelii}
\item 
\begin{stylelsSectionii}
Defining Extraction Patterns
\end{stylelsSectionii}
\end{listWWNumxxiilevelii}
\end{listWWNumxxiileveli}
\begin{styleStandard}
Once a dependency-parsed corpus database is created and indexed, it is possible to define a set of syntactic patterns to be used in the process of extracting a combinatorial database. Table 8 shows the result of using different extraction rules predefined in Treelets. The last two columns of the table show the number of extracted treelets and their cumulative frequencies. 
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.26635984in}|m{2.13236in}|m{1.2851598in}|m{1.1163598in}|m{1.2879599in}|}
\hline
\# &
\textbf{Pattern } &
\textbf{Dependencies } &
\textbf{Treelets } &
\textbf{Occurrences}\\\hline
1  &
Adjectival modifiers  &
amod  &
405 385  &
4 090 711\\\hline
2  &
Nouns with prep. objects  &
prep, pobj  &
368 357  &
1 906 280\\\hline
3  &
Direct objects  &
dobj  &
275 037  &
2 562 868\\\hline
4  &
Nominal subjects  &
nsubj  &
188 652  &
1 849 137\\\hline
5  &
Adjectival modifiers as direct objects  &
amod, dobj  &
82 311  &
314 510\\\hline
6  &
Nominal subjects with adj. modifiers  &
nsubj, amod  &
46 798  &
189 852\\\hline
7  &
Adverbial mods. of adjectives  &
advmod  &
43 233  &
58 0021\\\hline
8  &
Adjectival mods. as direct objects with prep. &
amod, dobj, prep  &
20 082  &
70 545\\\hline
9  &
Direct Objects with Prep. Objects  &
amod, dobj, prep, pobj  &
5 314  &
12 935\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 8 A summary of 8 syntactic types of subtrees extracted from BNC.
\end{styleStandard}

\begin{styleStandard}
Custom extraction rules can be defined using the editor shown in Fig. 3. In order to create a new extraction rule, which is essentially a dependency property subtree, it is necessary to define a directed tree graph as well as the aggregation keys of its vertices and edges. By default, the aggregation key is a combination of lemmas, part of speech tags and dependency types defined on the edges of the graph.
\end{styleStandard}

\begin{styleStandard}
  [Warning: Image ignored] % Unhandled or unsupported graphics:
%\includegraphics[width=6.2957in,height=3.3437in,width=\textwidth]{pezik-img003.png}
 
\end{styleStandard}

\begin{styleStandard}
Fig. 3 Designing extraction patterns in Treelets.
\end{styleStandard}

\begin{styleStandard}
The rule shown in Fig. 3 illustrates a special feature of Treelets which was implemented to deal with possible peculiarities of different dependency treebank annotation schemes. As hinted above, the validity of the Continuity Restraint, which assumes that phraseological units are lexicalized subtrees of the sentence dependency tree, may depend on the details of the dependency formalism. For example, in the current version of the Universal Dependency framework prepositions may function as case markers of their nominal heads. This means that the Continuity Restraint is not preserved for constructions such as \textit{have a direct bearing on}. This is because the governor nominal node whose ‘case is marked’ by the preposition \textit{on} according to this representation is not an ‘obligatory’ or ‘typical’ lexical node of this expression.\footnote{ This example also shows that the results of relational phraseology extraction depend on the syntactic framework used to annotate a given corpus.} To deal with such discontinuities, it is possible to use part-of-speech tags rather than lemmas to define the aggregation keys on whose values the extraction pattern is aggregated. In the example extraction pattern shown in Fig. 3, the aggregation key of the vertex marked as \textit{cw2} is therefore simply its part-of-speech tag (NOUN) rather than a combination of the tag and a lemma found on this vertex. In other words, the lemmas on the \textit{cw2} vertex are ignored in the aggregation process and constructions such as \textit{have a direct bearing on + NOUN} are counted as instances of the same lexicalized pattern. It is possible to test such rules on a selected reference corpus before using them for extraction. Once the ACD is generated it can be searched for both headwords such as \textit{role} as a noun and specific treelets of arbitrary length in which it is found such as \textit{play a specific role in the development of}. One of the results of a single query term for the string ‘role’ in the ACD search field is a view similar to Table 9, which summarizes the syntactic types of the lexicalized treelets of the noun \textit{role} in the ACD extracted from BNC\footnote{ \textrm{Currently, syntactic variants of recurrent treelets such as }\textrm{\textit{high hopes}}\textrm{ vs }\textrm{\textit{hopes were/are high}}\textrm{ are not explicitly related in the underlying database. However, they are usually dynamically related in user queries. For example a search for the lemma ‘hope’ will return both of the abovementioned syntactic configurations of high + hopes in the summary table of results similar to Table 9.}}. 
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.31985986in}|m{1.8816599in}|m{0.70875984in}|m{3.07056in}|}
\hline
\textbf{\# } &
\textbf{Rule } &
\textbf{Treelets } &
\textbf{Examples}\\\hline
\raggedleft 1 &
Nouns with prep. objects &
\raggedleft 1 182 &
\textit{role of state, role in society, role in process}\\\hline
\raggedleft 2 &
Adjectival modifiers &
\raggedleft 504 &
\textit{important role, major role, key role}\\\hline
\raggedleft 3 &
Adjectival mods. as direct objects &
\raggedleft 367 &
\textit{play important role, play major role, play key role}\\\hline
\raggedleft 4 &
Direct objects &
\raggedleft 334 &
\textit{meet role, play role, have role}\\\hline
\raggedleft 5 &
Adjectival mods. as direct objects with preps. &
\raggedleft 168 &
\textit{play important role in, play major role in, play key role in}\\\hline
\raggedleft 6 &
Nominal subjects &
\raggedleft 106 &
\textit{role be, role have}\\\hline
\raggedleft 7 &
Direct objects with prep. objects &
\raggedleft 91 &
\textit{play important role in development, play central role in development}\\\hline
\raggedleft 8 &
Nominal subjects with adjectival mods. &
\raggedleft 53 &
\textit{initial role be, former role be, final role be}\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 9 A summary of recurrent subtrees containing the noun \textit{role}.
\end{styleStandard}

\begin{styleStandard}
By clicking on a matching treelet, users are redirected to its dedicated page which currently consists of the following four sections:
\end{styleStandard}


\setcounter{listWWNumxxvileveli}{0}
\begin{listWWNumxxvileveli}
\item 
\begin{styleListParagraph}
The concordances of the recurrent treelet in the reference corpus; 
\end{styleListParagraph}
\item 
\begin{styleListParagraph}
The Statistics table with some statistical properties of the treelet, such as frequency, dispersion and strength of association;
\end{styleListParagraph}
\item 
\begin{styleListParagraph}
The dependency structure of the candidate construction;
\end{styleListParagraph}
\item 
\begin{styleListParagraph}
The Valency section, which features a tabular view of the directly subsumed and directly subsuming recurrent treelets. For example, the binary collocation \textit{important role} is hyperlinked to the entry page for \textit{play an important role}, which is linked to the entry for \textit{play an important role in}, etc.
\end{styleListParagraph}
\end{listWWNumxxvileveli}
\begin{styleStandard}
The Valency section showcases a simple application of the subsumption graph structure of the ACD entries generated with Treelets. More sophisticated representations of the higher-order constructions detected with this application are discussed in the next section.
\end{styleStandard}

\begin{styleStandard}
The current version of Treelets also supports extraction bases on untyped dependency tree patterns. In this mode, users only define lemmas for which all dependency subtrees up to a certain size (the current limit being six nodes) are extracted, aggregated and ordered by their frequency. In other words, only the ‘shape’ of extracted subtrees, i.e. directed edges between the nodes, is predefined in this case. Table 10 shows the results of such ad-hoc extraction of recurrent subtrees containing the noun \textit{factor }in BNC. Combinations of nouns joined by a preposition turn out to be the most productive pattern in which \textit{factor} is found in this corpus with 254 distinct recurrent treelets identified. The largest number of instances is yielded by combinations of adjectives modifying this noun with 7962 occurrences identified.
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.9094598in}|m{2.3441598in}|m{0.9115598in}|m{0.9115598in}|m{0.9115598in}m{0.9115598in}|}
\hline
\# &
Structure &
Subtrees &
Mass &
\multicolumn{1}{m{0.9115598in}|}{Examples} &
\\\hline
1~ &
v2-prep-v1-pobj-{\textgreater}v0~ &
254~ &
3104~ &
\multicolumn{2}{m{1.9018599in}|}{number + of + factor~

one + of + factor~

depend + on + factor~}\\\hline
2~ &
v0-amod-{\textgreater}v1~ &
218~ &
7962 &
\multicolumn{2}{m{1.9018599in}|}{key + factor~

important + factor~

major + factor~}\\\hline
3~ &
v1-dobj-{\textgreater}v0~ &
153~ &
1087~ &
\multicolumn{2}{m{1.9018599in}|}{take + factor~

identify + factor~

consider + factor~}\\\hline
4~ &
v1-nsubj-{\textgreater}v0~ &
120~ &
2730~ &
\multicolumn{2}{m{1.9018599in}|}{factor + be~

factor + include~

factor + influence}\\\hline
5~ &
v2-pobj-v0-amod-{\textgreater}v1~ &
74~ &
1292~ &
\multicolumn{2}{m{1.9018599in}|}{of + other + factor~

by + other + factor~

of + important + factor~}\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 10 Weakly-supervised extraction of dependency subtrees.
\end{styleStandard}


\setcounter{listWWNumxxiileveli}{0}
\begin{listWWNumxxiileveli}
\item 

\setcounter{listWWNumxxiilevelii}{0}
\begin{listWWNumxxiilevelii}
\item 
\begin{stylelsSectionii}
Exploring Valency Patterns
\end{stylelsSectionii}
\end{listWWNumxxiilevelii}
\end{listWWNumxxiileveli}
\begin{styleStandard}
To illustrate the exploratory potential of subsumption graph visualizations, let us consider graphs generated for two entries from two different corpora. Fig. 4 shows a subsumption graph generated for the noun \textit{role} from BNC using the eight extraction rules mentioned above. Only subtrees which occurred in this corpus at least two times are shown in this graph. The two vertices with the highest indegree in this graph represent the direct object binary collocations \textit{play a role} and \textit{have a role}. 
\end{styleStandard}

\begin{styleStandard}
  [Warning: Image ignored] % Unhandled or unsupported graphics:
%\includegraphics[width=4.3244in,height=4.4228in,width=\textwidth]{pezik-img004.png}
 
\end{styleStandard}

\begin{styleStandard}
Fig. 4 A subsumption graph of \textit{role} as a noun generated from BNC. Only selected treelets are labelled.
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.6143598in}|m{0.5184598in}|m{1.2656599in}|m{1.0038599in}|m{1.1025599in}|}
\hline
 &
\multicolumn{2}{m{1.86286in}|}{\textbf{Indegree}} &
\multicolumn{2}{m{2.18516in}|}{\textbf{Frequency}}\\\hline
ACD &
BNC &
COCA &
BNC &
COCA\\\hline
play a role &
99 &
292 &
2 591 &
16 225\\\hline
have a role &
96 &
136 &
1 042 &
2 955\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 11 Frequency and ‘productivity’ of play/ have a role in COCA and BNC.
\end{styleStandard}

\begin{styleStandard}
As shown in Table 11, \textit{play a role} is considerably more frequent than \textit{have a role} in COCA and only slightly more frequent in BNC. The indegrees of those vertices in the subsumption graph suggest that \textit{play a role} is used in various larger constructions in COCA and BNC. The exact structure of the subsumption graph depends on the size and composition of the corpus, especially if a raw frequency threshold is used to select the nodes. In principle, it is also possible to use any conjunction of frequency, strength of association, dispersion, independence scores to create more sophisticated criteria of selecting the vertices of a subsumption graph. For example, a subsumption graph could contain only vertices representing subtrees whose average strength of association score is greater than some minimum ‘significance’ threshold. Even a simple frequency-based subsumption graph may be helpful in formulating hypotheses to explain the differences between the varieties of English represented by the three corpora. One such hypotheses could be that \textit{play a role} is more frequent in American English than in British English or simply in the registers and text types represented in the two reference corpora.
\end{styleStandard}

\begin{listWWNumxxiileveli}
\item 
\begin{listWWNumxxiilevelii}
\item 
\begin{stylelsSectionii}
Source Databases
\end{stylelsSectionii}
\end{listWWNumxxiilevelii}
\end{listWWNumxxiileveli}
\begin{styleStandard}
The corpora and ACDs generated by Treelets are stored in a PostgreSQL database, which can be installed on any machine and in any location specified by the user. This means that more technical users can take full advantage of the dependency-parsed corpus database by querying it directly and from different client applications if necessary to obtain data views which have not yet been implemented in the Treelets web application. One example of such a query against the source corpus database is shown in Table 12. The purpose of the query is to find dependency subtrees which consist of the preposition \textit{of}, the noun \textit{force} as its object and an unspecified adjectival modifier or compound noun dependent of this object. Furthermore, the search is limited to texts which are marked as ‘SPOK’ (spoken register in the imported COCA corpus). The results are aggregated on lemmas and dependency types, counted and limited to matching prepositional phrases which only occur in at least 20 different spoken texts in the corpus.
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{6.4969597in}|}
\hline
SELECT

wc1.head\_lemma gov\_lemma, \ wc2.lemma dep\_lemma, \ wc1.lemma lemma, 

wc1.dep dep, wc2.dep dep\_dep, count(*) cnt, count(distinct(wc1.text\_id)) texts

FROM

word\_conllu wc1

JOIN word\_conllu wc2 on wc2.sentence\_id = wc1.sentence\_id and wc2.head\_id = wc1.id

JOIN text t on t.id = wc1.text\_id

WHERE

wc1.lemma = 'force' AND wc1.head\_lemma='of'

AND wc2.dep = ANY(ARRAY['amod','compound']) AND wc1.dep = ANY(ARRAY['pobj'])

AND t.genre = ANY(ARRAY['SPOK'])

GROUP BY wc1.lemma,wc1.head\_lemma, wc1.dep,dep\_dep,dep\_lemma

having count(distinct(wc1.text\_id)) {\textgreater}= 50

order by cnt desc;\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 12 An SQL query used to extract and aggregate recurrent prepositional attachments from the Treelets corpus database.
\end{styleStandard}

\begin{styleStandard}
As shown in Table 13, the most frequent prepositional attachments identified with this query include \textit{of armed forces}, \textit{of military forces}, \textit{of (det) work force}, etc. The query could be easily elaborated to identify more complex subtrees or return concordances of matching spans instead of aggregated summaries. It is also possible to relax the dependency joins defined in the query and use positional cooccurrence criteria in order to increase the recall of queries by matching unspecified or erroneously parsed dependency relations.
\end{styleStandard}

\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{0.8122598in}|m{0.82685983in}|m{0.65735984in}|m{0.65735984in}|m{0.74065983in}|m{0.65735984in}|m{0.65735984in}|}
\hline
\textbf{gov\_lemma} &
\textbf{dep\_lemma} &
\textbf{lemma} &
\textbf{dep} &
\textbf{dep\_dep} &
\textbf{cnt} &
\textbf{texts}\\\hline
of &
armed &
force &
pobj &
amod &
\raggedleft 197 &
\raggedleft\arraybslash 167\\\hline
of &
military &
force &
pobj &
amod &
\raggedleft 174 &
\raggedleft\arraybslash 155\\\hline
of &
work &
force &
pobj &
compound &
\raggedleft 134 &
\raggedleft\arraybslash 122\\\hline
of &
task &
force &
pobj &
compound &
\raggedleft 133 &
\raggedleft\arraybslash 112\\\hline
of &
U.S. &
force &
pobj &
compound &
\raggedleft 124 &
\raggedleft\arraybslash 112\\\hline
\end{supertabular}
\end{flushleft}
\begin{styleStandard}
Table 13 Recurrent prepositional attachments retrieved from the Treelets corpus database.
\end{styleStandard}

\begin{listWWNumxxiileveli}
\item 
\begin{stylelsSectioni}
Summary and future work
\end{stylelsSectioni}
\end{listWWNumxxiileveli}
\begin{styleStandard}
The starting assumption of this paper was the vast majority of phraseological units have an internal syntactic structure and that subsentential PUs an external valency. Using a dependency-based phraseology extraction approach, the paper then demonstrated how those properties of PUs can be at least partly accounted for in automatically combinatorial databases. It is hoped that the software tool implementing dependency-based phraseology extraction may help lexicographers and phraseologists “deal with the enormous structural variety of English idioms” (Cowie et al. 1993: 11) and possibly also explore prefabricated collocational chains as a noteworthy type of phraseological units. Future versions of Treelets will also include phraseology detection features (Pęzik 2018) to enable more advanced reference corpus-based indexing of idiomatic expressions.
\end{styleStandard}

\begin{listWWNumxxiileveli}
\item 
\begin{stylelsSectioni}
References
\end{stylelsSectioni}
\end{listWWNumxxiileveli}
\begin{styleStandard}
Biber, Douglas and Conrad, Susan. 1999. Lexical Bundles in Conversation and Academic Prose. \textit{Language and Computers} 26, 181–190.
\end{styleStandard}

\begin{styleStandard}
Burger, Harald. 2003. \textit{Phraseologie: eine Einführung am Beispiel des Deutschen} (Grundlagen der Germanistik (GrG), Band 36). Berlin: Erich Schmidt Verlag.
\end{styleStandard}

\begin{styleStandard}
Boguslavsky, Igor. 2003. On the passive and discontinuous valency slots. In: \textit{MTT Conference Proceedings}, 16-18 June 2003, Paris.
\end{styleStandard}

\begin{styleStandard}
Boguslavsky, Igor. 2016. On the non-canonical valency filling. In: \textit{Proceedings of the Workshop on Grammar and Lexicon: interactions and interfaces (GramLex)}. Osaka, Japan: The COLING 2016 Organizing Committee, 51–60.
\end{styleStandard}

\begin{styleStandard}
Cowie, Anthony Paul, and Ronald Mackin (eds). 1975. \textit{Oxford Dictionary of Current Idiomatic English}. London: Oxford University Press.
\end{styleStandard}

\begin{styleStandard}
Cowie, Anthony Paul, Ronald Mackin, and I. R McCaig. 1993. \textit{Oxford Dictionary of English Idioms}. Oxford; New York: Oxford University Press.
\end{styleStandard}

\begin{styleStandard}
Cowie, Anthony Paul. 1998. \textit{Phraseology[202F?]: Theory, Analysis, and Applications}. Oxford: Oxford University Press.
\end{styleStandard}

\begin{styleStandard}
Crowther, J., S. Dignen, and D. Lea. 2003. \textit{Oxford Collocations Dictionary: For Students of English}. Oxford: Oxford University Press.
\end{styleStandard}

\begin{styleStandard}
Evert, S. 2005. \textit{The Statistics of Word Cooccurrences. Word Pairs and Collocations}. Unpublished PhD dissertation. Stuttgart
\end{styleStandard}

\begin{styleStandard}
Hausmann, Franz Josef. 2004. “Was Sind Eigentlich Kollokationen.” In: K. Steyer (ed.), \textit{Wortverbindungen - mehr oder weniger fest}. Berlin: De Gruyter, 309–334.
\end{styleStandard}

\begin{styleStandard}
Heid, Ulrich, and Rufus H Gouws. 2006. A Model for a Multifunctional Dictionary of Collocations. In: \textit{Atti Del XII Congresso Internazionale Di Lessicografia}: Turin: 6-9 September 2006, 979–988.
\end{styleStandard}

\begin{styleStandard}
Moon, Rosamund. 1998. \textit{Fixed Expressions and Idioms in English: A Corpus-Based Approach}. Oxford: Clarendon Press.
\end{styleStandard}

\begin{styleStandard}
Moroz, Andrzej. 2013. Zależność a kookurencja – dwa różne sposoby wiązania wyrażeń. (Dependency and co-occurrence - two different ways of expressions binding). \textit{Studia Językoznawcze} 12, 121{}--132.
\end{styleStandard}

\begin{styleStandard}
Mel[2B9?]čuk, Igor[2B9?]. 1988. \textit{Dependency Syntax: Theory and Practice} (\textit{SUNY Series in Linguistics}). Albany: State University of New York Press..
\end{styleStandard}

\begin{styleStandard}
O’Grady, William. 1998. The Syntax of Idioms. \textit{Natural Language \& Linguistic Theory} 16: 2, 279–312.
\end{styleStandard}

\begin{styleStandard}
Pęzik, Piotr. 2014. Graph-Based Analysis of Collocational Profiles. In: V. Jasensek and P. Grzybek, \textit{Phraseologie Im Wörterbuch Und Korpus} (Phraseology in Dictionaries and Corpora). Maribor: Zora, 227–243. 
\end{styleStandard}

\begin{styleStandard}
Straka, Milan, and Jana Straková. 2017. Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe. In: \textit{Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.} Vancouver, Canada: Association for Computational Linguistics, 88–99. 
\end{styleStandard}
\end{document}
