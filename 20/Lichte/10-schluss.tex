%!TEX root = main.tex
\chapter{Zusammenfassung und Fazit} 

Der Forschungsgegenstand dieser Arbeit war der Status lexikalischer Valenzeigenschaften in Syntaxmodellen, wobei der Betrachtungsschwerpunkt auf solchen Syntaxmodellen lag, denen die Baumadjunktionsgrammatik (TAG) und ihre Varianten zugrundeliegen. Zunächst wurde die fest etablierte und auch im TAG-Framework dominante Sichtweise nachvollzogen, derzufolge sich die Valenzeigenschaften strukturell oder derivationell direkt in der Syntax niederschlagen. Die valenztheoretische Trichotomie aus Valenzträger, Ergänzung und Angabe führt hier also zu einer Ungleichbehandlung im Syntaxmodell. Bedingt durch diese enge konzeptuelle Verzahnung und durch die formalen Eigenschaften der Syntaxmodelle werden bestimmte Formen der Valenzrealisierung idealisiert, andere Realisierungsvarianten hingegen marginalisiert. Ich habe in Abschnitt~\ref{sec-valenzrealisierung} drei solche Idealisierungen der Valenzrealisierung beschrieben: die Idealisierung der Vollständigkeit, die Idealisierung der Kontinuität und die Idealisierung der Funktionalität, wobei letztere im Weiteren ausgeklammert wurde.

Diese Idealisierungen stehen im Widerspruch zu bestimmten, sich überschneidenden Datenklassen, nämlich kohärenten Konstruktionen und Ellipsen, mit diskontinuierlichen oder unvollständigen Valenzrealisierungen. In Kapitel~\ref{chap-kohaerenz} und~\ref{chap-ellipse} habe ich beide Datenklassen unter valenztheoretischen Gesichtspunkten ausführlich dargestellt. Um Kohärenz und Ellipse im Geltungsbereich dieser Idealisierungen modellieren zu können, wurden in anderen Arbeiten bereits Ausweichstrategien entwickelt: Bewegung und Valenzvereinigung bei Kohärenz; Reduktion, Füllung und Verschmelzung bei Ellipse. In dieser Arbeit habe ich mich jedoch für Syntaxmodelle interessiert, die aus diesem Raster herausfallen und in denen diese Valenzidealisierungen ganz oder teilweise fehlen.

Zunächst habe ich mich der direkten Repräsentation diskontinuierlicher Valenzrealisierungen zugewandt, die TAG alleine schon aufgrund der erweiterten Lokalitätsdomäne und der Adjunktionsoperation bis zu einem gewissen Grad leisten kann. Um über die Ausdrucksstärke von TAG hinaus Diskontinuität modellieren zu können, habe ich in Kapitel~\ref{sec-ttmctag} die TAG-Erweiterung TT-MCTAG entwickelt, die einige Schwächen vergleichbarer TAG"=Erweiterungen (TL-MCTAG, V-TAG, DTG, SegTAG, SN-MCTAG) ausräumt. Im Anschluss habe ich ein TT-MCTAG-Fragment für kohärente Konstruktionen im Deutschen skizziert. Es enthält auch als kritisch eingestufte Daten wie die mehrfach"=partielle Voranstellung, diverse Verbalkomplexstellungen und das Fernpassiv. Das macht es meines Wissens zum  umfangreichsten Grammatikfragment mit einer direkten Repräsentation diskontinuierlicher Valenzrealisierung, das bisher beschrieben worden ist. Es zeigt auch, dass Ansätze mit diskontinuierlichen Konstituenten den Daten und der linguistischen Intuition durchaus gerecht werden können -- entgegen den Zweifeln, die in der HPSG-Literatur in den letzten Jahren die Oberhand gewonnen haben.

Das TT-MCTAG-Fragment folgt zwar nicht der Idealisierung der Kontinuität, aber der Idealisierung der Vollständigkeit: Obligatorische Bestandteile eines Valenzrahmens können nicht weggelassen werden, ohne auf leere Wörter oder spezielle Elementarstrukturen zurückzugreifen. Für das TT-MCTAG-Fragment habe ich in Abschnitt~\ref{sec-deanchoring-ttmctag} beispielhaft einen Reduktionsansatz ausgeführt, der zur Implementierung der internen und externen Bedingungen auf elegante Weise vom Ableitungsbaum Gebrauch macht. 

Gibt man auch die Idealisierung der Vollständigkeit auf und strebt eine genuin unvollständige Repräsentation von  Ellipsen in der Syntax an, dann scheint mir die Trennung von Syntax und Valenz unvermeidbar zu sein. Das führt dazu, dass die Syntax  keinen Unterschied macht zwischen Valenzträgern, Ergänzungen und Angaben. Mit STUG  habe ich erstmals einen baumbasierten Grammatikformalismus dieser Art vorgeschlagen und auch gezeigt, wie damit Ellipse und Kohärenz im Deutschen modelliert werden können. Doch STUG hilft nicht einfach nur, den Einsatz leerer Wörter vollständig zu vermeiden, sondern scheint auch eine Reihe weiterer erfreulicher Eigenschaften zu besitzen. Genannt wurden die mögliche Inkrementalität der Verarbeitung und die stärkere Faktorisierung der Grammatik. Eine genaue Untersuchung dieser und anderer Eigenschaften von STUG, insbesondere der Komplexitätseigenschaften, steht jedoch noch aus. Und auch das skizzierte STUG-Modell für das Deutsche ist natürlich längst nicht zufriedenstellend, wie die im letzten Kapitel formulierten Desiderata gezeigt haben. Ein Anfang ist aber gemacht.

Doch auch der Valenzbegriff selber hat sich im Verlauf dieser Arbeit gewandelt. In der Jacob'schen Explikation war Valenz bestimmt als direkter Reflex der semantischen Prädikat"=Argument"=Struktur des Valenzträgers (d.\,h.\ durch die ARG-Beziehung). Der valenztheoretischen Trichotomie ging also eine semantische Trichotomie aus Prädikat, Argument und Modifizierer voraus. Dagegen reflektiert die zuletzt bei STUG eingeführte Valenzstruktur keine derartige semantische Trichotomie. Ihr liegt konzeptuell vielmehr Storrers Situationsvalenz und Fillmores Scenes-and-Frames-Semantik zugrunde, die keine explizite Differenzierung zwischen Ergänzung und Angabe (bzw.\ zwischen Argument und Modifizierer) machen. Valenz gilt hier als Potential der Situationsrollenspezifizierung und \mbox{-perspektivierung}, wobei die Spezifizierung der Valenzstruktur aus unterschiedlichen Richtungen und nicht nur durch den Valenzträger erfolgen kann. 

Es bleibt abschließend die Frage, welcher Weg der bessere ist. Soll man nun die Idealisierungen vermeiden, Syntax und Valenz trennen, das Valenzkonzept überdenken? Oder soll man besser auf den gewohnten Pfaden bleiben? In dieser Arbeit bin ich bis jetzt dieser Frage ausgewichen (so mein Eindruck), wiewohl die Sympathien ungleich verteilt waren und sicherlich erkennbar der Idealisierungsvermeidung galten. Mein Anliegen war zunächst einmal, den (eigenen) Horizont zu erweitern und die Alternativen überhaupt erst greifbar zu machen. Die Absicht war, zu verstehen. Dafür ist es nicht nötig, gleich vehement gegen andere Ansätze zu argumentieren. Ich habe auch an der einen oder anderen Stelle durchscheinen lassen, dass ich skeptisch bin, dass solche grammatik- und frameworkübergreifenden Vergleiche leicht zu bewerkstelligen sind. 

%\largerpage%
Leider ist selbst 60 Jahre nach der Etablierung formalisierter Grammatikmodelle noch immer nicht klar, wie man zwischen zwei konkreten, nicht-trivialen Grammatiken entscheiden soll.\footnote{Chomsky widmet sich dieser Frage bereits einigermaßen ausführlich in seinen frühen Klassikern (\citealt[Kapitel~2]{Chomsky:64}; \citealt[Kapitel~1]{Chomsky:65}). Leider wird diese Art der fundamentalen Methodenkritik aus meiner Sicht viel zu selten betrieben. Eine erfreuliche Ausnahme ist \cite[Kapitel~20]{Mueller:07}.} Das Offensichtliche, nämlich die Abdeckung zu betrachten (d.\,h.\ Chomskys "`observational adequacy"'), reicht nicht aus, da potentielle Erweiterungen berücksichtigt werden müssen \citep[Kapitel~20]{Mueller:07}.\footnote{Wenn wir allein die aktuelle Abdeckung als Kriterium in Betracht ziehen, hat es keinen Sinn, neue Grammatiken zu entwickeln, einfach weil es schon Grammatiken mit einer größeren Abdeckung gibt. Mit anderen Worten: Die abdeckungsgrößte Grammatik gewinnt sofort.} Die Erweiterung in der Zukunft hat aber nichts mit der Abdeckung jetzt zu tun. Auch der Vergleich anhand anderer quantitativer Eigenschaften, etwa der Anzahl und Mächtigkeit der eingesetzten Grammatik"-elemente, ist nur sehr eingeschränkt möglich, da die Abdeckung identisch und die Grammatiken tatsächlich idempotent sein müssen. Und wie will man, selbst wenn das zutrifft, unterschiedliche Grammatikelemente wie Merkmale und Regeln gegeneinander verrechnen? Ähnlich, nur noch schlimmer, verhält es sich mit der von \cite[403]{Mueller:07} vorgeschlagenen Erweiterbarkeit, d.\,h.\ mit dem Aufwand, eine Grammatik so zu erweitern, dass eine bestimmte Veränderung in der Abdeckung erreicht wird. Auch hier ist das Problem die Gewichtung unterschiedlicher Typen, nämlich von konkreten und virtuellen Grammatiken.

Man bedenke außerdem, dass der Vergleich nochmals schwerer fällt, wenn die Grammatiken unterschiedlichen Frameworks angehören. Gerade hier geschieht es leicht, dass man Urteile fällt (z.\,B.\ gegen Transformationen, leere Elemente, oder eben diskontinuierliche Konstituenten), die letztlich zu selektiv sind und vor allem den persönlichen Geschmack widerspiegeln. Geschmacksurteile mögen methodologisch unvermeidbar sein, weil sie Ausdruck einer Anfangsmotivation sind; doch wissenschaftlich sind sie wertlos. Schon Chomsky hat die Situation kommen sehen, zwischen zwei Grammatiken wählen zu müssen, die nach quantitativen und qualitativen Maßstäben ("`observational adequacy"') gleichwertig erscheinen. Sein Vorschlag besteht darin, auf zusätzliche Kriterien zurückzugreifen, die weniger auf Abdeckung oder Sparsamkeit der Mittel abzielen, als auf die Kompatibilität mit der Vorstellung, dass Grammatik ein kognitives Instrument ist und dass dieses Instrument mit beschränkten kognitiven Ressourcen gelernt und beherrscht werden muss. Die Rede ist von der Erklärungsadäquatheit ("`explanatory adequacy"') einer Theorie oder Grammatik. 

Was würde nun dafür sprechen, dass sich die Valenzidealisierungen positiv auf die quantitativen Eigenschaften einer Grammatik auswirken. Werden damit die Grammatiken kleiner und deren Abdeckung größer? Eher im Gegenteil; wir haben ja "`Sonderfälle"' gesehen, in denen dann Erweiterungen vorgenommen werden müssen. Könnten die Idealisierungen aber vielleicht Kriterien der Erklärungsadäquatheit darstellen? Das würde bedeuten, dass sie angeboren wären, quasi als Prinzipien der Universalgrammatik. Es spricht nicht viel für diese nativistische Deutung, denn warum sind dann die inkompatiblen "`Sonderfälle"' überhaupt möglich?

Während der Nutzen für konkrete Grammatiken nur schwer zu erkennen ist, ist der methodologische Schaden im Umgang mit den Daten offensichtlich. Die apriorische Annahme, dass Satzglieder unterschiedlichen valenztheoretischen Kategorien angehören (Valenzträger, Ergänzung und Angabe), die sich wiederum syntaktisch sehr unterschiedlich verhalten, wirkt sich auf die Wahrnehmung der Daten aus. Sieht man einen Valenzträger, so unterstellt man ihm unwillkürlich Dominanz und eine determinierende Funktion; über Angaben sieht man dagegen leicht hinweg. Damit geht eine gewisse Neutralität gegenüber den Daten verloren. Und natürlich setzt man sich lieber mit Daten auseinander, die mit den durch die Idealisierungen geweckten Erwartungen zusammenpassen -- Theoriebildung und Datenauswahl gehen eben gerne Hand in Hand. 

Statt Idealisierung, d.\,h.\ der apriorischen Beschränkung durch den Grammatikformalismus, sollten wir etwas anstreben, was man als Generalisierung bezeichnen könnte. Generalisierungen beziehen sich auf Eigenschaften von Primärrepräsentationen (das sind im Idealfall möglichst neutrale, beobachtungsnahe Repräsentationen syntaktischer Einheiten), geben diese aber nicht vor oder schränken diese ein. Generalisierungen sind vielmehr nachgeordnete Beschreibungen von Primärrepräsentationen. Ein Beispiel dafür ist das Verhältnis zwischen dem abgeleiteten Baum in TAG (der Primärrepräsentation) und seiner Faktorisierung qua Elementarbäume (der Generalisierung); oder das Verhältnis zwischen der flachen Syntaxrepräsentation in STUG und einem die Fusion regulierenden endlichen Automaten. Diese konzeptuelle Trennung hat den Vorteil, dass die Primärrepräsentationen auch dann erhalten bleiben, wenn sich die Generalisierungen grundlegend ändern oder unterscheiden. Generalisierungen sind möglicherweise nicht in der Art reversibel wie es quantitative Abstraktionen à la \cite{Stokhof:Lambalgen:11} sind. Aber der ontologische Wechsel vollzieht sich, wenn überhaupt, nur auf der Ebene der Generalisierungen, nicht auf der Ebene der unmittelbaren Datenrepräsentation. Leider hat dieser Hoffnungsschimmer  eine nicht unerhebliche Schattenseite, denn gerade die Form der Primärrepräsentationen ist strittig. Es lässt sich aus meiner Sicht immerhin sagen, dass die STUG-Strukturen (flache Syntaxbäume verlinkt mit Valenzstrukturen) dem Ideal der Beobachtungsneutralität näher zu kommen scheinen als beispielsweise die üblichen abgeleiteten Bäume in TAG, geschweige denn die syntaktischen Strukturen der GB-Theorie, einfach weil bei STUG die Valenzidealisierungen fehlen.

Abschließend kann man festhalten: Auch wenn all diese Überlegungen hier notgedrungen vage bleiben müssen, legen sie doch nahe, den mit dieser Arbeit eingeschlagenen Weg weiterzugehen, nämlich Valenzidealisierungen zu vermeiden, Syntax und Valenz zu trennen, das Valenzkonzept zu überdenken -- oder wirklich gute Gründe zu liefern, das nicht zu tun. 



