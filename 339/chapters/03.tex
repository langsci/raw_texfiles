\chapter{Empirical approach to clitics in BCS}
\label{Empirical approach to clitics in BCS}
%wir haben hier keine Verweisungen auf weitere Kapiteln...
%Here, we give a brief overview of the methods and subdisciplines in which they are used. 1) Segmentation experiments such as unit counting and slash and pause insertion are used in phonology research and the latter is also used in morphology; 2) rating/scaling experiments are applied in the study of phonology, morphology, syntax and semantics; 3) string manipulation experiments such as unit substitution are used to empirically investigate phonetics, while string manipulations such as unit inversion and word blending are used to study phonology; 4) miniature artificial real language subsystem (“Berko-type studies”) experiments are conducted in morphology, word-formation and language change; 5) concept formation experiments are used to study phonology, morphology, syntax and semantics; 6) recall and recognition experimental methods such as proactive interference are used to investigate the nature of lexical-semantic relations, while false memory experiments are used to study phonology and semantics.

\section{Introduction}
\label{Introduction:4}
The goal of this chapter is to present the ongoing discussion on data gathering practices in syntactic research on the one hand and to justify our choice of strategy on the other.

The methodological literature points out that the most widely used data source in syntactic research is speakers’ intuitions \textcolor{black}{\citep[27]{SchutzeSprouse13}}. In order to collect evidence which will enable them to describe syntactic structures, \textcolor{black}{``}syntacticians often rely on their own judgments, or those of a small number of their colleagues \textcolor{black}{''}, about the acceptability of a structure/sentence in question \citep[1]{Dabrowska10}. Some linguists (e.g. \citealt[48ff]{Newmeyer83}, \citealt[353]{Fanselow07}, \citealt[370]{Grewendorf07}, \citealt{Phillips10}) have argued that this kind of data is the most reliable and that it allowed the rapid development of linguistics. Others (e.g. \citealt{Schutze16}, \citealt{Cowart97}, \citealt{Keller00}, \citealt{Featherston07}) have replied that this can be problematic and that such an informal approach to collecting data leaves linguistics on shaky empirical ground.\footnote{\citet{PhillipsLasnik03} try to defend generative grammar and to show that it is not built upon empirically weak foundations by presenting different kinds of experiments which have been used in the generative framework.} As  \citet[25]{ClarkBangerter04} put it, using \textcolor{black}{introspective} methods ``you imagine a wide range of utterances and situations and draw your conclusions. You are limited only by what you can imagine, but that turns out to be quite a limitation.''

There are several crucial points in which typical informal linguistic judgments differ from the methodologically standardised practise of data gathering: 
\newpage

\begin{enumerate}
	\item judgments collected from very few speakers,\footnote{As \citet[39]{SchutzeSprouse13} point out this can be by necessity. “In the case of languages spoken in remote locations and languages with few remaining speakers, collecting data from just one or two speakers may be all that a linguist can practically do [...]”. However, as we argue later, there is no reason to treat Bosnian, Croatian, and Serbian as languages for which more data could not or should not be collected.}\textsuperscript{,}\footnote{\textcolor{black}{\citet[]{SchutzeSprouse13} do not define how many exactly makes a very few speakers. For us ``very few" means not enough to perform significance tests which allow estimating the probability that the result is replicable in another sample. However, we do not think that everybody in the world needs to work using statistical methods. Therefore, we could retreat from the expression ``very few" in favour of ``unspecific number of speakers" or ``judgments obtained in an unsystematic" or ``not documented way". Of course, the judgments of two speakers might be perfectly in line with those of 102 speakers, but the question remains: how can we obtain confidence about that.}} 
	\item linguists as participants,
	\item judgments collected only for few tokens of a structure of interest,
	\item relatively unsystematic data analysis \citep[cf.][30]{SchutzeSprouse13}. 

\end{enumerate}
	In the following, we discuss some of these points.
	
We agree that stable measures of acceptability (or grammaticality, see below) can be obtained only if we average responses which were provided by a number of informants \citep[cf.][1f]{Dabrowska10}. As \citet[2]{Dabrowska10} points out ``[a]nother problem with linguists’ reliance on their own intuitions is observer bias: the possibility that judgments can be influenced by the observer’s beliefs and expectations''. A further reason why linguists and non-linguists tend to evaluate the same sentences differently can be disparities in experience. \citet{Hiramatsu99} and \citet{Snyder00} experimentally demonstrated the existence of \textsc{syntactic} \textsc{satiation} – a phenomenon where participants (linguists or students of linguistics) are prone to accept some types of ungrammatical or borderline structures due to repetitive exposition to such structures. Furthermore, some scholars (e.g. \citealt[47]{Schutze16}, \citealt[60]{Cowart97}, \citealt[575]{Snyder00}) warn about the additional danger of judgments made by linguists: it is possible that ``correct answers'' could have been learned from the linguistic literature or in the course of education.

Surely, if the intuitions of every native speaker are based on the same hardwired language faculty which according to early generativist assumptions fully represents his or her linguistic competence, consulting a vast number of speakers about the same language phenomenon would only result in replication of one and the same answer, which, of course, would be a waste of time and human resources \citep[85]{BuchstallerKhattab13}. Already in the 1970s it was recognised that such an approach can be problematic: \citet[74]{Sampson75} stated that ``[o]ne of the unfortunate consequences of Chomsky’s mentalist view of linguistics is that in recent years a number of younger linguists have indulged very heavily in arguments based on their intuitions about quirks of their personal idiolects''. Similarly later he claimed that ``[w]e do not need to use intuition in justifying our grammars, and as scientists, we must not use intuition in this way'' \citep[135]{Sampson01}. 

The methodological literature mentions several problems with introspective data \textcolor{black}{collection}. One serious problem was identified by \citet[199]{Labov72} and more recently repeated by \citet{Schutze16}. They point out that it might be dangerous to produce theory and data at the same time. \citet[5]{Schutze16} warns that if linguists continue to produce theory and data at the same time, what is to stop them from purposely or accidentally manipulating the introspection process in order to substantiate their own theories?

To sum up: intuition-based judgments can suffer from bias, unreliability, and narrowness \citep{Schutze16}. These problems are described in quite some detail in relation to data on CLs in BCS in our book. Specifically, data based on linguists’ informal judgments very often turned out to be contradictory and flawed.\footnote{To understand this problem fully, compare the data from the literature in Chapter \ref{Clitics and variation in grammaticography and related work} with the empirical data from spoken varieties in Chapters \ref{Clitics in dialects} and \ref{Clitics in a corpus of a spoken variety}. The gap between data based on informal judgments and empirical data is even better illustrated in our Chapters \ref{A corpus-based study on CC in da constructions and the raising-control distinction (Serbian)}--\ref{Experimental study on constraints on clitic climbing out of infinitive complements}.} In Section \ref{From researchers' intuition to triangulation of methods} we present how these problems can be overcome through triangulation of methods. A detailed discussion of the empirical approach chosen for this monograph together with a concise overview of corpus linguistic and psycholinguistic methods can be found in Section \ref{Empirical approach in the current study}. The chapter ends with a presentation of the experiment chosen for our study.

\section{From researchers' intuition to triangulation of methods}
\label{From researchers' intuition to triangulation of methods}
\subsection{Triangulation of methods}
\label{Triangulation of methods}
The pitfalls of studies based exclusively on intuition mentioned above can be overcome through triangulation of methods, an approach well-established in the social sciences.  Following the definition of \citet[154]{YeasminRahman12}, triangulation ``is a process of verification that increases validity by incorporating several viewpoints and methods''. The importance of triangulation for linguistic studies has only recently been acknowledged. \citet[100]{Hoffmann13} and \citet[311]{FordBresnan13} recommend triangulation of methods to provide corroborating evidence and to capture language usage most accurately. According to \citet[33]{Angouri10}, mixed methods designs (i.e. combining or integrating quantitative and qualitative elements) arguably contribute to a better understanding of the various phenomena under investigation, since quantitative research is useful for generalising research findings, while qualitative approaches are particularly valuable in providing rich in-depth data.

\begin{sloppypar}
\citet[293]{Rosenbach13} argues that the combination of different methods eliminates the restrictions which emanate from the limitations of a given single  method. For instance in corpus-based data it is hard to control influencing factors, a problem which we can overcome when we use experimental elicitation of data. The problem of limited context and the lack of naturalness, i.e. ecological validity (see section below), which accompanies experimental data, can be avoided if we supplement it with corpus data. 
\end{sloppypar}

However, combining different methods has disadvantages as well, since it generates higher overall costs than applying a single approach: it is more time consuming and requires expertise in both methods (\citealt[293]{Rosenbach13}, \citealt[311]{FordBresnan13}).\footnote{\textcolor{black}{Sometimes, as we argue in Section \ref{Methods},  the cost of combining different methods may be lower. This happens, for example, when retrieving rare structures from corpora is far more complicated than elicitation and may lead to the problem of negative evidence.}} Hence, the last disadvantage often means involving more researchers, which, can in itself be counted as an advantage, since independence of scholars improves research objectivity. Moreover, there is no standard methodology in the field of triangulation and incorrectly combined methods do not fulfil their assumed function, which can pose some additional problems. Finally, one should also be aware that repeating a study involving several methods is less likely to happen than if a single method had been applied.


\subsection{Research validity}
\label{Research validity}
As mentioned above, the main strength of triangulation of methods lies in providing robust evidence of real language use, and it is a reliable method for verifying results.\footnote{\textcolor{black}{As already explained in Chapter \ref{Introduction with overview} under the expression ``real language use'' we understand observable language data as opposed to language data obtained by introspection.}}

In our study, ecological validity is supported in \textcolor{black}{three} ways. First, we retrieve fully uncontrolled material from web corpora, which guarantees observations from a fully natural environment, without any influence on language users from investigators. Secondly, the examples obtained from corpora are used as model sentences for acceptability experiment. \textcolor{black}{Finally, we conducted a pilot study where we asked native speakers to evaluate our target sentences. The results of their feedback were used to improve stimuli to sound as natural as possible.} This solution should ensure that constructed examples are not entirely artificial, and hence are likely to appear in real-life situations.\footnote{\textcolor{black}{For more information see Section \ref{Ecological validity of stimuli in our study}.}}

When talking about research validity, \citet{Brewer00} also distinguishes internal validity, sometimes called construct validity – ``the degree to which a study allows unambiguous causal inferences'', and external validity – ``the degree to which a study ensures that potential findings apply to settings and samples other than the ones being studied'' \citep{Brewer00}. These two types of validity rarely apply to a single study. This is because while for example words and sentences in corpora are not without their broader contexts, words and sentences in acceptability judgments are usually elicited in isolation \citep[3]{Myers17}. In our case, the findings obtained from laboratory experiment guarantee high internal validity, while structures retrieved from corpora support ecological and external validity.

\section{Empirical approach in the current study}
\label{Empirical approach in the current study}
\subsection{Chosen strategy}
\label{Chosen strategy}
The current work is language-use oriented and we follow the scheme intuition\slash theory – observation – experiment. Many theoretical claims concerning CLs in BCS are contradictory (see: Chapters \ref{Theoretical Aproaches to the Study of Clitics in BCS}, \ref{Clitics and variation in grammaticography and related work}, \ref{Approaches to clitic climbing}, and \ref{Constraints on clitic climbing in Czech compared to Bosnian, Croatian and Serbian (theory and observations)}). Therefore, in these chapters we first verify them against empirical data collected from corpora – our first source of observation. Since corpus data can be analysed quantitatively, some hypotheses can be also verified at this stage.\footnote{The quantitative methods we use are discussed in Chapter \ref{Introductory remarks to corpus studies on CC}.} This procedure is applied mainly in Part \ref{part3} of the book, which focuses on the understudied phenomenon of clitic climbing, but also in Part \ref{part2}, where we analyse the behaviour of CLs in spoken Bosnian. 

Nevertheless, the high level of ecological validity typical of corpora is also their drawback, as internal validity in large collections of spontaneously produced texts is quite low. Very often the influence of extralinguistic factors cannot be ruled out, e.g. due to the lack of information about the social background of the authors. Nevertheless, hypotheses formulated on the basis of corpus material can be further tested in acceptability judgment experiments where the level of control on particular factors can be adjusted. Additionally, corpora as recordings of natural language production can be nicely supplemented with experimental data such as acceptability judgment data because while they both provide evidence about syntax, the kind of evidence differs. While corpora reflect language production, acceptability data primarily reflect language comprehension \citep[3]{Myers17}.

\subsection{Corpus studies}
\subsubsection{Corpus linguistics}

Generally speaking, in the current work we understand corpus linguistics as a language-use oriented research approach which utilises collections of texts produced in a natural communicative situation, called corpora, and applies quantitative and qualitative analytical tools and techniques to them. ``Over the last few decades, corpus-linguistics methods have established themselves as among the most powerful and versatile tools to study language acquisition, processing, variation and change'' \citep[257]{GriesNewman13}. We decided to use corpus linguistics methods, as an alternative to intuitive acceptability judgments made by one person, only since they offer (more) objective, quantifiable, and replicable findings \citep[cf.][257]{GriesNewman13}. Contrary to what most works \citep{THG05} present, corpus linguistics has more to offer than a simple opportunity to extract authentic examples for the purpose of introspective research. In the following section we present the main approaches to corpus research and describe our own.

\subsubsection{Hybrid approach to corpus linguistics}

Investigations incorporating corpus linguistic methods are traditionally divided into corpus-driven and corpus-based. \citet[328]{Gries10} provides three typical features of the \textsc{corpus-driven} approach:

\begin{enumerate}
\item building theory from scratch as an aim, without any theoretical assumptions,
\item basing theories exclusively on corpus data,
\item (often) rejecting corpus annotation.
\end{enumerate}

Scholars treat these three elements differently, so corpus-driven is still a rather fuzzy term. In its most extreme form the corpus-driven approach allows only the assumption of word forms and requires a purely distributional analysis of the corpus in order to identify any linguistic units (\citealt[cf.][329]{Gries10}, \citealt[201]{Biber15}). Thus, we agree that ``\textit{truly} corpus-driven work seems a myth at best'' \citep[330]{Gries10}. In contrast, the \textsc{corpus-based} approach is often understood as the reverse of the corpus-driven approach in the sense that here the corpus is treated as a source of examples and possibly frequency information needed to confirm or disprove some existing theory or hypothesis \citep[cf.][15]{Meyer14}.  

So the question is: which type does our study belong to? The use of corpus material in the present work is versatile. On the one hand, we confront the existing theoretical claims with empirical evidence, indicate counterexamples, and test hypotheses, which brings us close to the corpus-based approach. As it does to \citet{Biber15}, corpus analysis offers us the perfect methodology for identifying the most frequent and most rare patterns in the given discourse variety, often counter to prior expectations. 

We are not the first to observe that expressions labelled ``ungrammatical'' by linguists have been found to be used by native speakers (\citealt[cf.][]{Sampson01}, \citealt{Stefanowitsch07}, \citealt{BresnanNikitina09}) or to be accepted by non-linguists (\citealt[cf.][]{WasowArnold05}, \citealt{Bresnan07}).

On the other hand, as we explained in Chapter \ref{Introduction with overview}, our study is rather data- than theory-oriented. We do not limit ourselves to examining known patterns: we also aim to explore the occurrences which have not yet appeared in theoretical approaches to CLs. \textcolor{black}{In that way, corpus-driven approach helps us, at least partially, to overcome the problem of false negatives (rejection of true hypothesis), the matter which is usually neglected in the studies dealing with the accuracy of introspective and experimental data \citep[cf.][611--612]{SprouseAlmeida12}.}
Furthermore, all the claims that we formulate are based on material from corpora and further tested by statistical methods and/or additionally verified in cautiously designed psycholinguistic experiment in order to achieve higher control of particular factors and to reject observations which occur due to error. In this sense, our study meets some criteria of a corpus-driven study. 

Hence, instead of drawing a sharp border between the two approaches, we are in favour of a \textsc{hybrid} \textsc{approach} \citep{Biber15} which on the one hand admits the validity of predefined grammatical categories and syntactic features (such as CLs and CC), but involves corpus-driven methods in the inductive analysis of corpora on the other. 

\subsubsection{Corpora as a source of authentic data}

As shown in Chapters \ref{Constraints on clitic climbing in Czech compared to Bosnian, Croatian and Serbian (theory and observations)}, \ref{A corpus-based study on CC in da constructions and the raising-control distinction (Serbian)}, and in \ref{A corpus-based study on clitic climbing in infinitive complements in relation to the raising-control dichotomy and diaphasic variation (Croatian)} on the one hand we encounter large disagreement among scholars concerning the possibility of CC in certain contexts, and on the other we see an absolute lack of empirical studies. For many studies on CLs in BCS the following statement applies: ``you imagine examples of language used in this or that situation and ask yourself whether they are grammatical or ungrammatical, natural or unnatural, appropriate or inappropriate'' \citep[25]{ClarkBangerter04}. In contrast, we believe that authentic data can help form and test hypotheses as well as settle ongoing disputes. Our first source is corpus data, which mainly fulfil the observatory function. We use corpora to provide counterexamples to theoretical claims. 

Since some of the syntactic constructions we wanted to investigate, such as CC out of \textit{da}\textsubscript{2}-complements, tend to have extremely low absolute numbers of occurrences, we decided to turn to large web corpora \{bs, hr, sr\}WaC.\footnote{For basic information on \textit{da}-complements see Section \ref{Types of complements} and for empirical data on CC out of \textit{da}\textsubscript{2}-complements see Chapter \ref{A corpus-based study on CC in da constructions and the raising-control distinction (Serbian)}.}\textsuperscript{,}\footnote{For a detailed description of corpora available for BCS and our reasons for choosing to work with those corpora over others, see Chapter \ref{Corpora for Bosnian, Croatian and Serbian}. For the queries used in our corpus studies see Chapter \ref{Introductory remarks to corpus studies on CC}.} Such corpora are collections of texts extracted from the world wide web and include many spontaneously produced, unedited texts, which gives prospects for valuable findings unlikely to be encountered in literary texts, often reviewed by editors with respect to some ``standard'' of language \citep[cf.][259]{GriesNewman13}.

\subsubsection{Limitations of corpus linguistics}
\label{Limitations of corpus linguistics}
We have to be aware that corpus linguistic methodology has its limitations arising mainly from the nature of the data with which it deals. First, one well-known drawback is no possibility of providing evidence of absence. In other words, the lack of occurrence of a certain structure in the corpus is not proof of its unacceptability, as the reason for it may be purely accidental. Additionally, while statistical tests may show that a given construction is improbable, they cannot give a reason for this improbability \citep[cf.][]{Stefanowitsch06}.
Secondly, corpora contain records of speech, and therefore all tests concern  language users’ performance but not their competence. Furthermore, it is hard to assess the acceptability of the occurring structures ad hoc. Corpora also include accidental forms (e.g. mispronunciations or typing/writing errors) which can be misinterpreted as rare but possible forms.
The usual assumption in big data is that the most frequent structures are the most grammatical while noise is rather infrequent \citep[9]{KilgarriffGrefenstette03}. Retrieving rare and complex structures is nevertheless challenging, and in the case of web corpora problems related to information retrieval accuracy measures – precision and recall – are impossible to overcome.\footnote{\textcolor{black}{For example in our study this is the case of CC out of object-control CTPs (see Section \ref{Methods} and Chapter \ref{Experimental study on constraints on clitic climbing out of infinitive complements}).}}
In order to gather more high-quality data we give preference to acceptability judgment tasks over elicitation of naturally occurring data through trigger questions in interview-based corpora, because we assume that the latter will still  not provide us with numerous occurrences of the relevant structure and will not include all the context we are interested in. Although interview-based corpora would provide more ecologically valid data, we decided to systematically collect data which would fulfil all conditions necessary for inferential statistical methods in order to be able to make more robust generalisations. Furthermore, acceptability judgment experiments as an empirical approach seem more appropriate to us since they enable us to generalise from many individual ratings. This provides more accurate answers to the research questions addressed than  uncounterbalanced interview data or data from a single linguist would.

\subsection{Psycholinguistic experiments}
\subsubsection{Types of psycholinguistic tasks}
\largerpage
\label{Types of psycholinguistic tasks}
Next to corpus data (i.e. observational data), other techniques of collecting empirical data are available, for instance psychological responses to linguistic stimuli. We can divide the many experimental tasks into \textsc{non-speeded} (non-chro\-no\-me\-tric) tasks where reaction or response times are not collected and analysed as data, and \textsc{speeded} (chronometric) \textsc{tasks} \citep[cf.][237]{DAH09}. While the former reflect only the final outcome of the psychological processes, the latter can reflect the time course of language processes \citep[3]{Myers17}. 

Reaction time was first introduced by \citet{Donders68}, whose main idea was that more complex cognitive tasks take more time to complete. Donders believed that cognitive operations are additive, i.e. that more complex tasks take longer because more cognitive operations are recruited. In accordance with this belief, he proposed the famous method of subtracting reaction times in a series of tasks that differed in only one cognitive operation, in order to determine the time taken by the additional cognitive operation. Although the original hypothesis on the additive nature of cognitive operations has been abandoned, the idea of reaction time as the indicator of cognitive load or processing cost has survived. Reaction time is one of the most frequently used behavioural measures in psychology and psycholinguistics \citep{Luce86}.
  
Non-chronometric  tasks include:\footnote{A detailed description of each task and some examples of the concrete experiments conducted can be found in \citet{DAH09}.}

\begin{enumerate}
	
 \item segmentation tasks,
 \item rating tasks,
 \item string manipulation tasks (or ``experimental word games''),
 \item manipulation of miniature artificial real language subsystems, 
 \item stimulus classification (or ``concept formation''),
 \item recall and recognition tasks \citep[240]{DAH09}. 
 
\end{enumerate}

 One of the most popular rating/scaling experiments used in syntax is the acceptability judgment task \citep[cf.][244]{DAH09}, which is used in order to indirectly access grammaticality.  

In \textcolor{black}{order to avoid problems related to obtaining linguistic data exclusively from informal acceptability judgments, which we discussed in sections above,} we decided to conduct what we call an acceptability judgment experiment with non-linguists. In the literature this method is also referred to by the terms well-formedness, nativeness, naturalness and grammaticality \citep[cf.][2]{Myers17}.\footnote{Wordlikeness is a term often used in morphology and lexical phonology research \citep[2]{Myers17}.} For the reasons given in the following subsection we use the term acceptability. 

\subsubsection{What exactly does an acceptability judgment test measure?}
\largerpage%long distance
Traditionally speakers’ reactions to sentences have been called ``grammaticality judgments'' \citep[27]{SchutzeSprouse13}, but in our view this term is misleading. Based on \citet[4, 11f]{Chomsky65} linguists generally agree that grammaticality and acceptability are two distinct concepts.\footnote{\citet{Chomsky65} clearly distinguishes between competence (grammar knowledge) and performance (a decision based on grammar knowledge). “We thus make a fundamental distinction between competence (the speaker-hearer’s knowledge of his language) and performance (the actual use of language in concrete situations). Only under the idealisation set forth in the preceding paragraph is performance a direct reflection of competence. In actual fact, it obviously could not directly reflect competence” \citep[4]{Chomsky65}. “Acceptability is a concept that belongs to the study of performance, whereas grammaticalness belongs to the study of competence” \citep[11]{Chomsky65}. “The notion “acceptable” is not to be confused with “grammatical”''  \citep[11]{Chomsky65}.} The former refers to whether a sentence conforms to the rules of grammar, while the latter, to the degree to which a sentence is judged by native speakers to be permissible in their language. On the one hand, sentences which are perfectly grammatical can be evaluated as unacceptable because they violate some prescriptive or pragmatic rules. On the other hand, sentences which are ungrammatical can be evaluated as acceptable depending on the informants’ ability to imagine necessary, though missing, context. Therefore, some scholars such as \citet[701f]{Featherston05} propose abandoning the mentioned difference between acceptability and grammaticality, and argue that grammaticality can be operationalised only in terms of acceptability (\citealt[cf.][674, 701f]{Featherston05}, \citealt[624]{Riemer09}). 
Following the latter approach, we can prove or falsify existing and potential syntactic theories, since the results of carefully constructed, relative acceptability judgments used as empirical data approximate grammaticality, which normally is not directly accessible, possibly closely (\citealt[51]{Newmeyer83}, \citealt[26]{Schutze16}, \citealt[402f]{Featherston07}). In applying this experimental method we must not forget that  informants’ judgments are not influenced only by grammatical, but also by extragrammatical factors. In order to avoid or neutralise the influence of the latter, researchers take various steps. For instance, they try to balance stimuli for length, lexical content, processing difficulty, plausibility, etc. as much as possible (see \citealt{Schutze16}, \citealt{Cowart97}, \citealt{Featherston05} for further discussion).\footnote{To balance does not necessarily mean to suppress those factors: as \citet[47]{Cowart97} puts it, they can be controlled for if they are uniformly spread across all the stimuli.}

\subsubsection{Different types of judgment tasks}
\label{Different types of judgment tasks}
Acceptability judgments involve explicitly asking speakers to ``judge'' whether a particular string of words or graphemes/phonemes is a possible utterance of their language \citep[28]{SchutzeSprouse13}. Acceptability judgments can be divided into two main categories: non-numerical or qualitative tasks, and numerical or quantitative tasks. While the former group includes yes-no and forced choice tasks, the latter group comprises the magnitude estimation task, Likert scale task, and the thermometer task, which have been designed to give us information about the size of the difference between the structures of interest \citep[cf.][33ff]{SchutzeSprouse13}. 

The acceptability of a sentence can be judged using the Likert scale task. Participants are given a numerical scale (usually from 1 to 5, from 1 to 7 or from $−3$ to $+3$) whose endpoints are labelled acceptable or unacceptable, and they are asked to rate each stimulus on the scale \citep[cf.][33]{SchutzeSprouse13}. In this kind of experiment, the researcher normally provides examples for the highest (ceiling) and lowest (floor) point of the scale, i.e. completely acceptable and completely unacceptable, which helps participants to take decisions during the experiment \citep[cf.][37]{SchutzeSprouse13}. 

In the magnitude estimation experiment a reference sentence (called standard) with an arbitrary value (called modulus) is presented to participants and they are asked to ascribe values to all other stimuli in comparison to the standard, so if the new stimulus is twice as good as the standard, it has to be assigned a number which is also twice as high as the modulus, etc. \citep[cf.][34]{SchutzeSprouse13}.\footnote{The other possibility is to give a reference stimulus and ask participants to assign a number to it themselves \citep[100]{Hoffmann13}.} In order to be able to express all their judgments relative to the standard stimulus, participants must have access to the standard sentence and its value (modulus) throughout the whole time of the experiment \citep[101]{Hoffmann13}.

\citet{Featherston08, Featherston09} proposed the thermometer task, which combines the intuitive nature of point scales with the sensitivity of the magnitude estimation task. In this kind of experiment, participants are presented with two reference sentences and their values (ceiling and floor of acceptability). Afterwards the values ascribed to the stimuli are plotted on a line relative to those two points.  

The fourth solution is to let participants evaluate stimuli on a binary scale: acceptable vs unacceptable. In these so-called yes-no tasks, it is important for participants to be exposed to polarised sentences; therefore, besides target sentences, they should get target-like incorrect sentences. Fillers have to be polarised as well, otherwise the participants will start to evaluate acceptable sentences as unacceptable.\footnote{Filler items are items (i.e. words or sentences) which are not related to the research question. Their main purpose is to reduce the chances of participants figuring out which sentence type is being tested, i.e. to avoid conscious response strategies \citep[39]{SchutzeSprouse13}.}

\largerpage
The fithth possibility is the forced-choice task in which participants are faced with two (or more) sentences, and they are asked to select the most (or the least), in their opinion, acceptable sentence \citep[cf.][31]{SchutzeSprouse13}.

While considering which type of acceptability judgment task to choose, we had to bear in mind the following advantages and disadvantages of each of them. For instance, the magnitude estimation task is more sensitive to fine contrasts between different types of structures and the results can be statistically evaluated with parametric tests \citep[cf.][8]{Dabrowska10}.\footnote{“Parametric tests involve statistical approximations and rely on the sampled data being distributed in a particular way” \citep[322]{Gries13}. “There are differences between the inferences licensed by parametric and non-parametric tests. For example, when all of the assumptions are met, parametric tests can be used to make inferences about population parameters from the samples in the experiment. Non-parametric tests, which do not assume random sampling, can only be used to make inferences about the sample(s) in the experiment itself” \citep[44]{SchutzeSprouse13}.} Furthermore, the magnitude estimation task allows participants to rate stimuli on their own scales and not on the scale provided by the researcher, i.e. artificial limitation of rating is avoided \citep[103]{Hoffmann13}. Compared to the Likert scale task, magnitude estimation is more time consuming and less intuitive. Namely, participants have to decide how many times better the stimulus is than the standard rather than deciding if a particular stimulus is closer to the ``good'' or ``bad'' end in the Likert scale (cf. \citealt[8]{Dabrowska10}, \citealt[33, 35]{SchutzeSprouse13}). Additional argument against such a time-consuming task comes from recent studies which showed that even in the case of magnitude estimation which should allow insight into fine differences between various kinds of structures, participants use a small set of numbers repeatedly instead of rating every stimulus differently. Thus it seems that they treat the magnitude estimation task as a type of Likert scale task \citep[cf.][34f]{SchutzeSprouse13}. Although some researchers object to the use of parametric tests in the case of Likert scale tasks, others argue that parametric tests are quite robust and that violations of the intervalness assumption have relatively little impact on the results. Thus, the use of parametric tests with data obtained using the Likert scale has become standard (cf. \citealt{Blaikie03,Pell05}).\footnote{Furthermore, z-score transformation has been suggested as a possible solution, since it allows each participant’s response to be expressed on a standardised scale \citep[cf.][34, 43]{SchutzeSprouse13}. } Yes-no and forced-choice tasks were designed to qualitatively compare  at least two conditions, but they do not catch the fine-grained differences between acceptable and borderline structures. On the other hand, they allow both the participants and the researchers to work quickly, which is important in the case of complex experiment design and shortage of participants \citep[cf.][31ff]{SchutzeSprouse13}.\footnote{It seems that forced-choice tasks are much easier to develop and later on, conduct as an experiment since they do not need fillers \citep[cf.][32]{SchutzeSprouse13}.}  
Finally, it is worth pointing out that since in all judgment tasks participants are asked to do the same cognitive task, the data yielded by different kinds of tasks are likely to be very similar, especially in the case of large sample size (e.g. twenty-five participants or more), so the choice of task is relatively inconsequential \citep[cf.][36]{SchutzeSprouse13}.

\subsubsection{Acceptability thresholds for different types of judgment tasks}
\label{Acceptability thresholds for different types of judgment tasks}
First of all, we need to state that acceptability is not a categorical, but a graded phenomenon \citep{LCL17}. Data from acceptability tasks with various modes of presentation converge to form such a conclusion. If speakers are presented with an acceptability judgment scale, their average ratings will be distributed across the scale values. If the speakers are presented with a binary acceptability judgment (yes-no), a single speaker will always either accept or reject a sentence, but the proportion of speakers who accepted (or rejected) a sentence will differ. 

Having in mind the continuous nature of acceptability, we face the problem of interpreting acceptability data. Extreme values are clearly easily interpreted as acceptable and unacceptable. However, the problem remains of how to interpret the middle ground. To the best of our knowledge, there is no established linguistic strategy that we could rely upon. Therefore, we look at the practices that are firmly established in empirical psychology, namely the measurement of sensation.

In psychology, if the task is to detect a stimulus (so-called detection task; e.g. “press yes if you hear something”), a stimulus is at the threshold value if it is detected in 50\% of the trials \citep{Weber34, Fechner60, Smith08, JWH09, Goldstein10}. If the task is to choose between two alternatives (so-called “two forced choice task”, where participants are presented with two alternatives -- two stimuli and the task to pick one), 75\% is taken as the threshold, as in this case 50\% denotes guessing. Given that binary acceptability judgments cannot be treated as two forced choice tasks, as only one stimulus is presented at a time, 50\% acceptance should be interpreted as the threshold. The definition of the threshold which applies is that it is the smallest intensity of stimulation for which 50\% of participants declare that they were able to detect it \citep{Smith08, Goldstein10}.

With all this in mind we decided to adopt a 50\% acceptance rate (i.e. acceptance by 50\% of the speakers) as the threshold of acceptability. It is important to note that we do acknowledge the fact that acceptance is a graded phenomenon (as demonstrated by \cite{LCL17}) and we do not imply that there is a strict line between acceptable and unacceptable sentences. We intend to use this threshold only for the purposes of orienting.

\subsubsection{Pros and cons of judgment data}
\label{Pros and cons of judgment data}
According to the literature, judgment data can provide \textsc{negative} \textsc{data} and data which cannot be collected otherwise, i.e. on infrequent structures that fail to appear even in a very large corpus (such as web corpus)  (\citealt[117]{Hoffmann13}, \citealt[92]{KrugSell13}, \citealt[280]{Rosenbach13}, \citealt[29]{SchutzeSprouse13}). In other words, introspection experiments such as acceptability judgments allow rare phenomena to be investigated and negative data to be obtained \citep[100]{Hoffmann13}. Moreover, judgment data can be used whenever there is no corresponding corpus at all or to complement corpus data \citep[117]{Hoffmann13}. Furthermore, if we compare judgment data with spontaneous usage data, we should emphasise that the latter include some proportion of production errors (slips of the tongue/pen/keyboard) which can later be misinterpreted as evidence for rare structures \citep[29]{SchutzeSprouse13}. Another advantage is that researchers can influence and control the kind and amount of data which is being collected and later evaluate it relatively quickly \citep[92]{KrugSell13}. Additionally, we should underline that the accumulation of many informants’ judgments produces supra-individual, less erratic intuition-based ratings, i.e. this kind of introspective data is claimed to be objective (\citealt[117]{Hoffmann13}, \citealt[92]{KrugSell13}). 

However, there are also disadvantages to such an approach; for instance, experiment and stimuli preparation can be time-consuming as experiments have to be carefully designed (\citealt[117]{Hoffmann13}, \citealt[92]{KrugSell13}). Furthermore, researchers do not collect natural speech/writing, and stimuli are not usually taken from spontaneously produced language material \citep[92]{KrugSell13}. Acceptability judgments rely on informants’ ratings and intuition and are not a direct investigation of actual language use; moreover, generalisations are limited to the specific conditions (combinations of observed factors) which were tested (\citealt[92]{KrugSell13}, \citealt[282]{Rosenbach13}).   

\subsubsection{Outlook: production experiments}
Finally, we would like to note that ``in an ideal world'' without human and funding restrictions we would have obtained naturalistic production data, which are versatile and have high ecological validity, like the data from WaC corpora.\footnote{For more information on BCS WaC corpora see Section \ref{WAC}.} Production experiments in the narrow sense as standardised procedures are an ideal case for researchers who want to be able to systematically manipulate some variables and control for the effect of others in order to collect data suitable for quantitative analysis \citep[cf.][11]{Eisenbeiss10}. Such experiments can be non-speeded or speeded. Widely used tasks include:
\begin{enumerate}
	
 \item elicited imitation experiments -- participants are asked to imitate\slash repeat\slash paraphrase spoken sentences,
 \item elicited production experiments -- involve prompts to produce target forms,
 \item speeded production experiments -- participants must produce target forms,
 \item syntactic priming -- it is observed whether participants repeat syntactic structures of unrelated utterances,
 \item the input/feedback experiment -- participants get input or both input and feedback on correct form.\footnote{For more details and examples of tasks used in each of the mentioned methods including  descriptions of procedures see \citet{Eisenbeiss10}.} 
\end{enumerate}

 One of the most used elicited production experiments in syntactic research is a paper-pencil task in which participants are asked to fill in gaps with target items. We will leave production tests for future research.	

\subsection{Experiment chosen for our study}
\label{Experiments chosen for our study}
Since the magnitude estimation task can show fine-grained differences between the tested items and conditions, it is often considered the most appropriate measure of acceptability. Because of the assumption that grammaticality is gradient, it seems important to measure acceptability either with tasks like magnitude estimation or at least with Likert scales with many levels of measurement which would allow insights into this gradience. 

However, \citet[253]{WeskottFanselow11} accurately point out that a certain degree of gradience may also be captured with binary yes-no scales. They emphasise that if each experimental condition is tested with at least four items, even the resulting mean values of the binary measures exhibit variability to some degree: a mean of four binary judgments can take on five different possible values (0, 0.25, 0.50, 0.75 and 1) \citep[253]{WeskottFanselow11}. Thus, it seems that even fixed-scale judgments with a small number of points like binary scales can, depending on the number of observations gathered, exhibit a certain range of variability, and are not per se less suited to represent gradient acceptability than for instance magnitude estimation \citep[253]{WeskottFanselow11}.

Since several studies (e.g. \citealt{BaderHaussler10}, \citealt{WeskottFanselow11}, \citealt{FGMB12}) showed that acceptability judgment tasks with different response types give very similar results and since binary scales can capture gradience in a similar way to numerical scales, we decided to use speeded yes-no acceptability judgment. 

As we showed in the previous subsections, the discussion of the best acceptability judgment task boils down to a trade-off between ease of application by the participants, statistical power, and time consumed by preparation and data processing. In respect of this, although the yes-no task is more demanding for the researcher in terms of data collection (as it requires more participants and more items per condition), it was our task of choice due to its advantages from the perspective of participants and the methodological advantages related to eliminating strategic responding. We refer to the ease with which participants can grasp the basic idea behind the task, i.e. what is expected from them. By informing the participants that the time allowed for each trial, although more than enough for their decision, will nevertheless be limited, we additionally strengthen the explicit instruction to reply intuitively, without overthinking. This way we also reduce the possibility of the participants building some kind of strategy while responding. In addition to being less likely to involve overthinking of each response, making a simple yes-no decision is also less time consuming, thus allowing a larger number of responses to be collected during the same total time. Although the need to collect data on more items per condition (which is more strongly recommended for the yes-no task compared to some other tasks) may seem a disadvantage of this task, it can also be viewed as an advantage, or even as an obligation. As \citet{Clark73} noticed, the peculiarity of psycholinguistic research is double-sampling. While sampling from the population of speakers, researchers also sample from the population of language items. In other words, the researchers’ aim is to be able to generalise their conclusions to all speakers, but also to all items of a chosen type (as opposed to relating conclusions only to the specific examples presented in the experiment). Therefore, as well as including multiple speakers in the experiment, one must also include multiple items per condition. 

Finally, the yes-no task (as a simple form of binary choice for participants) enables us to record response time, i.e. the time taken by participants to categorise each item as acceptable or as unacceptable. The long history of empirical research in psychology has demonstrated that complex tasks incur longer response latencies. In terms of language research – items that are rarely encountered, unusual, or complex take longer to process. Therefore, we expect items that are accepted by more participants to also elicit shorter response latencies, and vice versa (those that are rarely accepted should elicit longer reaction time). Having two measures (acceptance and response latency) for each token of interest we obtain two indicators of the same underlying speakers’ disposition, thus increasing the reliability of our research. Also, it should be noted that whereas participants’ responses could potentially be affected by response strategies, it is hard to imagine how speakers could build a strategy to control their processing time. 
