\addchap{\lsAbbreviationsTitle{} and conventions} 

\begin{tabbing}
	MMMMMM \= Antecedent\kill
	ANTE	\> Antecedent  \\
	BERT	\> Bidirectional Encoder Representations from Transformers  \\
	BF	\> Bayes Factor  \\
	BiGRU	\> Bi-directional Gated Recurrent Unit neural network  \\
	CAtt	\> Concatenative Attention mechanism \\
	Cb \> Backward-looking Center	  \\
	Cf	\> Forward-looking Center \\
	CL	\> Computational Linguistics  \\
	Cp	\> Preferred Center \\
	CT	\> Centering Theory  \\
	CRF \> Conditional Random Field \\
	E2E \> End-to-end \\
	GloVe \> Global Vectors \\
	HierAtt \> Hierarchical Attention mechanism \\
	IA \> The Incremental Algorithm \\
	LSTM \> Long Short-Term Memory \\
	ML \> Machine Learning \\
	MLP \> Multilayer Perceptron \\
	NeuralRFS \> Neural Referential Form Selection. \\
	NLG \> Natural Language Generation \\
	NLP \> Natural Language Processing \\
	NeuralRFS \> Neural Referential Form Selection. \\
	NLG \> Natural Language Generation \\
	NLP \> Natural Language Processing \\
	NP \> Noun Phrase \\
	ONF \> OntoNotes Normal File \\
	OOB \> Out-of-bag \\
	POS \> Part-of-speech \\
	PT \> Prominence Theory\\
	RCS \> Referential Content Selection \\
	RDF \> Resource Description Framework \\
	RE \> Referring Expression \\
	REF \> Referent \\
	REG \> Referring Expression Generation \\
	ReLU \> Rectified Linear Unit \\
	RF \> Referential Form (Referring Expression Form) \\
	RFI \> Random Forest Importance \\
	RFS \> Referential Form Selection \\
	RNN \> Recurrent Neural Network \\
	SHAP \> SHapley Additive exPlanations \\
	Seq2Sep \> Sequence-to-sequence decoder \\
	SFS \> Sequential Forward Search \\
	SOTA \> State-of-the-art \\
	WSJ \> Wall Street Journal
\end{tabbing}


\noindent This book is written in American English. An exception is the prefix \term{non-} which is joined to a word by means of a hyphen, e.g., \term{non-pronominal}. \noindent Chapters \5 and \7 are written in first-person plural (\intext{We}). \chapref{chap8} is a mix of first-person singular and plural (\intext{I} and \intext{We}). Other chapters are mostly written in first-person singular. In what follows, I give an overview of the writing conventions I have followed throughout the book.

\begin{description}
	\item[In-text examples:] In-text examples appear in \textit{italics}, such as \intext{here is an example.}
 \item[Translations and glosses:] Translations and glosses are surrounded by single quotation marks.
	
	\item[Terms:] New terms are introduced in \textit{italics} for the first time. Further use of these terms is in normal type. New terms include linguistic terms (e.g., \term{coreferential chain}), computational terms (e.g., \term{Natural Language Generation}), and computational methods used (e.g., \term{sequential forward search}).
	
	\item[Name of models and corpora:] The name of models and corpora, either from the literature or defined in this book, are in \textsc{small caps} (e.g.,\modname{webnlg}).
	
	\item[Values:] The different values that a linguistic category can take are written in the \texttt{typescript} mode (e.g., a 3-way classification consists of three values: \val{pronoun}, \val{proper name}, and \val{description}). The category itself is written mostly in normal font (e.g., grammatical role, animacy). If a value is mentioned in the text flow, it is also written in normal font. In the following example, the values \val{proper name} and \val{human} are written in normal font: we use a proper name to refer to a human referent for the first time.
	
	\item[Symbols:] The symbols used to represent a category or a feature are in the \texttt{typescript} mode. For instance, in tables and graphs, grammatical role is presented as \texttt{gm}.
	
\end{description}



