\chapter{Psycholinguistics and natural language processing} \label{ch:psycholinguistics and cl}

\section{The connection with psycholinguistics} \label{sec:psycholinguistics}

The foregoing analysis presents the broad framework for (locutionary) communication. Psycholinguists study the actual processes of utterance production and comprehension. So the Communication Game $\Gamma_u$ has to be specialized and refined in various ways to fit with their experimental data. Fortunately, the basic approach of Equilibrium Semantics is largely compatible with the findings reported in, for example, the textbook by \citet[Chapters~5--8]{fsc:fp} and elsewhere. 

As confirmed by \citet{swinney:ladsc,os:aladsc}; and \citet{kawamoto:ndrla}, multiple senses of ambiguous lexical items are activated and disambigu\-ated exactly as dictated by games of partial information. \citet{rgmw:mssa} found that homonyms, that is, words like \Expression{bank} with unrelated conventional meanings take more time to process than polysemes, words like \Expression{eye} or \Expression{school} with related senses. \citet{plm:rp} and \citet{frisson:sulp} suggest that the conventional meanings of polysemous words are likely to be relatively abstract and impoverished underspecified cores relative to the full meanings they are given during processing. This implies some further alterations to our conception of conventional meaning that I discuss in \chapref{ch:modulation}. These findings are based on ingenious experiments rather than mathematical models and so this provides an opportunity to connect the latter from Equilibrium Semantics with the former. This kind of work cannot be pursued here but I say a little about one refinement to my model to illustrate the point.

As I said in \sectref{sec:solving locutionary global games}, the meaning and parse vectors $x$ and $y$ are parts of the \emph{content vector} $z = (x, y)$ with $2n$ components where $n$ is the number of words in the sentence. This vector ranges over all the possible lexical meanings and parses of $\varphi$ uttered in $u$. I repeat Equation~\ref{eq:simple} from \sectref{sec:maintheorem} below for the reader's convenience.

\[ z^\star = \argmax_{z} P(z_k \cond z_{-k}; u) = \argmax_{z} P(z; u), \quad k = 1, \dots, 2n \]

\noindent This can be broken down into two symmetric equations, one for meanings and one for parses, as follows:
\begin{equation}
x^{\star} = \argmax_{x,y} P(x_i \cond x_{-i}, y_i, y_{-i}; u) \quad i = 1, \dots, n \label{eq:simple1}
\end{equation}
\begin{equation}
y^{\star} = \argmax_{x,y} P(y_i \cond x_i, x_{-i}, y_{-i}; u) \quad i = 1, \dots, n \label{eq:simple2}
\end{equation}

\noindent In real-time computations, the meanings and parses of earlier words can affect those of later words but not vice versa unless backtracking is required owing to a garden-path sentence or similar problem. So the symmetric equations above can be modified to reflect this temporality.
\begin{equation}
x^{\star} = \argmax_{x,y} P(x_i \cond x_1, x_2, \ldots, x_{i-1}, y_1, y_2, \ldots, y_{i-1}, y_i; u) \quad i = 1, \dots, n \label{eq:simpler1}
\end{equation}
\begin{equation}
y^{\star} = \argmax_{x,y} P(y_i \cond x_1, x_2, \ldots, x_{i-1}, x_i, y_1, y_2, \ldots, y_{i-1}; u) \quad i = 1, \dots, n \label{eq:simpler2}
\end{equation}

\noindent Equations~\ref{eq:simpler1} and \ref{eq:simpler2} capture some of the constraints on real-time sequential processing. These can be further refined to reflect different experimental results (e.g.\ the fact that the comprehension of the underspecified core sense of a polysemous word occurs instantaneously but the full meaning is realized only at the end of the sentence, if at all -- \citealt{frisson:sulp}).

At a more general level, it is far from clear from the data whether the various games themselves are played or whether just equations like the ones above involving probability comparisons are solved directly. Secondly, I have developed the broadest construal of Generation Games as including a model of the addressee and the various partial information games. This is likely to require some modifications. The Content Selection Game appears to be completely absent in psycholinguistic studies because the experiments start with an already selected content to be conveyed. It is also unclear whether and how meanings and parses interact in actual processing especially when we take account of the different sites in the brain where such activity might occur. For example, it may be that the utterance situation plays a greater role in the determination of optimal meanings than in the determination of optimal parses. The same observation also applies to phonetic processing. In other words, while the abstract framework of Equilibrium Linguistics makes possible the interdependence of phonetics, syntax, and semantics in relation to an utterance situation -- which may be useful in building artificial agents -- the psycholinguistic evidence may suggest that these three types of content are not so closely coupled. If this turns out to be so, then for psycholinguistic purposes a slightly less integrated framework can be devised keeping its basic principles intact.

Such observations and others suggest the possibility that an entire suite of psycholinguistic and neurolinguistic experiments may be conducted in light of Equilibrium Linguistics and there is much scope for fruitful interactions between the two. This could lead to one kind of unification of the separate fields -- psycholinguistics and semantics -- of the science of language mentioned in \sectref{sec:equilibrium semantics} and at the end of \chapref{ch:romantic tradition}. Current work in semantics is so remote from psycholinguistic practices that there is no easy way to bring them together. It is to the credit of Equilibrium Linguistics that its results are both more general and more precise than what exists in semantics and are therefore more directly testable.


\section{The connection with natural language processing} \label{sec:cl}

Not only does my framework connect with psycholinguistics and neurolinguistics, it also offers multiple bridges to computational linguistics and natural language processing. Artificial intelligence involves different kinds of constraints as compared with real-time human processing and is generally divided into logical,  statistical, and hybrid approaches, although most contemporary methods involve some use of probabilities.

\citet{hobbs:anlu} provides an overview of logical techniques involving abduction\is{abduction} in natural language understanding. Recall from footnote~\ref{foot:abduction} in \sectref{sec:communication as rational activity} that in abduction we conclude from an observable $Q$ and a general principle $P \Rightarrow Q$ that $P$ must be the underlying reason that $Q$ is true. We assume $P$ because it explains $Q$. In practice, there may be many such possible $P$'s, some contradictory with others, and therefore any method of abduction must include a method for evaluating and choosing among alternative explanations. This provides an inference to the \emph{best} explanation. Indeed, as I show now, the choice-based approach of Equilibrium Semantics is just a species of abduction in which the choice structure is far more sophisticated and detailed than that provided by Hobbs or in the literature.

Consider the game shown earlier in Figures~\ref{fig:semantic lexical game g1} and \ref{fig:semantic lexical game g1 again} and repeated for convenience in Figure~\ref{fig:semantic lexical game g1 again again}. Recall that $\varphi_1 = \Expression{Bill}$, $\sigma_1 = \emph{Bill Smith}$, and $\sigma'_1 = \emph{Bill Jones}$.

\begin{figure}[htbp] 
\input{figures/pix4varphi1algebraictruncated.tex}
\caption{Semantic lexical game $g_1$}
\label{fig:semantic lexical game g1 again again}
\end{figure}

The speaker is conveying $\sigma_1$ in $s_1$ and $\sigma'_1$ in $s_{1'}$. Both facts imply that $\cal A$ utters $\varphi_1$. So if we identify $P$ with \emph{$\cal A$ is conveying $\sigma_1$} and $P'$ with \emph{$\cal A$ is conveying $\sigma'_1$} and, further, if we identify $Q$ with \emph{$\cal A$ utters $\varphi_1$}, we can write $P \Rightarrow Q$ and $P' \Rightarrow Q$. The pure abductive task is then to infer which of $P$ and $P'$ best explains the observable $Q$. But the game $g_1$ in Figure~\ref{fig:semantic lexical game g1 again again} resolves precisely this question \emph{in conjunction with} the other partial information games associated with the whole utterance. As we have seen, this complex strategic inference results in the Fundamental Equation. Thus, strategic inference is a more refined form of abduction.

Those pursuing purely logical approaches do not seem to realize that their set\-ups miss a great deal of structure that is present in the problem. This is part of the reason they tend to be rather brittle and do not scale well in real natural language understanding applications. This lack cannot be rectified simply by adding in a few costs here and there as Hobbs attempts to do. A strategic inference, on the other hand, is a very tightly knit structure containing all kinds of relations among the various bits of information in the problem. That is why we get something as elegant and powerful as Equation~\ref{eq:simple}.

Incidentally, these observations are orthogonal to the issue of how so-called real world knowledge is best represented as I also said in \sectref{sec:solving locutionary global games}. Both pure abduction and more refined strategic inference require it as any full-fledged approach would. Some of it may be present in the encyclopedic entries for lexical items and some of it in some kind of knowledge base. It would enter the computation of meaning both through conventional (lexical) meanings and through the prior probabilities in the locutionary global game. But, importantly, the need for explicit representation of some of it would be circumvented by using corpus-based probabilities for different alternatives. But it cannot be avoided altogether. Consider the following example:

\ea The weight went through the wall because it was made of iron.\z

\noindent Here, some world knowledge has to be used to set the conditional probabilities for the two possible referents of the pronoun. The probability for \emph{weight} would presumably be higher than for \emph{wall}. We also used such world knowledge in the earlier examples, including the simple sentence \Expression{Bill ran} (e.g.\ knowing that politicians run in elections). Humans have ready access to it so it can be assumed to be available. For the sentence above, it is enough to say that most human interlocutors would have the background knowledge that iron would go through many ordinary materials and so the possible referent \emph{weight} has a higher probability. This is really all that is required for a semantic theory for humans in my opinion. It is not a gap in the theory to say we cannot assign higher or lower probabilities without such background assumptions. Artificial agents, on the other hand, do not have such knowledge so it has to be made explicit. This is best done in the context of actually building such agents. In any case, such orthogonal matters cannot be addressed here but suffice it to say that Equilibrium Linguistics offers a hybrid approach to the problem.

Statistical natural language processing \is{statistical NLP|(} has exploded in the last two decades and the textbooks by \citet{ms:fsnlp} and \citet{jm:slp, jm:slp2} have been quickly outpaced by ongoing work.

Perhaps the key connection between Equilibrium Linguistics and contemporary approaches in statistical NLP is that they both see the resolution of ambiguity broadly construed as the common feature of many problems that arise in communication and other tasks. \citet[Section~1.2]{jm:slp2} make this observation up front and it is something I have emphasized in all my work. Ambiguity is not just lexical or structural or phonetic but occurs wherever there are multiple possible contents that can be phonetic, syntactic, or semantic, the last category of which includes all of pragmatics and discourse.

In my view, and as \citet[592]{jm:slp2} also say, this key problem remains unsolved despite many advances in computational lexical semantics and in parsing. Many models such as Naive Bayes, Hidden Markov Models, Maximum Entropy Models, Conditional Random Fields, and others have been developed to address a variety of tasks.\footnote{See \citet{r:hmm,sm:crf}; \citet[Chapters~7, 9]{ms:fsnlp}; and \citet[Chapters~5, 6, 20]{jm:slp2}.} 

The results of \sectref{sec:solving locutionary global games} show that Equilibrium Linguistics can potentially offer a more satisfying theoretical foundation for such statistical models by grounding them in rational human agency in a way that is philosophically sound and empirically adequate and can also offer more general computational techniques. All the models listed in the previous paragraph make the basic assumption that communication is a stochastic process with no further constraints. The Communication Games developed in this book capture the further constraint that communication is more or less \emph{rational}. This brings in more cutting power and enables sharper conclusions to be drawn as Theorem~\ref{thm:simple equation}
and the more general Theorem~\ref{thm:compatibility with payoffs} show.

For example, it is easy to see that Equation~\ref{eq:simple} is more general than the Naive Bayes equation for word sense disambiguation:
\begin{eqnarray*}
s' & = & \argmax_{s_k} P(s_k \cond c) \\
& = & \argmax_{s_k} [\log P(s_k) + \sum_{v_j \in c} \log P(v_j \cond s_k)]
\end{eqnarray*}
\noindent where the $s_k$ are the word senses, the $v_j$ are the words in the context $c$, and the $P$ represents probability.\footnote{See \citet[Section~7.2.1]{ms:fsnlp} for more details.} The key generalization is that the conditioning (and conditioned) variables in my equation are the ``senses'' and parses themselves, as opposed to words, because of the circular and fixed point nature of my conception.

While Hidden Markov Models are also not sufficiently general as they rely essentially on Bayesian inference, Maximum Entropy Markov Models and Conditional Random Fields \emph{are} sufficiently general and do allow in principle for the kind of fixed point inference that occurs in Equation~\ref{eq:simple}. All these models involve manipulations of conditional probabilities and the latter two make it possible to condition on a wide range of features including the ones required by Equation~\ref{eq:simple}. But because these models are not based on rationality, they cannot derive Equation~\ref{eq:simple} itself and cannot conclude that the probability distributions involved in communication are \emph{compatible} distributions. In practice, moreover, semantic and syntactic (and phonetic) contents are seldom thrown in together as they are in Equilibrium Linguistics so even though the latter two models \emph{could} include the relevant conditioning variables, they almost never do.

To illustrate how bringing in rationality can generalize such purely probabilistic considerations, consider the problem of classifying a piece of text as spam or not spam.\is{text classification}\is{text classification!spam} This example also shows, incidentally, how the tools of Equilibrium Linguistics can be extended to related tasks such as text classification as mentioned earlier in \sectref{sec:generation game}. A rudimentary version of the spam identification task can be modeled by the simple game shown in Figure~\ref{fig:text classification game}. Being spam or not spam is just a more abstract kind of content of an utterance.

\begin{figure}[h] 
\input{figures/pix4textclassification.tex}
\caption{Text classification game $g_{tc}$}
\label{fig:text classification game}
\end{figure}

Clearly, the Nash solution involves comparing expected values of payoffs\linebreak rather than pure probabilities. By adjusting the $\cal B$ payoffs we can get different, more fine-grained results. For example, ordinarily we would want false positives (i.e.\ classifying some text as spam when it is not spam) to be penalized more heavily. If it is assumed that the sender in $s$ is spamming and in $s'$ is not spamming then such a heavier penalty can be realized by making $c'_{\cal B}$ sufficiently low, that is, by making the cost of interpreting a text that is not spam as spam high. If, instead, we were doing sentiment analysis,\is{text classification!sentiment analysis} a false negative, that is, a negative sentiment in $s$ interpreted as positive may be penalized more heavily by making $c_{\cal B}$ sufficiently low. And so on.  Needless to say, this only scratches the surface of the text classification problem but it shows how Equilibrium Linguistics offers additional degrees of freedom. This flexibility carries over to the much harder task of inferring the content of communication as well, as made clear by Figure~\ref{fig:scaled compact form 2} and Theorem~\ref{thm:compatibility with payoffs}, because it is also just a classification problem. It can also help in natural language generation and in building conversational agents and in other tasks such as machine translation (see \chapref{ch:translation}) and information extraction.

I hope I have convinced the reader that Equilibrium Linguistics has something to offer to statistical NLP. Its main contribution is its comprehensive definition of the problem and its better grounded and more general solution. I believe it is the closest any theory in  semantics has come to computational accounts.

As one would expect, the illumination goes in the other direction from statistical NLP to Equilibrium Linguistics as well. Recall from \sectref{sec:generation game} where the Flow Constraint and partial information games were first discussed that the probabilities $\rho$ were taken to be the prior conditional probabilities that $\cal A$ was conveying a certain content \emph{given} that he was conveying the other meanings and parse trees of the rest of the sentence in the utterance situation $u$. This assignment was based on the empirical observation that the meanings and parses of words are interdependent in a rational utterance. As statistical NLP shows, such conditioning variables can be thought of very generally as a wide variety of \emph{features}. For example, if the word \Expression{Brown} is capitalized in a written sentence, it helps the reader to see that it is likely to be a name rather than an adjective. So features like capitalization can further aid disambiguation. In human understanding, too, such features can supplement the primary features of semantic, syntactic, and phonetic contents. All that is required is to augment the conditioning variables in the prior probabilities by such extended features and then carry through the game-theoretic analysis as before.

This shows that if the approaches of Equilibrium Linguistics and statistical NLP are combined then both can be enriched, the former by adding more features to the list of conditioning variables, and the latter by bringing in both rationality as well as a more satisfying and foundational basis for the results.\is{statistical NLP|)}\\\\
I believe \partref{part:III} gives a more or less complete solution to the problem of locutionary communication that is, as advertised in \sectref{sec:equilibrium semantics}, philosophically sound, mathematically solid, computationally tractable, and empirically ade-\linebreak quate. The Setting Game, Content Selection Game, Generation Game, and Interpretation Game that constitute a (locutionary) Communication Game are developed in detail. So are the four Constraints: Phonetic, Syntactic, Semantic, and Flow. The solution I offer shows, moreover, that partial information games are universal in the algebraic sense discussed in \sectref{sec:universality}. Frege's two historic principles of compositionality and context are both reconciled and transcended. I develop a theory of \isi{vagueness} and address the sorites, relate \isi{vagueness} to essentially contested concepts, and model vague communication. Finally, as promised in \sectref{sec:equilibrium semantics}, I connect arguments in philosophy, linguistics, psychology, and computer science, unifying them through the framework of Equilibrium Linguistics.

In the next Part, I consider illocutionary communication and explore the wider varieties of meaning.
