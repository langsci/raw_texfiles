\chapter{An example with syntactic ambiguity} \label{ch:syntactic ambiguity}

\section{The example}

In the simple example I have just finished analyzing, there was no syntactic ambiguity and so the syntactic games of partial information were all trivial. In order to fully justify Theorem~\ref{thm:simple equation} in \sectref{sec:solving locutionary global games}, I now consider an example with both semantic and syntactic ambiguity. I will focus exclusively on the locutionary global game $LG_u$ in the overall Communication Game $\Gamma_u = (SG_u, CSG_u, GG_u, UG_u, G_u)$. 

%Since it is a part of both $GG_u$ and $UG_u$, these larger games will implicitly figure in what follows.

Suppose $\cal A$ is a reporter for the \emph{New York Times} and writes the headline ``Fed raises interest'' and $\cal B$ is a reader who reads the headline the next morning. Here $\varphi$ is the whole sentence and $\varphi_i$ its $i$th word. The situation $u$ includes the general goings-on of the week and also related financial news.\footnote{I will be using the same symbols as before to avoid multiplying symbols unnecessarily.}

Intuitively, there are three possible meanings of this utterance, of which I will consider two:

\ea \itshape the Federal Reserve augments the interest rate \z
\ea \itshape the Federal Reserve's increments arouse curiosity \z

\noindent The second meaning seems improbable but is nevertheless a real possibility. The third possible meaning -- \emph{the Federal Reserve augments curiosity} -- will be ignored here to keep things simple. If it is assumed as part of $u$ that there have been no recent increases in the interest rate, the intended meaning of the reporter's utterance would be the first one above. As before, the utterance and its interpretation involve four constraints:
\begin{itemize}
\item Phonetic Constraint
\item Syntactic Constraint
\item Semantic Constraint
\item Flow Constraint
\end{itemize}
In general, the Phonetic Constraint generates all the possible words associated with the speech wave corresponding to $\varphi$ in $u$, the Syntactic Constraint generates all the possible parse trees of $\varphi$ in $u$, and the Semantic Constraint generates all the possible meanings associated with $\varphi$ in $u$. The Flow Constraint then disambiguates all these ambiguities simultaneously and identifies the equilibrium or intended content, that is, the words uttered, their parse, and their meaning.

As the example involves a written sentence, the Phonetic Constraint can be omitted. There would be a corresponding Graphic Constraint that involves recognizing the characters, but for our purposes it can just be assumed that the words in the sentence $\varphi$ are immediately available to $\cal B$.


\subsection{The Syntactic Constraint}\largerpage

The two possible parses of the whole sentence generated by an appropriate background grammar $G$ or algebraic system of trees $({\cal T}, \star)$ are:\footnote{The grammar and algebraic system are naturally different from the ones in Sections~\ref{sec:algebraic system of trees} and \ref{sec:generation game}. Also this sentence has a syntactic ambiguity at the lexical level itself so we do not need recourse to the feature-based grammmars of \sectref{sec:complication}.}

\begin{itemize}

\item $[_{\mathrm{S}}[_{\mathrm{NP}}[_{\mathrm{N}}\, \mathrm{Fed}]][_{\mathrm{VP}}[_{\mathrm{V}}\,\mathrm{raises}][_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{interest}]]]]$

\item $[_{\mathrm{S}}[_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{Fed}][_{\mathrm{N}}\,\mathrm{raises}]][_{\mathrm{VP}}[_{\mathrm{V}}\,\mathrm{interest}]]]$

\end{itemize}

\noindent It is the first parse above that is optimal as it corresponds to the first meaning above. Thus, \Expression{Fed} has two trees, \Expression{raises} has two trees, and \Expression{interest} has two trees. The Syntactic Constraint gives us:

\ea \Expression{Fed}:\\
\begin{itemize}
\item $\varphi_1 \longrightarrow [_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{Fed}]] = t_1$ 
\item $\varphi_1 \longrightarrow [_{\mathrm{N}}\,\mathrm{Fed}] = t'_1$ 
\end{itemize}
\z

\noindent \Expression{Fed} is a noun in both cases but in the first case it forms a noun phrase by itself and in the second it forms a compound noun phrase with \Expression{raises}. In the first case, it can be chained one level up to the NP level owing to a rule such as NP $\to$ N whereas, in the second, no chaining is possible because the branch arising from \Expression{raises} is encountered owing to a rule such as NP $\to$ N N.

\ea \Expression{raises}:\\
\begin{itemize}
\item $\varphi_2 \longrightarrow [_{\mathrm{V}}\,\mathrm{raises}] = t_2$ 
\item $\varphi_2 \longrightarrow [_{\mathrm{N}}\,\mathrm{raises}] = t'_2$
\end{itemize}
\z


\ea \Expression{interest}:\\
\begin{itemize}
\item $\varphi_3 \longrightarrow [_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{interest}]] = t_3$ 
\item $\varphi_3 \longrightarrow [_{\mathrm{VP}}[_{\mathrm{V}}\,\mathrm{interest}]] = t'_3$
\end{itemize}
\z

The Syntactic Constraint uses the elementary trees in $({\cal T}, \star)$ to derive the possible parses of each word, phrase, and the whole sentence. Earlier, there was no syntactic ambiguity so there was just one syntactic content per word and for the whole sentence; now, there are two syntactic contents for each word.


\subsection{The Semantic Constraint}

The Semantic Constraint consists of two subconstraints, the Conventional Constraint and the Referential Constraint. The first maps words into their conventional meanings and the second maps each of these conventional meanings into their potential referents relative to $u$.

Each word in $\varphi$ has multiple conventional meanings. I will assume the first word \Expression{Fed} has just the one relevant conventional meaning and the latter two words have just the two relevant conventional meanings each.\footnote{This is just to keep the discussion simple without any loss of generality.} Accordingly, the Semantic Constraint gives us:

\ea \Expression{Fed}:\\
\begin{itemize}
\item Referential Use: $\varphi_1 \longrightarrow P^{\varphi_1} \stackrel{u}\longrightarrow \emph{the Federal Reserve} = \sigma_1$  
\end{itemize}
\z

\ea \Expression{raises}:\\
\begin{itemize}
\item Predicative Use: $\varphi_2 \longrightarrow P^{\varphi_2}_1 \stackrel{u}\longrightarrow \emph{augments} = \sigma_2$ 
\item Referential Use: $\varphi_2 \longrightarrow P^{\varphi_2}_2 \stackrel{u}\longrightarrow \emph{increments}\footnote{This corresponds to a noun, not a verb.} = \sigma'_2$ 
\end{itemize}
\z


\ea\Expression{interest}:\\
\begin{itemize}
\item Referential Use: $\varphi_3 \longrightarrow P^{\varphi_3}_1 \stackrel{u}\longrightarrow \emph{interest rate} = \sigma_3$ 
\item Predicative Use: $\varphi_3 \longrightarrow P^{\varphi_3}_2 \stackrel{u}\longrightarrow \emph{arouse curiosity} = \sigma'_3$
\end{itemize}
\z

\noindent The intermediate conventional meanings are denoted by the various properties $P^{\varphi_i}_j$.


\subsubsection{The Flow Constraint}

The Flow Constraint gives us six lexical games, three semantic and three syntactic, corresponding to the three words.

\begin{figure}[H] 
\input{figures/pix5varphi1algebraic.tex}
\caption{Semantic lexical game $g_1$}
\label{fig:semantic lexical game g1 - new example}
\end{figure}

In Figure~\ref{fig:semantic lexical game g1 - new example} showing the first semantic lexical game $g_1$, $s_1$ is the initial situation for the writer $\cal A$ though it also includes the later facts about potential readers, including $\cal B$. The only action available to $\cal A$ in $s_1$ is uttering $\varphi_1 = \Expression{Fed}$ and, since there is only one possible meaning for \Expression{Fed}, the addressee has just one possible interpretation $\sigma_1$ as specified by the Semantic Constraint. When that action is taken, it leads to the terminal situation at which two payoffs $a_{\cal A}$, $a_{\cal B}$ are awarded to the two interlocutors. 

The initial symbol $\rho_1$ represents the prior conditional probability that $\cal A$ is conveying $\sigma_1$ \emph{given} that he is conveying the other meanings and parse trees of the rest of the sentence in the utterance situation $u$ as we have seen before. In other words, $\rho_1 = P(\sigma_1 \cond x_2, x_3, y_1, y_2, y_3; u) = 1$ where the $x_i$ are variables standing for the possible corresponding meanings $\sigma_2$, $\sigma'_2$ and $\sigma_3$, $\sigma'_3$, and the $y_i$ are variables standing for the possible corresponding parse trees $t_1$, $t'_1$ and $t_2$, $t'_2$ and $t_3$, $t'_3$. The situation parameter $u$, which is \emph{not} a random variable, also influences the probability. Again, $\rho_1$ is a \emph{function} of the conditioning variables and parameter but because there is just a single probability, its value is always 1 independent of the values of the variables. 

As before, this semantic lexical game $g_1$ is obtained by applying the semantic game map $g_u$ to the word $\varphi_1$. That is, $g_u(\varphi_1) = g_1$.

\begin{figure}[h] 
\input{figures/pix4varphi1algebraictrees.tex}
\caption{Syntactic lexical game $g'_1$}
\label{fig:syntactic lexical game g1' - new example}
\end{figure}

The syntactic game in Figure~\ref{fig:syntactic lexical game g1' - new example} is now a little more complex than before as there are two possible contents rather than just one. Notice how all the primes on the symbols work, especially symbols such as $s'_{1'}$ and $\rho'_{1'}$ where there is a prime on the symbol and on the subscript. There is no easy way to avoid this and once it is accepted, all the other symbols that arise flow naturally out of these conventions.

In $s'_1$ in Figure~\ref{fig:syntactic lexical game g1' - new example}, the speaker is assumed to be conveying $t_1$, although because $\varphi_1 = \Expression{Fed}$ is syntactically ambiguous between $t_1$ and $t'_1$, the addressee could potentially interpret the word as having the parse $t'_1$ as well. This choice is represented by two branches emanating from the upper intermediate situation inside the information set, the elongated oval, rather than just one as we had in $g_1$. The payoffs for the two agents are $a_{\cal A}$ and $a_{\cal B}$ for making the right choice $t_1$ as this is what $\cal A$ is conveying in $s'_1$, and they are $c_{\cal A}$ and $c_{\cal B}$ for making the wrong choice $t'_1$ as this is not what $\cal A$ is conveying in $s'_1$. The $a$ payoffs are therefore again greater than the corresponding $c$ payoffs as they represent the correct interpretation by $\cal B$.

In $s'_{1'}$ in the lower half of $g'_1$, a similar choice presents itself to the addressee but this time the payoffs are reversed because in this \emph{hypothetical} situation $\cal A$ is conveying the other possible parse $t'_1$. Again, the primed $a$ payoffs are greater than the corresponding primed $c$ payoffs.

The prior probabilities are $\rho'_1 = P(t_1 \cond x_1, x_2, x_3, y_2, y_3; u)$ and $\rho'_{1'} = P(t'_1 \cond x_1,\allowbreak x_2,\allowbreak x_3, y_2, y_3; u)$ and these must sum to 1 as there are only two possible parse trees the speaker could be conveying. Since these probabilities are also functions of the conditioning variables and parameter $u$, there are different \emph{versions} of essentially the same game $g'_1$ when, say, $x_3$ or $y_2$ take on different values. These versions differ only in the numerical values of the prior conditioned probabilities; in all other respects, they are identical.

This syntactic lexical game $g'_1$ is obtained by applying the syntactic game map $g'_u$ to the word $\varphi_1$. That is, $g'_u(\varphi_1) = g'_1$.

As I have remarked before, Equilibrium Linguistics sees both meanings and parse trees as different types of content communicated in an utterance and they are treated analogously. All the possible meanings and all the possible parses of an utterance codetermine one another to yield the equilibrium meaning and parse. Not only does semantics reflect syntax as Frege believed, but syntax also reflects semantics. Now, I show the next set of lexical games for the second word.

\begin{figure}[h] 
\input{figures/Ch9pix4varphi2algebraic.tex}
\caption{Semantic lexical game $g_2$}
\label{fig:semantic lexical game g2 - new example}
\end{figure}


\begin{figure}[h] 
\input{figures/pix4varphi2algebraictrees.tex}
\caption{Syntactic lexical game $g'_2$}
\label{fig:syntactic lexical game g2' - new example}
\end{figure}

It should be relatively straightforward to interpret Figures~\ref{fig:semantic lexical game g2 - new example} and \ref{fig:syntactic lexical game g2' - new example}. In $s_2$ in Figure~\ref{fig:semantic lexical game g2 - new example}, the speaker is conveying $\sigma_2$, although because $\varphi_2 = \Expression{raises}$ is semantically ambiguous as mentioned above, the addressee could potentially interpret the word as conveying $\sigma'_2$ as well. As one would expect from the foregoing, this choice is represented by two branches emanating from the intermediate situation rather than just one as we had in $g_1$. The payoffs follow the same pattern as in $g'_1$.

In $s_{2'}$ in the lower half of the same game, a similar choice presents itself to the addressee but this time the payoffs are reversed because in this hypothetical situation $\cal A$ is conveying the other possible meaning $\sigma'_2$. Again, the primed $a$ payoffs are greater than the corresponding primed $c$ payoffs.

The prior probabilities by analogy with the earlier ones are $\rho_2 = P(\sigma_2 \cond x_1, x_3,\allowbreak y_1, y_2, y_3; u)$ and $\rho_{2'} = P(\sigma'_2 \cond x_1, x_3, y_1, y_2, y_3; u)$ and these must sum to 1 as there are only two possible meanings the speaker could be conveying. Since these probabilities are also functions of the conditioning variables and parameter $u$, there are different \emph{versions} of essentially the same game $g_2$ when, say, $x_3$ or $y_2$ take on different values.

The same kinds of choices and payoffs occur in Figure~\ref{fig:syntactic lexical game g2' - new example} except that this time a syntactic ambiguity has to be resolved as in $g'_1$. Now the prior conditional probabilities are $\rho'_2 = P(t_2 \cond x_1, x_2, x_3, y_1, y_3; u)$ and $\rho'_{2'} = P(t'_2 \cond x_1, x_2, x_3, y_1, y_3; u)$ which must again sum to 1.

The games $g_2$ and $g'_2$ are obtained by applying the maps $g_u$ and $g'_u$ to $\varphi_2$. That is, $g_u(\varphi_2) = g_2$ and $g'_u(\varphi_2) = g'_2$.

The third set of lexical games is shown in Figures~\ref{fig:semantic lexical game g3 - new example} and \ref{fig:syntactic lexical game g3' - new example}.

\begin{figure}[h] 
\input{figures/pix4varphi3algebraic.tex}
\caption{Semantic lexical game $g_3$}
\label{fig:semantic lexical game g3 - new example}
\end{figure}

\begin{figure}[h] 
\input{figures/pix4varphi3algebraictrees.tex}
\caption{Syntactic lexical game $g'_3$}
\label{fig:syntactic lexical game g3' - new example}
\end{figure}

By now, it should be very easy to interpret the diagrams. Again, $\varphi_3 = \Expression{interest}$ has two possible meanings and parse trees as discussed above and so the game trees look similar to the ones for $\varphi_2$. Just for clarity, I mention that the prior conditional probablities for $g_3$ and $g'_3$ are 
$\rho_3 = P(\sigma_3 \cond x_1, x_2, y_1, y_2, y_3; u)$, $\rho_{3'} = P(\sigma'_3 \cond x_1, x_2, y_1, y_2, y_3; u)$ and $\rho'_3 = P(t_3 \cond x_1, x_2, x_3, y_1, y_2; u)$, $\rho'_{3'} = P(t'_3 \cond x_1, x_2,\allowbreak x_3,\allowbreak y_1, y_2; u)$, respectively, both sets of which must sum to 1.

This completes the discussion of semantic and syntactic lexical games. The next step is to look at the  phrasal games that arise as a product of these lexical games. There are two nontrivial phrases, one in each parse, the verb phrase $[_{\mathrm{VP}}[_{\mathrm{V}}\,\mathrm{raises}][_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{interest}]]]$ in the first parse and the noun phrase $[_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{Fed}]\allowbreak[_{\mathrm{N}}\,\mathrm{raises}]]$ in the second parse shown above. I will start with the semantic phrasal game for the second phrase as it is the simplest. It is shown in Figure~\ref{fig:semantic phrasal game g12}.

\begin{figure}[h] 
\input{figures/pixproduct12algebraic.tex}
\caption{Semantic phrasal game $g_{12}$}
\label{fig:semantic phrasal game g12}
\end{figure}

As before, the tree diagram is a ``product'' of the trees of $g_1$ and $g_2$.\footnote{The game tree product is, of course, different from the parse tree product described earlier in \sectref{sec:algebraic system of trees}.} Next, there are two initial situations $s_{12} = s_1 \cup s_2$ and $s_{12'} = s_1 \cup s_{2'}$. The latter situations $s_1$, $s_2$, and $s_{2'}$ are the initial situations of the two games $g_1$ and $g_2$ that are being multiplied. In other words, the initial situations of the product are derived from the initial situations of the multiplicands as they should be and $1 \times 2 = 2$ such combinations are possible from one initial situation of $g_1$ and two initial situations of $g_2$. 

The speaker's utterance $\varphi_1\varphi_2$ is a concatenation of the individual utterances $\varphi_1$ and $\varphi_2$ from $g_1$ and $g_2$. And the addressee's possible actions are $\sigma_1\sigma_2$ and $\sigma_1\sigma'_2$, where again the components are obtained from the respective games. As before, the payoffs corresponding to the relevant branches in the multiplicands are added to give the payoffs in the relevant branch of the product. For example, the payoff for $\cal A$ in the topmost branch corresponding to the path $s_{12}$, $\varphi_1\varphi_2$, $\sigma_1\sigma_2$ is $a_{\cal A} + a_{\cal A}$, which is the sum of the $a_{\cal A}$ in $g_1$ corresponding to the path $s_1$, $\varphi_1$, $\sigma_1$ and the sum of the $a_{\cal A}$ in $g_2$ corresponding to the path $s_2$, $\varphi_2$, $\sigma_2$.

Finally, the priors $\rho_{12} = P(\sigma_1, \sigma_2 \cond x_3, y_1, y_ 2, y_3; u)$ and $\rho_{12'} = P(\sigma_1, \sigma'_2 \cond x_3, y_1,\allowbreak y_2, y_3; u)$ which must sum to 1. Again, it should be easy to see how these probabilities are generated from the corresponding probabilities in the multiplicands $g_1$ and $g_2$ where the priors are $\rho_1 = P(\sigma_1 \cond x_2, x_3, y_1, y_2, y_3; u)$ and $\rho_2 = P(\sigma_2 \cond x_1, x_3,\allowbreak y_1, y_2, y_3; u)$, $\rho_{2'} = P(\sigma'_2 \cond x_1, x_3, y_1, y_2, y_3; u)$.

The product game $g_{12} = g_1 \otimes g_2$ can also be obtained directly by applying the semantic map $g_u$ to the phrase $\varphi_1 \circ \varphi_2$. In other words, $g_{12} \equiv g_u(\varphi_1 \circ \varphi_2) = g_u(\varphi_1) \otimes g_u(\varphi_2) \equiv g_1 \otimes g_2$. 

Turning to the corresponding syntactic game, recall that the two trees of \Expression{Fed} are $t_1 = [_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{Fed}]]$ and $t'_1 = [_{\mathrm{N}}\,\mathrm{Fed}]$, and the two trees of \Expression{raises} are $t_2 = [_{\mathrm{VP}}[_{\mathrm{V}}\,\mathrm{raises}]]$ and $t'_2 = [_{\mathrm{N}}\,\mathrm{raises}]$. This means there are four potential parses of the phrase $\varphi_1\varphi_2$:
\begin{eqnarray*}
t_1 \star t_2 & = & [_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{Fed}]] \star [_{\mathrm{V}}\,\hbox{raises}] = t_0\\
t_1 \star t'_2 & = & [_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{Fed}]] \star [_{\mathrm{N}}\,\mathrm{raises}] = t_0\\
t'_1 \star t_2 & = & [_{\mathrm{N}}\,\mathrm{Fed}] \star [_{\mathrm{V}}\,\mathrm{raises}] = t_0\\
t'_1 \star t'_2 & = & [_{\mathrm{N}}\,\mathrm{Fed}] \star [_{\mathrm{N}}\,\mathrm{raises}] = [_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{Fed}][_{\mathrm{N}}\,\mathrm{raises}]] = t_{1'2'}
\end{eqnarray*}

The only nonzero product corresponding to \Expression{Fed raises} is the compound noun under an assumed grammatical rule like NP $\to$ N N or, equivalently, a tree such as $[_{\mathrm{NP}}[_{\mathrm{N}}\,][_{\mathrm{N}}\,]]$ which would left multiply $t'_1$ and then $t'_2$ as follows:
\begin{eqnarray*}
t'_1 \star t'_2 & = & [_{\mathrm{N}}\,\mathrm{Fed}] \star [_{\mathrm{N}}\,\mathrm{raises}]\\
& = & ([_{\mathrm{NP}}[_{\mathrm{N}}\,][_{\mathrm{N}}\,]] \lhd [_{\mathrm{N}}\,\mathrm{Fed}]) \lhd [_{\mathrm{N}}\,\mathrm{raises}]\\
& = & [_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{Fed}][_{\mathrm{N}}\, ]] \lhd [_{\mathrm{N}}\,\mathrm{raises}]\\
& = & [_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{Fed}][_{\mathrm{N}}\, \mathrm{raises}]]\\
& = & t_{1'2'}
\end{eqnarray*}

Notice how the primes on $t_{1'2'}$ work: we need to prime both subscripts 1 and 2 as the corresponding subscripts on both the multiplicands are primed. A product such as $t_1 \star t'_2 = t_{12'} = t_0$ and likewise with the other products as there are presumably no rules that correspond to such products in $G$.

The resulting syntactic phrasal game is shown in Figure~\ref{fig:syntactic phrasal game g12'}.

\begin{figure}[p] 
\input{figures/pixproduct12varphi1varphi2trees.tex}
\caption{Syntactic phrasal game $g'_{12}$}
\label{fig:syntactic phrasal game g12'}
\end{figure}

This product should be straightforward to interpret. It follows the same pattern as Figure~\ref{fig:semantic phrasal game g12}. For example, there are four initial nodes as there are two initial nodes $s'_1$, $s'_{1'}$ in $g'_1$ and two initial nodes $s'_2$, $s'_{2'}$ in $g'_2$ and $2 \times 2 = 4$. These situations are the corresponding unions of the initial situations of the multiplicands, and so on with the rest of the entities. We have again $g'_{12} \equiv g'_u(\varphi_1 \circ \varphi_2) = g'_u(\varphi_1) \otimes' g'_u(\varphi_2) \equiv g'_1 \otimes' g'_2$.

There is a similar semantic phrasal game $g_{23} \equiv g_u(\varphi_2 \circ \varphi_3) = g_u(\varphi_2) \otimes g_u(\varphi_3) \equiv g_2 \otimes g_3$ with $2 \times 2 = 4$ initial nodes as can be seen from Figures~\ref{fig:semantic lexical game g2 - new example} and \ref{fig:semantic lexical game g3 - new example}. Likewise, there is a four initial node syntactic phrasal game $g'_{23} \equiv g'_u(\varphi_2 \circ \varphi_3) = g'_u(\varphi_2) \otimes' g'_u(\varphi_3) \equiv g'_2 \otimes' g'_3$ as can be seen from Figures~\ref{fig:syntactic lexical game g2' - new example} and \ref{fig:syntactic lexical game g3' - new example}. In the case of the latter game, it should be possible to see by inspection that the only nonzero parse tree product is $t_2 \star t_3$ corresponding to the verb phrase $[_{\mathrm{VP}}[_{\mathrm{V}}\,\mathrm{raises}][_{\mathrm{NP}}[_{\mathrm{N}}\,\mathrm{interest}]]]$. I will not tarry to draw these games.

The next two games are the semantic and syntactic sentential games corresponding to the sentence $\varphi_1\varphi_2\varphi_3$. Both games can be obtained in either of two ways. For the syntactic sentential game, we stipulate that $g'_{123} = g'_1 \otimes' g'_{23} = g'_{12} \otimes' g'_3$ despite the lack of associativity of $\star$. This is done by mandating that when forming a three-term product such as $(t_1 \star t_2) \star t_3$ or $t_1 \star (t_2 \star t_3)$, which have different results, an intermediate nonzero grouping should be favored and the other one dropped. Since $(t_1 \star t_2) = t_0$ but $(t_2 \star t_3) \neq t_0$, it is the latter that should be retained in place of the former (even if the final three-term product proves to be zero). If this were not stipulated, there would be two distinct syntactic sentential games which is undesirable because then the two possible parses of the sentence $t_1 \star (t_2 \star t_3)$ and $(t'_1 \star t'_2) \star t'_3$ cannot be directly compared in the same game. This turns out to be felicitous for other reasons too as the set of syntactic games remains associative with respect to $\otimes'$. The reader is urged to write out the sixteen possible parse tree products of the three lexical trees and see how eight of them are discarded in forming the syntactic game. This stipulation is just part of the overall way in which the product of syntactic games is defined, that is all.

In exactly the same way, for the semantic sentential game too, we stipulate that $g_{123} = g_1 \otimes g_{23} = g_{12} \otimes g_3$ despite the lack of associativity of $\odot$. This game also has eight initial nodes just like the syntactic sentential game, and eight zero products equal to the zero $\mathbf{0}$ of the infon lattice introduced in \sectref{sec:information} are discarded. As I have not described this operation in any detail in this book, I will simply say the matter is analogous to what happens in the case of the syntactic sentential game.

Finally, we would form the full mixed semantic-syntactic sentential product as before by multiplying the semantic sentential product with the syntactic sentential product. This is straightforward to do and I leave it to the reader to confirm that it would have as many as $8 \times 8 = 64$ initial nodes and, likewise, $64$ interpretive branches emerging from each of the $64$ intermediate nodes in the information set for $\cal B$. This confirms that these full mixed sentential product games grow rapidly in size as also mentioned on page~\pageref{page:mixed products} in \sectref{sec:generation game}. Recall that we do not need a Pareto-Nash equilibrium for these full mixed products; just the Nash equilibrium will do, and if there is more than one, we just assign them equal probabilities.

We can collect all these interdependent games as before into the locutionary global game $LG(\varphi) = \{g_1, g_2, g_3, g'_1, g'_2, g'_3\}$ where the product games have been dropped as all the required information is contained in the lexical games as indicated by Theorem~\ref{thm:one} in \sectref{sec:solving locutionary global games}. I have chosen to describe the distributed form of $LG(\varphi)$ rather than the compact form which would involve a $2n = 2 \times 3 = 6$ dimensional matrix as there are $n = 3$ words in the sentence uttered. Such a high-dimensional matrix can be represented by several two-dimensional matrices.

All that remains is to show that the global Pareto-Nash equilibrium of all these games yields the intended meaning $\sigma = \sigma_1(\sigma_2\sigma_3)$ and parse $t = t_1(t_2t_3)$ for the utterance. This can be seen informally by noting which prior probabilities are higher in each of the lexical games and ensuring the corresponding solutions are compatible with one another as indicated by Theorem~\ref{thm:simple equation} in \sectref{sec:solving locutionary global games}. The fact that this is easy to do by inspection is reassuring because the calculations would happen in milliseconds in the heads of the writer and reader of the sentence as only a few probabilities have to be compared in a compatible manner.

Assuming just basic facts about ontology, the language, grammar, and partial rationality, we have been able to compute the intended parse and meaning of the utterance in the presence of both syntactic and semantic ambiguities. This approach can be extended to any utterance in principle. Aspects of syntax such as \emph{movement} (e.g.\ \emph{wh}-movement) require some small adjustments that are straightforward to devise.

The full content conveyed by $\cal A$ is the proposition $p = (c \vDash \sigma)$ where $c$ is the described situation as before and could be something like the (financial) news of the day. As noted earlier, the latter's boundaries are somewhat indeterminate.

I leave it to the reader to see how the solution to the locutionary global game fits into the Generation Game on the speaker's side and into the Interpretation Game on the addressee's side and to make appropriate assumptions to construct a Content Selection Game for the example, which could be assumed to be trivial.


\section{Locutionary meaning} \label{locutionary meaning}

The locutionary meaning of an utterance is the meaning that comes ``directly'' from the words in the relevant utterance situation. As such, finding it involves just disambiguating between alternative referential meanings of words that have homonymous or polysemous conventional meanings and fixing the reference of pronouns. It does \emph{not} involve other phenomena such as free enrichment, modulation, implicature, and direct and indirect illocutionary force. All of the latter belong to the illocutionary meaning of an utterance, although modulation straddles locutionary and illocutionary meaning as will be seen in \chapref{ch:modulation}. I will say more about this distinction between locutionary and illocutionary meaning and the crosscutting distinction between literal meaning and implicature at the end of \partref{part:IV}.

In the foregoing, I have shown in a very thorough manner how lexical and structural disambiguations work. This can be extended to fixing pronoun reference in a straightforward way as that problem is also just one of disambiguation among alternative possible referents. All aspects of communication involving alternative possibilities, whether of a locutionary or illocutionary kind, are just problems of disambiguation and the Flow Constraint treats them all uniformly. The only difference that arises is in how the possibilities are generated. In the case of fixing pronoun references, if the pronouns are anaphoric, the possible referents come from the sentence or the discourse, and if they are deictic, they are available directly in the utterance situation $u$ or in induced resource situations $r_u$ that depend on $u$. The rudimentary model described in \citet{cp:ga} can easily be extended to cover all such cases so I will not say more about this problem here (except briefly in \sectref{sec:a complete example}) and will take the problem of locutionary communication as fully solved.

It is fitting to first solve this problem completely in a detailed way and then look at some general philosophical results that concern the system of locutionary communication and, in particular, games of partial information. I now turn to this latter task.

