\chapter{Introduction}\label{ch:introduction}
While all professions are affected by technological innovation, the impact has been particularly profound on the language professions of translation and interpreting (T\&I). New information and communication technologies (ICTs) have shaped the ``\textit{micro}- and \textit{macro}-systems'' \citep{o2013impact} of T\&I, which is to say, both the way the core tasks of these professions are performed and the social and physical contexts in which translators and interpreters operate. While new technologies threaten to replace human interpreters and translators, on the one hand, they also afford new T\&I services and support the work of translators and interpreters, with the ambition to increase professionals’ productivity and service quality, on the other hand.

The impact of ICTs on written translation became evident in the noughties, leading scholars to speak of a ``technological turn'' \citep{chan2007taking,cronin2010translation,o2013impact}, although the work on ICTs with revolutionary power (such as machine translation) began in the 1950s. In interpreting, it is not until the last decade that the ``technological turn'' \citep{fantinuoli2018a} was announced by scholars, even though previous technological breakthroughs had already shaped the developments of the profession, for instance leading to the birth of simultaneous interpreting (SI). Compared to previous phases of change following technological developments, today, interpreting technologies are increasing in number and penetrating the profession at a pace that became exponential a few years ago \citep{fantinuoli2018a} and was further sped up by the Covid-19 pandemic. It is likely that ICTs will soon become so deeply entrenched in the interpreter’s workflow that we will start viewing all of interpreting as a form of human-computer interaction (HCI), as scholars already argued of translation \citep{o2012translation}.

Among the technologies that have the potential to shape the ways in which interpreting is performed and alter the very nature of the underlying cognitive sub-processes, computer-assisted interpreting (CAI) tools and their use during SI are of particular significance to the profession. Like computer-assisted translation (CAT) tools for translators, CAI tools are designed with the intent to ease and enhance the work of interpreters, increasing interpreters’ efficiency (in the preparation phase) and delivery accuracy (in the interpretation or in-booth phase).

In the \textit{in-booth} phase, the latest generation of CAI tools is aimed at supporting interpreters in the rendition of particularly demanding and error-prone linguistic items. These are named entities, numbers, specialised terms, and acronyms. Previous research on these dreaded ``problem triggers'' \citep{gile2009basic} has shown both the staggering error rates in interpreters’ delivery when these items occur in the speech to be translated (cf. \citealt{desmet2018simultaneous} and \citealt{frittella2017numeri,frittella2019a} on numbers, for instance) and reported interpreters’ feelings of mental effort and stress associated with interpreting these items (e.g. \citealt{alessandrini1990translating}). The increased difficulty forces interpreters to adopt coping tactics during SI in the presence of problem triggers, such as manually searching for named entities, terms and acronyms in the interpreter’s glossary or external sources such as electronic dictionaries or databanks, writing down numerals, or asking the colleague working in the booth with them to help them perform these tasks accurately. Because all these processes entail considerable risk for human error and must be attended to by the interpreter, while s/he is performing a task that is in itself complex and cognitively demanding, they risk leading to errors and a disruption in the interpreting activity. The recent integration of \textit{automatic speech recognition} (ASR) and \textit{artificial intelligence} (AI) technology into CAI tools offers the opportunity to help interpreters better cope with problem triggers. By automatically displaying named entities, numbers, specialised terms and acronyms on the interpreter’s laptop screen in real-time, ASR- and AI-powered in-booth CAI tools have the potential to provide interpreters with a reliable “virtual boothmate”, relieving them from the stress and mental effort in dealing with problem triggers and increasing delivery accuracy.

However, the complexity of the mental task that CAI tools aim to support constrains the potential effectiveness of the artificial boothmate. The synchronized execution of multiple mental operations, which partly compete for the same limited cognitive resources, makes SI a sensational stunt of mental acrobatics. During SI, human interpreters receive a “source speech” in one language, mentally process it and turn it into a “target speech” in a different language. All this happens in real-time, under time pressure and at a pace that cannot be controlled by the interpreter. It is possible that the introduction of yet another source of information into the already cognitively taxing activity of SI may cause interpreters to reach a mental saturation point and disrupt the task. While details matter in all user interfaces and even seemingly minor particulars may disrupt the user, this may be even truer of CAI tool interfaces. Maximum \textit{usability} seems to be an imperative for CAI tools to support rather than disrupt the delicate balance of SI. This assumption is shared amongst leading scholars in this area, who postulated that a CAI tool should ``offer ergonomic ways to present extracted information'' (\citealt[26]{fantinuoli2017speech}), if the tool is not to exert the opposite effect and ``slow down delivery and place a burden on interpreters’ mental processing'' (\citealt[2]{defrancq2021automatic}). However, so far, the set-up of technical requirements for CAI tools and their UI design has been mainly driven by developers’ (usually interpreters themselves) intuition, without any ``experimental support in design decisions''  (\citealt[160]{fantinuoli2018b}). The overall process has lacked ``test and improvement phases, which are crucial to achieve software maturity and find the golden standard in terms of functionalities desired by the professional group'' (\citealt[164]{fantinuoli2018a}).

The importance of research as an instrument to increase the usability of CAI tools becomes apparent at a comparison with CAT tools and their development. Also in the case of CAT tools, scholars extensively argued the importance of usability: ``The link should be quite obvious: While CAT tools exhibiting a high usability should enhance the translator’s cognitive performance, tools exhibiting a low usability will probably tend to decrease it'' (\citealt[115]{kruger2016contextualising}) and ``If CAT tools are easy to use, then more time and cognitive capacity should be available for the decision-making and problem-solving processes integral to translation work. If such tools or certain features are complicated and/or non-intuitive, then human-computer interaction can be compromised, which usually results in less than optimal use and dissatisfaction with tools''  (\citealt[2]{kappus2020ergonomics}). However, the development of the first CAT tools, which later became and today still represent the market standard, was market-driven rather than translator-driven, i.e., emerging from the initial disillusionment with ``unmanned'' machine translation and motivated by the need for larger translation volumes at reduced time and costs. Therefore, translators’ needs were not adequately accounted for in the development of the first CAT tools (cf. \citealt{moorkens2017assessing, o2017irritating} inter alia). While virtuous examples of systematic, translator-centred CAT tool development do exist, already existing solutions are difficult to change for economic reasons, even where their UI features are found to be ``irritating'' \citep{o2017irritating} and inefficient for translators, hence decreasing their productivity. Several studies identified poor usability as a major reason for translators’ resistance to using CAT tools (e.g. \citealt{leblanc2013translators,o2017irritating}). Nonetheless, because it is often the agency or the client who decides which CAT tool is to be used, translators may feel obliged to use a solution that they find too expensive and unusable just to access jobs \citep{garcia2015computer}. Forced to adopt solutions that do not fully consider their needs may contribute to translators’ feeling of ``dehumanisation'' and ``devaluation'' \citep{o2012translation} following the introduction of CAT tools into their workflow. 

Given that the economic case for developing CAI tools was much less compelling than CAT, the initial development of these solutions has emerged ``from the bottom'', i.e. from the initiative of interpreters themselves, rather than being imposed from above. However, this situation appears to be changing. Large remote-simultaneous interpreting (RSI) providers\footnote{See the introduction of Interpreter Assist \citep{fantinuoli2022kudo} into Kudo, currently the world’s number one RSI provider.} who own the largest share of the RSI market are beginning to integrate CAI tools into their platforms. Large institutional clients such as international organisations\footnote{For instance, the CAI tool InterpretBank \citep{fantinuoli2016interpretbank} is currently being used for interpreters’ terminology management at institutions such as the OECD (\href{https://www.interpretbank.com/site/}{https://www.interpretbank.com/site/}).} are increasingly integrating CAI tools into their work processes. The fact that CAI tool design today still ``reflects more the ideas and habits of the respective developer, generally an interpreter himself, than the needs of the interpreter community'' \citep[164]{fantinuoli2018b} and that a strand of usability-focused CAI research is still missing threaten to lead to unsustainable technological developments. What has been missing from the development of CAI tools is the involvement of the community, the validation of design assumptions and the incorporation of data into the development process to improve the solutions. A recurring slogan under which CAI tools are marketed is ``developed \textit{for} interpreters \textit{by} interpreters''. What is missing from the equation is ``\textit{with} interpreters'' -- the engagement of the community in conceptualising and creating the tools intended to be used by professionals in their everyday work. The distance of the interpreting community from the development of CAI tools may be a factor contributing to the traditional disengagement and mistrust towards these tools. Time and again, practitioners express concerns about the possible negative impact that CAI tools may have on their interpreting process and final output, the changes that these may cause on their professional status and remuneration, and even that the tools may take over, confusing CAI with the application of ASR- and AI in machine interpreting to replace, rather than support, human service provides. Like translators perceived CAT tools as a “black box” \citep{o2012translation} also interpreters seem to fear a possible dehumanisation and devaluation following the introduction of CAI tools into their work. While some have interpreted professionals’ aversion to technological change as ``defense of old accrued power, dressed in the guise of quality'' (\citealt[4]{pym2011technology} cited in \citealt[155]{fantinuoli2018a}), it may also be interpreted as a consequence of the lack of community engagement in the development of CAI tools.\largerpage[2]

This book presents a case study of interpreter-centred development of an ASR- and AI-powered CAI tool--\textit{SmarTerp}, developed within the European Union funding line EIT Digital. It represents the first case in which a usability engineering (or “user-centred design”) approach was used to develop a CAI tool, starting from the analysis of users’ needs to develop requirements, define UI features, and test and refine the solution iteratively to integrate users’ feedback and data into the design of the tool at an early stage of its development. The process exemplifies the role of research as an instrument for the inclusion of the interpreting community into the development of technological solutions.

The focus of the book will be on the empirical evaluation of the CAI tool through the usability testing method. Usability testing consists of the observation of representative users performing representative tasks with the product or a high-fidelity functioning prototype. In HCI and usability engineering processes, it is regarded as a fundamental method in the development of interactive systems because it makes it possible to identify the challenges that users encounter during usage and change the UI features that cause those impasses in the interaction. Usability testing, especially at this initial stage of in-booth CAI tool development, may represent a crucial method in the empirical evaluation of these systems, to identify those seemingly minor details that may make a major difference during SI and gain a better understanding of interpreters’ needs.

By making the knowledge and materials developed through this research experience available to a wider audience, I hope that this book may pave the way for the development of a strand of usability-focussed empirical CAI research, contribute to innovation in the field and promote high standards of methodological and scientific rigour. At a time when ``interpreting is about to go through a transformation phase driven by socio-technical change'' \citep[8]{fantinuoli2018a}, it is a priority to ensure that technology advances \textit{with} interpreters.\largerpage[2]

The book is structured as follows. Chapter \ref{ch:usability_engineering} on usability engineering provides the conceptual and methodological framework of the inquiry. Chapters \ref{ch:translation_technology} and \ref{ch:interpreting_technology} respectively review the history of translation and interpreting technology (particularly, CAT and CAI) and the methods that were used to evaluate the usability of these tools. Chapter \ref{ch:smarterp} presents the background of the SmarTerp project. Chapter \ref{ch:methods} opens the empirical section of the work with a detailed presentation of the study design and materials. Chapters \ref{ch:first_iteration} and \ref{ch:second_iteration} present the results of the two usability tests -- the pilot study (involving 5 conference interpreters as study participants) and the main study (10 conference interpreters) -- leading to design recommendations and the improvement of the CAI tool prototype. The following discussion (Chapter \ref{ch:discussion}) summarises the broader implications of the study findings in terms of gains in the field’s scientific understanding of interpreter-CAI interaction and usability principles. The discussion also addresses the limitations of the present work, points to possible future research trajectories and provides methodological recommendations for future researchers wishing to build on this study. The most salient aspects of the work and the future outlooks are summarised in the conclusion (Chapter \ref{ch:conclusion}).
