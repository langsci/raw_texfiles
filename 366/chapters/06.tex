\chapter{Usability test of SmarTerp: Methods}\label{ch:methods}
This chapter opens the empirical section of the work by presenting the methods used in the two cycles of prototyping, testing and improvement of the SmarTerp prototype (cf. Chapter \ref{ch:smarterp}). The chapter first clarifies the study aims, choice of research approach and study design. It then details the rationales for the development of study materials and their characteristics. After discussing the participant selection criteria and procedures, the chapter presents the data collection procedures and, finally, the analysis criteria.

\section{Aims}

As explained in the previous chapter (cf. Chapter \ref{ch:smarterp}), the testing of SmarTerp aimed to achieve goals of different nature: improving the UI, assessing the extent to which the tool was useful and satisfactory to users and contributing to scientific understanding of interpreter-CAI interaction. To better define and operationalise the goals of the study, I specified them as follows. Starting from the goal of improving the interface, I defined the research question as follows:
\begin{quote}
    Do problems systematically occur when interpreters use SmarTerp’s CAI tool? Can these problems be attributed to design features or technical specifications of the CAI tool?
\end{quote}
I specified the aim of evaluating the impact of the CAI tool on users and their satisfaction as follows, drawing on \citegen{iso2018} definition of usability
\begin{quote}
    Evaluating the extent to which the CAI tool SmarTerp may support representative users in completing representative tasks in a way that is effective, efficient and with a high degree of satisfaction.
\end{quote}
Because the study aimed to fulfil not just the practical aim of developing recommendations for the improvement of SmarTerp but also a scientific aim, the analysis aimed to identify the usability factors that have an impact on interpreters’ performance and, possibly, explain why. In other words, the aim above could be reformulated as the following questions:
\begin{quote}
    To what extent was SmarTerp effective\slash efficient\slash satisfactory to test participants? Why was it (not)?
\end{quote}


\section{Choice of research approach}

The aims of the study explained above required the collection of both performance and perception data and their analysis with both quantitative and qualitative methods. Performance data was required to evaluate the tool’s actual effectiveness and efficiency, since participants’ perception may not be regarded as a reliable indicator of these aspects. Perception data was required to ascertain participants’ satisfaction with SmarTerp. Effectiveness, efficiency, and satisfaction had to be evaluated quantitatively to answer the research questions concerning their extent, developing an overall description of what happened in the test and obtaining benchmark values against which to compare future testing results. These usability aspects also had to be evaluated qualitatively to identify deeper patterns that could not be grasped at a mere quantitative description and unveil reasons behind users’ observable behaviour and reported perceptions.

These requirements are consistent with the double practical and scientific aim of the study. On the practical side, the need was to extrapolate design recommendations from the collected evidence, which required an interpretive process whose robustness increases if multiple data sources are used. This is particularly necessary for the exploration of complex systems, such as the interpreter-CAI tool interaction, in which we assume both human factors and tool functions to exert an influence on the outcome making it necessary to isolate the phenomena caused by tool-related issues from those arising from contingent and idiosyncratic factors. On the scientific aim, contributing to the field’s understanding of interpreter-CAI tool interaction required a level of rigour and detail that is usually not achieved in usability tests conducted for purely commercial purposes (cf. \cite{barnum2020usability}).

\begin{sloppypar}
In the light of these requirements, I opted for a mixed-method approach. Mixed\hyp{}methods is a research approach that emerged around the late 1980s and early 1990s informed by the pragmatic worldview that research design should be driven by the study objectives rather than rigid canons, which justifies the combination of data from multiple sources and methods from across the quan\-ti\-ta\-tive\nobreakdash-qual\-i\-ta\-tive spectrum \citep[294--324]{creswell2017research}. This is considered as a particularly adequate methodology to explore complex systems that must be studied from different perspectives to gain a holistic and reliable understanding.
\end{sloppypar}

This design does not only offer benefits but also poses challenges for the inquirer, including the need for extensive data collection, the time-intensive nature of analysing both qualitative and quantitative data, the requirement for the researcher to be familiar with both quantitative and qualitative forms of research and the complex flow of research activities \citep[298]{creswell2017research}. To cope with these challenges, I opted for a small sample, in line with the aim to obtain a complete, robust interpretation over statistically generalisable results. I also dedicated great attention to the design of materials and procedures to have a clear and efficient organisation.


\section{Study design}

In order to address the aims, the study design had to fulfil some requirements. The first was involving \textit{representative users} reflective of the target group for whom SmarTerp was designed, i.e. practising conference interpreters fulfilling a series of selection criteria. The second was using the \textit{working} prototype of SmarTerp in action during the simultaneous interpretation of a speech. The third was ensuring that during the test the users would perform tasks representative of those that they would normally perform with the support of the tool, which required a special speech design strategy. Because of all these characteristics, I chose \textit{usability testing} as a method.

The individual test sessions consisted of (1) an \textit{interpreting test} in which participants were asked to interpret simultaneously an ad-hoc designed speech with the support of a mock-up of the SmarTerp prototype, (2) a \textit{usability questionnaire}, and (3) a semi-structured \textit{interview}. The interpreted test was aimed at gathering performance data to evaluate the tool’s (actual) effectiveness and efficiency. The questionnaire aimed to gather a quantitative measure of participants’ perception of the tool and its usability. The interview was aimed at deepening the analysis of both performance and perception and providing a starting point for the patterns that emerged in the delivery.

To abstract usage problems from the bulk of data collected in the study and develop design recommendations, I followed the steps proposed by \citet{seewald2004kritischen}. In the \textit{experience} (or \textit{data collection}) phase, I collected data concerning participants’ performance in using SmarTerp as well as their perception. In the construction (or data analysis) phase, I analysed the data using the methods detailed in \sectref{sec:data_analysis}. The data sources, materials used to collect them, and the outcome of the analysis are summarised in the table below (\tabref{tab:2}).




\begin{table}
%\resizebox{\linewidth}{!}{
\small
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{.2\textwidth}QQQ}
\lsptoprule
 & \multicolumn{3}{c}{{Study design}}     \\
\cmidrule(lr){2-4}
&  {CAI-tool assisted SI test}    & {Post-task questionnaire}    & {Post-task interview}   \\
 \midrule
{Type of data gathered}     & Quantitative and qualitative performance data    & Quantitative perception data   & Qualitative perception data   \\
\tablevspace
{Data description}     & Participants’ video-recorded interpretation with SmarTerp support: delivery, observations, and interview quotes & Questionnaire data    & Interview data    \\
\tablevspace
{Data collection materials} & Source speech video with integrated SmarTerp interface (mock-up)   & Digital questionnaire (7-point Likert scale items)   & Semi-structured interview protocol \\
\tablevspace
{Outcome of analysis}      & a) Quantitative:  success rates on tasks

 b) Qualitative:  error patterns  & Participants’ agreement with statements related to general satisfaction and pragmatic usability criteria & Users’ explanation of usage problems and reflections about their experience using SmarTerp \\
\lspbottomrule
\end{tabularx}
%}
\caption{Overview of the study design: Data sources, aims and materials}
\label{tab:2}
\end{table}


In the \textit{interpretation} phase, I identified recurring issues in participants’ performance (e.g. a frequent pattern of error) or that were reported by participants (e.g. a frequent complaint). To identify recurring issues, I used the following processes. Starting from the task success rates, I looked for patterns in tasks with a particularly low score. Looking at the distribution of error categories, I searched for patterns where a given error category occurred with high frequency in a given task. I complemented this search with the observations I made during the test, participants’ questionnaire responses, and recurring themes in the interview data. I considered whether data from the performance and perception sets converged into an explanation for the given issue (e.g. several participants reported to have struggled with a specific tool feature in a specific task where I could identify a recurring error pattern). This way, I developed an interpretation of recurring usage problems.

In the \textit{prioritisation} phase, I ranked the usage problems by their frequency and impact. Impact ranks the issue by the severity of its consequences on successful task completion (i.e. complete and accurate interpretation). The levels of impact are:
\begin{enumerate}
    \item \textit{High}: prevents the user from completing the task and gave rise to critical errors in the test.
    \item \textit{Moderate}: causes user difficulty and gave rise to non-critical errors in the test.
    \item \textit{Low}: a minor problem that does not significantly affect task completion.
\end{enumerate}
Frequency is given by the percentage of participants who experienced the issue: \begin{enumerate}
    \item \textit{High}: 30\% or more of the participants experienced the issue.
    \item \textit{Moderate}: 11--29\% of participants experienced the issue.
    \item \textit{Low}: 10\% or fewer of the participants experienced the issue.
\end{enumerate}
In the \textit{recommendation} phase, I proposed some design recommendations that could help solve the issues identified in the study. I finally discussed the proposed recommendations in a workshop with the SmarTerp HCI and development team to verify and finalise them.

\section{Materials}




\subsection{Test speech design}




Usability testing requires the observation of users’ interaction with the product in tasks that are relevant to them and representative of the tasks they would normally perform with the product. In the context of a CAI tool, the tasks are defined by the problem triggers in the source speech that will prompt interpreters to use the CAI tool. This implies that the usability test of a CAI tool requires the careful drafting of the test speech to create the necessary conditions for complete and meaningful observations.

I posited that the source speech design should satisfy five criteria. First, it should include \textit{all problem trigger classes} for which CAI tool support is offered. Second, the problem triggers should be \textit{sufficiently challenging} for all participants to increase the likelihood that they will consult the tool during SI. Third, the problem triggers in the source speech and their distribution should vary, creating tasks of \textit{varying complexity} that make it possible to ascertain in which conditions and to what extent CAI tool support may be effective. I posited this criterion because my past research experience on the SI of numbers \citep{frittella2017numeri,frittella2019a} showed that the density of numerals in a speech unit and the complexity of the elements that accompany it contribute to determining the difficulty in interpreting the information. Because no previous study on the CAI tool-supported SI systematically analysed the impact of task complexity on the delivery, I identified this as a potentially significant variable to examine. Fourth, the \textit{content} should not be so complex and unfamiliar to interfere with CAI tool consultation. Fifth, the \textit{speech structure} should be clear in logic and alternate test tasks with discursive speech passages to prevent the difficulty in one task from impacting the following one and confounding the analysis. A similar criterion for the design of controlled test speeches had been articulated by \citet{seeber2012cognitive} as well as \citet{prandi2017designing}. Sixth, further speech \textit{characteristics} (e.g., audio quality, syntactic complexity, speed, etc.) should prevent factors other than the problem triggers to confound the analysis.

Starting from these principles, I designed the test speech to include all problem trigger classes for which SmarTerp provided support, i.e. acronyms (henceforth abbreviated as AC), named entities (NE), numbers (NU) and specialised terms (ST). Each item (e.g. the acronym, numeral, etc.) should be challenging enough to prompt the interpreter to look at the tool -- hence, common acronyms (e.g. EU), well-known named entities (e.g. Barack Obama), one-digit numerals (e.g. 3), and specialised terms that are likely to be known to interpreters (e.g. artificial intelligence) were excluded. For each problem trigger class, more than one condition was chosen to create tasks of varying complexity. For instance, based on previous research on numbers (e.g. \cite{frittella2017numeri,frittella2019a,frittella2019b}), I identified a 20-word sentence containing one numeral and no other factor of complexity to be a less problematic condition than a 60-word speech passage containing eight numerals accompanied by specialised terms. This way, I conceptually defined the tasks that should constitute my test speech.

In the tasks \textit{isolated named entity} (NE), \textit{isolated acronym} (abbreviated as AC), \textit{isolated term} (TE) and \textit{isolated numeral} (NU), the given problem trigger occurs in a “simple sentence, which is defined as a sentence of approximately 20--30 words, with simple syntax and logic and no further problem trigger. In the task \textit{numeral and complex referent} (NR), one numeral is accompanied by a complex referent (i.e., another problem trigger such as an acronym/ a named entity/ specialised term/ numerical value) in a simple sentence (defined as above). The task \textit{numerical information unit} (NIU) is a sentence of approximately 30 words constituted of a complex referent, a complex unit of measurement (i.e., an acronym/ a named entity/ specialised term/ numerical value) and five consecutive numerals, as in the structure (referent) increased/decreased by (X\%) from Y in (time1) to Z in (time2). The task redundant number cluster (NCR) is a number-dense speech passage with the following characteristics: (1) constituted of three subsequent NIUs of approximately 10-15 words each; (2) the time and place references remain unvaried and are repeated in each NIU; (3) the unit of measurement and the referent remain unvaried; (4) the numeral changes in each NIU. The task non-redundant number cluster (NCN) is a number-dense speech passage with the following characteristics: (1) constituted of three subsequent NIUs of approximately 10--15 words each; (2) time, place, referent, unit of measurement and numeral change in each NIU; (3) either the referent or the unit of measurement is complex. I included two number-dense speech passages with different degrees of redundancy into the test speech to see whether the use of the tool would encourage participants to use a strategy, such as omitting repeated elements to reduce the requirements involved in processing the speech. The task \textit{term list} (TL) includes three specialised terms occurring in the form of a list. The task \textit{terms in a semantically complex sentence} (TS) is constituted of three specialised terms connected by complex logical links, e.g., implicit logical passages where comprehension requires relevant background knowledge and inference. The task \textit{complex speech opening} (SO) is the address to participants and comprises three unknown named entities of people, their charges and two acronyms. Finally, in the task \textit{conference programme} (CP), the programme for the rest of the conference day is presented and it includes several named entities and other problem triggers. The test speech may be consulted in the appendix. Table \ref{tab:3} summarises the task names and the corresponding abbreviations.

\begin{table}[p]
\begin{tabular}{ll}
\lsptoprule
Code & Name\\\midrule
AC        & Isolated acronym                         \\
NE        & Isolated   named entity                  \\
NU        & Isolated   numeral                       \\
NR        & Numeral and   complex referent           \\
NIU       & Numerical   information unit             \\
NCR       & Redundant   number cluster               \\
NCN       & Non-redundant   number cluster           \\
TE        & Isolated   term                          \\
TL        & Term list                                \\
TS        & Terms in a semantically complex sentence \\
SO        & Complex   speech opening                 \\
CP        & Conference   programme                   \\
\lspbottomrule
\end{tabular}
\caption{Test speech tasks\label{tab:3}}
\end{table}

\begin{table}[p]
\begin{tabularx}{\textwidth}{llQQ}
\lsptoprule
Code & Name    & Definition & Source speech segment\\ \midrule
AC   & Isolated acronym & One acronym in a \textit{simple} sentence (20--30 words, with simple syntax and logic and no further problem trigger). & The signing of the AfCFTA {[}\textit{the African Continental Free Trade} \textit{Area}{]} by the African Member States significantly strengthens the movement towards this goal.\\
\lspbottomrule
\end{tabularx}
\caption{Test speech task example\label{tab:4}}
\end{table}

The test speech tasks may be grouped by their level of complexity as follows:

\begin{enumerate}
    \item \textit{Tasks of low complexity}: AC, NE, NU, and TE are the tasks of lowest complexity, consisting of a 20–30-word sentence characterised by simple syntax and logic and presenting only one problem trigger.
    \item \textit{Tasks of medium complexity}: TL, TS and NR are slightly more complex given that several problem triggers co-occur within a sentence.
    \item \textit{Tasks of high complexity}: the remaining tasks may be regarded as highly complex given the co-occurrence of several problem triggers in the speech passage constituting the task.
\end{enumerate}

After conceptually defining the tasks, I chose a topic and a communicative context that I predicted would be unknown to most interpreters to increase the likelihood that the items would be unknown to most of my study participants (i.e., a speech taking place at the African Union). After selecting the topic, I started creating speech passages matching my previously defined task criteria, as represented in Table \ref{tab:4}.

Once I crafted all my tasks, I assembled them into a speech alternating tasks and discursive speech passages following a random order. The speech opened with a 45-word introduction free of problem triggers as a ``warm-up time'' for interpreters. It then presented the sections: greetings to participants, conference programme, body, and conclusion. In the body of the speech, tasks were followed by a ``control sentence'', providing a spill over region, and 0.3-second pause, when the speech was video-recorded. Each control sentence was approximately 30--50 words long and contained no problem trigger nor factor of difficulty. The whole speech is provided in the appendix of this volume.
\begin{quote}
\textit{Task}: The continent currently has a \textit{gross domestic product of USD 3.42 trillion.}\\
\textit{Control Sentence}: This represents a remarkable achievement if we consider the fast pace of our economic growth over the past decades. [0.3 pause]
\end{quote}
Once it was ready, the test speech was video recorded in high resolution. I video recorded the speech for the first test (the pilot study) reading it out in a native-like pronunciation. The speech was video recorded again by a native speaker of English and interpreter trainer for the second test (the main study). I repeated the same procedure to design the speech used for the practice section of the training module, which may be seen as comparable in structure and tasks.



\subsection{Test video}



The test speech was video recorded to generate a video. The speech video was then entered into a prototype of the SmarTerp CAI tool to generate the automatic aids. The output was video recorded to generate the final video showing the speaker’s video with the synchronised visual aids at constant latency of 0.2 seconds. I chose to use a recorded video rather than the live tool in the test to provide all participants with the same input. Controlling this variable was necessary for me to focus my analysis on UI design principles.



\subsection{Post-task questionnaire}


The usability of the tool is determined not just by users’ performance but also by their mental state while using the product. In the usability test of SmarTerp, a post-test questionnaire aimed to gather quantifiable data capturing interpreters’ satisfaction with the tool after having tried it in the interpreting test.

In developing the questionnaire, I considered utilising pre-existing validated tools, such as the User Experience Questionnaire. The advantage of using re\-search\nobreakdash-val\-i\-dat\-ed tools is that they offer a high degree of construct validity. However, at a preliminary assessment within the research team and the project consultant (three conference interpreters and researchers and one HCI expert), we found existing tools to be inadequate for our evaluation scenario. For instance, the UEQ requires users to express a judgment on properties of the tool through bipolar adjectives, such as ``not understandable/understandable'', ``inferior/valuable'', ``not secure/secure'', etc. We found some adjectives to be ambiguous in the context of a CAI tool, which made it necessary to either change the items or opt for a self-designed tool. However, the questionnaire could not be adjusted without compromising its validity. For this reason, I decided to design a questionnaire ad hoc for the test.

My questionnaire required users to express their agreement with statements related to some usability quality of the tool on a 7-point Likert scale. The questionnaire was hence scaled $-$3 to +3, where $-$3 represents the most negative response, 0 a neutral response and +3 the most positive. I chose a 7-point Likert scale considering the well-documented users’ tendency to avoid extreme judgments -- which is also the rationale behind the construction of the UEQ \citep{laugwitz2008construction}. The first question asked participants to express their overall satisfaction with SmarTerp to capture their overall perception of the product. The following questions asked participants to express their judgment concerning pragmatic attributes of the product: its \textit{effectiveness} (how effective SmarTerp was in supporting the SI task in participants’ view), \textit{ease of use} (how easy it was for interpreters to use SmarTerp during the test task), \textit{ease of learning} (how easy it was for interpreters to learn to use SmarTerp), \textit{timeliness} (whether interpreters found the support provided by SmarTerp well-timed), and \textit{dependability} (whether interpreters felt that they could rely on SmarTerp during the test task). Finally, I asked participants to express a judgement of the likelihood that they would include an ASR- and AI-based CAI tool into their SI workflow in the near future. I intentionally repeated this question, which I included in the enrolment questionnaire too, with the aim to explore whether the self-reported likelihood to use ASR-powered CAI tools changed after testing SmarTerp. The questionnaire is structured as follows. As shown below, I divided the questions into sections and gave them a heading for clarity, but these were not displayed in the original questionnaire.

\begin{quote}
    \textit{Part I: General Satisfaction}\\

On the whole, how satisfied are you with the CAI tool's support during the test?  --  Options: from 1 (very dissatisfied) to 7 (very satisfied)\\

\textit{Part II: Satisfaction by Pragmatic Usability Criteria}\\

Do you agree with the following statements?  --  from 1 (strongly disagree) to 7 (strongly agree)
\begin{itemize}
    \item [-] The CAI tool was easy to use (perceived ease of use)
    \item [-] The CAI tool helped me improve the accuracy of my delivery (perceived effectiveness)
    \item [-] No training is required to use the CAI tool effectively (perceived ease of learning)
    \item [-] The input provided by the CAI was timely (perceived timeliness)
    \item [-] I felt that I could rely on the CAI tool's support (perceived dependability)
\end{itemize}

\bigskip
\textit{Part III: Likelihood to Use the Tool}\\

How likely are you to use ASR-integrated CAI tools as a support during simultaneous interpreting in the near future?  --  from 1 (very unlikely) to 7 (very likely)

\end{quote}




\subsection{Semi-structured interview protocol}

In the SmarTerp usability test, a semi-structured interview complemented the analysis of users’ perceptions through the questionnaire. I decided to include an interview for three main reasons. The first was to expand the analysis of users’ needs and requirements and update the CAI tool features based on such deeper understanding. The second was to shed light on the reasons for students’ perception of the tool’s usability expressed through the questionnaire. The third was to integrate participants’ self-reported data into my interpretation of critical incidents and identification of usage problems, hence adding strength to the analysis. While in usability tests this is often accomplished by asking users to ``think aloud'' while performing test tasks, this is obviously not possible in the context of a simultaneous interpreting task.

I conducted the semi-structured interviews based on the protocol below. Interview questions in parts I and II of the protocol were aimed at shedding light on the reasons for participants’ responses to the post-test questionnaire. Part III was aimed at gleaning insight into participants’ perception of the tool’s support in the rendition of individual problem triggers. Part IV asked participants to express their judgment on the functions and design features of the tool. It is important to note that although, as discussed earlier, users’ opinions alone cannot be used to reliably derive design principles, within a mixed-method study, users’ recommendations may be compared with researchers’ observations to see whether they converge or not. Part V explored participants’ perception of the tool in a real context of use. During the interview, I adapted the questions (by reformulating them, asking for specifications, asking to report a critical incident, making open-ended statements that invited participants to fill the gap, etc.) to probe into issues of interest without influencing participants’ responses.

\begin{quote}
    \textit{Part I: General Satisfaction}\\

Your self-reported satisfaction with the tool was ... [user’s evaluation in the post-task questionnaire] Could you please explain this choice?\\

In the enrolment questionnaire, your self-reported likelihood of using a CAI tool of this kind was ... In the post-test questionnaire, it was ... Why is that?\\

\textit{Part II: Satisfaction by Pragmatic Usability Criterion}\\

Based on the enrolment questionnaire, your perceived ease of use / effectiveness / ease of learning / timeliness / dependability is (...). Could you motivate your choice? Can you recall an example?\\

\textit{Part III: Satisfaction by Problem Trigger Class}\\

Based on the enrolment questionnaire, your self-reported difficulty with acronyms / named entities / numerals / specialised terms is (...). How do you normally deal with this problem trigger? Do you believe that the tool helped you in the test? How? Can you recall an example?\\

\textit{Part IV: Design-related Recommendations}\\

What did you like the most about the tool and its interface? Why?\\

What did you like the least about the tool and its interface? Why?\\

\textit{Part V: Perception of the Tool in Context}\\

In your view, how could a CAI tool of this kind make a difference to you as an interpreter?\\

How does the tool differ from a human boothmate?\\

Which one is more reliable?\\

If you were to choose whether to use it in an online or on-site assignment or both, which option would you choose?\\
\begin{itemize}
    \item [-] If on-site or both: would you still use the help of a human boothmate? What would his/her role be in that constellation?
\end{itemize}
\end{quote}




\section{Participants}


\subsection{Recruitment}


The participants in the study were recruited through an open call for participants made circulate through the research team’s professional network and SmarTerp’s communication channels. Prospective participants who expressed their interest in participating in the study received an \textit{informed consent} to sign. The document included key information about the study objective and scope, the deadlines and the data collection and treatment procedures. They were also asked to fill out a digital \textit{enrolment questionnaire}. The questionnaire was aimed to profile and select participants based on our selection criteria: (a) 30--50 years of age, (b) more than 10 years of professional experience as an English-Italian conference interpreter in the simultaneous mode; (c) no less than 10 workdays as an English-Italian simultaneous interpreter each year, (d) no identifiable connection with the topic of the source speech (to create equal conditions for all participants). The first ten prospective participants who responded to our call and were found to fulfil our selection criteria were notified of their inclusion and provided access to the online training module.

\begin{quote}
    \textit{Part I: Participant’s Personal Information}\\

What are your name and surname?\\

What is your chosen pseudonym? (You can choose any name different from your given name as the pseudonym which will identify your data to protect your privacy. If you do not choose any pseudonym, the research team will assign one to you.)\\

What is your country/ region?\\

What’s your age group? Options: below 30 -- excluded; 30--40; 40--50; above 50 -- excluded.\\

\textit{Part II: Qualifications and Professional Activity}\\

Do you hold a Master's degree in Conference Interpreting or equivalent academic qualification? Options: yes / no.\\

Are you a member of a professional association? If yes, which one(s)?
How many years of professional experience as an English-Italian conference interpreter in the simultaneous mode do you have? Options: less than 10 -- excluded; 10--19; 20--30; above 30.\\

On average, how many days do you normally work as an English-Italian simultaneous interpreter each year? Options: less than 10 -- excluded; between 10 and 20; between 20 and 30; above 30.\\

Do you have any fields of specialisation? If yes, which one(s)?
What is the country/region of your main clients? If Africa -- excluded.\\

\textit{Part III: Current Use of Technology}\\

How many days have you worked in the remote simultaneous interpreting mode over the past 12 months? Options: less than 10 -- excluded; between 10 and 20; more than 20.\\

In how many of your last three on-site simultaneous interpreting assignments did you bring a laptop with you in the booth and use it to browse your glossary or search for unknown terms / information while you were interpreting? Options: none; 1 assignment out of three; 2 assignments out of three; 3 assignments out of three.\\

Have you ever used a computer-assisted interpreting (CAI) tool? If yes, which one and for what purpose?\\

\textit{Part IV: Perception of Problem Triggers}\\

Do you find it difficult to interpret the following items? Please, rate their difficulty on a scale from 1 (very easy) to 7 (very difficult): acronyms, named entities, numbers, specialised terms.\\

\textit{Part V: Prospective Use of Technology}\\

How likely are you to start using a technological tool with integrated automatic speech recognition during simultaneous interpreting in the near future? Choose the most suitable option on a scale from 1 (very unlikely) to 7 (very likely).

\end{quote}



\subsection{Training}


After enrolment, participants completed an asynchronous (self-paced, approx. 1.5 hours) e-course which I had previously developed and made available via the LMS Moodle. The e-course comprised the following units: (1) Theoretical introduction to in-booth CAI tool support, explaining what CAI tools are and how in-booth support works; (2) Introduction to the CAI tool SmarTerp, presenting key functions of the tool (e.g., type of support provided and latency) and key interface features (e.g., location of elements in the interface, order of appearance, mode of display, etc.); (3) Practice session, consisting in a CAI-tool assisted SI exercise of a speech equivalent to the test speech in structure and complexity but different in topic and terminology. Participants were given over seven days to complete the e-course and their activity was monitored through the user data collected by the LMS. Quizzes were embedded in each theoretical section to test participants’ knowledge. The practice session required them to upload their delivery to ensure that they did indeed complete the required training. Note that the word ``training'' in the context of this study refers to a combination of fundamental technical information with a practice session. No guidance was provided on how to effectively integrate the tool into the SI process (e.g., making participants aware of possible threats, suggesting the use of interpreting strategies, etc.).



\subsection{First iteration: Pilot study}


The participants in the first iteration of prototyping, testing and revision (i.e. the \textit{pilot study}) were five Italian conference interpreters with English as their working language (either B or C). Because the main aim of conducting a pilot study was validating the research methodology and providing an initial orientation to the design work, the inclusion criteria were not strictly held in the recruitment phase. The only criteria for inclusion were holding an MA degree in conference interpreting and being a practising conference interpreter (ITA-A, ENG-B/C). All participants signed the informed consent and completed the training phase.



\subsection{Second iteration: Main study}


The participants in the main study were selected following the criteria for inclusion detailed earlier in this section. Accordingly, all main-study participants are conference interpreters with at least 10 years of professional experience, 20 RSI working days over the past twelve months, and 30 interpreting assignments as an English-Italian simultaneous interpreter each year.


\section{Procedure}


At the time of the test, the Covid-19 pandemic made it impossible to conduct presential sessions. I also judged that conducting the test remotely would not change the nature of the interpreting task, as the SmarTerp CAI tool was intended to be used on the interpreter’s laptop and primarily in an RSI setting. I conducted the test remotely via the web conferencing platform Zoom and tested the CAI tool on each participant individually.

When the participant logged in at the agreed time, I welcomed him/her and reminded him/her of the purpose of the study. I then asked permission to video-record the session. Once recording started, the participants shared their screen and accessed the test video via a link I shared with them in the Zoom chat. The video started with a slide containing information about the communicative context of the speech (i.e. event name, speaker’s name, title of the speech, time and place) which remained visible for one minute. During this time, the participant could reflect on the communicative context but could not search for more information. After one minute, an acoustic signal announced that the speech was starting. At that point, the participant started to interpret the speech simultaneously and could use the CAI tool SmarTerp. Through screen sharing, I could record the integrated view of participant’s face and the CAI tool operating.

While the participant was interpreting, I noted significant patterns in an observation sheet that I had previously prepared. The observation sheet consisted of a table that listed the source speech tasks in their order of appearance in the test speech. The tasks were identified through a code. The empty column to the immediate left was dedicated to notes of errors or phenomena of interest that I observed while the participant was talking. Further to the left, an ``interview'' column provided a space to note comments on the critical incidents that participants spontaneously discussed during the interview. The structure of the observation sheet is depicted below (Table \ref{tab:10}).

\begin{table}
%\resizebox{\linewidth}{!}{%
\begin{tabularx}{\textwidth}{llQQQ}
\lsptoprule
{Task code} & {Item code} & {Source speech segment} & {Observations}                                                             & {Interview}                                                        \\
\midrule
task                                                         & subtask                                                     &                                                                          & notes made during the interpreting test & notes made during the interview \\
\lspbottomrule
\end{tabularx}
%}
\caption{Structure of the interview sheet\label{tab:10}}
\end{table}

After the speech finished, I asked the participant to stop sharing his/her screen and to complete the post-test questionnaire that I sent via a link in the chat. After that, the participant took a 10-minute break. In the meantime, I read their answers and integrated them into the interview protocol to ask more personalised questions (for example, instead of asking ``what do you think of the tool’s efficiency?'' I asked, ``you rated the tool’s efficiency as 5, why is that?''). After the break, I conducted the interview. I made notes on the protocol while interviewing participants but checked and completed the notes after each session, replaying the recorded interview. When, during the interview, participants spontaneously made comments on specific aspects of their delivery, or if I had a specific question for them on critical incidents, I noted the conversation in the ``interview'' column of the observation sheet.



\section{Data analysis}\label{sec:data_analysis}


\subsection{Performance data}

\subsubsection{Evaluation approach}

In \chapref{ch:interpreting_technology}, I discussed the limitations of performance measures that have traditionally been used in empirical CAI research to evaluate tool effectiveness. Terminological accuracy and number accuracy may not be taken as a measure of overall interpreting ``accuracy'' or ``quality'' and hence tool effectiveness because a term or number that is correctly interpreted but wrongly contextualised still corresponds to an incorrect interpretation. Therefore, I decided to adopt a more comprehensive approach to the evaluation of interpreters’ performance in the test, which I call, for lack of a better term, the \textit{communicative approach}.

I developed this approach in my previous research on the SI of numbers \citep{frittella2017numeri,frittella2019a}. In such work, I identified a limitation in previous studies on the SI of numbers that had analysed the interpreted numeral only. By doing so, they failed to capture severe errors such as implausible numbers, sentences left unfinished, etc. In search of a more comprehensive analysis framework, I developed the \textit{Processing Ladder Model for the SI of Numbers} \citep{frittella2017numeri,frittella2019a} inspired by \citet{chernov2004inference} \textit{Probability Prediction Model}. The Processing Ladder Model analyses several levels of meaning of interpreted numerals to identify a broader range of phenomena and error patterns.

Given that empirical CAI research is still in its infancy and most previous studies focused on the rendition of terms and numerals in isolation, I decided to apply the framework I previously developed to the present analysis. In evaluating interpreters’ performance during the test, I took into consideration the following ``layers of meaning'' in their delivery:

\begin{description}
    \item[Word:] was the item (i.e. the acronym, named entity, number or specialised term) accurately rendered?
    \item[Sentence:] was the item accurately embedded in a well-formed and complete sentence?
    \item[Text:] is the delivery internally consistent and congruous in meaning with the source speech?
    \item[Context:] does the delivery make sense and is it plausible against the background knowledge of an informed audience?
    \item[Function:] is the interpreter’s delivery equivalent in function to the message intended by the speaker?
\end{description}

I consider a delivery segment as fully accurate only if all questions above may be answered positively. I applied this rationale both in defining criteria to calculate task success rates and in the analysis of error patterns.



\subsubsection{Preparation}


To analyse participants’ performance in the test, I started from the observation sheet (in Excel) and expanded it. The worksheet hence presented the relevant parts of the source speech (i.e. the ``tasks'') that were further segmented into smaller units (i.e. the ``subtasks'') corresponding to the individual items (i.e. the problem triggers) and the surrounding elements. I transcribed the relevant segment of participants’ delivery to the right of the source speech segment. Then, I duplicated the spreadsheet and formatted one version for the quantitative analysis of task success rates and the other for the qualitative analysis of error categories.


\subsubsection{Task success rate}

To calculate the success rate of participants’ rendition of each task, I first coded the delivery segments corresponding to each subtask using the following notation:
\begin{itemize}
    \item Accurate rendition, space is left blank.
\item Error, marked as ``e'' and identifying severe semantic errors in which the meaning of the source speech passage is substantially changed in the delivery (i.e. considering the levels word to function of the communicative analysis framework described above).
\item Omission, marked as ``o'' and identifying an omission of the item.
\item Other issue, marked as ``x'' and identifying any other error of secondary importance (i.e. not corresponding to a substantial semantic error).
\end{itemize}

I then attributed a success rate to each subtask based on the following criteria:

\begin{itemize}
    \item Correct rendition (100\%): The delivery is correct and complete.
 \item Strategy (100\%): The interpreter uses a strategy that does not change the meaning of the message and does not cause a loss of information, e.g., In 2021 $\longrightarrow$ this year.
\item Minor error or missing detail (95\%, marked as ``x''): The delivery is accurate and complete, apart from a minor imperfection or a missing detail, such as a missing adjective, speech disfluency, etc., e.g.,	Giovanie Biha $\longrightarrow$ Giovanie ``Beha''.
\item Partial rendition (proportional to task content, marked as ``x''): Some element of the subtask is missing but the overall meaning is still comprehensible, e.g., Ministry of Trade and Industry $\longrightarrow$ Ministry of Trade ($-$30\%).
\item Generalisation or summarisation (30\%, marked as ``o''): The interpreter omits the item and summarises the information, e.g., coal-bed methane has a huge potential $\longrightarrow$ natural gas has a huge potential.
\item Omission (0\%, marked as ``o''): The item is omitted without the use of a strategy, which causes the whole message and its informative content to go lost.
\item Semantic error (0\%, marked as ``e''): The delivery is implausible, inconsistent or nonsensical, e.g., Africa has a GDP of USD 3.42 trillion $\longrightarrow$ Africa has a GDP of USD 3.42 million.
\end{itemize}

Finally, I calculated the task success rate as the mean value of the success rates of its constitutive subtasks. The codes and corresponding success rates are summarised in Table \ref{tab:11} below.

\begin{table}
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{.4\textwidth}lY}
\lsptoprule
{Error / Phenomenon}             & {Error Code} & {Success Rate}        \\ \midrule
{Correct rendition}              &              & 100\%                        \\
{Strategy}                       &              & 100\%                        \\
{Minor error and missing detail} & x            & 95\%                         \\
{Partial rendition}              & x            & Proportional to task content \\
{Generalisation, summarisation}  & o            & 30\%                         \\
{Omission}                       & o            & 0\%                          \\
{Semantic error}                 & e            & 0\%                          \\ \lspbottomrule
\end{tabularx}
\caption{Task success rate -- evaluation criteria}
\label{tab:11}
\end{table}



\subsubsection{Error patterns}


To identify qualitative error patterns, I started from the communicative error categories presented in Frittella \citeyearpar{frittella2017numeri,frittella2019a}. In this framework, each level of analysis of the \textit{Processing Ladder Model} presented corresponding error categories. I chose this framework because it is in line with the general communicative approach I adopted in my analysis. The levels of analysis are \textit{word, sentence, text,} and \textit{context}. I added \textit{strategy} as a separate category to ease the identification of patterns at this level. At each level of analysis, the specific codes are the following.

Analysis at the word level focuses on the delivery of problem triggers (acronyms and terms, numbers and named entities) in isolation, for which the CAI tool provides support. At the word level, the error categories are \textit{omission} (if the problem trigger is not interpreted), \textit{error} (if the problem trigger is incorrectly interpreted), \textit{partial rendition} (if only part of the problem trigger is rendered), \textit{pronunciation error} (if the problem trigger is incorrectly pronounced), \textit{gender error} (if a person’s gender is misinterpreted).

Analysis at the sentence level focuses on the accuracy of the sentence containing the problem trigger for which CAI tool support is provided. The first error category is \textit{misattribution}, if sentence components are incorrectly linked in the interpretation, for example, the address ``Honourable Soraya Hakuziyaremye, Rwanda’s MINICOM Minister; Ms. Giovanie Biha'' is interpreted as ``Honourable Soraya Hakuziyaremye; Ms. Giovanie Biha, Rwanda’s MINICOM Minister''. Sentence fragment occurs when the interpreted sentence is grammatically incomplete, for instance the referent of a numeral is missing as in the example ``Namibia produced 2.52 million carats [‘of diamonds’ is omitted]''. An error at the sentence level is registered if the problem trigger is correctly interpreted but an accompanying element of the sentence is misinterpreted, as in the sentence ``Namibia imported [rather than produced] 2.52 million carats of diamonds''.

Analysis at the text level focuses on the meaning and consistency of the speech passage containing the problem trigger. A first error category is \textit{inconsistency}, when parts of the delivery are mutually contradictory, as in the delivery sample ``by 2030, the African continent will have about 295 million people aged 15-to-64. In 2030, there will be 1 billion people aged 15-to-64 in Africa'', where the second statement contradicts the first. A second error category is \textit{distortion}, if the delivery, albeit internally consistent, differs substantially in meaning from the source speech, as in the following example:

\begin{quote}
    \textit{Source}: \ldots\, makes coal-bed methane reservoirs advantageous for commercial operations.\\
\textit{Delivery example}: coal-bed methane is hence important to boost the growth of the energy sector.
\end{quote}
Analysis at the context level focuses on the external plausibility of the interpreted message. \textit{Plausibility errors} correspond to implausible interpretation, for instance ``by 2030, Africa will be home to 1 billion people''. The error category \textit{nonsense} is attributed when the delivery does not express a logical statement, as in the example below:

\begin{quote}
    \textit{Source}: Though its contribution to the total energy mix is still modest, coal-bed methane has impressive potential.\\
\textit{Delivery example}: The energy mix is still in its first stage, but this bears great potential too.
\end{quote}
Finally, strategy categories are omission of redundant item (if the interpreter omits an item that is repeated within the numerical task), lexical substitution (if the interpreter replaces the item with an equivalent lexical element), generalisation (if the interpreter produces a sentence with general meaning), and summarisation (if the interpreter summarises the meaning of the speech unit).

The table below summarises the categories of error and strategy used in the study and shows their correspondence to the quantitative analysis measures presented in the previous section.

\begin{table}
\begin{tabularx}{\textwidth}{lrQ}
\lsptoprule
{Error Code} & {Success Rate}                                                  & {Name}              \\
\midrule
o                                                             & 0\%                                                                    & Omission                   \\
e                                                             & 0\%                                                                    & Error (word level)         \\
x                                                             & proportional to task content & Partial rendition          \\
x                                                             & 95\%                                                                   & Pronunciation error        \\
x                                                             & 95\%                                                                   & Gender error               \\
e                                                             & 0\%                                                                    & Misattribution             \\
e                                                             & 0\%                                                                    & Sentence fragment          \\
e                                                             & 0\%                                                                    & Error (sentence level)     \\
e                                                             & 0\%                                                                    & Inconsistency              \\
e                                                             & 0\%                                                                    & Distortion                 \\
e                                                             & 0\%                                                                    & Plausibility error         \\
e                                                             & 0\%                                                                    & Nonsense                   \\
                                                              & 100\%                                                                  & Omission of redundant item \\
                                                              & 100\%                                                                  & Lexical substitution       \\
o                                                             & 30\%                                                                   & Generalisation             \\
o                                                             & 30\%                                                                   & Summarisation              \\
\lspbottomrule
\end{tabularx}
\caption{Categories of error and strategy in the deliveries}
\label{tab:12}
\end{table}


\subsection{Questionnaire data}


To analyse the data gathered in the questionnaires, I entered them into an Excel spreadsheet and converted the 7-point Likert scale into a $-$3 to +3 scale. For each questionnaire item, I calculated the measures of central tendency: the mean (i.e. the average of the dataset), the median (i.e. the middle value in the dataset) and the mode (i.e. the value that occurs most often). While the mean is the most commonly used measure of central tendency in a numerical data set, it may not be a fair representation of the data because it is easily influenced by outliers, which is particularly problematic in small datasets. The median and mode are more robust, i.e. less sensitive to outliers. I, therefore, calculated all three measures of central tendency in the main study (10 participants) but only the mean in the pilot study (5 participants) because the sample size in the latter case was too small to calculate the median and mode. To compensate for this and make it possible to identify the influence of outliers on the mean, I decided to report all individual values of the questionnaire responses. To interpret the questionnaire results, I adopted the standard interpretation of the scale means in the UEQ \citep{laugwitz2008construction}:
\begin{itemize}
    \item $< -0.8$: negative evaluation
\item $[-0.8 , 0.8]$: neutral evaluation
\item $> 0.8$ positive evaluation
\end{itemize}


\subsection{Interview data}

To code the interview data, I turned the interview protocol into an Excel spreadsheet, in which each line corresponded to a question and each column to a participant. I transcribed participants’ responses into the relevant cell of the spreadsheet, paying particular attention to quoting the exact words that participants used. I partly took notes during the interview and then replayed the whole interview afterwards to complete the transcription. When I found the answer ambiguous during the interview, I asked participants to reformulate their answer and then I asked for confirmation of my interpretation of their words. I then analysed the interview data as follows. For each question, I calculated how often a certain concept emerged in participants’ answers. I divided the concepts into factors influencing participants’ perception (e.g. of the CAI tool’s features and usability) positively and negatively. Within each category, I counted how often the same concept had been expressed by study participants. I summarised the outcome in a table and translated representative quotes for the most important concepts into English.
