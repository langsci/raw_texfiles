\chapter{Conclusion}\label{ch:conclusion}
The interpreting profession is facing a phase of change due to technological advancement. CAI tools are one such technology that has the potential to become deeply entrenched in the way SI is performed. Thanks to the recent introduction of ASR- and AI-technology into CAI solutions, it is now possible for conference interpreters to utilise these tools during the complex cognitive task of SI to receive automated support for the rendition of particularly problematic items, known as ``problem triggers''. The occurrence of acronyms, named entities, numbers, and specialised terms in the source speech systematically corresponds to higher-than-normal rates of omissions, generalisations, approximations as well as severe errors and is thought to increase the processing requirements of the interpreting task. ASR-integrated CAI tools, representing an artificial boothmate for interpreters, have the potential to increase accuracy and alleviate some of the mental effort in processing these problematic items. However, CAI tools are used during a task, simultaneous interpreting, that is already extremely complex from the point of view of the numerous sub-processes taking place concurrently in the mind of the interpreter. Details matter in the design of any user interface, where seemingly small features can significantly impact users’ performance. This is all the truer in the case of a tool designed to be used in a task that is as cognitively complex as SI. Therefore, CAI tool UI design is of utmost importance to ensure that these potentially useful tools are supportive, not disruptive, for the interpreter. 

Despite the importance of CAI tool usability, the design of CAI tools has been intuitive rather than systematic and evidence based. While empirical research, for how scarce, has been conducted on the use of CAI tools in the booth, no previous study systematically evaluated the CAI tool interface with the aim to provide empirical evidence for design principles that could make the UI more usable. Differently from CAT research, which has been enriched by cross-fertilisation with the field of HCI, empirical CAI research is still in its infancy and has been mostly initiated by scientific aims. Empirical CAI research has predominantly addressed exploratory research questions (e.g. \textit{how do untrained students use CAI tools?}) or experimental hypotheses (e.g. \textit{do CAI tools lead to an improvement in interpreters’ performance?}). Study designs and measures have been consistent with the experimental research tradition in interpreting studies, but no study has explicitly drawn on a methodological and theoretical HCI framework to evaluate the usability of existing interfaces and inform their future development. Previous studies have created our current knowledge base on CAI and some methodological contributions paved the way for possible new lines of research (e.g. \cite{prandi2022a}, which offers the starting point for cognitive CAI research). However, they have not specifically addressed the tool’s usability and insights concerning these aspects were obtained incidentally rather than as a major and intended output -- with some recent exceptions \citep{montecchio2021,eabm2021b}, which, however, must be considered in the light of their limitations. Hence, it may be more appropriate to say that empirical CAI research so far has mostly provided evidence for the tool’s \textit{utility} rather than usability.

This book presented a case study of interpreter-centred design and development of an ASR- and AI-powered CAI tool, SmarTerp, and detailed the application of usability testing methods to the empirical evaluation of this solution. After a literature review of usability research methods in usability engineering, translation technology (CAT tools), and interpreting technology (CAI tools), the empirical part of the work shed light on the rationale for the development of methods and materials. The work then presented the results of the two usability tests (i.e. the \textit{Pilot Study} and the \textit{Main Study}) that were conducted with two groups of conference interpreters (no 5 and 10, respectively) to develop design recommendations and improve the solution. The study presented a convergent mixed-method design in which quantitative and qualitative performance and perception data were gathered through a CAI-tool supported SI test, a post-task questionnaire and a semi-structured interview. Other than fulfilling the practical aim of improving the UI of SmarTerp’s CAI tool, the study contributed to the field’s scientific understanding of interpreter-CAI interaction and moved some steps forward towards the development of data-driven usability heuristics for CAI tool design, as argued in the discussion. Nevertheless, given its novelty and interdisciplinary nature, the study represents a methodological contribution to empirical CAI research. The work may, hopefully, pave the way for a strand of usability-focused empirical CAI research. To this aim, the transcript of the test speeches are provided in the appendix and the study limitations, as well as possible future trajectories and methodological recommendations for future work, are addressed in the discussion.

Usability testing, along with other user research methods, is a fundamental tool to ensure that technological solutions meet conference interpreters’ needs and are suitable for the complex cognitive task they intend to support. A usability-focused line of empirical CAI research contributes to ensuring the development of interpreter-centred solutions, that may help professionals leverage technological affordances, achieve better service quality and keep up with changing market requirements.

Future work should focus not only on the ``machine side'' of interpreter-CAI interaction but also on the human side. Previous research highlighted problems that are not caused by usability issues but rather by interpreters’ improper use of CAI tools, such as their ``overreliance'' \citep{defrancq2021automatic} on the visual aids. The present work confirmed that interpreters encounter several difficulties in the use of CAI tools during SI. Despite the apparent simplicity of these tools, given the automaticity offered by ASR and AI technology, the effective integration of visual aids into the SI process appears to be far from simple. Exploring the complexity in interpreter-CAI interaction and identifying the root causes of the problems experienced by interpreters to inform the development of training resources seems highly desirable. Supporting interpreters in the use of CAI tools through training does not only offer practical benefits, i.e. increasing the effectiveness of interpreter-CAI interaction, but also has an ethical dimension. As concerns multiply with new technologies quietly nudging aside humans, assuming greater roles in the T\&I industry, helping humans leverage technological innovations to enhance their service represents a contribution to a more thoughtful, ethical system with human interpreters as the pivot of progress.

Research on CAI tool usability and research informing CAI tool training are hence complementary in supporting the profession at a time of unprecedented change. In this crucial time of a ``technological turn'', research on interpreting technology (be its focus usability or training) is needed to inform change in the field and ensure that technological development is fair and sustainable. This need calls for a redesign not just of tools but of the very role of research and the researcher. To live up to its social responsibility, research will need to work as the link between different stakeholders, ensuring that their concerns are central to technological development.
