\chapter{Usability engineering}\label{ch:usability_engineering}

This chapter is an introduction to usability engineering -- a set of practices used in the development of interactive systems to ensure their adequacy to users’ needs and their usability. It provides the rest of the work with conceptual and methodological instruments that will then be applied to the analysis of previous research on translation technology (Chapter \ref{ch:translation_technology}) and interpreting technology (Chapter \ref{ch:interpreting_technology}), as well as to design of the study on the CAI tool SmarTerp (Chapter \ref{ch:methods}) and the analysis and interpretation of findings (Chapters \ref{ch:first_iteration} to \ref{ch:discussion}). The present chapter first defines the concept of “usability” and how this abstract concept may be specified into measurable components. It then characterises usability engineering as the different research activities that are conducted in the process of designing and developing interactive systems by this approach. It then reviews different evaluation methods that are used at different development stages to validate and refine concepts and prototypes. Finally, the chapter presents the key methodological tenets of usability testing, a crucial method in usability engineering.


\section{The concept of usability}


Usability is a crucial concept in the study of human-computer interaction (HCI) and the development of the concept of usability and that of the discipline have gone hand-in-hand. HCI is fundamentally ``the study of people, how they think and learn, how they communicate, and how physical objects are designed to meet their needs'' \citep[7]{lazar2017research}. It is a discipline concerned with the theoretical aspects of product design, with the procedural aspects of how to achieve a good design and with the practical aspects of informing specific products. In this respect, \citet[10]{lazar2017research} distinguish between \textit{technical HCI research} (focused on interface building) versus \textit{behavioural HCI research} (focused on cognitive foundations). Irrespective of the specific focus of different strands, HCI is concerned with the \textit{practical relevance} of research outputs: ``HCI research must be practical and relevant to people, organizations, or design. The research needs to be able to influence interface design, development processes, user training, public policy, or something else'' \citep[7]{lazar2017research}.

The birth of HCI as a discipline is believed to coincide with the first conference on Human Factors in Computing Systems in Gaithersburg (Maryland, United States) in 1982 \citep[1]{lazar2017research}. In the previous decade leading up to the conference, computers had been slowly moving from research laboratories into the home and the office. With this move, the use of computers was no longer the exclusive realm of technicians and, consequently, interfaces had to be designed to be used by laypeople: ``The interaction between the human and the computer was suddenly important. Nonengineers would be using computers and, if there wasn’t a consideration of ease of use, even at a basic level, then these computers were doomed to failure and nonuse'' \citep[2]{lazar2017research}.

At that time, ``computer vendors first started viewing users as more than an inconvenience'' \citep[4]{nielsen2010}. They realised that excessive complexity put people off and represented a major barrier to the reception of a product. Nielsen convincingly explains the problem in the context of web design: ``If a website is difficult to use, people \textit{leave}. If the homepage fails to clearly state what a company offers and what users can do on the site, people \textit{leave}. If users get lost on a website, they \textit{leave}. If a website's information is hard to read or doesn't answer users' key questions, they \textit{leave}'' (\citealt{nielsen2012}, emphasis in the original). This realisation led to the emergence of the concept of “user-friendliness”, later “usability” \citep{bevana1991usability,nielsen2010}, and to intensified efforts to make products “usable”. The launch of the Apple Macintosh made a strong case for the importance of usability:
\begin{quote}
    Introduced to the public during the 1984 Super Bowl, the Macintosh seemed to make the case that ease of use sells. It was the symbol of a well-designed product. While usability was only one of the factors that made the Macintosh a household word, it was the most often mentioned factor in the growing, but small, market share that the Macintosh captured. In my view, the Mac changed the level of the argument for investing in usability. Before this period, I had to argue with clients that usability itself was important. Afterward, the high-tech world simply assumed that usability was essential. \citep[56]{dumas2007great}
\end{quote}
The term usability seems to have first emerged in the 1980s as a substitute for the term ``user friendly'' which engineers realised had ``a host of undesirably vague and subjective connotations'' \citep{bevana1991usability}. It was deemed inadequate for mainly two reasons:
\begin{quote}
    First, it is unnecessarily anthropomorphic – users don’t need machines to be friendly to them, they just need machines that will not stand in their way when they try to get their work done. Second, it implies that users’ needs can be described along a single dimension by systems that are more or less friendly. In reality, different users have different needs, and a system that is ``friendly'' to one may feel very tedious to another. \citep[4]{nielsen2010}
\end{quote}
\citet{bevana1991usability} distinguish different views of what usability is and how it is measured. Based on the product-oriented view, usability can be measured in terms of the ergonomic attributes of the product. This view implied that ``usability could be designed into the product, and evaluated by assessing consistency with these design guidelines, or by heuristic evaluation'' \citep[143]{bevan2015iso}. Consistent with this view, major research efforts in the 1980s and 1990s were invested in identifying the attributes that made a product usable. According to the \textit{user-oriented view} \citep{bevana1991usability}, usability can be measured in terms of the mental effort and attitude of the user. In the \textit{user performance view} \citep{bevana1991usability}, usability can be measured by examining how the user interacts with the product, with particular emphasis on either ease-of-use (how easy the product is to use) or acceptability (whether the product will be used in the real world). The user-oriented and user-performance views emphasise that usability is contingent upon who used the product and with what goals \citep[143]{bevan2015iso}. Representatives of this approach advocated that usability should be operationalised and evaluated in terms of actual outcomes of people interacting with the product and their satisfaction \citep{whiteside1988usability}. These views are complemented by the \textit{contextually-oriented view} \citep{bevana1991usability}, that usability of a product is a function of the particular user or class of users being studied, the task they perform, and environment in which they work. Consistent with a contextually-oriented view of usability, Lewis highlights that usability is ``an emergent property that depends on the interactions among users, products, tasks and environments'' \citep[1267]{lewis2012}. Hence, usability should not be considered as a fixed concept. It emerges from the interaction among different agents, called \textit{product} (i.e. the design element, also called \textit{interactive system}) and \textit{user} (i.e. the person for whom the product is designed) and it is contingent upon the physical as well as the extended context in which the interaction occurs (the \textit{environment}) and goal-directed behaviour (the \textit{task}) that the user attempts to enact via the product.

Foregrounding the utilitarian aspect of using a product, usability may be considered as a  component of a product’s acceptability, as proposed by \citet{nielsen2010}, a reference figure in the field both as a scholar and as a practitioner and founder of the Nielsen Norman Group (NNG). Nielsen's conceptualisation links usability to other components of the product and highlights the considerations that usability must trade off against in a development project  \citep{nielsen2010}.  In Nielsen’s view, ``usability is a narrow concern compared with the larger issue of system acceptability, which basically is the question of whether the system is good enough to satisfy all the needs and requirements of the users and other potential stakeholders, such as the users’ clients and managers''  \citep[4]{nielsen2010}. Based on this conceptualisation, usability is linked to a product’s acceptability, usefulness, and utility (\figref{fig:1}). The overall \textit{acceptability} of a product is a combination of its social acceptability and its practical acceptability \citep{nielsen2010}. \textit{Social acceptability} concerns issues such as ethics and safety that need to be fulfilled before we can even consider other aspects \citep{nielsen2010}. For instance, there is not much point in worrying about the usability of an app for firearm trafficking. Provided that a product is socially acceptable, we can further analyse its \textit{practical acceptability} within various categories, including cost, support, reliability, compatibility with existing systems, as well as the category of usefulness  \citep{nielsen2010}. \textit{Usefulness} is the issue of whether the system can be used to achieve some desired goal and it can be further broken down into the categories of utility and usability \citep{nielsen2010}. \textit{Utility} is the question of whether the \textit{functionality} of the system in principle can do what is needed and \textit{usability} is the question of how well users can use that functionality \citep{nielsen2010}. We could say that utility is a matter of concepts and functionality and usability of how that concept is translated into design features.

\begin{figure}
 %   \includegraphics[width=0.8\linewidth]{figures/Picture1.png}
    
    \begin{forest}
    forked edges,
    for tree = {
    grow' = east,
    anchor=west,
    font = \small,
    }
    [System\\acceptability
        [Social\\acceptability,align=left]
        [Practical\\acceptability,align=left
            [Usefulness
                [Utility]
                [\textit{Usability}
                    [Easy to learn]
                    [Efficient to use]
                    [Easy to\\remember,align=left]
                    [Few errors]
                    [Subjectively\\pleasing,align=left]
                ]
            ]
            [Cost]
            [Compatibility]
            [Reliability]
            [etc.]
        ]
    ]    
    \end{forest}
    \caption{A model of the attributes of system acceptability (from \cite[6]{nielsen2010})}
    \label{fig:1}
\end{figure}

Usability itself is further defined by specific \textit{attributes}, which represent the starting point for operationalising the concept, identifying measures and indicators, hence allowing to evaluate the system. Different sources propose different attributes to define the usability concept. For instance, \citet{nielsen2010} proposes that a product’s usability is given by its \textit{learnability} (``the system should be easy to learn so that the user can rapidly start getting some work done with the system''), \textit{efficiency} (``the system should be efficient to use so that once the user has learned the system, a high level of productivity is possible''), \textit{memorability} (``the system should be easy to remember so that the casual user is able to return to the system after some period of not having used it without having to learn everything all over again''), \textit{errors} (``the system should have a low error rate so that users make few errors during the use of the system, and so that if they do make errors they can easily recover from them. Further, catastrophic errors must not occur''), and, finally, users’ \textit{satisfaction} (``the system should be pleasant to use so that users are subjectively satisfied when using it'').

The norm  9241-11 on the Ergonomics of Human Computer Interaction \citep{iso2018}, previously \citet{iso1998}, attempts to combine the user-oriented view of usability with the performance-oriented and the contextually-oriented views. In the norm, usability is defined as ``the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use'' \citep{iso2018}. Hence, to be defined as ``usable'' a product must support \textit{specified users} (i.e. not just any user but those for whom the product was designed) in accomplishing \textit{specified goals} (i.e. not just to perform any task but those tasks that the product aims to support). The outcome of using the product should be both \textit{objectively} successful (i.e. effective and efficient) and \textit{subjectively} successful (i.e. satisfactory). Within the same standard, more specific definitions of the effectiveness, efficiency and satisfaction with which a goal is achieved are provided. \textit{Effectiveness} is defined in terms of the accuracy and \textit{completeness} of users’ performance and the protection from severe consequences in case of negative outcomes (inaccurate and incomplete performance). \textit{Efficiency} is defined in terms of the resources (time, human effort, costs and material resources) that are expended when users attempt to achieve the goal (e.g. the time to complete a specific task). \textit{Satisfaction} is defined as ``the extent to which attitudes related to the use of a system, product or service and the emotional and physiological effects arising from use are positive or negative'' \citep{iso2018}. Measures for the components of effectiveness, efficiency and satisfaction defined in ISO 9241-11 are provided in ISO/IEC 25022 on the measurement of quality in use \citep{iso2016}, as summarised below in \tabref{tab:1}.

\begin{table}
\begin{tabular}{lll}
\lsptoprule
Effectiveness & Efficiency & Satisfaction\\\midrule
Tasks completed &  Task time & Overall satisfaction \\
Objectives achieved & Time efficiency & Satisfaction with features \\
Errors in a task & Cost-effectiveness & Discretionary usage \\
Tasks with errors & Productive time ratio & Feature utilisation \\
Task error intensity & Unnecessary actions & Proportion of users \\
 & Fatigue &\hspace*{1em}complaining\\
 &  & Proportion of user complaint\\
 &  & \hspace*{1em}about a particular feature\\
 &  & User trust \\
 &  & User pleasure \\
 &  & Physical comfort
\\ \lspbottomrule
\end{tabular}
\caption{Measures of effectiveness, efficiency and satisfaction (from \cite{bevan2016new})\label{tab:1}}
\end{table}

As explained by \citet{bevan2016new}, the inclusion of the satisfaction dimension in the revised version of the ISO standard acknowledges the increasing call from several authors to go beyond usability in purely utilitarian terms and consider the complexity of goals that people aim to achieve through products, not just of practical but of emotional and social nature (e.g. \cite{burmester2002usability}). Proponents of this view encouraged an increased attention for users’ motives for using products and their emotional response to the product. This led to the introduction of the term \textit{user experience} (UX) alongside usability which may be defined as ``a person’s perceptions and responses resulting from the use and/or anticipated use of a product, system or service'' \citep{iso2010}. Some authors incorporate usability within UX. For instance, in \citegen{hassenzahl2003thing} UX model, usability is a \textit{pragmatic attribute} of the user experience alongside utility. By contrast, \textit{hedonic attributes} (stimulation, identification, and evocation) emphasise individuals’ pleasure in the use of the product.

\section{Usability engineering}\largerpage

At the origins of HCI, the question of how to ensure that a product is usable was first addressed with the methods of experimental psychology \citep{lewis2012} -- suffice it to say that \citegen{shneiderman_1980} \textit{Software Psychology} is considered to be one of the first books on the topic of HCI. The first graduate programmes in HCI emphasised the use of research methods from the behavioural sciences \citep[55]{dumas2007great}. HCI later grew to incorporate methods and characteristics of other disciplines, such as ethnographical methods derived from the social sciences \citep{lazar2017research}. This evolution characterises HCI as a multifaceted and interdisciplinary field until today. However, at its onset, the discipline lacked an identity of its own: ``HCI was viewed, I believe, as a new area in which to apply traditional methods rather than a one requiring its own unique methods'' \citep[55]{dumas2007great}. In Dumas’ reconstruction, the ``leap forward'' in the discipline’s development happened when a series of publications argued for a new approach to product design and evaluation, which they called \textit{usability engineering} \citep{whiteside1988usability}.

The main driving force that separated HCI, and usability engineering specifically, from the standard protocols of other research traditions was the need to inform the design of products rapidly, at an early stage and throughout the development process \citep[5]{lewis2012}. As Buxton and Sniderman recount:

\begin{quote}
    Faced with a problem whose solution can not be derived from the literature, the designer of the human interface is confronted with two alternative strategies. On the one hand, a scientific approach can be followed, in which formal experiments are run, in an attempt to fill in existing gaps in our knowledge. On the other hand, an engineering approach can be taken, in which some ad hoc strategy is followed in order to deliver a system which was typically needed ``yesterday''. Due to pragmatics, the latter approach is by far the most prevalent. \citep[2]{buxton1980iteration}
\end{quote}

There were several arguments in favour of an engineering approach to product design and development. One pragmatic argument was related to costs: ``Under typical resource constraints, modifications will be feasible only in the prototyping stage. It is much too expensive to change a completely implemented product, especially if testing reveals the need for fundamental changes in the interface structure'' \citep[13]{nielsen2012}. Furthermore, once the product-oriented view of usability (see above) revealed its limitations, HCI experts realised that usability could not simply be designed into a product by following a set of pre-defined and universally valid rules \citep[143]{bevan2015iso}. Awareness developed that ``design is complex, and there simply is no cookbook approach to design that can rely on general principles and guidelines alone'' \citep[918]{mayhew2007requirements}. While some fundamental principles and analogue precedents could inform the design, they could not alone ensure that a product was usable. Each product had to be treated as a unique piece and considered in its context to evaluate and improve its usability. What makes each project unique is that the design fundamentally involves human beings ``who are, to put it mildly, complex'' \citep[1]{lazar2017research}. Unlike in other research areas, in usability engineering the complexity of individuals cannot be discarded as a confounding variable. The designer is guided in his/her design choices by that very complexity.

The reasons why a usability engineering approach is needed are convincingly explained in \citet{nielsen1993usability} ``usability slogans'' -- of which I will report and discuss the most relevant for the present discussion. A basic reason for the existence of usability engineering is that (1) \textit{Your Best Guess Is Not Good Enough}: ``it is impossible to design an optimal user interface just by giving it your best try. Users have infinite potential for making unexpected misinterpretations of interface elements and for performing their job in a different way than you imagine'' \citep[10]{nielsen1993usability}. The designer should be aware that any initial attempt at a user interface design will include some usability problems and acknowledge the need to modify the original design to accommodate the user’s problems. In other words, (2) \textit{The User Is Always Right}: ``The designer's attitude should be that if users have problems with an aspect of the interface, then this is not because they are stupid or just should have tried a little harder'' \citep[11]{nielsen1993usability}. If the user is always right, then why not ask them what they want to see in an interface, use the most recurring requests as a bottom line and provide plenty of customisation flexibility to accommodate the variability in users’ wishes? The reason is that, unfortunately, (3) \textit{The User Is Not Always Right}. Users ``do not know what is good for them ... Users have a very hard time predicting how they will interact with potential future systems with which they have no experience ... Furthermore, users will often have divergent opinions when asked about details of user interface design.'' \citep[11--12]{nielsen1993usability}. Another reason why the interface design cannot be left up to the user is that (4) \textit{Users Are Not Designers}:

\begin{quote}
    Studies have shown, however, that novice users do not customize their interfaces even when such facilities are available [Jergensen and Sauer 1990]. One novice user exclaimed, ``I didn't dare touch them [the customization features] in case something went wrong.'' Therefore, a good initial interface is needed to support novice users. Expert users (especially programmers) do use customization features, but there are still compelling reasons not to rely on user customization as the main element of user interface design. \citep[12]{nielsen1993usability}
\end{quote}
At the same time, designers cannot just trust their intuition because (5) \textit{Designers Are Not Users} and because they are not users, they will not understand users’ needs and struggles with the interface:

\begin{quote}
    When you have a deep understanding of the structure of a system, it is normally easy to fit a small extra piece of information into the picture and interpret it correctly. Consequently, a system designer may look at any given screen design or error message and believe that it makes perfect sense, even though the same screen or message would be completely incomprehensible to a user who did not have the same understanding of the system. Knowing about a system is a one-way street. One cannot go back to knowing nothing. It is almost impossible to disregard the information one already knows when trying to assess whether another piece of information would be easy to understand for a novice user. \citep[13]{nielsen1993usability}
\end{quote}
To understand why ``knowing about a system is a one-way street'', think of the picture puzzles where an animal picture is hidden. It may take a long time to figure out where the animal hides, but once you recognise it, you can’t help seeing it. In the same way, designers know what is the best way to use the product they designed. They know what actions lead to a desired result and how to solve problems when they occur. Hence, they will not encounter the problems that a real user would. Unfortunately, when designing interactive systems, (6) \textit{Details Matter} \citep[15]{nielsen1993usability}, which means that even seemingly minor interface details can drastically alter the overall usability of the interface, a further reason why a systematic usability engineering approach is needed to ensure that the product meets users’ needs and is usable.

The fundamental traits of a usability engineering process were first articulated by \citet{gould1985designing}. The first trait is the \textit{early focus on users and tasks}: the designer first seeks to understand who the users are, in terms of their cognitive, behavioural, social and attitudinal characteristics, and the nature of the work to be accomplished. The second is \textit{empirical measurement}: intended users, i.e. representative of the users for whom the product is designed, should use product simulations and prototypes early in the development process to carry out real work, and their performance and reactions should be observed, recorded, and analysed. The third trait is \textit{iterative design}: when problems are found in user testing, they must be fixed improving the solution. This means that the development process takes the form of several cycles of design, testing and improvement. Through each cycle, the solution is progressively refined.

We can hence see that usability engineering consists of a series of what we may call “design-focussed research activities” spanning throughout the whole product development process. The process for the ``design of usable systems'' was later conceptualised by \citet{gould1988design} as consisting of four phases, which he called the ``gearing-up phase'', the ``initial design phase'', the ``iterative development phase'', and the ``system installation phase''. Today, the traits of the engineering approach may be found in popular design models such as the \textit{Usability Engineering Lifecycle} (e.g. \cite{mayhew2007requirements,nielsen1992usability,nielsen1993usability}), \textit{User Centred Design} (e.g. \cite{still2017fundamentals}) and \textit{Design Thinking} (e.g. \cite{brown2009}). Although they operationalise the design and development process differently, they all roughly present phases similar to Gould’s conceptualisation. For the sake of simplicity, I will call these phases \textit{analysis}, \textit{design}, \textit{development}, and \textit{implementation}. In the \textit{analysis} phase, research activities focus on users and their tasks, as well as other aspects of the context in which the product is developed, such as existing competitor products. In the \textit{design} phase, a concept, serving as the blueprint for the development, is created and validated. In the \textit{development} phase, prototypes of increasing complexity are developed, tested, and refined iteratively. In the phase of \textit{implementation}, the finished product is evaluated. Although these phases are presented as sequential, a substantial difference exists between processes in which these phases are executed in sequence (i.e. the Waterfall or Liner Sequential Life Cycle Model) or in an iterative, cyclical fashion (i.e. Spiral or Agile models; cf. \cite{van2017barriers}).

The research activities in each phase have a different focus ad require different methods. The analysis phase draws on methods such as survey, interview, and contextual inquiry to understand users, their tasks and needs that the product should fulfil. Other activities such as review of relevant standards and product requirements are conducted to establish initial design principles. The evaluation of the product concept and its prototypes during the design and development phases may be described as \textit{formative evaluation} whereas evaluation of the finished product, in the implementation phase, may be described as \textit{summative evaluation} (cf. \cite{pyla2005ripple}) -- concepts imported from educational science \citep{scriven1967methodology}. In the context of usability engineering, formative evaluation is explicitly aimed at informing design decisions and improving the solution \textit{during} product design and development. Summative evaluation is conducted to ascertain some quality aspect of the product or its impact \textit{after} it has been developed. Different types of evaluation at different stages of the development process call for different methods.


\section{Empirical evaluation in usability engineering}

Empirical evaluation during and after product development is the fundamental tool to ensure the usability of a product. For all the reasons explained above, each product is unique. Because designers do not know what will make that specific product satisfactory for the target users, constant empirical evaluation is needed to bridge the gap between designers and users.

Methods are usually broadly classified as either inspection or testing (e.g. \cite{holzinger2005usability}). During \textit{usability inspection} \citep{nielsen1994usabilityinsp}, an evaluator inspects a functioning or non-functioning prototype of the product. Inspection is aimed at generating results quickly and at low costs. Usability inspections are generally used early in the design and development phase and may involve non-functioning prototypes, such as paper prototypes. Paper prototypes capture the essential features of what the product will look like and allow for the evaluation of the initial concept before resources are invested in the development. Inspection methods generally involve experts and do not require them to accomplish real tasks with the functioning prototype. Typical methods include \textit{feature inspection}, in which the evaluator reviews the list of product features intended to accomplish tasks, the \textit{cognitive walkthrough}, in which an evaluator progresses through the steps in accomplishing the task through the product and simulates the problem-solving that the user engages in, and \textit{heuristic evaluation}, in which evaluators compare the prototype against some pre-defined requirements (called heuristics).

By contrast, \textit{testing} requires that the system be used by real users. Users should be representative of the target group not only in broad demographic terms (such as age, educational experience, technical experience, etc.), but also in terms of the task domain; for instance, that means that to study interfaces designed for lawyers, you must actually have practising lawyers taking part in the research \citep[6]{lazar2017research}. Testing offers higher reliability than inspection methods, but it is more time consuming and costly. Testing is used to identify and correct usability problems or to optimise the design.

Based on the stage of development and the specific aims, testing methods may vary in characteristics such as the setting (whether it is artificial or naturalistic), what is being evaluated (whether they are aimed at capturing users’ performance or perception), how it is evaluated (whether predominantly qualitatively or quantitatively). The test \textit{setting} may be either artificial, such as the company’s premises or a laboratory, or naturalistic, i.e. the real context of use. Studies in fully naturalistic settings are only possible at the very final stage of development or in the implementation phase. An example of a fully naturalistic test is a \textit{field study}, in which users are observed naturally interacting with the product after its release. An artificial setting usually also calls for a higher degree of experimental control in the test tasks. Examples include \textit{usability testing}, which requires test users to use the functioning prototype to accomplish a set of tasks, \textit{A/B testing} when two design alternatives are tested in controlled conditions (or A/B/C testing if three alternatives are involved), \textit{think aloud}, where users are required to think aloud as they perform real tasks with the functioning prototype.

In line with the definition of usability being related both to how users perform on tasks through the product (the dimensions of efficiency and effectiveness) and what they think of the product (the dimension of satisfaction), testing may gather data both related to users’ \textit{performance} with the product and \textit{perception} of the product. Both types of data have their own limitations and both represent necessary evidence for the product’s usability \citep{bevana1991usability}. Performance data is more reliable as it shows what actually happens when users interact with the product. For instance, while a user may claim to find the product “helpful”, an analysis of his/her performance may reveal that s/he performs better without the product. At the same time, performance data, especially of quantitative nature, can only be used to describe what happens but it can hardly clarify why. The internal state of users is important in providing further evidence for a products’ usability. For instance, in some circumstances, accurate performance may be achieved by the user only at the cost of considerable mental and physical effort. Furthermore, especially qualitative perception data may help develop an explanation of why certain phenomena occur in the interaction with a product. However, perception data alone can hardly be a reliable and accurate descriptor of a product’s usability.

The analysis of performance may focus on two main aspects \citep[1]{lewis2012}: one is \textit{measurements} related to the accomplishment of task goals, i.e. of quantitative nature, and the other is related to the identification of “impasses” (such as signs of discomfort and confusion) in the interaction with the product, called \textit{usage problems}, i.e. of rather qualitative nature. Examples of performance measures that are commonly used to evaluate the usability of a product are those provided in ISO/IEC 25022 on the measurement of quality in use \citep{iso2016}, as explained earlier. The identification of usage problems requires the observation of users’ performance and an interpretive process on the researcher’s side, as will be explained in greater detail in the following section on usability testing. \textit{Quantitative performance} data may be used for (semi-)summative purposes to identify to what extent the product or the prototype meets usability goals. Furthermore, this type of analysis is only applicable to those tasks that can be separated out, quantified, and measured \citep[8]{lazar2017research}. \textit{Qualitative performance} data is used for formative purposes and it is deemed as essential to improve the product: ``Dr White’s team knows that they cannot determine if they’ve met the usability performance goals by examining a list of problems, but they also know that they cannot provide appropriate guidance to product development if they only present a list of global task measurements'' \citep[3]{lewis2012}.

Data related to users’ perception, also called “attitude”, is most commonly gathered through post-task questionnaires and interviews. Questionnaires may include both open and closed questions. Most commonly, pre-designed questionnaires like the User Experience Questionnaire (UEQ, \citealt{laugwitz2008construction}), the Systems Usability Scale (SUS, \citealt{brooke1996sus}), the NASA Task Load Index (NASA TLX, \citealt{hart1988development}) are used to obtain a quantitative description of users’ perception. These instruments were developed and validated through research and are commonly used both in scientific work and in the evaluation of industrial products. Quantitative perception data may be used for formative purposes, to observe how users’ evaluation of the products’ usability changes as some design elements are modified through iteration cycles. It may be used for summative purposes by comparing the results against users’ evaluation of analogous products of the same class or by matching them against industry benchmarks. Qualitative perception data gathered through interviews is used to gain a deeper understanding of what drives users’ perception, thereby yielding a deeper understanding of target users, which may feed back into the analysis phase.



\section{Usability testing}

Among all methods that may be used to improve a product’s usability, usability testing is often described as the royal road to usability. For example, Nielsen qualified it as ``the most fundamental usability method'' and claimed that it is ``in some sense irreplaceable, since it provides direct information about how people use computers and what their exact problems are with the concrete interface being tested'' \citep[165]{nielsen1993usability}. The major goal of usability testing is ``to be practical and have a major impact'' \citep[266]{lazar2017research}, or in other words to identify problems and propose data-driven solutions capable of improving the design of the product.

Since the term is used with some ambiguity, a terminological clarification is required before talking about its specific characteristics. Some authors have used the term in a broad sense to indicate any process or activity that aims to improve the ease of use of an interface, hence as a synonym of usability engineering \citep[267]{lazar2017research}. Other authors have referred to usability testing as ``user testing'' (e.g. \cite{nielsen1993usability}), which may refer to any type of testing involving real users (as opposed to inspection methods). Another alternative has been ``user research'', although user research, as explained in \citet{goodman2012observing}, actually has a broader meaning and includes a family of inquiry methods and elements of design and development, such as personas, user profiles, card sorting, and competitive research which are not regarded as testing. Furthermore, in usability testing, the primary research focus is the product, not the user, although as Lazar and colleagues point out it ``can be used to learn more about how people interact with interfaces, even when the goal is not fixing the interface, but instead learning more about users and interactions. So, in usability testing, maybe we are researching the user?'' \citep[263]{lazar2017research} 

Having clarified what it is not, usability testing may be defined as a test method conducted on representative users attempting representative tasks, on early prototypes or working versions of products \citep{lewis2012}. This definition entails the key tenants of usability testing. First, usability testing involves representative users. Given that usability is established in relation to the relevance of a product for a particular user and aim, study participants should be representative of the target users for which the product was designed. During the test, users work with a working prototype or the product in action to accomplish representative tasks. This means that the test tasks should be real and meaningful to them, and they should represent as closely as possible the tasks that users typically accomplish with the product in real life. Furthermore, the test task should lead participants to interact with all product functions for which testing is required.

\citet{seewald2004kritischen} define usability testing as the systematic observation of users in a controlled environment to ascertain how well they can use the product. Hence, usability testing possesses methodological characteristics both of experimental research (cf. also \cite[23]{rubin2008handbook}), because test tasks should be adequately designed to be representative, and of ethnography, in the use of observation. Furthermore, the analysis of performance is often corroborated with perception data to tap into participants’ perception and interpretation of their experience with the product, which may be gathered both through post-task questionnaires and interviews (\cite[267]{lazar2017research}, \cite[65]{rubin2008handbook}). Verbal protocols, such as think-aloud, may also be used to uncover the mental processes inherent to task execution. However, thinking aloud impacts some aspects of performance (such as time on task) and therefore it excludes the possibility to use such metrics in the analysis. Usability testing may hence be defined as a type of convergent mixed-method research \citep{creswell2017research}.

Users’ performance on test tasks may be analysed \textit{quantitatively} -- on usability metrics discussed earlier such as time on task, success rate, etc. -- to ascertain the extent to which a certain product feature leads to desired performance. However, usability testing usually has a strong \textit{qualitative}, \textit{observational} component \citep{seewald2004kritischen}. The observation of users’ interaction with the product is aimed at identifying ``flaw areas of the interface that need improvement'' \citep[264]{lazar2017research}. An interface flaw is some aspect of the interface which is confusing, misleading, or generally suboptimal \citep[264]{lazar2017research}. Interface flaws should cause a problem for most people. It is not about font style or colour preferences, but rather about an element that most people stumble upon with consequences on performance. The manifestation of problems during the interaction is called a \textit{critical incident} \citep{seewald2004kritischen}. A critical incident is constituted of a task-related goal, an ``activity'' of the user (an action as well as a verbal or non-verbal expression) which is discordant with the goal, and a consequence \citep{seewald2004kritischen}. An error in the execution of the task, users’ facial expressions (e.g. frowning, squinting at the screen, etc.) and verbal reactions (e.g. ``what?!'' or ``where is the button?'') all point to them experiencing some form of problem or discomfort with the product and conflicts with the goal of effective and efficient interaction. The interpretation of a common class of critical incidents as being caused by a specific design element leads to the identification of usage problems \citep{seewald2004kritischen}.

The identification of usage problems lies at the centre of usability testing. According to \citegen{seewald2004kritischen} model, the usability testing process is constituted of the following five steps:
\begin{enumerate}
    \item \textit{Experience} (data collection): critical incidents are identified through observation and questioning of study participants.
    \item \textit{Construction} (data analysis): the critical incidents are coded or classified.
    \item \textit{Interpretation}: a possible cause for a common class of incidents, representing a usage problem, is identified.
    \item \textit{Prioritisation}: usage problems are ranked based on the researcher’s judgment of the ``severity'' or ``impact'' of the problem or more subjective measures such as the ``frequency'' of the problem. An indicator of the stability of the occurrence (``Auftretensstabilität'', \cite[145]{seewald2004kritischen}) can be the number of participants who made a certain mistake, as well as quantitative performance indicators such as task completion rate, success rate, error ratio, etc.
    \item \textit{Recommendations}: based on the interpretation of usage problems, the researcher develops recommendations on how to improve the product, often collaboratively with the design team.
\end{enumerate}
Given that usability tests are often conducted with observational techniques and data are collected combining multiple sources, the studies may be very time intensive and therefore allow for only a limited number of participants. The ideal number of study participants for a usability test is still a matter of debate in the scientific community. \citet{nielsen1993mathematical} developed a mathematical model suggesting that a test on five users can uncover 85\% of usage problems. More recent studies have suggested that the ideal sample size is $10\pm2$ users \citep{hwang2010number} or even more than ten users \citep{schmettow2013many} for high-risk systems. It is generally accepted that qualitative usability testing can be based on around five users if the primary aim is to improve a product and/or to deepen understanding of the user through in-depth interviews. Quantitative usability testing, by contrast, aims to find metrics that predict the behaviour of a larger population and requires a larger sample size.
