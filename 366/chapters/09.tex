\chapter{Discussion}\label{ch:discussion}

The analysis of usability test data in Chapters~\ref{ch:first_iteration} and \ref{ch:second_iteration} yielded practical recommendations for the improvement of SmarTerp’s UI. Taking SmarTerp as a case study of interpreter-CAI tool interaction during SI and interpreting the findings against the backdrop of previous research, this chapter discusses the scientific contribution of this work. First, the chapter summarises the study’s contribution to the field’s current knowledge of interpreter-CAI interaction, both from the point of view of understanding of users’ performance and of users’ perception of CAI tools. Afterwards, the chapter discusses possible evidence about the need to train novice users of CAI tools. In a next step, based on the insights gained from this study, the chapter suggests (albeit only tentatively) general heuristics of CAI tool UI design and summarises open UI design questions. The chapter then discusses the limitations of the work, traces future research trajectories and, finally, provides methodological recommendations for future studies wishing to use the methods developed in the present work.

\section{General principles of interpreter-CAI tool interaction}


\subsection{Users' performance}


\subsubsection{Mediating variables affecting users’ performance}

This study found that several mediating variables external to the CAI tool’s UI design can influence the outcome of interpreter-CAI interaction. The first is \textit{CAI tool accuracy}. The data gathered in this study suggests that an inaccurately displayed item or an omission of the CAI tool is likely to trigger either an error or an omission in interpreters’ delivery. For instance, CAI tool omissions corresponded to delivery omissions in 93\% of cases (i.e. across 3 cases of omission interpreted by 5 participants) and in 60\% of cases in the main study (in one instance of CAI tool omission interpreted by 10 participants). The second mediating factor is \textit{task complexity} (a variable that had not been accounted for in the design of previous studies, cf. \chapref{ch:interpreting_technology}). In the present study, task complexity was defined by the density of problems triggers in the speech passage to be interpreted and the complexity of the semantic relationships connecting them, whereas other potential factors of complexity, such as complex syntax, were controlled for (cf. \chapref{ch:methods}). High task complexity was associated with much lower success rates and with the occurrence of severe semantic errors. This means that, while one isolated problem trigger in a simple sentence is likely to be accurately rendered with the support of the CAI tool, speech passages dense in problem triggers or where problem triggers are connected by complex logical relations (requiring greater analysis effort from the interpreter) are more likely to be misinterpreted. A further mediating variable is \textit{CAI tool latency}. The large majority of study participants perceived the tool’s latency as excessive. They ascribed errors in their interpretation to ``having to wait for the tool'', which, in their view, caused them to forget or fail to process other elements of the unfolding messages. Given the intense use of working memory (WM) during SI (e.g. \cite{cowan2000processing,mizuno2005process}), a possible explanation for the supposed impact of latency on interpretation lies in WM overload. It is possible that if the interpreter waits too long for the tool’s input to start interpreting the items held in working memory, WM saturation is reached so that already-processed items disappear from memory, or it is not possible to process additional information. Future research should confirm the impact of these mediating variables and possibly identify other significant ones exerting an influence on CAI.

\subsubsection{Level of performance affected and error analysis}


In the present analysis, errors were detected extending the analysis beyond the level of the isolated term/numeral and including larger units of analysis (the sentence, the coherence and cohesion of the speech passage, the plausibility of the message etc.), i.e. adopting a ``communicative approach'' to the analysis of deliveries (cf. \chapref{ch:methods}). This is a major difference of this study compared to previous empirical CAI research, which focused on the accuracy of interpreted numbers and terms only, without focusing on the overall meaning of the interpreted message (cf. \chapref{ch:interpreting_technology}). A significant proportion of errors in participants’ deliveries was detected analysing the delivery beyond the mere problem trigger, which means that CAI tool use may lead to overall incorrect delivery even when individual items were correctly rendered. Examples of errors detected at a communicative analysis were problem triggers that were accurately rendered but wrongly contextualised in the delivery, omission or misinterpretation of information accompanying the problem trigger or implausible delivery.


\subsubsection{Effectiveness and efficiency of the interaction}

In usability studies, efficiency and effectiveness are typically evaluated with different metrics (such as success rates for the former and time on task for the latter) and regarded as two distinct concepts: while an interactive system may not be very efficient (i.e. sub-optimal in terms of the time and effort investments required of users to complete the task) it may still be evaluated as effective (i.e. it allows users to complete the task successfully). In the case of in-booth CAI tools, the distinction of efficiency and effectiveness as two separate concepts seems not to be as neat. Given that SI is a complex cognitive activity, the slightest interference may disrupt task execution. Hence, a decrease in efficiency, producing a delay in the delivery and causing the cognitive load to increase, impacts the tool’s effectiveness. For instance, study participants claimed that excessive CAI tool latency delayed their interpretation and contributed to their failure to process other elements of the unfolding speech.


\subsection{Users' perception}



\subsubsection{Perceived usefulness}

The perception of the CAI tool’s effectiveness (described by study participants as its contribution to a complete and accurate rendition of problem triggers, reduced effort in the interpretation of these elements and a greater feeling of security) seems to have been a major driver of user activation, i.e. a positive first-time experience with the CAI tool. In other words, in order to perceive the tool as satisfactory, users need to feel that the tool helps them achieve better outcomes than they would by themselves and with less effort. To achieve this goal, the tool must be effective not just in easy problem situations, for instance, an isolated numeral in a simple sentence, which we may expect professional conference interpreters to be capable of solving themselves, but rather in the most complex ones, such as the interpretation of long and complex named entities, high number density and the co-occurrence of several problem triggers. Study participants explained that complex tasks are the ones where they were most in the need of CAI tool support.




\subsubsection{Perceived dependability}

Issues in interpreter-CAI tool interaction may have interfered with the other sub-processes such as monitoring the plausibility of the own delivery and checking the plausibility of the tool’s suggestions. Participants stressed that during SI ``even a split second makes a difference'' and reported that, in this context, they found it difficult to monitor both themselves and the tool. This perception is reflected in the high rate of delivery errors corresponding to CAI tool errors in the pilot study (48\%, across 5 cases of CAI tool error interpreted by 5 participants). Interpreters perceive the CAI tool as a source of immediate and reliable help when in need. Hence, they regard the tool’s dependability as a fundamental prerequisite for them to adopt the tool in real life.

\subsubsection{Perception of human vs artificial boothmate}


The ambition of ASR-integrated CAI tools is to represent an ideal virtual boothmate. To fulfil this role, the virtual boothmate should possess some characteristics of the human boothmate perceived by interpreters as ``ideal'' -- although the help provided by the virtual and the human boothmate currently are and are likely to remain different in nature, as acknowledged by our study participants too. The study participants described a ``good'' boothmate as one who is available, reliable and knows what type of help the individual interpreter needs. Participants spoke of trusted colleagues who know their interpreting style, their preferences and needs well and provide help accordingly. The personalisation of help is regarded as crucial because excessive or unnecessary input may be disruptive during SI. Therefore, it seems recommendable for a CAI tool to provide sufficient customisation options for the individual interpreter to adjust the amount and type of support received to meet their individual needs. However, because users most commonly do not use the customisation options available in the tools they use, it seems recommendable to instruct CAI tool users on how to adjust the options to best suit their needs.













\section{Training needs of CAI tool users}


While ASR- and AI-powered CAI tools address existing needs of interpreters, which makes them potentially very useful, error patterns recurrently occurring in interpreters’ delivery warn us from taking the success of CAI tool use for granted. Despite the apparent simplicity and intuitiveness of CAI tools, achieving an effective integration of CAI tools into SI appears to be a complex task. Such complexity risks to offset the potential gains of utilising these tools, if users are not instructed to use them appropriately. At present, both the content and methods of CAI tool training (i.e. \textit{what should we teach?} and \textit{how should we teach it?}) remain to be defined.

All study participants in this study had completed an e-course introducing them to the UI features and technical specifications of SmarTerp before taking the test. They had had a chance to practise on the CAI tool in an interpreting exercise of equal length, complexity and structure. However, several errors and problems occurred anyway in their delivery. This suggests that the content of in-booth CAI tool training cannot be reduced to mere information about the tools and unguided practice.

While further research is needed to precisely define the content of CAI tool training, some issues that emerged in the study may point to potential learning needs. It must be stressed that not all study participants were aware of their learning needs. Some participants contradictorily claimed that ``no training is needed to use a CAI tool effectively'' but, at the same time, they reported several difficulties in the use of the tool, such as ``getting used to it'' and developing specific strategies to integrate it into SI. Some users also claimed that they did not think the tool had any negative impact on their delivery also where they made considerable errors. Developing awareness for potential problems in CAI tool use and analysing one’s performance may be a first learning need to be addressed in training.

Drawbacks of using the CAI tool that were reported by study participants, and may point to learning needs, include:
\begin{itemize}
    \item Difficulty in ``ear-eye coordination'', i.e. attending to both the CAI tool and the speaker simultaneously.
\item Loss of concentration on the overall meaning of the message, in favour of an excessive concentration on the problem trigger.
\item Being prompted by the tool, i.e. interpreting an item that participants saw appear on the screen as an impulsive, immediate reaction to the visual input, although participants were aware of not having understood how to contextualise the item.
\item Indiscriminate consultation, i.e. consulting the tool also when an alternative strategy may have been more effective.
\item Knowing the tool enough to formulate realistic expectations about what items will be displayed.
\item Reliance-agency balance, i.e. striking a balance between using the aids provided by the tool and remaining vigilant.
\end{itemize}












\section{CAI tool UI design}

\subsection{Tentative heuristics}

The results of the usability tests conducted in this study point to some heuristic principles that may guide the UI design of CAI tools. Although these must be corroborated by further evidence, they may provide hypotheses for future usability-focussed studies on CAI.


\subsubsection{Display numerals as a mix of Arabic digits and target-language orders of magnitude, if larger than thousand, but watch out for rare orders of magnitude}

Previous studies on the CAI of numbers displayed numerals entirely in the Arabic code or using a mix of Arabic code for digits and \textit{source} language phonological code for orders of magnitude above ‘thousand’ \citep{canali2019technologie,pisani2021measuring}. Presented with this graphic representation of numerals, users still made some transcoding errors in the rendition of orders of magnitude (e.g. ‘billion’ $\longrightarrow$ ‘million’). In this study, numerals were presented using a mix of Arabic code for digits and \textit{target} language phonological code for orders of magnitude above ‘thousand’. The fact that no transcoding errors for orders of magnitude until ‘billion’ were found in the dataset validates this design principle. However, the order of magnitude ‘trillion’ represents an exception. This order of magnitude is rare and its translation into Italian may be ambiguous. Therefore, the solution we chose confused users and caused severe errors in the delivery. The TL translation of rare orders of magnitude in in-booth CAI tools should hence be carefully chosen and its clarity should be verified with users.

\subsubsection{Make the UI interface as consistent as possible}

Actions on the UI interface (e.g. where new items appear on the screen) should be as consistent and predictable as possible. An issue recurrently mentioned by study participants during the interviews is a difficulty in ``finding information on the screen''. The activity of locating relevant information amongst all other irrelevant stimuli on the screen is a cognitive process known in psychology as visual search \citep{davis2004visual}. Our study participants reported that the additional effort and delay caused by non-automatic visual search caused them to focus excessively on the visual input and fail to attend to the acoustic input. A user interface that is maximally consistent should ease the development of automatic search behaviour.



\subsubsection{De-highlight irrelevant items}

In order to facilitate users’ identification of relevant items on the UI, it seems recommendable to de-highlight items that are no longer relevant.

\subsubsection{Favour precision over recall}

Because CAI tool dependability seems to be a fundamental need of users and some study participants even claimed that they were unable to check the accuracy of the aids during SI, it seems recommendable to favour the precision of displayed aids over recall, as suggested by \citet{fantinuoli2017speech}.



\subsubsection{Signal the origin of specialised terms and acronyms}

Users were mistrustful of the terms suggested by the CAI tool. In the case of CAI tools that search for terminology in external sources (e.g. electronic dictionaries, databanks etc.), it seems necessary to signal the origin of the displayed term (e.g. via colour-codes or icons) so that users may decide whether to use the information at a quick glance.


\subsection{Open design questions}


In this study, we could not find any empirical evidence for some principles that guided the design of SmarTerp. It is still to be demonstrated empirically whether these UI features ensure that CAI tool interface is usable. Each of these critical features could be the object of a dedicated study.

The first open question relates to the separation of problem triggers into distinct interface sections (``modules''). Study participants’ opinions of this design feature diverged. Some users found that the division into modules makes the UI better organised and more consistent. Other users found that it made the interface excessively and unnecessarily cluttered. The impact of this UI feature could be explored comparing different interface options in comparable and controlled tasks (A/B testing).

The second open question is about the display of terms and acronyms both in the source and target language. Users’ behaviour did not point to any use of the source-language version of the displayed item. This could depend on the artificial nature of the study: interpreters may have taken the accuracy of items for guaranteed and not performed the accuracy check that they would normally perform in a real assignment. However, some users commented that they would have never had the time and concentration to check whether the tool suggestion was accurate.

Open questions also relate to the display of named entities. Participants often mispronounced them or misinterpreted their gender (in the case of people). Furthermore, the excessive concentration required to read out the named entity from the screen recurrently led to errors or a loss of other fundamental information components in the following sentence segment. Pronunciation errors are more likely to occur in the interpretation of complex named entities of a foreign language that the interpreter does not master. Gender errors may be more likely when languages that are gender-neutral in spoken speech (such as Mandarin Chinese) are interpreted simultaneously into languages that are gender-sensitive. To cope with pronunciation errors, some study participants suggested using not the official written form of named entities but a sound-like transcription similar to that produced by publicly available ASR systems (like Otter.ai or Google Translate). One study participant (Carlo) suggested adding a ``Netflix-style pop-up'' displaying additional information about the people mentioned, such as a person’s picture, gender, age etc.

A further question related to interpreters’ difficulty in interpreting not just numerals but the whole numerical information unit correctly. A question is whether displaying the transcript of the sentence in which the numeral occurs in response to mouse hovering over the numeral might represent a more effective support for interpreters rather than the isolated number.

Both in the display of named entities and numerals questions related to the amount of information provided to users and whether more information should be made accessible when users request it through an interaction with the system (e.g. hovering over or clicking on the item). There seems to be a parallel with intelligibility features in translation memories, that allow users to find out more about the aid provided to them.





\section{Limitations}

This study has several limitations. From a general scientific perspective, the study design was adequate to account for some possible mediating variables, that were incorporated into the design of the test speeches, but not all possible variables. Further variables such as the impact syntactic complexity, delivery pace, language combination, remain currently unexplored. Furthermore, given the artificial nature of the test, an evaluation of CAI tool use in a real-life assignment may reveal further insights and, possibly, yield a better understanding of actual users’ needs.


From a usability perspective, the main limitations of the work derive from the small sample size and the broad exploratory character of the inquiry. A larger sample size is required to give more robustness to the design recommendations that were developed within this work. Furthermore, to develop more robust UI design heuristics, focused studies are required to zoom in on specific interface principles, for instance through usability testing.

Given these limitations, the UI design heuristics and the principles of in\-ter\-pret\-er-CAI tool interaction that were identified based on the interpretation of the study findings should be regarded as initial hypotheses requiring further exploration



\section{Future work}

Empirical CAI research is still in its infancy and several research questions remain to be addressed by future studies. Future work may have three major orientations: scientific, pedagogical, and usability. The \textit{scientific orientation} consists in exploring the CAI-tool supported SI to contribute to the field’s scientific knowledge. The use of a CAI tool adds a further element of complexity to the already complex cognitive task of SI. Therefore, this may represent a vehicle to increase understanding of SI from a cognitive perspective -- and, possibly, be of interest to cognitive psychologists. More in general, scientific orientation may be seen as the basic research providing the fundamental knowledge applied in the usability and pedagogical orientations. A major and necessary contribution to basic scientific knowledge may be obtained pursuing the following objectives: (1) Developing a cognitive model of interpreter-CAI interaction that may explain which structures are activated during CAI-tool assisted SI and why errors occur; (2) Defining the impact of CAI tools on users with varying interpreting expertise (e.g., students vs professionals); (3) Exploring the impact of specific classes of problem trigger and identifying moderating variables; (4) Ascertaining the psychophysiological impact of CAI tool availability, for instance, exploring the hypothesis that it may reduce stress.

The \textit{pedagogical orientation} comprises all research conducted to define the content and methods of instructional interventions on in-booth CAI tool use in a scientific and systematic way. This orientation is of interest both for the development of CPD solutions for professionals and the training of new generations of interpreters. To advance towards the development of research-based solutions, the following research gaps should be filled: (1) Modelling the skills and knowledge structures underlying effective interpreter-CAI interaction; (2) Defining effective instructional strategies to train those skills.

Finally, the \textit{usability orientation} aims at developing recommendations for the further development of CAI tools, identifying the optimal UI features and technical specifications. Some research gaps that should be filled to develop this orientation are: (1) Developing research-validated tools and measurements: for instance, usability studies rely on a number of validated questionnaires; using these tools across studies increases their construct validity and allows for comparability and replicability; (2) Developing industry benchmarks allowing to put the evaluation of a CAI tool in perspective; (3) Exploring the impact of particular interchangeable UI elements, which are currently selected based on the personal intuition of interpreters/designers with little scientific justification. These are, for instance, the use of a running transcript vs isolated problem triggers, unitary field vs division of suggestion into modules etc.


\section{Methodological recommendations}


The present study represents an example of usability-oriented empirical CAI research. Future studies may build on the methods used in the present work to evaluate the usability of CAI tools. I recommend that these studies consider the following methodological recommendations:
\begin{enumerate}
\item \begin{sloppypar}\textit{Participant selection}: to develop valid recommendations, participants should be representative of the target users for which the product was designed. If non-representative users are selected for convenience or to achieve other scientific aims, this should be specified in the limitations of the study; in this case, usage problems and possible solutions should serve as hypotheses rather than definite design recommendations.\end{sloppypar}
\item \textit{Sample size}: given the time-intensive nature of usability tests, it is common to have small samples, and it is believed that 5 participants are sufficient to detect major usage problems \citep{barnum2020usability,nielsen1993mathematical}. However, the limitations of a small sample should be acknowledged. If a quantitative research question and statistical validity are pursued with a small sample, an experimental design collecting a large number of data points on the impact of one specific variable may be preferable to a usability test. Alternatively, other HCI testing methods should be considered.
\item \textit{Test speech design}: because usability tests should involve tasks representative of real-life challenges that users would overcome through the use of the product, a sufficient degree of experimental control is required on all actions that users perform with the product during the test session to evaluate the usability of the tool on those tasks. In the evaluation of a CAI tool, the design of the test speech is crucial because it is the source of the tasks that users accomplish with the support of the tool. In the limitations of the study, it should be specified that this design strategy represents a limitation to the ecological validity of findings. We recommend specifying the precise characteristics of the test tasks and, if possible, disclosing test materials to encourage scientific scrutiny and replicability.
\item \textit{Approach to the analysis of deliveries}: the identification of critical incidents and usage problems through a process of abstraction requires the analysis of interpreters’ performance. A micro-analysis focusing solely on the interpreted problem trigger may be useful to respond to specific research questions. However, a broader analysis of the interpreting product is required to infer the impact of the CAI tool on the interpreting process and derive considerations on possible tool-related problems. It is recommendable to adopt an analysis framework that allows for such an in-depth nuanced analysis  --  which we obtained adopting a communicative approach and an adaptation of the Redundancy Ladder Model \citep{frittella2019a}. If a micro-unit of analysis is used in the study (e.g., only interpreted numerals or specialised terms are analysed without) it should be specified that the results of the analysis are not reflective of broadly-conceived ``delivery accuracy\slash quality'' and are not measurements of ``CAI tool effectiveness''.
\item \textit{Inferring the cause of critical incidents}: human factors always influence the outcome of users’ interaction with a product. This may be particularly true of SI  --  a cognitively taxing task of bilingual communication performed by human interpreters in real-time and under time pressure. The influence of contingent and idiosyncratic factors represents a major potential threat to the usability testing of a CAI tool as a critical incident may be caused by multiple factors. In my study, I found it particularly helpful to integrate observations and performance metrics with interview passages in which participants explained why, in their view, particular critical incidents occurred. I considered their explanation as a form of participant triangulation strengthening the reliability of my interpretation.
\item \textit{Interpreting users’ recommendations}: while study participants’ recommendations may be useful to surface their perceived problems and needs, they should be evaluated in the light of a whole range of possible biases: users may be unable to locate or adequately explain a problem, they may misattribute a problem to the wrong cause, report that they want something which actually does not work in practice etc. In sum, while it is important to listen to users’ opinions, one should not ``ask users to be designers'' \citep{barnum2020usability}. Their recommendations may be taken as a starting point for further exploration or a source of participant triangulation but not as an infallible source of design recommendations.
\item \textit{Validity of findings}: recommendations for the design and development of a product through usability testing are a powerful tool to improve a product. Nonetheless, it should be made clear that these are based on small-scale studies which may have yielded only a partial understanding or biased view of the issue. Like all scientific research, the results should be held as valid only until disproved by further evidence and scientific scrutiny should be encouraged through a transparent and well-documented process of (a) formulating research questions on UI design elements and technical specifications of the tool, (b) collecting adequate data to explore these questions, (c) providing a detailed, objective description of findings, (d) and explaining researchers’ interpretation of underlying usage problems.
\item \textit{Generalisability of findings}: the results of small-scale inquiries are, by their own nature, not generalisable in a statistical sense. However, we suggest that the concept of analytical generalisation \citep{Yin2013} may apply to the design principles identified in usability tests. Analytical generalisation involves making projections about the results of a study not to a population but to a theory. Evidence for the applicability of a principle gathered through multiple studies strengthens the generalisability of the given principle to other similar cases. Usability testing was used in the present work in line with the aim to derive recommendations on the tool’s UI design and technical specifications. Future work may focus on applying other user research methods in line with different aims and research questions, such as contextual inquiry and focus groups to deepen understanding of users’ needs, A/B testing to compare two different designs etc.
\end{enumerate}
