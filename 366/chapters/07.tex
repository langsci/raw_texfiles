\chapter{First iteration: Pilot study}\label{ch:first_iteration}

The purpose of the pilot study in the usability test of SmarTerp was to validate the methodology and provide initial orientation to the design. It hence represents both a tool for methodological validation and the first iteration of prototyping, testing and improvement. It was conducted with five participants, who were Italian (A) – English (B/C) conference interpreters (see the description of study participants’ characteristics in \chapref{ch:methods}). Because this test was conducted using a prototype, the tool was not always accurate. It omitted and misrepresented some source speech items, as described below. This first analysis hence shed light on some patterns of error that may occur when users are presented with inaccurate input by the CAI tool.

\section{CAI tool inaccuracies}

Due to imperfections in the CAI tool prototype, the test speech presented 15 issues in a total of 52 displayed items. These may be categorised as follows:

\begin{itemize}
    \item \textit{Errors} (code `e'), no. 5: An item is displayed differently than it should have, e.g., ``58,000'' is displayed as ``58.00.00''.
    \item \textit{Omissions} (code `o'), no. 3: The CAI tool does not display an item that should have been displayed.
    \item 7 CAI tool \textit{limitations} (code `x'): The CAI tool does not display an element that participants consistently found significant, e.g., it does not display charges, like President, Minister, etc.
\end{itemize}
While errors and omissions are actual inaccuracies in the prototype performance, what I categorised as `limitations' are elements that were not displayed by the tool because it was not programmed by default to provide such aids. I added this category to my analysis because, during the test, I observed that participants repeatedly stumbled on these items.  I hence decided to dedicate particular attention to this recurring pattern which I interpreted as potentially significant.

\section{Users' performance}


\subsection{Task success rates}

\tabref{tab:14} reports participants’ success rates in speech tasks as well as the mean success rate on each task. As discussed in the method section, AC, NE, NU, and TE may be regarded as the tasks of lowest complexity, TL, TS, and NR as tasks of medium complexity, and NIU, NCR, NCN, SO, and CP may be expected to be of higher complexity. An asterisk identifies the speech tasks in which the test CAI tool prototype produced an error or omission.

\begin{table}
\begin{tabular}{llr}
\lsptoprule
{Code} & {Task}                         & {Mean} \\
\midrule
{AC}            & {Isolated acronym}                      & {100\%}         \\
{NE}            & {Isolated named entity}                 & {96\%}          \\
{NU}            & {Isolated numeral}                      & {80\%}          \\
{NR}            & {Numeral and referent}                    & {100\%}         \\
{NIU}           & {Numerical information unit*}            & {48\%}         \\
{NCR}           & {Redundant number cluster*}              & {53\%}         \\
{NCN}           & {Non-redundant number cluster*}          & {42\%}         \\
{TS}            & {Terms in a semantically complex sentence} & {45\%}          \\
{TL}            & {List of three unknown terms*}           & {65\%}         \\
{TE}            & {Isolated term}                         & {78\%}          \\
{SO}            & {Complex speech opening*}                & {78\%}         \\
{CP}            & {Conference programme}                  & {91\%}          \\
\lspbottomrule
\end{tabular}
\caption{Mean task success rates (pilot study). “*”: CAI tool error, omission or limitation is present in the task}
\label{tab:14}
\end{table}

Two outliers may be found in the high success rates that participants obtained on low-complexity tasks, i.e. Diana’s interpretation of NU and Sally’s interpretation of TE. Based on the interviews, these were due to a distraction due to an interpretation error in the previous passage (Sally), and to the display of the order of magnitude ``trillion'' ($10^{12}$), which the tool displayed as ``bilione'' (Diana). While ``bilione'' is the correct Italian translation of ``trillion'', it is rarely used -- common alternatives are: ``trilione'' ($10^{18}$), incorrect translation but used with increasing frequency as a loanword from English, and ``mille miliardi'', a correct and more native alternative but we expected it to be more difficult to process during SI.


\subsection{Response to CAI tool inaccuracy}

This section of the analysis shows what happened when participants received inaccurate aids by the CAI tool, or an expected aid was missing.  \tabref{tab:15} is a contingency table showing errors, omissions and other issues that occurred in participants’ deliveries when the CAI tool provided a correct and an incorrect suggestion, omitted a suggestion that should have been provided or did not display a component of the information because of its intrinsic functional limitations.

\begin{table}
    \begin{tabular}{l *4{r@{~}>{(}r<{)}}}
        \lsptoprule
        & \multicolumn{8}{c}{Delivery issues}\\\cmidrule(lr){2-9}
        & \multicolumn{2}{c}{Total} & \multicolumn{2}{c}{Errors} & \multicolumn{2}{c}{Omissions} & \multicolumn{2}{c}{Other}\\
        \midrule
        {Correctly displayed items (185)} & 56 & 30\% & 15 & 8\% & 16 & 9\%  & 25 & 13\%\\
        {CAI tool issues (75)} & 52 & 69\% & 19 & 25\% & 29 & 39\% & 4   & 5\%\\
        {CAI tool errors (25)} & 14 & 56\% & 12 & 48\% & 1 & 4\% & 1 & 4\% \\
        {CAI tool omissions (15)} & 14 & 93\% & 0 & 0\% & 14 & 93\% & 0 & 0\% \\
        {CAI tool limitations (35)} & 24 & 69\% & 7 & 20\% & 14 & 40\% & 3 & 9\%\\
        \lspbottomrule
    \end{tabular}
    \caption{Contingency table: Accuracy of CAI tool and delivery}
    \label{tab:15}
\end{table}

The variation of errors, omissions, and other issues in the delivery with CAI tool accuracy is represented in \figref{fig:16}.
\begin{figure}
 %   \includegraphics[width=\textwidth]{figures/Picture16.png}
    
    \begin{tikzpicture}
    \begin{axis}[
    ybar,
    axis lines* = left,
    width=\textwidth,
    height = 6cm,
    ymin=0,
    ymax=100,
    ylabel={\%},
    xtick={0,1,...,3},
    xticklabels = {Delivery issues,Delivery errors,Omissions,Minor issues},
    x tick label style = {font =\small},
    enlarge x limits={.2},
    nodes near coords,
 %   legend pos = north east,
    legend style = {at={(0.5,-0.15)},anchor=north,font=\small},
    legend columns = {2},
    legend cell align=left
    ]
    \addplot+[lsDarkBlue]
    coordinates{
    (0,30)
    (1,8)
    (2,9)
    (3,13)
    };
    \addplot+[lsDarkOrange]
    coordinates{
    (0,56)
    (1,48)
    (2,4)
    (3,4)
    };
    \addplot+[lsLightBlue]
    coordinates{
    (0,93)
    (1,0)
    (2,93)
    (3,0)
    };
    \addplot+[lsLightOrange]
    coordinates{
    (0,69)
    (1,20)
    (2,40)
    (3,9)
    };
    \legend{Correctly displayed items, CAI tool error, CAI tool omission, CAI tool limitation}
    \end{axis}
    \end{tikzpicture} 
    \caption{Column chart: Accuracy of CAI tool and delivery }
    \label{fig:16}
\end{figure}

When the item displayed by the tool was correct, 70\% of interpreted items were evaluated as correct; if we consider only delivery errors and omissions and exclude ``other issues'' from the count, 84\% of interpreted items were rendered correctly. However, it should be taken into account that the tasks where no issue occurred were those of lowest complexity; therefore, the results cannot be seen as a direct impact of task complexity. When the CAI tool presented an issue, only 31\% of interpreted items were evaluated as correct. If we exclude CAI tool limitations from the count and only consider the instances of errors and omissions, the proportion of correctly interpreted items equals 30\%.







\subsection{Error patterns}

This section of the analysis reports the patterns of error that most frequently occurred both when the CAI tool provided participants with correct aids and when the aids were incorrect or missing.



\subsubsection{Correctly displayed items}

When the item was correctly displayed by the CAI tool, nearly half (44.5\%) of all delivery issues were categorised as ``other issues'' (cf. analysis criteria detailed in \chapref{ch:methods}). One of these frequently occurring issues was \textit{pronunciation errors} (no. 16), i.e., mispronunciation of a named entity (no. 13) and a specialised term (no. 3). The most mispronounced named entities were “Soraya Hakuziyaremye” (subtask code SO-2-1), mispronounced by 4/5 participants, and “Felix-AntoineTshisekedi Tshilombo” (subtask codes NE-2 and CP-1-2), mispronounced 7 times by 4/5 participants. The mispronounced terms were ``praseodymium'' (subtask code TL-1), mispronounced by 2/5 participants, and ``terbium'' (TL-2), mispronounced by 1 participant. Other recurring issues were \textit{gender errors} (no 4), i.e., a person mentioned in the source speech is attributed the wrong gender by the interpreter, and a \textit{partial rendition} of an acronym. Apart from ``other issues'', participants made a similar number of omissions (no 16) and more severe semantic errors (no 15) when the CAI tool displayed items correctly. No clear pattern of distribution of errors and omissions may be identified as both occur in the same tasks indiscriminately, i.e. in NCR, NCN, NIU and TS.



\subsubsection{CAI tool errors}

Most CAI tool errors led to a \textit{severe semantic error} in the delivery, as shown in the example below. In the delivery example provided, it is possible to see that the CAI tool error disrupted the participant to the extent that she committed a plausibility error -- considering that the global nickel production amounted to about 2.5 million tons in 2021, it is impossible that Madagascar alone produced 58 million. The subtask where the CAI tool error occurred had a mean success rate of 26\%, with 2 plausibility errors, 2 misattributions and 1 summarisation strategy.

\begin{quote}
\begin{sloppypar}
\textit{Source}: Madagascar alone produced approximately 58,000 [displayed as 58.00.00] metric tons of nickel in 2021.\\
\textit{Delivery example} (Minerva):  Madagascar alone produced 58 million tons of nickel.
\end{sloppypar}
\end{quote}




\subsubsection{CAI tool omissions}

In nearly all cases, an item omitted by the CAI tool was omitted by study participants too (14/15, or 93\% of cases). In the example below, all participants (5/5) omitted the item that was not provided by the tool and interpreted the ones provided:

\begin{quote}
\textit{Source}: Our soil is rich in high-value praseodymium [displayed], dysprosium [not displayed], and terbium [displayed].\\
\textit{Delivery example} (Logan): Our soil is rich in praseodymium and terbium.
\end{quote}

\subsubsection{CAI tool limitations}

Both errors and omissions were triggered by what I designated as ``CAI tool limitations'', which must be differentiated from CAI tool omissions because a given item is not left out because of a tool error but rather because of the way the tool functions. In the example below, for instance, the CAI tool did not display the referent ``diamond production'' because this was not recognised as a technical term. The mean success rate in this subtask was 46. 2 of 5 interpreters could not interpret the referent, which had been said by the speaker but not displayed by the tool, producing an incomplete sentence with a missing referent, as in the delivery example below. One other interpreter resorted to a strategy and summarised the overall meaning of the sentence without detailing the precise information: ``Namibia’s production increased too'' (Carlo).

\begin{quote}
\textit{Source}:  Namibia's diamond production [not displayed] amounted to 2.52 million carats.\\
\textit{Delivery example} (Sally): Namibia produced 2.52 million carats [missing referent].
\end{quote}

Another case of tool limitation may be found when a person is introduced in the speech: his/her name and the agency s/he works for are both displayed but not their charge, as this is not recognised as a specialised term. While charges are lexical items for which professional conference interpreters should have a readily available equivalent in the target language, these were found to be often omitted or misinterpreted by study participants, as in the example below.

\begin{quote}
\textit{Source}: His Excellency Paul Kagame; Honourable Soraya Hakuziyaremye, Rwanda [not displayed] MINICOM Minister [not displayed], Ms Giovanie Biha UNECA Deputy Executive Secretary [not displayed].\\
\textit{Delivery example} (Minerva): His Excellency Paul Kagame, Honourable Soraya ``Akugiaramie'', Trade and Industry Minister, Mr Giovanie Biha Economic Commission for Africa’s Secretary.

\end{quote}


\section{Users' perception}




\subsection{Post-task questionnaire}


\tabref{tab:17} reports the results of the post-task survey, which explored participants’ satisfaction with SmarTerp and their perception of its usability. The table also shows participants’ self-reported likelihood to use ASR-based CAI tools in the near future before and after the test with SmarTerp. Given the small number of survey responses, only the mean value was calculated. It is noticeable that the mean interest in ASR-powered CAI solutions (reflected by participants’ self-reported likelihood to use such tools before and after testing SmarTerp) increased by a mean value of 0.8 points.


\begin{table}
\begin{tabular}{lr}
\lsptoprule
Usability aspect      & Mean \\\midrule
Overall satisfaction           & 1.8           \\
Ease of use                    & 2             \\
Effectiveness                  & 2.2           \\
Ease of learning               & 1.4           \\
Timeliness                     & 1.8           \\
Dependability                  & 2             \\ \midrule
Likelihood to use CAI (before) & 1.2           \\
Likelihood to use CAI (after)  & 2             \\ \lspbottomrule
\end{tabular}
\caption{Questionnaire results (pilot study)\label{tab:17}}
\end{table}




\subsection{Interviews}

\subsubsection{SmarTerp’s UI and technical specifications}


From the point of view of the tool’s aesthetic qualities, participants generally referred to the ``smart interface'' (mentioned by four participants) as a factor increasing the appeal of SmarTerp also in comparison with other existing ASR-based CAI tools, although perceptions of aspects such as colour, font size and amount of information displayed vary among participants. For instance, two participants liked the colour choice (dark background with white typography), whereas two found it unpleasant.

The structural feature that received the most negative comments was the order of appearance of items. four participants said that they were confused by the fact that items did not appear at the top of the interface and believed that the feeling of uncertainty and distraction caused by this UI feature contributed to their errors during the test. Furthermore, one participant found the division of the interface into three modules “overloading”, and one thought that terms and acronyms should be displayed in the target language only.

Given that most participants immediately mentioned the tool as ``easy to use'', ``intuitive'', and ``seamless'', as well as suitable for everyone ``even for the tech\-nol\-o\-gy\nobreakdash-dam\-ag\-ed'' (in Italian: ``è a prova di tecnoleso'', Minerva) right in opening the interview, it seems that they perceived the ease of use as a major strength of the tool, which is in line with the high scores in the post-task questionnaire. In participants’ view, ease of use was given by the ASR system, which ``is even better than a human boothmate: you don’t even need to ask for help'' (Sally). The technical specification that was most criticised by users is the tool’s latency, which was defined as “too long” (three participants) or “a bit too long” (one participant).



\subsubsection{Usefulness}

When asked what was, in their opinion, the main advantage in using the CAI tool, all participants (5/5) emphasised the support in dealing with interpreting problem triggers, or the ``pain points of simultaneous interpreting'' (Minerva). In the words of another participant: ``the good thing is knowing that the most difficult part of the sentence will be there'' (Sally). All participants reported that they found the tool most helpful for ``unknown items'', i.e. information that you cannot infer from the context and where you cannot apply a strategy: ``you can round a number but you cannot approximate a named entity'' (Minerva). The tool was found to be especially needed because of the lack of preparation (Diana, Carlo, Minerva), for the following reason: ``I usually do not need support in the rendition of terminology: if the terms are in my glossary, I know them. The four most important things are usually written on my notepad'' (Minerva). Participants associated the availability of a support system for problem triggers with increased delivery accuracy (which justifies the high effectiveness scores in the post-task questionnaire) and reduced mental effort:

\begin{quote}
    The good thing is that you know that the most difficult part of the sentence is going to be provided by the tool. This way, you don’t have to put any effort into energy-consuming tasks such as writing down numerals. (Sally)
\end{quote}
Two participants also spoke of a greater feeling of security in using the tool and defined it as a ``safety net'' (Minerva) and a ``further confirmation'' (Sally, Diana).

When asked if they could recall particular speech passages where they found the tool most helpful, participants mentioned most often: (1) the \textit{conference programm}e (task code “CP”), given the high density of unknown named entities: ``If I hadn’t had the tool, I would have probably said either the persons’ charge or their name but not both'' (Sally); (2) \textit{Number-dense passages} (task codes “NCR” and “NCN”): (3) \textit{Unknown terms}, in particular in a list (task code “TL”). However, this is in contrast with the fact that at least two study participants reported that the tool was helpful in dense passages while they actually made severe errors.

A recurring theme in the interviews was the issue of trust. One participant commented in the interview that ``trusting the tool is the prerequisite for using it. It’s like having a GPS: you must trust it, otherwise what’s the point of having one?'' (Diana). In answering the question ``what’s the difference between the CAI tool and a human boothmate?'', 3 participants declared that they would trust the tool more than a human interpreter: ``The computer gives you the impression of being infallible. You can trust the human boothmate if you know her personally, but it does happen that you turn to your partner to request help and she gives you a blank stare back'' (Sally).

Logan and Carlo were the participants who gave the lowest dependability scores to the tool. However, their low level of trust in the CAI tool had different grounds. Carlo showed an overall positive attitude towards technology, in general, and CAI tools, in particular. He reported in the interview to have tested and used several new technological solutions in the past and to be keen to integrate a tool similar to the one we tested into his workflow; he also showed understanding of how ASR and AI technology works and made predictions on the tool’s performance based on his knowledge. Based on this knowledge, Carlo explained that he would need to ``learn to trust the tool'':

\begin{quote}
    I would trust the colleague to provide me with a hint on a specialised term I never saw before but not the tool because the latter cannot evaluate the adequacy of a solution in context. I would probably check the tool’s suggestion after four consecutive assignments and if they consistently prove reliable, with time, I would learn to trust it. (Carlo)
\end{quote}
Quite interestingly, Carlo was the only one of the participants who attempted to make selective use of the tool’s prompts. When he did, he was observed looking away from the tool and using strategies such as abstraction and generalisation of the overall meaning of the passage whilst omitting the hint provided by the tool. When asked, in the interview, what motivated his behaviour he explained that he realised that he was having difficulty understanding the overall message and hence looked away, presumably to concentrate on his comprehension of the source speech beyond the individual problem triggers displayed by the CAI tool: ``I did so because I knew that the aid was pointless if I couldn’t understand the meaning in the first place ... The tool is helpful because I can ignore it'' (Carlo).

Logan, instead, showed distrust and a low degree of familiarity with CAI tools and their functioning. Before agreeing to join the study, he asked whether such tools would one day replace human interpreters. In the interview, he mentioned the tool not providing the referent “diamond production” (whereas the item is not provided because it is not a technical term) as well as “Agenda 2063” appearing in the named entity column instead of the number column as two tool errors. He mentioned that, in several cases, he expected the tool to provide him with suggestions that did not come up and, in his view, impacted his delivery negatively: ``my too-high expectations betrayed me'' (Logan).


\subsubsection{Difficulty in using the tool}

Despite the apparent simplicity and intuitiveness of CAI tools and the advantages reported by participants, most of them also pointed out difficulties and potential pitfalls in using CAI tools. Coming to the disadvantages reported by participants, three main themes emerged from our analysis: (1) ear-eye coordination, (2) CAI tool as prompt, (3) adjusting to CAI tool use. These three themes are explained in the discussion below.

\subsubsubsection{Ear-eye coordination}

4/5 participants (all of them except for Carlo) reported that they had difficulty splitting their attention between listening to the source speech and using the support of the CAI tool, which we will call \textit{ear-eye coordination}. Participants explained this issue differently. Some of them reported a loss of grit and self-regulation: ``It’s just that I didn’t put effort into listening anymore. The CAI tool made me lazy'' (Logan). Another participant further elaborated on this same phenomenon: ``because you know that the most difficult part of the sentence, the one that usually demands so much of your attentive control, is going to be provided by the CAI, you do not care of it anymore. For instance, in my case, I was not listening to numbers anymore, I just expected the CAI to do the work for me'' (Sally). In other instances, participants provided an alternative explanation for the ear-eye coordination difficulty reported: ``you must \textit{learn} [italics added] to use it as a confirmation rather than your primary source of information ... you \textit{must know how to }[italics added] distribute your attention between listening and looking at the tool'' (Sally). Another participant reflected on the fact that the need to monitor one’s interpreting process and output (e.g., in the case of plausibility errors) increases when a CAI tool is used: ``the trade-off for the simplicity of use of the tool may lie in the need to monitor yourself more closely'' (Diana). She went on to explain that ``while without CAI an item that you don’t know or hear is just gone, CAI puts you facing a choice.''

\subsubsubsection{Tool as prompt}


While, in theory, it is up to the interpreter to choose whether to use the CAI tool input or not, all participants except for Carlo reported that they felt prompted by the tool to say whatever they saw appear on the screen. Two participants defined the tool as ``a temptation'' (Diana, Sally) and added that they were tempted to use the tool’s suggestion even in situations that would have better been dealt with otherwise, such as through the use of omission, generalisation, approximation and other similar strategies. The situations mentioned by participants were the following:

\begin{enumerate}
    \item A high density of information (Sally), where a reduction in cognitive load was needed.
    \item Redundant information, such as repeated numbers and examples (Sally), that participants felt could be left out strategically without compromising the overall message.
    \item Loss of the overall meaning of the message, which makes the decontextualised suggestion unusable (Diana, Sally, Minerva, Carlo): ``the tool was useful where I didn’t need to understand the sentence, I just needed the hint. In some cases, I had the hint but not the understanding and so I still needed to generalise'' (Carlo).
    \item Long décalage or fast delivery speed: ``in the case of acronyms, although the extended form of the acronym is helpful to provide a complete and accurate rendition, it is a double-edged sword: it triggered me to say the whole acronym also when it was necessary, or I had better saved time'' (Minerva).
\end{enumerate}
Participants hence suggested that the tool should be used selectively, while failure to make goal-directed choices about how to use the CAI tool’s suggestions may lead to mistakes.


\subsubsubsection{Getting to know the tool}

Most participants expressed the need to ``get to know the tool'', or ``to know what to expect from it''. Participants explained the importance of knowing the tool as follows: ``it would reshape your expectations: knowing how the tool works, knowing its limits, you also know how it can help you and you behave accordingly'' (Minerva). With this expression, participants may have referred to several distinct concepts.

One meaning of ``getting to know the tool'' may be knowing where items appeared and how they would be displayed. One participant (Logan), for instance, explained that he expected the referent “diamonds” in the numerical task NCN to appear and was disappointed not to be provided with it by the CAI tool.

With this expression, one participant stressed the need for more hands-on experience allowing one to develop a practical feel and understanding of the tool’s functioning: ``one way is to read `latency is two seconds' and another thing is to experience it while interpreting'' (Carlo). Participants also expected that, through experience and repeated use, one would learn how to integrate the tool into the SI process more effectively, for instance by accommodating one’s décalage to the tool’s latency and using strategies to integrate the hint into one’s rendition while waiting (Carlo, Diana, Logan).

As a further dimension of ``getting to know the tool'', participants engaged in reflection about the fact that the CAI tool did not help them process the speech semantically:

\begin{quote}
    Let’s use the metaphor of a crutch: the CAI tool will be very helpful to walk, but it won't help you run! (Minerva)\\
You are the one interpreting, which means understanding the link between pieces of information; the tool can only support you in the tasks that are challenging for the human brain. (Diana)
\end{quote}
Participants reported a certain ``disappointment'' when they found out that the tool could not help them to the extent they had previously assumed:

\begin{quote}
    It was probably my high expectations that betrayed me. (Logan) \\
If you're not understanding what the speaker is on about, and you expect the tool to be your ``phone-a-friend lifeline'', you will be disappointed! (Minerva)\\
At some point in the speech, a series of technical terms came up [referring to the task ``technical terms embedded in a semantically complex sentence'', task code “TS”] and I was lost because I didn’t know anything about the topic. I somewhat expected that the tool would help me out, but that obviously wasn’t the case: CAI tools can provide you with terms but they do not stitch them together in a sentence for you! (Diana).
\end{quote}

Possible errors and omissions of the tool were also mentioned as a negative factor. Participants were capable of recalling specific points of the test where they believed that the tool omitted an item or provided a wrong suggestion, which implies that CAI tool inaccuracies are strongly perceived by users. In regard to omissions, participants admitted that they got complacent after some exposure to the tool (``the CAI tool made me lazy'', Carlo): ``I expected problematic items to be displayed and so after a while, I stopped listening carefully to them; if they were not provided by the tool, I was dumbfounded'' (Sally). In saying so, participants seemed to attribute the problem of CAI tool omissions leading to delivery omissions to their fault, at least partly. In contrast, errors seem to be much more negative for the tool’s perceived dependability. Diana, who suggested that terms should be provided only in the target language to reduce the load of visual information, explained this point as follows:

\begin{quote}
    Trust is the fundamental prerequisite for using a CAI tool -- it’s like having a GPS: if you don’t trust it, why use it? In the case of specialised terms in a real assignment, if a term is in my glossary, it is also in my head. If I consult the tool, it is because I need it: I need readily available, immediate support. I cannot waste time assessing the validity of the suggestion, if it’s a term I’ve never heard before, I don’t even have the knowledge to do so. I can only trust it. (Diana)
\end{quote}
These difficulties are in contrast with participants’ overall perception of the ease of learning of the tool, which was very high for most participants except for one. Participants who reported that little or no training is required to use the tool explained: ``I don’t think that there’s much to be learnt: you literally don’t have to do anything'' (Logan). This may signal that novice CAI tool users may lack the awareness of their own difficulties and errors, which makes them unable to recognise their learning needs.


\section{Usage problems and design recommendations}

This first test iteration highlighted recurring error patterns that may be interpreted as usage problems, leading to recommendations for the improvement of SmarTerp. The first usage problem that was identified in this round of testing was that the order of appearance of items made it difficult for users to locate new information as it appeared on the screen. This usage problem had both high impact and frequency. 4/5 participants reported that the order of appearance of new items (ABCD, E→A, F→B etc.) appeared illogical and was confusing to them. The following is a representative quote from an interview:

\begin{quote}
    Why don’t items appear in their logical chronological order? I would expect new items to appear at the top of the list with the others scrolling down so that I can check the previous item as well if I need to. The highlighting system is good, but items must appear in chronological order too. (Sally)
\end{quote}
Participants explained that this feature decreased the tool’s efficiency because they had to ``look for bits of information on the screen'' (Diana). A critical incident associated with the extra difficulty caused by the non-linear display system is the following.

\begin{quote}
\begin{sloppypar}
    \textit{Source}: His Excellency Paul Kagame; Honourable Soraya Hakuziyaremye, Rwanda's MINICOM Minister; Ms Giovanie Biha, UNECA Deputy Executive Secretary.\\
    \textit{Delivery example} (Logan): His Excellency Paul Kagame; Soraya ``Hakuziaremiei'' [leans forward to read]; Giovanie Biha, Vice-Minister for Trade and Industry.
\end{sloppypar}
\end{quote}
This feature seems to decrease both the perceived efficiency of using the tool and the actual effectiveness of the interaction. Given its high frequency and severe impact, we recommend that the order of appearance of new items on the screen be changed into chronological order, with the new item appearing at the top of the list and the others already displayed scrolling down.

A further recurring error pattern, of medium impact but high frequency, was that users consistently interpreted people’s names without the corresponding professional charge. This pattern could be interpreted both as a human error, given by the fact that interpreters could not distribute their attention effectively, and as a result of the fact that names were displayed by the tool but not charges, contrary to users’ expectations. We decided that it would be appropriate to reassess the problem after the second round of testing.

Finally, an issue of interest was the fact that both low accuracy and low recall impacted participants’ delivery. Nearly half of the errors made by the CAI tool resulted in a severe semantic error in participants’ delivery. At the same time, 93\% of items (14/15) omitted by the CAI tool were omitted by study participants too. There are two ways to interpret these findings from the point of view of CAI tool development. On the one hand, one could notice that 93\% is a much higher inaccuracy rate than nearly 50\%. On the other hand, one could consider the impact of semantic errors as more severe than that of omissions. The severity of the issue may be further compounded by the lack of awareness of errors. A previous study observed that CAI tool users may ``copy'' tool errors without noticing them \citep{defrancq2021automatic}. Also in this study, at least two participants reported that the CAI tool was ``very useful'' in the rendition of passages where they had committed severe errors. Finally, one may consider the crucial impact that wrong suggestions have on the perceived dependability of the tool: study participants tended to blame themselves for omissions made when the CAI tool omitted an item but blame the tool when it made an error. Considering all these aspects of the issue, it seems recommendable to continue following the principle ``accuracy over recall'' until disproved by future evidence.



\section{Methodological validation}

Along with conducting an initial formative evaluation of the tool, a  purpose of the pilot study was to validate our research methodology and materials. The speech design was validated in that the tasks were challenging enough to prompt CAI tool consultation and the speech structure, alternating complex passages with passages not containing problem triggers, gave participants sufficient time to conclude the interpretation of a task before the next began, as expected. For the second iteration, we recorded another video with a more stable prototype and the updated CAI interface, i.e., after the design recommendations presented above were implemented.
An element of the study materials that we decided to revise before the second iteration was the  questionnaire section asking participants to express their satisfaction with the support provided for individual problem triggers. In the interview, participants were not able to motivate the reason for this choice. For example, Logan and Carlo declared that they gave a low score to acronyms and specialised terms because ``there were only one or two of them in the test speech''. Since we did not find the outcome of these questions to produce reliable information, we decided to exclude them from the questionnaire in the main study and to dig deeper into users’ perception of the usefulness of SmarTerp for the individual types of problem triggers through the post-test interview.

The interview questions allowed us to gain insight into participants’ perspective, as we were expecting, and therefore the interview protocol remained unaltered.

Overall, the convergence of methods appears to be a strength of the study as it allows to mitigate the confusion that may arise from the interplay of several uncontrolled variables, related both to the machine side (e.g., CAI tool accuracy, functionalities, design features) and to the human side (idiosyncratic factors) of the interpreter-CAI tool interaction
