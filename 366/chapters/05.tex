\chapter{SmarTerp}\label{ch:smarterp}

This chapter presents the case study on which this work is based: the CAI tool SmarTerp. The chapter clarifies the background of the project, outlines its design and development process, and details the analysis activities conducted to identify users’ requirements and define UI design features. Finally, it clarifies the role of testing within the design and development project, which will be presented in detail in the following empirical part of the work (Chapters~\ref{ch:methods}--\ref{ch:second_iteration}).

\section{Background}

The SmarTerp\footnote{\url{www.smarter-interpreting.eu}} project is an Innovation Activity funded by the European Union in the framework of the EIT Digital BP2021. Its goal is to develop a remote simultaneous interpreting (RSI) platform with an integrated ASR- and AI-powered CAI tool. The project was started by the Spanish conference interpreter Susana Rodríguez. Under her coordination, an interdisciplinary team started working on SmarTerp in the autumn of 2020. The EIT Digital funding period ended in December 2021. The timeline of the research and development activities is depicted in \figref{fig:fig_timeline}.

\begin{figure}
    \includegraphics[width=\textwidth]{figures/fig_timeline.png}
    \caption{Timeline of research activities \label{fig:fig_timeline}}
\end{figure}

The early development of SmarTerp under the EIT Digital grant began in the Autumn of 2020. The Activity Leader Susana Rodríguez organised a series of think tanks with several stakeholders to begin preparing a document on interpreters' requirements for the development of the CAI tool and the RSI platform.

The design, development and research activities began in January 2021 and continued until the end of the year. They were conducted by a consortium of universities, institutes and external consultants working from Spain, Italy, Germany and New Zealand. As for the Spanish side, the teams involved are \textit{Next Generation Internet Group} (GING) and \textit{Ontology Engineering Group} (OEG) from the \textit{Technical University of Madrid} (UPM). The former was responsible for telecommunication and the development of the RSI platform, and the latter was responsible for language technologies and natural language processing. The Italian teams involved are the Trento-based \textit{Fondazione Bruno Kessler} (FBK) and the \textit{University of Bologna/Forlì}. FBK, in particular the SpeechTek Research Unit, was responsible for the Speech Recognition engine and its integration into the AI module. The University of Bologna was responsible for piloting the solution with their students. Within the Spain-based HCI team, Giona Fossati led the user research activities and created a design concept and Iciar Villamayor was responsible for the development.

I began collaborating on the project first as a volunteer in the focus group organised by Susana Rodríguez in the Autumn of 2020, which opened the analysis activities prior to CAI tool design (see below). From January 2021, I began to collaborate as an external researcher. My role within the team was of User Researcher. I was presented with the need to test the CAI tool empirically to improve its usability at an early stage of its development.  I was responsible for designing a study that would match this practical need. At the same time, as an academic researcher, it was my interest to ensure a level of scientific rigour required to yield insights capable of informing current scientific understanding of interpreter-CAI interaction and, possibly, making progress towards the development of general principles for the design of CAI tools in line with interpreters’ needs. After designing the study, I developed the study materials, procedures and analysis criteria, recruited participants, collected and analysed data and, finally, developed design recommendations which I discussed with the rest of the SmarTerp team in workshops after each round of testing and presented in a final report. I shared my study materials and methods with the other members of the Consortium so that they could be used in subsequent rounds of testing.

At the time of writing, the RSI platform reached the Technology Readiness Level\footnote{\url{https://www.esa.int/Enabling\_Support/Space\_Engineering\_Technology/Shaping\_the\_Future/Technology\_Readiness\_Levels\_TRL}} (TRL)9, with the actual system proven in an operational environment and ready to be marketed. The CAI tool reached TRL7 and TRL8 and is expected to reach TRL9 by the end of 2022. In the meantime, the system is being further improved with new research conducted on the development of an end-to-end system replacing the cascaded system behind the CAI tool. At the time of writing, the CAI tool works only integrated into the RSI platform. However, in the future, the development of the CAI tool as a stand-alone solution decoupled from the RSI platform is foreseen. More research is also being conducted to improve the accessibility of the tool for visually impaired interpreters.


\section{Design and development process}


The design and development process of the CAI tool SmarTerp was inspired by the usability engineering approach to product development (cf. \chapref{ch:usability_engineering}). The process was envisaged to incorporate users’ requirements, developed through various data sources, early into the design of the tool. It was also planned to incorporate data into the whole process to refine the design through cycles of testing and improvement. The detailed design process and all the research activities conducted by the HCI team are described in detail in the Master’s degree thesis by \citet{fossati2021smarterp}. The core phases of the design and development process and the activities that were conducted in each phase are detailed in \citet{fossati2021smarterp}. In the remainder of this chapter, I will summarise the key requirements identified through the research activities I was actively involved in, namely the focus group and the literature review concerning the CAI tool.


\section{Need analysis}


\subsection{Focus group and contextual inquiry}

The first phase, from September 2020 to March 2021, consisted in the \textit{analysis of users’ needs and requirements} through the expert focus group, contextual inquiry, users’ personas and literature review methods. The expert focus group was directed by the project coordinator Susana Rodríguez and took place from September to November 2020. The participants were 15 practising conference interpreters and interpreting studies scholars with expertise in interpreting technology and related fields. In four distinct sessions of approximately 1.5 hours each, they provided input on the needs of interpreters and the requirements for an in-booth CAI tool. As a result of the focus group discussions, the product owner drafted a report, in which she detailed the needs expressed by participants and their requirements on the CAI tool. The document was then sent to the focus group participants for them to check its accuracy and completeness. After this stage, from January to March 2021, SmarTerp’s user experience researcher, Giona Fossati, expanded the analysis of users needs through contextual inquiry and cognitive walkthrough methods with expert interpreters (cf. \cite{fossati2021smarterp}). The user needs identified from this stage of analysis are reported below.
\begin{quote}
    \textit{Numbers:} users need to...
    \begin{itemize}
        \item[-] See the numeral in its final version (and not propose partial renditions of it, i.e. the provisional ASR results, which are seen as a distractor)
        \item[-] See the numeral with punctuation based on the target language, e.g. 2020 (years) vs. 2,020 (quantity)
        \item[-] See the numeral and the element it refers to
        \item[-] See numerals converted in the target-language system, e.g. 1 billion (EN) $\longrightarrow$ mil millones (ES)
        \item[-] Have units of measurement converted and rounded, e.g. 54 gallons $\longrightarrow$ 204,41 litres
    \end{itemize}
    \textit{Terms:} users need to...
    \begin{itemize}
        \item[-] See both the source language and target language, so that they can detect potential machine errors (acronyms, homonyms, minimal pairs or a pair of words that differ in only one element of their pronunciation such as pin-bin)
        \item[-] See the origin of the suggested term (whether it comes from the interpreter’s glossary, a specialised dictionary, a database, etc.)
        \item[-] Review and validate/modify displayed terms record online/at the end of the session and store/download for future occasions
    \end{itemize}
    \textit{Named entities:} users need to...
    \begin{itemize}
        \item[-] See further information about the place, person, thing, etc. being referred to, in an expandable link
    \end{itemize}
\end{quote}
Further needs expressed by focus group participants were for items to be clearly identifiable and always visible on the screen. As predictable, considering that ``users are not designers'' (cf. \chapref{ch:usability_engineering}), when it came to the exact design features opinions diverged substantially. Some interpreters hypothesised that they may find the whole running transcript most helpful, others that they would find individual elements most helpful. Some speculated that all problem triggers should be displayed in a single interface field, others would want to see them divided into three separate fields. While all interpreters agreed that some visual signalling would make the items clearer to identify, some suggested doing so through colours, others through size, brightness, etc. All focus group participants confirmed the utility of the CAI tool in its core function that consists in displaying named entities, numbers, as well as specialised terms and acronyms




\subsection{Design-focussed literature review}


While the contextual inquiry proceeded in parallel, from January to February 2021, I conducted a literature review of empirical CAI research. The focus of my literature review was on previously highlighted requirements for in-booth CAI tool use as well as possible design hypotheses emerging from the findings of previous studies. The key issues emerging from the literature review were discussed in a workshop with the UI design team to integrate relevant information into the developing design concept.

A first section of the literature review focused on previous studies on CAI tools and the general requirements for CAI tool development. I reported the general principles defined by \citet{fantinuoli2017speech} for ASR-based CAI tools:
\begin{quote}
    To be used with a CAI tool, an ASR system needs to satisfy the following criteria at minimum:
    \begin{itemize}
        \item[-] be speaker-independent
        \item[-] be able to manage continuous speech
        \item[-] support large-vocabulary recognition
        \item[-] support vocabulary customisation for the recognition of specialised terms
        \item[-] have high performance accuracy, i.e. a low word error rate (WER)
        \item[-] be high speed, i.e have a low real-time factor (RTF)
    \end{itemize}
    [...]
    As for CAI tools, in order to successfully support the integration of a ASR system, the tool needs to satisfy the following requirements:
    \begin{itemize}
        \item[-] \begin{sloppypar}high precision, precision being the fraction of relevant instances among the retrieved instances\end{sloppypar}
        \item[-] high recall, recall being the fraction of relevant instances that have been retrieved over the total amount of relevant instances present in the speech
        \item[-] if a priority has to be set, precision has priority over recall, in order to avoid producing results that are not useful and may distract the interpreter
        \item[-] deal with morphological variations between transcription and database entries without increasing the number of results
        \item[-] have a simple and distraction-free graphical user interface to present the results.
    \end{itemize}
    \citep{fantinuoli2017speech}
\end{quote}
I also reported the findings of the EABM survey \citep{eabm2021b}. I reported that most respondents to the survey expressed the following preferences:
\begin{itemize}
    \item A vertical layout where new items are added under previous items (59.24\% of respondents).
    \item Items remain on screen as long as possible and only disappear when there is no longer any room left on screen (82.29\%).
    \item Terms are on the left, numbers on the right (39.62\%) or both items are in the same interface section (39.43\%).
    \item New items appear in a bold font (38.11\%), a larger font size (23.32\%) and/or in a different colour (27.87\%).
\end{itemize}
As predictable, survey respondents’ opinions diverge substantially on concrete UI features. Furthermore, because ``users are not designers'' and ``users do not know what’s best for them'' (cf. \chapref{ch:usability_engineering}), the design team agreed that the EABM survey findings should be used as a starting point to develop hypotheses about users’ requirements but not directly applied as design principles.

I then reviewed relevant findings concerning the interpretation of numbers and the inherent sources of error -- a topic which I had researched extensively in the past \citep{frittella2017numeri,frittella2019a} and training conference interpreters on \citet{frittella2019b}. The HCI team felt that a dedicated literature review was needed on this specific topic because the design of CAI tool UI for numbers required choosing between a variety of options (e.g. should numerals be displayed as a word or an Arabic numeral? In the source and the target language? What else should we display? etc.)

My literature review provided a brief summary of the causes why errors occur in the SI of numbers (cf. \cite{frittella2017numeri,frittella2019a}) to shed light on possible design principles. First, the cognitive processing of numerals during SI may be simplistically modelled as the sub-processes of \textit{decoding} (comprehension of the source-language numeral), \textit{transcoding} (turning the mental representation into a graphic numeral, e.g., in the Arabic code), \textit{recoding} (turning the graphic numeral into the target-language numeral). All these sub-processes are likely to require some degree of cognitive control, i.e., not to be automatic -- which, among other evidence, is revealed by the fact that errors may occur in each of these phases. Another example where it is possible to see the non-automaticity of these sub-processes is the fact that interpreters often write down large source-language numerals not as a full Arabic numeral but rather as a mixed code of Arabic numerals, for the digits, and phonological code, for the order of magnitude -- most commonly when above “thousand” (e.g., three million $\longrightarrow$ 3Mio). The mixture of codes is likely to be utilised to simplify the transcoding and recoding processes. In their recent study on the CAI-tool supported SI of numbers, \citet{pisani2021measuring} displayed the numeral as a combination of graphic Arabic code for digits and \textit{source-language} phonological code for orders of magnitude above thousand. The results show errors in participants’ recoding of the order of magnitude (e.g., they saw “million” appear on the screen and interpreted it as “billion”). This error pattern is predictable in light of the discussion above. I hence postulated that, to be maximally efficient, a CAI tool should support the interpreter in all phases of item processing. In the case of numbers, this means supporting the whole process until the recoding phase. Therefore, we decided to display numerals as a mixture of graphic Arabic code for digits and target-language phonological code for orders of magnitude above thousand.

Second, interpreting numbers does not only mean transcoding digits. Numerals are only one part of a \textit{numerical information unit} (NIU). To express the meaning of the NIU, the interpreter must render the other components of the information unit as well as the numeral. These are the \textit{referent} (i.e., the entity that the numeral refers to), the \textit{unit of measurement} (the standard measure of the given value), the \textit{relative value} (e.g., increase, decrease, etc.), as well as its \textit{temporal} and \textit{spatial} location. Ideally, a CAI tool should help the interpreter reconstruct the whole NIU since numerals alone out of context do not convey meaning. However, this is not possible at the current stage of technological development. Currently, item extraction is based on word recognition or may take place through a syntactic analysis of the speech. Identifying the components of the NIU would require a real-time semantic analysis, which current algorithms cannot perform. Therefore, I recommended that the CAI tool display at least the numeral together with the following element in the sentence, which is usually either the referent (as in 3 \textit{people}) or the unit of measurement (as in $3m^2$).









\section{Design requirements and features}

The user requirements derived from the focus group discussions, along with relevant previous research and the knowledge of the HCI experts within the project, led to the formulation of principles for the design and development of SmarTerp. Below, I will summarise key user requirements extrapolated from the various sources used in the need analysis phase and map the chosen design principles onto those requirements. The principles are divided into the categories (1) general UI features, (2) problem triggers, and (3) technical specifications.



\subsection{General UI features}


The principles concerning the general UI features of SmarTerp refer to the ways in which elements are organised on the interface. Before detailing the principles, a brief terminological clarification is necessary at this point. The design team referred to the individual elements (terms, numbers, etc.) as \textit{items} and to the graphic unit containing an item as \textit{item box}. A \textit{module} is a user interface section containing items of the same category. \figref{fig:8} identifies these key interface units on the first version of the SmarTerp interface.

\begin{figure}
    \includegraphics[width=\textwidth]{figures/fig_SmarTerp.png}
    \caption{SmarTerp's interface -- first prototype \label{fig:8}}
\end{figure}

\begin{sloppypar}
The first requirement concerning general UI features for SmarTerp that emerged from the focus group discussions was that as many items as possible should be simultaneously visible. For this reason, the design team decided to structure the interface into three modules. In left-to-right order, the modules were named entities, terms and acronyms, and numbers.
A further requirement was that items should be clearly and immediately identifiable. To fulfil this requirement, the design team decided to place items into an item box and highlight new items in a different colour.
\end{sloppypar}

Finally, a requirement of users was that the interface should be as unobtrusive as possible. The design team hypothesised that displaying items as a scrolling list, with new items appearing on top and causing already displayed items to scroll down, may attract the attention of users causing unnecessary distraction. They therefore opted for a mode of display where a new item replaces the last one to appear on the screen. For example, if items A, B, and C are already displayed, item D will replace A, E will replace B, etc.

\subsection{Item classes}

The design principles concerning item classes led the design team to defining how the problem trigger classes should be presented. The design team decided that \textit{named entities} should be transcribed using the official transcription in the target language because this emerged as the preferred option from the focus group discussions. Coming to \textit{terms} and \textit{acronyms}, users expressed the need to be able to check the accuracy of the tool’s suggestions. For this reason, the design team decided to display terms and acronyms both in the source- and the target language, with the TL version being highlighted to make it more easily identifiable. Users also wished to be presented both with the short and the long version of acronyms, so that they could select which version to use based on the audience’s prior knowledge and time constraints in interpreting. When deciding how to display numbers, the requirements were derived both from the focus group and from the literature review (presented above). Because of the complexity of all phases of numerical transcoding, where errors can emerge also in the transcoding from Arabic graphic numeral to the TL numeral, I formulated the design requirement that numerals should be displayed as a mix of graphic Arabic code (for digits) and TL verbal code (for orders of magnitude above “thousand”). Where the SL and TL code do not correspond, the numeral should be recoded into the TL (e.g., 1 billion should be displayed as “mil millones” in Spanish). Furthermore, because errors in the interpretation of numbers may be caused not just by the numeral itself but, rather, by the whole numerical information unit, I recommended that the numeral be accompanied by the unit of measurement and the referent in the same item box. Because this was technically unfeasible, the developers proposed to display the following element in the sentence in the item box, which could be either the unit of measurement or the referent depending on the sentence structure.

\subsection{Technical specifications}

A final set of principles concerned the technical specifications of the tool. Based on the literature review, in particular the requirements formulated by \citet{fantinuoli2017speech}, we posited that the latency should not exceed two seconds and that precision should be favoured over recall, i.e., items should only be displayed if accurate and complete.



\section{Testing}


Based on the requirements and features presented above, the HCI team designed an interface concept, which was reviewed internally. Then, they developed a high fidelity prototype, which was then tested empirically in two usability tests (see timeline in \figref{fig:fig_timeline}).  Because the tests took place at an early stage of the CAI tool’s development, we referred to the testing cycles as \textit{Early Testing}, of which I was responsible. The testing consisted in two cycles. The first cycle, which we called \textit{pilot study}, aimed at gathering initial evidence by testing the prototype on 5 conference interpreters. We expected that this first round of testing would allow us to detect and correct a number of bugs and basic usage problems that we could correct as early in the development as possible. The pilot study also aimed at validating the testing methodology, which was a necessity because the materials, procedures and methods are novel in the field of interpreting technology and CAI tool research. The second cycle, which we called \textit{main study}, aimed to gather evidence on a larger sample (10 conference interpreters) using the CAI tool UI polished from the issues identified in the pilot study. We expected that, working with a more mature prototype and validated methods and materials, we would identify deeper usage problems and focus on more fine-grained design principles.



Before defining the most suitable testing method and designing the study, I discussed the aims of testing with the whole team to make sure that expectations and key objectives would be met. Through the discussions, I realised that the team had objectives of different types that they hoped to achieve through the early testing. The central objective was of validating or discarding the design principles that guided the UI design to improve it. Another objective of the HCI team was to ascertain the extent to which the tool fulfilled its purpose -- that of supporting users in achieving an accurate performance, and whether it was perceived as satisfactory. A further objective was gaining a deeper understanding of users and their needs, revising and expanding our analysis. This aim emerged from the fact that interpreters’ needs related to the use of ASR-powered CAI tools in the booth have not yet been the object of dedicated empirical explorations. Therefore, questions related to issues such as users’ acceptance (why do interpreters introduce a CAI tool into their workflow? why not?), perceived utility (when is a CAI tool most helpful to interpreters? under which circumstances is it less helpful?), training needs (do interpreters need to be trained on using CAI tools? what type of training?) etc., are still largely unexplored. Finally, my personal objective was to inform scientific understanding of interpreter-CAI interaction and contribute to defining principles for the design of CAI tools’ interface adequate to interpreters’ needs. In the following chapter, I will discuss the methods of the study in detail.
