\chapter{Interpreting technology}\label{ch:interpreting_technology}

This chapter analyses interpreting technology with a focus on computer-assisted interpreting (CAI) tools and related research from a usability engineering perspective (discussed in Chapter \ref{ch:usability_engineering}). The analysis offered in this chapter positions the present work within previously conducted research, identifies the knowledge gap, and sheds light on the relevance of usability-focused CAI research. After a definition of ``interpreting'', this chapter examines the history of interpreting technology and the development of computer-assisted interpreting tools. It then provides a succinct review of CAI tool research, with a focus on studies dedicated to informing the design of CAI tools through the analysis of users’ needs and requirements as well as through empirical testing. The chapter concludes with a discussion of the extent to which a usability engineering approach is reflected in the development of CAI tools and the relevance of usability research for the future developments of the profession.

\section{Developments and current landscape}

While translation was defined as a written, textual operation (see \chapref{ch:translation_technology}), interpreting may be defined as a translational activity characterised by its immediacy: ``in principle, interpreting is performed “here and now” for the benefit of people who want to engage in communication across barriers of language and culture'' \citep[10]{pochhacker2004}. It is a form of oral translation enabling direct communication between two groups, especially where a lingua franca does not exist. Interpreting in all its forms is studied in the academic discipline of \textit{Interpreting Studies} (IS) which emerged as a subdiscipline of translation studies around the 1990s and since then has gained an increasingly distinct identity \citep{pochhacker2015a}.

There are several modes of interpreting. Liaison interpreting (also dialogue interpreting) is performed in a variety of business, community and diplomatic encounters in which the interpreter is physically and/or metaphorically ``the person in the middle'' of the communication and performs a connecting function \citep{merlini2015}. Consecutive interpreting is mainly performed in conference settings and involves the rendition of a whole speech from the source language (SL) into the target language (TL) \textit{after} it has been fully uttered by the speaker \citep{andres2015consecutive}. Simultaneous interpreting (SI) is the rendition of the message from SL to TL in real-time, \textit{while} it is being uttered by the speaker and typically takes place in conference and court settings \citep{pochhacker2015b}.  It can be further differentiated based on the supporting equipment being used \citep{pochhacker2015b} which distinguishes simultaneous interpreting (performed through special equipment) from its precursor form of \textit{whispered interpreting} (or \textit{chuchotage}, in which the interpreter whispered the interpretation into the ears of the audience) and from the more recent \textit{remote simultaneous interpreting} modality (also called distance interpreting and defined as ``interpreting of a speaker in a different location from that of the interpreter, enabled by information and communications technology (ict)'').

It is often said that interpreting is the second oldest profession in human history \citep[10]{baigorri2014paris}. Interpreting has existed as long as there have been different spoken languages in contact, long before written language and hence written translation. Written accounts of the activity of the interpreter through history are scarce but they may be found as early as 3000 BC in ancient Egypt, where the activity of interpreting was first institutionalised and designated by its own hieroglyphic (cf. \cite{falbo2016going,kurz1985rock}). Interpreters are mentioned several times in the Bible, for instance in Corinthians (14:28): ``If any man speak in an unknown tongue, let it be by two, or at the most by three, and that by course; and let one interpret. But if there be no interpreter, let him keep silence''.

\begin{sloppypar}
Despite such a longstanding tradition, conference interpreting began to emerge as a profession in Europe in the 20th century \citep{baigorri2014paris}. The first conference interpreters came to work in government departments after World War I as well as in early international organisations. At the time, the pluralism of languages began to characterise the world of diplomacy, which had previously been monolingual, with Latin in medieval and renaissance western Europe and French in the early 20th century being used by diplomats of different origins as lingua franca. During high-level international meetings, the early conference interpreters enabled participants speaking different languages to communicate with each other by turning their messages (from short statements to whole speeches) from one language into another in the consecutive mode.
\end{sloppypar}

Since then, the rapid evolution of the conference interpreting profession has been stimulated by historical events, the emergence of new political structures (especially international organisations), new societal needs and it has been propelled by technological advancement. Scholars define the impact of technology on the interpreting profession as a series of ``breakthroughs'' \citep{fantinuoli2018a,tripepi2010usefulness}.

The first breakthrough in the development of conference interpreting was marked by the birth of simultaneous interpreting. In the 1920s IBM patented a wired system for real-time speech transmission which was adopted by the Sixth Congress of the Comintern in the former Soviet Union, then at the International Labour Conference and irreversibly made popular by the Nuremberg trials \citep{fantinuoli2018a}. The advantage of this new system was that it curbed conference times and made it possible to host a conference in multiple languages at the same time. SI grew at a rapid speed despite the resistance of early conference interpreters, who feared that being relegated at the back of the room might diminish their prestige \citep{fantinuoli2018a}. Since then, SI became the standard interpreting mode employed at international meetings and organisations such as the United Nations and the European Union (EU)  --  the biggest employer of conference interpreters in the world \citep{european2016}.

The second breakthrough was introduced by the World Wide Web, which changed interpreters’ access to knowledge. To comprehend how the internet changed the profession, one must consider the crucial importance of preparation for interpreters:

\begin{quote}
    Interpreters are knowledge workers constantly moving back and forth between different contexts, languages and conceptual systems. To do this, they rely on their own knowledge base being complemented by external information sources explored in what can be called ``secondary knowledge work'' in order to properly perform the actual, ``primary'' knowledge work, i.e. the activity of interpreting work as such ... In-depth preparation is therefore all the more important. It involves specific terms, semantic background knowledge and context knowledge about the assignment. \citep{rutten2016professional}
\end{quote}
While prior to the internet interpreters had to acquire terminology and specialised knowledge for each assignment through paper-based sources, the internet made it possible to access a wealth of information at any time. According to some interpreters and scholars, the internet increased the efficiency of interpreters’ preparation: ``The greatest advantage of new information technology lies in the speedier collection of information from different sources and easier management so that preparation is more efficient and results in improved quality'' \citep[81]{kalina2010new}. Others, point to both pros and cons in the ``surfeit of data'' \citep{donovan2006interpreting} with which interpreters were suddenly confronted in their preparation. The risk for interpreters is to be carried away by the information overload in their preparation.

\begin{sloppypar}
According to \citet{fantinuoli2018a}, recent technological developments brought about a third breakthrough in conference interpreting with even greater transformative potential than the previous two: ``Bigger by one order of magnitude if compared to the first two breakthroughs, its pervasiveness and the changes that it may bring about could reach a level that has the potential to radically change the profession'' \citep[3]{fantinuoli2018a}. Given the breadth and depth of these changes, he suggests calling the recent third breakthrough in conference interpreting the \textit{technological turn}.
\end{sloppypar}

Today, the interpreting technology landscape presents a plethora of technological solutions that are involved in the technological turn in the profession. \citet{braun2019technology} distinguishes four categories of interpreting technologies. The first comprises all technologies used to deliver interpreting services and enhance their reach, such as RSI equipment, giving rise to \textit{technology-mediated interpreting}. The second category includes all technologies (both generic and bespoke for interpreters) that can be applied to support or enhance the interpreter’s preparation, performance, and workflow, leading to \textit{technology-supported interpreting}. The third category comprises all technologies that are designed to replace human interpreters, leading to \textit{technology-generated} or \textit{machine interpreting} (MI). Machine interpreting (also known as automatic speech translation, automatic interpreting or speech-to-speech translation) may be defined as ``the technology that allows the translation of spoken texts from one language to another by means of a computer program'' \citep[5]{fantinuoli2018a}. An audible version of the SL speech into a TL speech is generated combining automatic speech recognition (ASR), to transcribe the oral speech into written text, machine translation (MT), and speech-to-text synthesis (STT, \citealt[5]{fantinuoli2018a}). A fourth category of interpreting technologies in Braun’s classification comprises all forms of \textit{technology-enabled hybrid modalities}, such as respeaking.

\begin{sloppypar}
Technologies for technology-mediated and technology-supported interpreting, in Braun’s classification, correspond to setting-oriented and process\hyp{}oriented technologies in \citegen{fantinuoli2018b} classification, which further elaborates on the impact that these technologies exert on interpreting. Setting\hyp{}oriented technologies ``primarily influence the external conditions in which interpreting is performed'' \citep[155]{fantinuoli2018b}. In Fantinuoli’s view, these have an impact on the interpreter profession, its status, and working conditions, but do not radically change the nature of the mental task performed by interpreters. By contrast,\textit{ process-oriented technologies} ``support the interpreter during the different sub-processes of interpreting and, consequently, in the various phases of an assignment, i.e. prior to, during and possibly after the interpreting activity proper'' \citep[155]{fantinuoli2018b}. In Fantinuoli’s view, by becoming an integral part of the interpreting process, these technologies are directly linked to and might influence the cognitive processes underlying the task of interpreting.  The central process-oriented technologies giving rise to technology-supported interpreting are called \textit{computer-assisted interpreting} (CAI) tools.
\end{sloppypar}

\section{Computer-assisted interpreting (CAI) tools}


CAI tools may be defined as ``all sorts of computer programs specifically designed and developed to assist interpreters in at least one of the different sub-processes of interpreting'' \citep[155]{fantinuoli2018b}. Better specified, the aim of CAI tools is to ``improve the interpreters’ work experience, by relieving them of the burden of some of the most time-consuming tasks ... both during preparation and during the very act of interpreting'' \citep[4]{fantinuoli2018a}. By doing so CAI tools ultimately aim to increase the interpreters’ productivity as well as the quality of his/her performance.

This definition differentiates CAI tools from general software solutions that were not developed for interpreters, but that can be used by interpreters in their works, for instance, word or Excel files used to create glossaries. It also specifies that CAI tools aim to \textit{support} interpreters, not to replace them contrary to MI technology in one or more sub-processes of interpreting or phases of the interpreter’s workflow.

For the sake of simplification, these phases can be defined as preparation (\textit{pre-booth}), interpretation (\textit{in-booth}) and follow-up (\textit{post-booth}, \citealt{eabm2021a}). This definition is based on the breakdown of the interpreting workflow as \textit{pre}-, \mbox{\textit{peri}-}, \textit{in}- and post-process \citep{kalina2000interpreting}. While the follow-up phase is essentially about revising and expanding the interpreter’s knowledge based on the information gathered during the assignment, the two most important phases which have been central to the development of CAI tools are the pre-booth and the in-booth phases.

In the preparation or pre-booth phase, CAI tools aim to increase the efficiency of interpreters’ preparation for their assignments. Interpreters’ preparation generally consists of reading materials provided by the speaker and the conference organisers to gain language knowledge (i.e. terminology), content knowledge (i.e. information about the topic that will be interpreted), and context knowledge (i.e. information about the communicative setting in which interpreting takes place; \citep{rutten2007informations}. If preparation material is not provided or insufficient, interpreters have to find relevant material themselves. In this phase of the interpreter’s workflow, CAI tools can help interpreters create glossaries, manage them, and share them with colleagues. The extent to which the glossary creation process is supported essentially depends on the degree of sophistication of the CAI tool, as explained below.

In the interpretation or in-booth phase, CAI tools aim to help interpreters access information while interpreting, easing the burden related to the interpreting task and ultimately increasing delivery accuracy. While interpreters perform SI, they retrieve specialised terminology found in the preparation phase from their memory, which may imply a certain mental effort if the terminology is not so consolidated for the retrieval to be automatic. When in doubt, they search for the TL equivalent term in their glossary. If the term is not present in their glossary, they search for it in online databanks, dictionaries etc. In general, these processes are considered to be ``time-consuming and distracting in an activity that requires concentration and fast-paced decoding and delivery. The interpreter at work may not have the time or the cognitive ability to look up a word online or in his/her electronic dictionary, or detect and choose the correct translation of a specific term among the myriad of possible solutions that are generally offered by dictionaries'' \citep[90]{tripepi2010usefulness}. During SI, interpreters have to deal with not just specialised terminology but also other factors of complexity, known as ``problem triggers'' \citep{gile2009basic}. These elements of speech, such as acronyms, named entities, numbers and specialised terms, are generally considered to be highly problematic and are associated with higher-than-normal error rates. Support for problem triggers is sometimes provided by the booth colleague, who may look up an unknown term or jot a numeral down. However, this support is not always reliable and efficient:

\begin{quote}
    Even the help of the fellow colleague in the booth may sometimes prove useless in real time oral translation, and may even slow down the interpreting process. The interpreter, when hearing something unknown, is often alone and has nothing to resort to but his/her own memory and mind (ibid: 77). A simultaneous interpreter at work cannot wait for more than half a second for a missing word otherwise his/her narrative would sound broken and the short memory be overburdened ... The activity of searching for the right term may result in distraction and loss of concentration for the interpreter. \citep[91]{tripepi2010usefulness}
\end{quote}
Depending on their architecture and functionalities, CAI tools have been traditionally divided into \textit{generations} \citep{eabm2021a,fantinuoli2018b,prandi2022a,prandi2023}. \textit{First-generation CAI tools} are defined as ``programs designed to manage terminology in an interpreter-friendly manner'' \citep[164]{fantinuoli2018b}. They were designed to support interpreters in the pre-booth phase of preparation. They developed from the concept of terminology management systems for terminologists and translators but present a simpler entry structure which is more adequate to the interpreter’s terminology work. They offer basic functionalities to look up glossaries in the booth which are similar to the look up system in a word or Excel file. To query the glossary, the interpreter types the term or part of the term in the search field and presses the enter key.

\textit{Second-generation} tools offered ``a holistic approach to terminology and knowledge for interpreting tasks and ... advanced functionalities that go beyond basic terminology management, such as features to organise textual material, retrieve information from corpora or other resources (both online and offline), learn conceptualised domains, etc'' \citep[165]{fantinuoli2018b}. Other than such advanced functionalities for knowledge acquisition and management, the second generation of tools started bespoke functionalities for the in-booth phase. Advanced search algorithms were introduced to ease glossary query during SI, i.e. taking into account the time constraints and the complexity of the interpreting task. For example, \textit{fuzzy search} acts as an error correction mechanism for misspelling in the word typed by the interpreter or present in the glossary, \textit{stopwords exclusion} reduces the number of matches displayed as a query result, \textit{dynamic search} finds terms without interpreters having to press the enter button, and \textit{progressive search} searches for the term in other glossaries of the interpreter if it is not found in the glossary that is currently being queried \citep[166]{fantinuoli2018b}.

The introduction of artificial intelligence (AI) into CAI tools paved the way for a \textit{third generation} \citep{eabm2021a,prandi2022a}. First, this technology has made it possible to automate parts of the preparation task, such as by automatically compiling glossaries starting from interpreters’ preparation material. Second, the combined use of AI and automatic speech recognition (ASR) technology has made it possible to automate the query of interpreters’ glossaries and provide support for all major classes of problem triggers (c.f. \cite{fantinuoli2017speech}). Not only are the TL-equivalent of specialised terms automatically extracted from interpreters’ glossaries, but also acronyms, named entities and numbers may be displayed in real-time on interpreters’ laptop screens. Given the novelty of these tools, they currently present technological limitations. The major problems are the system’s \textit{latency}, which is the delay between the uttered word and its appearance on the interpreter’s screen, and its \textit{accuracy}, i.e. the possibility that the tool might produce errors in the displayed items or omit some information. Current tools are based on a \textit{cascaded system} of independent ASR and AI modules, which first recognise and transcribe the source speech, then extract problem triggers and finally convert them into a graphic representation that is displayed to the interpreter. This increases both system latency and the risk of inaccuracies due to the propagation of errors from one module to the next. To overcome these limitations, developers are currently working on an end-to-end system which, they suggest, may dramatically increase the potential of CAI tools and introduce a \textit{fourth generation} of tools \citep{gaido2021moby}.








\section{CAI tool design, development and reception}

The development of CAI tools has been initiated by conference interpreters with programming skills. in some cases, or by conference interpreters in tandem with developers, in other cases. A comprehensive review of past and present CAI tools is offered by \citet{stoll2009jenseits,costa2014comparative,will2015eignung}, and \citet{fantinuoli2016interpretbank} among others.

At the time of writing, the commercially available and actively maintained first-generation CAI tools are \textit{Interplex}\footnote{\url{https://interplex.com/}}, \textit{Interpreters’ Help}\footnote{\url{https://interpretershelp.com/}}, and \textit{InterpretBank}\footnote{\url{https://www.interpretbank.com/site/}} (which was later developed into a second-generation and then a third-generation CAI tool). Two CAI tools, InterpretBank and \textit{LookUp}\footnote{\url{http://www.lookup-web.de/index.php}}, were developed within doctoral research projects, but only InterpretBank has made it to the commercial stage. Interplex is a glossary builder and management tool for both translators and interpreters first released in 2003. It was designed by conference interpreter Peter Sand and developed by software developer Eric Hartner. After the first release, the software has been ``tweaked and re-tweaked'' by its creators, ``taking on board the many suggestions received from interpreters using it in the booth'' \citep{sand2015}. Interpreters’ Help is a cloud-based glossary builder and management tool for conference interpreters first released in 2013. It was designed and developed by software developers Yann Plancqueel and Benoît Werner with the consultancy of two conference interpreters who are part of the team. InterpretBank was first developed as a first-generation CAI tool and then became the first second-generation CAI tool. Its functions were glossary building, management, and look up during simultaneous interpreting. It was designed and developed between 2008 and 2012 by conference interpreter and scholar Claudio Fantinuoli as part of his doctoral research project at the University of Mainz/Germersheim \citep{fantinuoli2016interpretbank}. In its subsequent development, it was the first tool to introduced advanced search functions for in-booth use, which makes it particularly popular among interpreters. The starting point for the design was a review of the literature modelling interpreters’ workflow and analysing interpreters’ needs, as presented in the previous section.

Currently, the ASR- and AI-powered “third-generation” CAI tools on the market are \textit{InterpretBank ASR} and \textit{Kudo’s InterpreterAssist}\footnote{\url{https://kudoway.com/kudo-interpreter-assist-artificial-intelligence/}}  -- as well as SmarTerp (presented in Chapter \ref{ch:smarterp}). InterpretBank ASR \citep{fantinuoli2017speech} is the first-ever third-generation CAI tool. It is the eighth release of InterpretBank, in which ASR and AI technology was introduced to retrieve terms from interpreters’ glossaries and automatically display numerals as a cloud-based experimental feature. At present, users can choose between two UIs of InterpretBank ASR. In the first, numbers only are provided in the form of a scrolling list with new numerals placed on top of the list and highlighted (see \figref{fig:4}).

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/Picture4-new.png}
    \caption{Representation of InterpretBankASR (numbers only view)}
    \label{fig:4}
\end{figure}

In the second UI, specialised terms and numerals are provided in two distinct columns. New items appear at the top of the scrolling list and are highlighted (see \figref{fig:5}). In both visualisations, numerals are displayed in the source language.

A research paper \citep[5]{defrancq2021automatic} describes a third possible interface option for InterpretBank ASR, which displays the whole source-language transcript and highlighted units of interest -- numbers and terms (see \figref{fig:6}).

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/Picture5-new.png}
    \caption{Representation of InterpretBank ASR (numbers and terms view)}
    \label{fig:5}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/Picture6-new.png}
    \caption{InterpretBank ASR transcript interface, from \citet[7]{defrancq2021automatic}}
    \label{fig:6}
\end{figure}

Kudo InterpreterAssist \citep{fantinuoli2022kudo} is a recent CAI tool developed in collaboration with Claudio Fantinuoli to be integrated into Kudo’s RSI platform. Currently, no information is available on the UI of this CAI tool.


Compared to CAT tools, CAI tools have been used to a much smaller extent \citep[89]{tripepi2010usefulness}. One factor that has certainly contributed to this more modest spread is the fact that CAI tools, so far, have been meant to be used by individual interpreters based on personal interest. CAT tools, on the contrary, have been adopted by agencies and institutions, which forced the adoption by translators. However, the voluntary nature of CAI tool use may change in the next year, with their growing integration into RSI platforms. Organisations have also shown increasing interest in adopting CAI tools to streamline the management of terminology and preparation materials for interpreters’ assignments. CAI tools have been received by practitioners with mixed feelings:

\begin{quote}
    These opposite views are nothing new: whenever change comes, there are those who feel delighted with it and those whose emotions range from fear or panic to anger and aggression. In the case of ICTs we could say that the delight would come because they can be seen as tools that can make the work of the conference interpreter much easier, in any of the phases of the interpreting, while the fear or anger may come from hurt pride, as many interpreters feel it a part of their self-image as professionals to be able to manage without anything except their extraordinary memories. \citep[27]{berber2010information}
\end{quote}
A further reason for the limited reception of CAI tools by the interpreting profession has to do with concerns of different nature. The first concern is linked to the complexity of the simultaneous interpreting task. The use of ICTs in the interpreting process has been defined as ``unnatural'' (e.g. \cite{donovan2006interpreting}). Professionals have maintained that the use of a new technological instrument in the booth may exacerbate the cognitive load in SI (cf. \cite{prandi2017designing}), disrupt the task and lead to a deterioration of performance rather than quality improvement. \citet{Roderick2018}, cited in \citet{fantinuoli2018a}, for example, spoke of ``alienation due to the use of new technology'' and contended that ``[using ICTs in the booth] can lead the interpreter to lose sight of the first aim of interpreting as we learn it, namely conveying meaning and facilitating communication''. Another concern that emerges in the dialogue with practitioners is of economic nature. Interpreters often raise the question of whether the use of new technologies increasing the efficiency of interpreters’ preparation may lead clients to demand a reduction in the price of interpreting services, similar to what happened with the introduction of CAT tools. Finally, a concern that is frequently expressed by professionals may be caused by the lack of knowledge of CAI tools. The frequent question posed by interpreters during public discussions ``aren’t these tools aimed at replacing us?'' shows that they are not fully aware of the difference between technologies aimed at replacing human interpreters and supporting them.

The way in which CAI tools have originally been developed may also be a factor contributing to their limited acceptance by professionals. While the initial development of some CAI tools (in particular, InterpretBank, which is the most used CAI tool, and LookUp) has drawn on scholarly work on interpreters’ workflow and preparation needs, the UI design has been ``more or less based on the personal ideas of their developers, mostly interpreters themselves, and lack[s] any experimental support in [the] design decisions'' \citep[160]{fantinuoli2018b}. Hence, CAI tool UI design has been predicated upon developers’ personal intuition and rather vague heuristics. For example, \citet[62]{fantinuoli2017speech} defines a ``simple'' and ``distraction-free'' UI as a requirement for an effective CAI tool. Defrancq and Fantinuoli postulate that, in order to effectively support interpreters, the in-booth CAI tools should present the visual input ``in an ergonomically suitable format'' \citep[4]{defrancq2021automatic}. However, the exact features and characteristics that make a CAI-tool interface ``simple'' and ``ergonomic'' are not specified. Furthermore, \citet[164]{fantinuoli2018b} points out that the development of CAI tools has been based on partial and unsystematic knowledge and has lacked ``test and improvement phases, which are crucial to achieve software maturity and find the golden standard in terms of functionalities desired by the professional group.''


\section{CAI tool research}

CAI tool research may be positioned within the IS sub-field of interpreting technology research, alongside research on remote interpreting and the use of technology in interpreter training. The academic interest in interpreting technology remained scarce until the first decade of the 21st century \citep{berber2010information}. Until recent years, research on CAI tools remained underrepresented in the interpreting technology research landscape \citep{fantinuoli2018b}. It is noticeable that, with the exception of the study by \citet{desmet2018simultaneous} and \citet{defrancq2021automatic}, most empirical studies on the use of CAI tools in the booth were conducted within the framework of Master’s degree theses and PhD theses, in the case of Prandi. \citet[87]{prandi2022b} interprets this as a sign of the increasing interest in CAI tools among younger generations, but it could also be interpreted as a sign of the marginalised role that (in-booth) CAI tools have played in academic research until recent times. Over the past couple of years, the spread of RSI propelled by the Covid-19 pandemic and the introduction of ASR technology into CAI tools has renewed the interest in different ICTs in interpreting and in the in-booth CAI tool use, in particular.

CAI tool research is a nascent research strand and therefore no specific definition has yet been provided. \citet[85]{prandi2022a} points out that ``the publications devoted to CAI tools address a number of topics and issues inherent to the tools, ranging from a simple presentation and comparison of the available solutions to the first empirical tests conducted on one or more tools''. Since CAI tool research is a relatively new field of inquiry that has been much less prolific than CAT tool research, the research landscape is still largely lacking clear research strands and agendas. We could broadly define CAI tool research as a research strand that is concerned with the design and development of CAI tools and their impact on the interpreting product and process.

The review below addresses previous research on CAI tools that has implications for their design. As in the review of CAT research (cf. \chapref{ch:translation_technology}), the focus will be on the most common methods used to develop recommendations for the design and their further improvement. I will first consider research providing input on users’ needs and general CAI tool requirements. Then, I will examine research focused on the empirical evaluation of CAI tools via tool performance, users’ performance, and users’ perception.



\subsection{Need analysis and tool requirements}


As discussed earlier in this chapter, the initial “first-generation” CAI tools aimed to support interpreters in the preparation of their assignments. Some scholarly work preceded the development of the first CAI tools in the analysis of interpreters’ needs in preparation and identification of the pain points that could be addressed by a technological solution. The final aim, which is more or less explicit in the different studies, is to develop a set of requirements for the development of CAI tools.

The first studies preceded and accompanied the development of first-gen\-er\-a\-tion CAI tools and focused on interpreters’ needs inherent to preparation and terminology management. Because at the time of the first studies the term ``CAI tools'' was not yet in use, they refer to their object of inquiry with terms such as ``interpreting-specific technologies'', ``electronic tools for interpreters'' etc. An important theoretical input to the development of CAI tools in this early stage came from authors such as \citet{gile1987terminotique}, \citet{kalina2000interpreting,Kalina2007}, and \citet{will2009dolmetschorientierte}, who defined the workflow of interpreters and modelled the processes inherent to preparation (cf. also \cite{kalina2015}). \citet{fantinuoli2006specialized} proposed a model of Corpus Driven Interpreters Preparation and discussed how a CAI tool can facilitate this preparation process.

Parallel to the theoretical studies, some empirical studies explored interpreters’ preparation and terminology management. For example, \citet{moser1992terminology} conducted a survey about how conference interpreters handle terminology documentation and document control to identify some guidelines for the development of CAI tools for terminology and documentation management. Another study that used survey methods to analyse interpreters’ needs in the use of technology for terminology management was conducted by \citet{bilgen2009investigating}. The results further shed light on the difference between interpreters’ and translators’\slash terminologists’ terminological needs and the requirements that a terminology tool for interpreters should have (e.g. simple entry structure, flexibility in the search process etc.). The studies by \citet{berber2010information} and \citet{pastor2016survey} surveyed the use of ICTs by interpreters to portray the landscape of available solutions and identify possible gaps. A different design was chosen by \citet{rutten2007informations}, who moved from a field study in which she observed interpreters in their terminology acquisition and management process to derive requirements for CAI tool development.

When it comes to need analysis related to the use of CAI tools in the booth, studies describing interpreters’ in-booth behaviour \citep{chmiel2008boothmates,duflou2016ing,jensen2006strategic} may be considered to be precursor work to the design and development of CAI tools. Also research on the impact of ``problem triggers'' (cf. \cite{gile2009basic}), such as research on the impact of numbers on SI (e.g. \cite{braun1996inacuracy, frittella2019a, mazza2001numbers}), is often reported as a justification for the need for a CAI tool in the booth to access terminology and the integration of ASR to support the rendition of numbers (for instance, see the description of research gap and purpose statement in \cite{defrancq2021automatic,desmet2018simultaneous,fantinuoli2017speech}).

An in-depth ethnographic study conducted by \citet{rutten2018} collected and analysed the booth notes of 25 conference interpreters at the European Commission and Council. The study provided insights into which items are usually written down by interpreters during SI, how these are written, how the sheet of paper on which items are noted functions as a ``communication platform'' to exchange information between colleagues \citep[143]{rutten2018}. As the author herself suggests, information on how interpreters use non-digital supports during SI could provide valuable input to the design of CAI tools. However, detailed investigations aimed at informing tool development for in-booth support have been scarcer than analyses of preparation needs.

A recent EU-funded project led by the University of Ghent in collaboration with the University of Mainz/Germersheim attempted to develop recommendations for the UI design of third-generation CAI tools based on a survey in which they asked practising conference interpreters to express their preferences on a series of design options \citep{eabm2021b}. Participants were asked to express a preference based on graphic representations of the interface options and written descriptions but did not use the options.  Although the results of the survey should be treated as hypotheses rather than valid design specifications, this study testifies the growing interest in usability and UI design-focused CAI research.

Other authors attempted to define heuristic principles for the development of CAI tools. \citet{costa2014comparative} evaluated CAI tools for preparation by self-chosen features related to how glossaries can be compiled and presented. \citet{will2015eignung} proposed some features that characterise a CAI tool as ``simultaneous-interpreting compliant'' (simultanfähig) based on theory and logical reasoning. \citet{fantinuoli2017speech} proposed technical requirements for an ASR system to be successfully integrated into CAI tools and for a CAI tool to effectively support ASR integration. These requirements were partly derived from commonly used metrics measuring the speed and accuracy of ASR systems and partly based on logical reasoning as well as his practical experience as a developer.

\subsection{Evaluation research}

Empirical research aimed at the evaluation of in-booth CAI tools has been conducted by interpreting studies scholars via the analysis of (1) \textit{tool performance}, (2) \textit{users’ performance}, or (3) \textit{users’ perception}.

\subsubsection{Tool performance}

Thus far, only two published studies, both written by Fantinuoli as sole or first author, reported on the evaluation of CAI tool performance. Such evaluation has focused on the accuracy of the visual aids displayed by interpreters.

\citet{fantinuoli2017speech} first proposed metrics for the evaluation of in-booth CAI tool performance and applied them to the evaluation of the first version of InterpretBank with ASR integration. He used the terminology-dense speeches designed by \citet{prandi2017designing}, run the prototype and recorded the outcomes of term and number extraction. He used \textit{word-error rate}, i.e. the percentage of wrongly displayed items out of all items that should have been displayed, as a measure of the tool’s accuracy. He also uses the metrics of precision \textit{p} (i.e. the number of correct positive results divided by the number of all positive results) and the recall \textit{r} (i.e. the number of correct positive results divided by the number of positive results that should have been returned) to calculate the \textit{F1 score} (i.e. the harmonic mean of precision and recall expressed as a value comprised between 0 and 1).

After the recent release of Kudo InterpreterAssist, \citet{FantinuoliEtAlForthcoming} performed a technical evaluation of the tool’s performance in the automatic generation of glossaries and the ASR-powered CAI tool. To evaluate the glossary-generation feature, they automatically generated three 100-term glossaries in English\,>\,French and English\,>\,Italian in three specialised domains. They first asked three conference interpreters to evaluate the relevance and accuracy of extracted terms categorising them as either ``specialised term'', ``general term'', or ``error'' for incomplete or lexically invalid elements. Then, they asked three conference interpreters to evaluate the English\,>\,French glossary and three conference interpreters to evaluate the English\,>\,Italian glossary by marking the target-language translation of extracted terms as either ``acceptable'' or ``unacceptable''. For the evaluation of the CAI tool output, they selected some speeches representative of the speeches interpreted on Kudo. They evaluated both the final output and the intermediate stage of automatic speech transcription in terms of precision, recall and F1.

The evaluation of CAI tool performance is still scarce and rather limited in scope. Other aspects, such as the stability of CAI tool latency (e.g. based on the speaker’s accent and speaking pace, sound quality, word length etc.), have not been considered. A further limitation in previous evaluations of CAI tool performance is that the tool was tested in highly controlled conditions only (i.e. speeches characterised by high sound quality and standard native accents) whereas no evaluations have so far been conducted of tools in naturalistic conditions.


\subsubsection{Users’ performance}

The evaluation of the tool via users’ performance is by far the most frequent method of CAI tool evaluation. \citegen[89]{prandi2022b} research on ``the impact of CAI tools usage on the quality of SI'' is currently the most prolific type. Empirical research on the impact of CAI tools on users’ performance began to emerge in the second decade of the 21st century and gained momentum with the integration of ASR into CAI tools about five years ago. As \citet[87]{prandi2022b} notices, the evaluation of users’ performance has mostly focused on the interpreting product.

The assessment of the impact of the tool on users’ performance was mostly focused on the rendition of individual items (i.e. interpreted specialised terms or numerals) rather than larger units of analysis (e.g. the meaning of the interpreted sentence or speech passage) and more broadly conceived delivery quality, or even the perspective of the recipient of the interpreting service. For instance, in their study of the CAI tool-supported SI of numbers, \citet{desmet2018simultaneous} and \citet{pisani2021measuring} used the error classification proposed by \citet{braun1996inacuracy} which is concerned with the interpreted numeral only. \citet{defrancq2021automatic} chose the numeral as their unit of analysis too. This means that all other components of the \textit{numerical information unit}, such as the entity the numeral refers to (referent), its measure (unit of measurement), were left out from the evaluation, which makes the validity of the interpreted numeral a measure of CAI tool effectiveness questionable. In fact, contextualisation errors, i.e. where the numeral was correctly reported but wrongly contextualised in the speech, were incidentally reported in some studies \citep{canali2019technologie,pisani2021measuring}. Similarly, studies on the impact of in-booth CAI tool use on terminological accuracy considered the interpreted term as their unit of analysis. In sum, the rendition of isolated problem triggers has been the privileged measure of tool effectiveness. This is a major difference from CAT tool research, where a ``communicative approach'' was used to evaluate users’ performance holistically, considering aspects such as text coherence and consistency, fluency, as well as its clarity and acceptability for recipients (cf. \chapref{ch:translation_technology}).

As of today, \citegen{prandi2022b} doctoral dissertation may be regarded as the only study on the CAI tool-supported interpreting process. She compared the use of a ``traditional'' glossary with InterpretBank with manual-look up and an ASR simulation in the interpretation of three comparable speeches. Other than performance data (i.e. participants’ deliveries), she gathered eye-tracking data to ascertain whether a variation in cognitive load could be identified in the three conditions. This may be considered as a measure of tool efficiency, i.e. a usability property of different technological solutions and CAI tool generations.

While previous empirical CAI tool research has not focused on UI design or tool development at a fine-grained level, a notable exception is represented by Montecchio’s Master’s degree thesis \citep{fantinuoli2022defining,montecchio2021}. This study evaluated participants’ rendition of a number-dense speech with CAI tool support at a latency of 1, 2, 3, 4, and 5 seconds with the aim to ascertain what is the ideal and maximum acceptable latency. This study differs from the previous ones reviewed in that it was specifically designed to derive implications for CAI tool development. However, several methodological issues limit the reliability of the findings. First, the latency at which stimuli were presented was not randomised; the gradual increase may have caused a learning effect. Second, the numerals in the source speech were replaced by an acoustic signal (``beep'' sound) to ensure that participants would rely on the visual stimulus; while this is an interesting approach, we cannot assume that the outcome would be equal when users are presented with multimodal input. Third, the numerals at each latency level were not comparable, as discussed below. While this study remains interesting as an example of the growing interest in usability- and design-focused studies on CAI tools, the methodological limitations constrain the translation of findings into design principles.

Considering the design of test materials used in previous studies, like in empirical CAT research, the UI design of the CAI tool and the source speech are the test materials that require manipulation in order to gather data capable of responding to the research question. A further aspect to be added is CAI tool performance: since ASR-powered solutions are still in the prototype stage, their performance is not yet fully stable and may vary even when it is presented with the same speech. Common measures that have been taken to control for the variable of CAI tool performance is using mock-ups simulating peak performance of the CAI tool \citep{canali2019technologie,desmet2018simultaneous}. Researchers have controlled the performance of second-generation CAI tools (i.e. InterpretBank with manual look-up as in \cite{biagini2015glossario,gacek2015softwarelosungen,prandi2015a,prandi2022a} by developing a glossary for the study and feeding them to the CAI tool, so that all participants could have equal conditions. Live third-generation CAI tools \citep{defrancq2021automatic,pisani2021measuring,vanreconnaissance} have been used, too, in exploratory studies aimed at evaluating the overall feasibility of the tool.

Coming to the design of test speeches, the strategies used by researchers have been mostly aimed at preventing the impact of confounding variables on the delivery, hence making it impossible to ascertain whether a certain phenomenon (e.g. an error) was caused by CAI tool use or a factor inherent to the source speech (e.g. speed, syntactic complexity etc.) The speech design in Prandi’s study on terminology is the most elaborate \citep{prandi2017designing,prandi2022a}. She carefully selected the number of syllables and type of terms (e.g. avoiding cognates) to include in her test speeches so that the terms would be perfectly comparable. Following the method proposed by \citet{seeber2012cognitive}, she kept sentence length and syntactic complexity constant and alternated ``target sentences'' (i.e. those containing a problem trigger) and ``control sentences'' (i.e. providing a ``spillover region''). Apart from Prandi’s research, the test speech designs of previous studies were not fully controlled and aligned with the research questions. \citegen{vanreconnaissance} MA thesis at the University of Ghent used a speech with specialised terms without attention to their typology or distribution. \citet{desmet2018simultaneous} and \citet{canali2019technologie} numbers at random intervals equally distributed among simple whole numbers (e.g. 87 or 60 000), complex whole numbers (e.g. 387 or 65 400), decimals (e.g. 28.3) and years (e.g. 2012). \citet{defrancq2021automatic} delegated the preparation of their test speeches to a colleague, which resulted in one of their four speeches presenting nearly twice as many numbers as each of the other three speeches. In \citet{pisani2021measuring}, an English speech was selected from the European Commission website and more sentences containing figures were added. None of these studies took into consideration the distribution of figures in the speech in the design, as well as other factors such as what type of referents are associated to the number. In \citegen{fantinuoli2022defining} and \citegen{montecchio2021} study the numeral type and size is randomly distributed  across latency levels.


\subsubsection{Users’ perception}


Several studies have evaluated the CAI tool based on users’ perception. Typically, users’ perception has been gathered through post-task questionnaires designed by the authors (\cite{de2013uso,defrancq2021automatic,desmet2018simultaneous,gacek2015softwarelosungen,pisani2021measuring,prandi2015a,prandi2022a}) whereas no study no far used a usability questionnaire commonly used in HCI.


\section{Discussion: Usability engineering in CAI}


Differently from CAT tools (see Chapter \ref{ch:translation_technology}), CAI tools did not emerge as a response to market demands for higher productivity and efficiency. Rather, their development was first prompted by conference interpreters themselves in tandem with developers, or the tools were even developed from scratch by interpreters who were programmers themselves. With the recent integration of CAI tools into RSI platforms, the development of new tools is now being prompted by interpreting businesses with the aim to increase the efficiency of interpreters’ workflow and provide interpreters with a better user experience. However, the design and development processes continue to involve conference interpreters. For instance, Kudo’s Interpreter Assist was developed by Claudio Fantinuoli, the conference interpreter who developed InterpretBank. While the motivations behind the development of these tools were to serve interpreters, the community has not been much involved in their design and development. While some tools that have been developed within the framework of doctoral research work (i.e. InterpretBank and LookUp) have moved from previous interpreting research and theory, no study reports of dedicated data collection in order to better specify the needs of the community as a starting point for the design. As a consequence, ``tool design nowadays reflects more the ideas and habits of the respective developer, generally an interpreter himself, than the needs of the interpreter community'' \citep[164]{fantinuoli2018b}. Furthermore, no previous project reports an iterative process of prototyping, testing and revision, which is essential to develop a mature and optimal solution.

Empirical as well as theoretical work on the workflow of interpreters and the challenges that interpreters face in their work represents the first systematic analyses of interpreters’ needs. These studies may be considered as the initial input to the development of CAI tools and provided initial requirements for their development. Initial inquiries emerged in response to the absence of technological tools suitable for interpreters’ preparation: before the naughties, tools for terminologists and translators already existed but interpreters found them too complex and ill-suited to their preparation process. These concerns, alongside pedagogical needs, stimulated a series of theoretical and empirical inquiries into interpreters’ workflow, preparation strategies, and in-booth behaviour, which we may consider as a “forerunner” of CAI tool research and development.

Once tools began to be developed, the first empirical studies echoed the concerns that using a digital tool in the booth to access information during SI may be unfeasible and undesirable. The first empirical studies on the topic, most of which were Master’s degree theses (e.g. \cite{biagini2015glossario,de2013uso,gacek2015softwarelosungen,prandi2015a}) aimed to answer exploratory or experimental research questions concerning the hypothesised beneficial or detrimental impact of using a CAI tool during preparation or interpretation on the interpreters’ delivery. The same aim may be found in studies testing the feasibility of ASR integration in the booth, which, so far, have mostly focused on numbers \citep{canali2019technologie,defrancq2021automatic,desmet2018simultaneous,pisani2021measuring}. Alongside these studies on feasibility, \citet{prandi2018exploratory}, \citegen{prandi2022a} doctoral dissertation paved the way for a cognitive line of research on in-booth CAI tool use. This development seems only natural considering that the use of CAI tools in the booth ``is not only challenging the way interpreting is performed, but it may have an impact on the cognitive processes underlying the interpreting task, even on some basic assumptions and theories of interpreting, for example, the cognitive load distribution between different tasks during simultaneous interpreting'' \citep[153]{fantinuoli2018b}. Hence, it may be more appropriate to say that empirical CAI research so far has mostly provided evidence for the tool’s \textit{utility} rather than usability.

Only recently, researchers began to express an interest in research that is explicitly aimed at developing recommendations for CAI tool UI design (e.g \cite{eabm2021b}) or technical specifications, such as latency \citep{montecchio2021}. However, previous studies present several limitations. Tool performance was evaluated under controlled and optimal conditions only. Users’ performance was evaluated mostly by delivery accuracy. However, accuracy was measured by the rendition of isolated items only (i.e. specialised terms or numerals) without taking into account broader aspects of the interpreted message. Furthermore the impact of potentially crucial speech variables, such us the density of problem triggers in a given speech unit, has not been explored. When it comes to the evaluation of CAI tools via users’ perception, post-task questionnaires have been the preferred method. Since the questionnaires vary across studies, we do not have standardised data collection methods nor benchmark values. We could hence say that, differently from CAT research, which has been enriched by cross-fertilisation with the field of HCI, empirical CAI research is still to develop a usability-focused strand of inquiry.
