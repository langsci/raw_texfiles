\documentclass[output=paper]{langscibook} 
\ChapterDOI{10.5281/zenodo.2654367}
\author{Geoffrey K. Pullum\affiliation{University of Edinburgh}}
\title{Formalism, grammatical rules, and normativity}
\label{chap:pullum} 

\abstract{Formalism within logic and mathematics has indirect connections to modern formal linguistics in that the earliest attempt at realizing the formalist program for logic had the side effect of leading to the development of what today we call generative grammars. Syntactic theory has been dominated by the generative conception for six decades. Despite reference in the literature to ``rules'', generative grammars do not contain rules in the usual sense (under which a rule can be followed or disobeyed). It is not clear how work on generative grammars could make sense of the idea of normative principles of grammar. But the subject matter of grammar is indeed best taken to be normative: a grammar expresses statements about what is correct or incorrect, not claims directly about phenomena in the empirical world. Grammatical rules with normative force can nonetheless be rendered mathematically precise through a type of formalization that does not involve generative grammars, and normativity can be understood in a way that does not imply anything about obligations or duties. Thus there is some hope of reconciling the normativity of grammar with the enterprise of formalizing grammars for human languages and the view that linguistics is an empirical science.}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:pullum:intro}

The school of thought known as ``formalism'' in logic and mathematics takes these disciplines to be concerned solely with procedures for manipulating strings over a finite inventory of meaningless symbols.  Put like that, it sounds pointless: logic and mathematics were surely supposed to be \emph{about} something, not just meaningless games. But formalism has a point: its aim is to ensure that proofs of theoremhood are constructed in a way that is scoured clean of any question-begging hint of meaning or truth.  Then, if what is provable turns out to coincide with what is semantically tautologous, it can be shown that proof in the syntactic sense truly accomplishes something.

That is, ideally we want everything logically provable from true premises to turn out to be true given those premises, and everything that is logically true to be provable. To show that this has been achieved, without circularity, we need first a method of proof that pays no regard to meaning or truth, and second a way of demonstrating that it proves all and only the logical truths. In technical terms, we want a consistent and complete method of proof.

A crucial contribution to the formalist program in logic, just over a century old, was presented in an important book by Clarence Irving Lewis (1883--1964): \textit{A~Survey of Symbolic Logic}.\footnote{See section \textsc{iii} of Chapter 6 in the first edition, \citeyear{Lewis18}; the edition is crucial, because when the second edition by Dover was authorized, Lewis stipulated that Chapters 5 and 6 of his book were to be omitted; he felt that whatever value the book had did not depend on those two somewhat more speculative and heterodox chapters.} Lewis clearly saw that the crucially important work on reducing mathematics to logic, \textit{Principia Mathematica} (\citeyear{WhitRussPM}, henceforth \textsc{pm}) by Alfred North Whitehead (1861--1947) and Bertrand Russell (1872--1970), had failed to separate syntax from semantics in the logical system it assumed. The distinction between axioms and inference rules had not yet emerged: Whitehead and Russell subsumed them both under ``primitive propositions''. In consequence Modus Ponens was framed in a way that, from the formalist viewpoint, is shockingly confused, because it is semantically contaminated. It says: ``Anything implied by a true elementary proposition is true'' and, where $x$ is a real variable, ``When $\varphi x$ can be asserted [\ldots]\ and $\varphi x \supset \psi x$ can be asserted [\ldots]\ then $\psi x$ can be asserted''.

This precludes making legitimate use of the claim that $p$ implies $q$, unless we take $p$ to be true.  But $p$ might of course be a proposition we are by no means sure of.  Using logic to see what follows from a false assumption is an important technique of discovery that Whitehead and Russell's statement of Modus Ponens appears to disallow. Lewis understood that, if we want to be sure that our symbolic reasoning is trustworthy, we must have a purely syntactical method of deriving strings, one that does not in any way depend on meaning, and we must then show that the strings it derives are the right ones -- the ones that are true if and only if the initially adopted premises are true. Lewis sketched in ordinary English a statement of Modus Ponens that ruthlessly excluded any talk of meaning or truth, referring solely to positions of symbols in strings.

The program for making logic truly formal that Lewis urged was taken up in earnest by a PhD student in the Department of Mathematics at Columbia University. Emil Leon {\Post} (1897--1954) graduated with a mathematics BA from the City College of New York in 1917, and went on to do a PhD at Columbia under the philosopher and mathematician Cassius Jackson Keyser (1862--1947).\footnote{{\Post}'s intellectual and personal biography is well documented in broad outline. Sources that I have consulted include \citet{Davis94}, \citet{Stillwell04}, \citet{DeMol06}, \citet{Urquhart09}, and \citet{Jackson18}.} Lewis's book appeared during {\Post}'s first year as a graduate student, and appears to have influenced him considerably. The plan for a doctoral dissertation that he conceived involved turning Lewis's informally presented ``heterodox'' approach into a program within pure mathematics. {\Post} aimed to construct:

\begin{itemize}
\item[(I)]   a way of testing a formula of the propositional calculus used in \textsc{pm} to determine (via truth tables for connectives) whether it was a tautology (i.e., a logical truth); 
\item[(II)]  a system for deriving new formul{\ae} (intuitively, the theorems) from given formul{\ae} (\textsc{pm}'s axioms) that was totally independent of semantic or logical categories like ``constant'' or ``variable'' or ``connective'', working on strings of symbols without reference to their potential meaning; and 
\item[(III)] a proof that the set of tautologies as determined by (I) coincided with the set of derivable strings defined by (II).
\end{itemize} 

For the limited portion of  Whitehead and Russell's logic that he tackled, the propositional part, {\Post} actually achieved that goal (see the abridged version of his PhD dissertation published as \citealt{Post21}). He planned to go on and complete the job of dealing with the whole of \textsc{pm}'s logic, including its quantificational reasoning, in a postdoctoral year at Princeton, where he had obtained a Procter fellowship. In pursuit of that goal, he generalized his syntactical proof system further, and created a type of formal system that would revolutionize theoretical linguistics nearly 40 years later.

The system {\Post} worked out in 1921 was not described in a publication until \citeyear{Post43}, but there were reasons for that. The reasons had to do with a long battle against severe manic-depressive mental illness (now usually called bipolar disorder). Suffice it to say that the system was general enough that it could express any imaginable set of rules for deriving strings from other strings. Indeed, {\Post} rapidly came to regard it as fully capturing the intuitive notion ``set for which it is possible to envisage a way of systematically enumerating the membership''. Today such a set is usually referred to as \emph{recursively enumerable} (\emph{r.e.}).

Formalizing the inference rule Modus Ponens is a very simple application of {\Post}'s system. Assume a symbol inventory containing ``$\supset$'', ``$)$'', ``$($'', and letters like $p$ and $q$, and $P_i$ are to be interpreted as indexed variables over unbounded strings.  Assume that we have already been given some strings that do not need to be proved (intuitively, those correspond to axioms), and we are trying to build a list of other strings using them (those are the theorems).  Then Modus Ponens says that if we can find a string on our list that has a ``$($'' followed by some other stuff which we will call $P_1$, followed by the symbol ``$\supset$'', followed by some further stuff which we will call $P_2$, followed by a ``$)$'', and the stuff that we called $P_1$, on its own, is also on the list, then the stuff that we called $P_2$ can be added to the list.

The way {\Post} put it was that a string of the form ``$(\:P_1\:\supset\:P_2\:)$'' together with a string of the form ``$P_1$'' \emph{produce} a string of the form ``$P_2$''.

Modus Ponens is an extremely simple application of the idea of building strings systematically on the basis of already obtained strings (though it was crucial for {\Post}'s PhD dissertation project). But in 1920--1921 as a Procter fellow at Princeton, {\Post} started working on doing the rest of Whitehead and Russell's logic -- the reasoning that involved quantifiers -- and in connection with that he worked out a radical generalization of the notion of a rule of inference. There could be any finite number of ``premise'' lines, and both the premises and the conclusion could be of any finite length and contain any number of specified symbol strings and/or the variables over strings ($P_1, P_2$, etc.). He presented his generalized metaschema in a truly bewildering tableau. I give it here in a slightly modified form due to \citet{Davis82}, which (believe it or not) makes things slightly clearer:

\large 
\begin{displaymath}
\begin{array}{ccccccc} g_{1,0} & P_{1,1} & g_{1,1} & P_{1,2}  & . . . & P_{1,n_1} & g_{1,n_1} \\ g_{2,0} & P_{2,1} & g_{2,1} & P_{2,2}  & . . . & P_{2,n_2} & g_{2,n_2} \\ \multicolumn{4}{c}{\vdots}             & . . . & \multicolumn{2}{c}{\vdots} \\ g_{k,0} & P_{k,1} & g_{k,1} & P_{k,2}  & . . . & P_{k,n_k} & g_{k,n_k}\\[1ex]         &         &         &\Downarrow&       &           & \\[1ex] h_1 & P_{r_1,s_1} & h_2 \; P_{r_2,s_2} & . . . & h_j & P_{r_j,s_j} & h_{j+1} \end{array} 
\end{displaymath}
 

\normalsize\smallskip\noindent  Each of the $g_i$ and $h_i$ stand for specific strings of symbols that would be given in the production. The down arrow ``$\Downarrow$'' means ``produce'' in {\Post}'s sense. The $r_i$ variables tell us which premise line a variable comes from, and the $s_i$ tell us which variable we are talking about (counting from the left), so requiring the $r_i$ to be between 1 and $k$ (where $k$ is the total number of premises) and requiring the $s_i$ to be between 0 and $n_{r_i}$ (where $n_{r_i}$ is the total number of variables in the relevant line) guarantees that the last line will call only $P_i$ variables that are present somewhere in the earlier lines. Thus everything in the conclusion must be either an explicitly specified string or something drawn from the material covered by the $P$ variables of the premises. Hence the conclusion can say things like ``put the content of the $x$\textsuperscript{th} variable in premise number $y$ into the conclusion at this point'', while not allowing it to say ``put in some random stuff at this point'', which would make nonsense of the idea of representing logical reasoning.

I exhibit the above tableau merely to make the point that it represents a schema fully general enough to express arbitrary string edits. It is more than general enough to state anything from simple phrase structure rules (immediate constituent analysis), or categorial grammar rules, or Chomsky's most elaborate generalized transformations.

Thus a chapter of the history of formalism in mathematical logic turns out to relate to a crucial part of the prehistory of generative linguistics. For {\Post}'s specific design of a formalist proof system with axioms as inputs was to emerge later in Noam Chomsky's work under a new name: \isi{generative grammar}.

Chomsky hit upon the idea of rewriting systems as a mathematical technique for giving syntactic descriptions of human languages some thirty years after {\Post} developed his production systems.  Late in 1951, in the revised version of his MA thesis,\footnote{On the two versions of Chomsky's MA thesis \textit{The Morphophonemics of Modern Hebrew}, see the very careful comparative study by \citet{Daniels10}.} Chomsky (\citeyear{Chomsky51b}:\,3) used the verb ``generate'' for the relation between a grammar and a string of symbols -- for the first time in linguistics, as far as I have been able to determine.  By 1954 both Zellig Harris (1909--1992) and Charles {\Hockett} (1916--2000) had used ``generate'' in the same way (see \citealt{Harris54transfer}:\,260 and \citealt{Hockett54}:\,390). It is not clear whether they were influenced by Chomsky's usage, for although Chomsky had close contacts with Harris up to summer 1951, his December 1951 revision of the MA thesis was done during his first six months at the Society of Junior Fellows at Harvard (see \citealt{Chomsky75LSLT}:\,26), and was little known before its publication by Garland in 1979 (\citealt{Chomsky75LSLT}:\,30 says it met with an ``almost total lack of interest''). By 1955 \nocite{Chomsky55LSLT} Chomsky had completed a first draft of \textit{The Logical Structure of Linguistic Theory}, which proposed a theory of ``generative grammars'' in detail (though it was very little read at that time, and did not appear in print until twenty years later, as \citealt{Chomsky75LSLT}).

There is no citation or mention of Emil {\Post} in \citet{Chomsky55LSLT}, but from 1959 onward Chomsky has occasionally mentioned {\Post}'s name as an earlier source for the prior use of ``generate'' in the mathematical literature. Chomsky (\citeyear{Chomsky59CFPG}:\,137n) notes that he is ``following a familiar technical use of the term `generate'\thinspace'', citing \citet{Post44}, a paper that says almost nothing about how production systems work. The locus classicus on production systems is \citet{Post43}, a paper which Chomsky has never cited, probably because (as conjectured by Urquhart \citeyear{Urquhart09}:\,471), he learned about {\Post}'s work mainly or entirely from secondary sources like the \citeyear{Rosenbloom50} mathematical logic text by Paul C. Rosenbloom (1920--2005), which is cited in \citet{Chomsky75LSLT}. In Chomsky's hands over the following six decades, production systems, under the new name ``generative grammars'', became the overwhelmingly dominant type of framework for the study of syntax.

\section{Rules}
\label{sec:pullum:rules}

With generative grammars firmly established as mainstream in linguistics, both linguists and philosophers commonly speak as if generative grammars of the sort that Chomsky advocates contain something like rules of grammar of the traditional kind -- ``The verb agrees with the subject'' and so on. Chomsky even had an early paper called ``On the notion `rule of grammar'\thinspace'', which might tempt anyone to think that he was dealing with rules in some antecedently understood sense. I am not aware of anyone who has pointed out that it is simply not true. Linguists have completely overlooked a key fact about generative grammars: that they do not consist of rules in any sense that would be recognized by traditional grammarians or non-linguists like philosophers.

The ordinary intuitive understanding of a rule is something that we can follow (that is, behave in a way that complies with it) or break (that is, violate or disobey it).  It defines a regular pattern or practice, a way of ``going on in the same way''.  But nothing of the content of a \isi{generative grammar} has anything like this character. Chomsky actually recognizes this when he comes to respond to the discussion of rule-following in \citet{Kripke82} and observes that ``we would not say, as scientists, that a person follows the rule of phrase structure'' formulated as ``\textsc{vp}~$\rightarrow$~\textsc{v}~\textsc{np}~Clause'' \citep[243]{Chomsky86KL}. A rule of this sort (a context-free phrase structure rule, to be technical about it) is often thought of as saying ``a verb phrase may consist of a verb followed by a noun phrase followed by a complement clause'', but in fact it means nothing of the kind.  The presence of such a ``rule'' in a \isi{generative grammar} neither says nor implies that a \textsc{vp} always contains a \textsc{v} followed by a \textsc{np} and a Clause in that order.  It does not even say that this is possible.  It does not entail that a \textsc{vp} always contains a \textsc{v}, or even that it may contain a \textsc{v}.  These things may be true in a grammar with such a ``rule'', or they may not. Everything depends on what the rest of the grammar says.

There could be a transformational ``rule'' that always shifts \textsc{v} to the end of \textsc{vp} (as with the ``universal base'' analyses of the early 1970s that derived even \textsc{sov} languages from \textsc{vso} underlying structures), or the \textsc{v} could be shifted out of the \textsc{vp} altogether (as in much more recent transformational analyses).  In either case there would never be a \textsc{vp} containing a \textsc{v} with an \textsc{np} following it. Nothing is fixed by any individual statement in the grammar. Only the entire grammar, taken holistically, does anything at all; and what it does is to provide an instantaneous description of the entire set of well-formed sentences. No part of the grammar expresses any generalization about the shape of expressions in the language.

Through all of the last 60 years of linguistics, and especially the discussion of linguistics among philosophers, there has been talk of Chomsky-style generative grammars containing ``rules'' that is completely counter to the way generative grammars actually work. If we take a rule to be a statement that expresses some generalization about the form of linguistic expressions, then no proper subpart of a \isi{generative grammar}, of any scope or size, is a rule or contains a rule.\footnote{We can see a partial exception in the case of \citet{Chomsky81}, \textit{Lectures on Government and Binding} (\textsc{lgb}), but that is precisely because it is not fully generative (in the narrow sense of that term I assume here). In \textsc{lgb}, modules of grammar of a completely different sort are introduced. When the ``binding theory'' says that anaphors (like reflexive pronouns) must be bound in their governing category, it actually is talking about something that has to hold within the structure of any expression. It says that an \textsc{np} node with a reflexive pronoun as its lexical realization always has a coindexed node that c-commands it in the tree (roughly, is closer to the root, and dominated by a node that also dominates the reflexive pronoun).  This use in \textsc{lgb} of what linguists often call ``constraints'' is a departure that Chomsky made from his earlier theoretical work. It disappeared again after 1990 with the appearance of his ``minimalist program''.}

What I have said about phrase structure rules holds also for transformations. \textit{Wh}-movement cannot be followed or complied with. A transformation saying ``move a \textit{wh}-marked phrase to the beginning of the clause'' does \emph{not} say that the language has clause-initial \textit{wh}-words. The language might or might not exhibit them: there could be another transformation that moves them back again (the device known as ``reconstruction'' in post-1980 transformational grammar does exactly that when mapping s-structures to logical form), or a transformation that moves them to the very end of the sentence, or a rule that simply expunges them completely (which is actually what happens in bare relatives like \textit{the one I want}). Everything depends on the rest of the grammar and how all of its components interact.

It is very important, therefore, not to assume when we talk about people following particular rules, or languages having particular rules, that ``rule'' refers to anything found in a \isi{generative grammar}. The non-technical and informal notion of a rule is valuable (indeed, in my view it is essential), but it simply cannot be equated with any pieces or elements of generative grammars.

\section{Normativity}
\label{sec:pullum:normativity}

What I have just said entails that we are in a certain amount of difficulty when we come to consider the issue of whether the claims made by a grammar are (or are not) \emph{normative}.  A \isi{normative} statement is one that deals not with how things are but how they ought to be, or how it is appropriate for them to be given some set of values. Nothing in a \isi{generative grammar} has that property.

Some philosophers in effect question whether \isi{normativity} can arise in a physical universe. How could any physical distribution of elementary particles constitute a situation in which some things (drawn from an indefinitely large range) are objectively ``good'', or ``beautiful'', or ``right''? Such reflections lead to moral antirealist views under which ethical statements like ``That is morally wrong'' or ``You should apologize'' are regarded as having more in common with grunts or cries of pain than truth-evaluable statements like ``This is made of gold''.

My own views in metaethics incline towards moral realism. But at least one philosophically inclined linguist assumes we have to accept antirealist error theories of ethics. Replying to an article in which I mentioned that I think claims about grammaticality are \isi{normative} \citep{Pullum07}, Geoffrey {\Sampson} remarked:

\begin{quote}
I was at least assuming that grammatical description consists of statements that are correct or incorrect: but correctness is not a concept applicable to the domains of ethics or aesthetics. (As it is often put in the case of ethics, ``you cannot derive an ought from an is''.)
\citep[112]{Sampson07reply}
\end{quote}

{\Sampson} is assuming that claims like ``Torturing children is wrong'' or ``Bach's music is beautiful'' are not even truth-apt, and he thinks that my passing mention of ethics and aesthetics has committed me to the view that claims about grammaticality are likewise not statements of objective fact.  This is of course nothing like what I believe.  But it is instructive to read {\Sampson}'s views (restated in {\Sampson} and Babarczy \citeyear{SampBaba14}:\,96--99), because they are a reminder of how difficult the philosophical clarification of descriptive linguistics is going to be. While extreme prescriptivists seem to think that a construction can be held to be grammatically incorrect no matter how much natural usage conflicts with that claim, {\Sampson} represents the opposite pole, apparently holding that the only objective claims about language concern what has occurred in a corpus, and statements about what is grammatical or ungrammatical do not even have truth conditions.

Let me start by attempting to be clear about what I think \isi{normativity} is. Normativity is generally taken primarily as a property of statements, and then derivatively as a property of domains or subject matters in which \isi{normative} statements are the appropriate mode of discourse (see \citealt{Millar04}:\,93--96 for a careful discussion of how the two are related).

The claims of geology are not \isi{normative}; the system of table manners is. Number theory is not a \isi{normative} discipline; ethics is. Aeronautical engineering is not \isi{normative}; aesthetics is.\footnote{Logic is a rather interesting case, since on the one hand we want to say that it is a plain and undeniable fact that $P \rightarrow Q$ is logically equivalent to $Q \vee \neg P$, but on the other hand it also seems right and proper to reason logically rather than illogically. There is a philosophical literature on this but, regrettably, exploring it is  beyond the scope of this chapter.}

{\Millar} (\citeyear{Millar04}:\,92--99) points out that all the classic cases of \isi{normativity} involve \isi{normative} statements providing reasons for doing, feeling, believing, desiring, or intending something. I believe grammatical \isi{normativity} falls together with the classic cases. ``It is not good table manners to lick your knife'' offers a reason for not licking your knife; ``Torturing children is wrong'' implies a reason for not torturing children; ``Bach's music is beautiful'' suggests a reason for planning to attend a Bach concert; ``Attributive adjectives expressing colour always follow the noun in French'' provides a reason for positioning colour adjectives after the noun when seeking to be regarded as using normal French.

\section{Prescriptivism}
\label{sec:pullum:prescriptivism}

Touching on a rule of grammar that defines how to a speaker ought to position French adjectives brings us inevitably to a consideration of prescriptive grammars. Some discussion cannot be avoided, though in fact I will not have much truck with prescriptivism here. I make a terminological distinction that is not standard: although many have referred to old-fashioned ``don't-do-this'' grammar and usage books as ``\isi{normative} grammar'', and many have said that \isi{normative} statements are prescriptive rather than descriptive, I am going to use ``\isi{normative}'' and ``prescriptive'' quite differently.

\largerpage[1]With respect to the grammatical rules for a human language (especially one with a high-prestige standard variety), there is a crucial distinction between two stances or attitudes:

\begin{itemize} 
\item[--] \emph{descriptive} grammar involves the identification and statement of the rules or constraints that define the linguistic system (rules and constraints that I am going to argue are \isi{normative}); 
\item[--] \emph{prescriptive} grammar involves the issuing of injunctions or opinions or exhortations about what system ought to be used, or judging how well or poorly some use of language complies with a given system. 
\end{itemize} 

Prescriptivist grammarians certainly see language as a \isi{normative} domain, but not in the way I am interested in. What primarily marks out prescriptivists is that they see their role as advising or instructing or cajoling other language users to alter their linguistic behaviour.  They want to change the way we speak and write, to lead us out of error and towards the correct path.

We can set aside here the fact that prescriptivists often have the rules wrongly conceived or wrongly formulated.  They often doggedly maintain the validity of rules that do not match what they profess to regard as excellent usage, such as the usage of people they explicitly admire (Orwell, Strunk, White, whoever).  Often it can be shown that they defend a rule which they unknowingly and constantly violate in their own writing, which one might have thought was a knock-down drag-out argument that the rule cannot be right, at least for their own English.  They never accept such arguments, preferring to insist, irrationally, that even their own usage is to be condemned if it does not comply with the fictive rule. And they invariably ignore grammatical differences between dialects, treating non-standard English \data{He don't never come here no more} as simply incorrect standard English, as if it were a poorly executed attempt at saying \data{He does not ever come here any more}, when in truth languages or dialects that have negative concord working-class and low-prestige dialects of English around the world fall together with standard Italian, standard Polish, and other languages in which repeated morphological expressions of negation reinforce each other rather than cancelling out.

But all of this is basically a side issue, because even if the prescriptivists had all the rules exactly right, their enterprise would still be quite distinct from that of descriptive linguistics. They are in the critical and advisory business of evaluating language use as good or bad. I am not.

John Searle draws a relevant distinction (in his book \textit{Speech Acts}, \citeyear{Searle69}) between \emph{constitutive} and \emph{regulative} rules.  Constitutive rules define or set up the activities to which they apply; regulative rules are established to govern an activity that can proceed independently of them and in defiance of them.  Knocking other marathon runners down in order to get ahead of them is still clearly running in a marathon, because the rule that we should not use physical violence against other runners in a marathon is regulative. But moving a knight six squares directly towards the other end of the board is not playing chess: the rule that a knight moves to a second-nearest square of opposite colour is constitutive.

Prescriptivists take grammatical rules to be regulative. Criticizing other people's linguistic behaviour and attempting to get them to modify it is the goal.  That has nothing to do with my topic here, so I want to simply set the prescriptive stance aside.

The question I am concerned with is whether the \emph{descriptive} view of grammar also involves a \isi{normative} perspective of what the subject matter is.

What suggests \isi{normativity} in the subject matter of grammar, more specifically syntax, is the fact that there is (or at least, linguists assume there is) a distinction between well-formed and ill-formed expressions, and it holds over an indefinitely large range, certainly far too large for it to be a matter of list membership.

What a grammar has to do is not to summarize some finite set of observations or facts, but to use all available evidence to discover a definition, over an indefinitely large class of candidate objects (potential expressions), of the difference between those that are good or properly structured in the language under study and those that are bad or improperly structured.

This does not mean that linguistics fails to be empirical (contrary to {\Sampson}'s assumption). Its task is to find out what the right constraints are, and that is not an a priori matter. It can only be done empirically, ultimately by reference to the usual behaviours and reactions of the native users of the language when distractions and irrelevant extraneous factors do not intrude. This is true despite the fact that both intuitions and corpus attestations are fallible sources of evidence. The epistemology is therefore subtle. I have suggested elsewhere (see \citealt{Pullum17}) that it should be seen as based on the method of reflective equilibrium.

The way generative linguists usually view it, the grammar has to cover all the expressions of the language, and only those expressions, and it must do it in a way that tells us the status of novel expressions -- expressions we have never encountered before. That means making a description that is fully explicit about how the expressions of the language -- all of them, however many there may be -- are structured. And that calls for some kind of formalization of both the representation mode and the grammatical rule system.

\section{Formalization}
\label{sec:pullum:formalization}

What I mean when I refer to formalization in syntax is simply \emph{the use of mathematical and logical tools to make theoretical claims more explicit}. Talk about formalization is therefore not essentially connected to the ``formalist'' progamme. It has nothing to do with de-emphasizing the semantic, pragmatic, rhetorical, or aesthetic aspects of human languages, or with assigning more importance to form than to function, or with Carnap's project of eliminating meaning from the language of science, or with Hilbert's doomed project of reducing all of mathematics to questions of logical truth in some decidable formal logic.

The tools that formally inclined linguists have borrowed from logic for use in framing syntactic theories over the last few decades have included rewriting systems, automata, graphs (most importantly trees), and model theory.  More recently, 21st-century linguistics has been increasingly employing tools from statistics and probability theory.

But there is truth in the familiar remark about how to a three-year-old with a hammer everything looks like a nail.  Syntacticians have become so completely engrossed in working with generative grammars that they see everything in terms of derivations, and cannot conceive of what life would be like in any other terms.

They have paid very little attention to the fact that a \isi{generative grammar} of $L$ says absolutely nothing about the structural properties of any non-sentence of $L$. They have ignored the fact that the sharp boundaries of any set defined by a \isi{generative grammar} fly in the face of the widely accepted intuitive view of ill-formedness as gradient -- the fact that one ungrammatical sentence can be more ungrammatical than another.

They have also paid little or no attention to the fact that a \isi{generative grammar} makes syntactic properties depend crucially on the contents of a finite lexicon: a derivation that does not terminate in a string of items belonging to the relevant lexicon is not a derivation at all, so there is no way for a \isi{generative grammar} to represent an example like Carnap's \textit{Pirots karulize elatically} as grammatical unless the lexicon contains a noun \data{pirot}, a verb \data{karulize}, and an adverb \textit{elatically} -- which for standard English it does not.

These are not problems for formalized syntax in general. For one thing, when we are talking about the invented languages of logic and computer programming, the worries I just expressed about generative grammars turn into virtues. For proving theorems about logical systems -- completeness, consistency, compactness -- it is absolutely crucial that the formul{\ae} of the logic should be sharply defined to form a specific set with a known cardinality. And for proving correctness of a computer program, the same is true. There can be no gradient levels of ill-formedness, or potential tolerance of minor deviance, or uncertainty about the finite list of allowable symbols, when we are talking about logics or programming languages.  This is the grain of truth in Michael Tomasello's capsule summary of ``Chomskian \isi{generative grammar}'' (\citeyear{Tomasello03}:\,5), which he says is ``a~`formal' theory, meaning that it is based on the supposition that natural languages are like formal languages''. In a way, though I suspect not in the way he intended, he is correct.\footnote{The rest of his summary is inaccurate and confused. He says \isi{generative grammar} uses ``a~unified set of abstract algebraic rules'' (they are actually of diverse types, not at all unified); and they ``are both meaningless themselves and insensitive to the meanings of the elements they algorithmically combine'' (but algebraic operations always need interpretations, as \citealt{McCawley68BASE} carefully shows, and grammar rules can be written to build semantic representations simultaneously with syntactic ones, as in \citealt{Montague73PTQ},      \citealt{Montague74} or \citealt{GaKlPuSa85}). Finally he says the rules come with ``a lexicon containing meaningful linguistic elements that      serve as variables in the rules'' (but I see no sense in which lexical items serve as variables).}

The sense in which generative grammars do treat natural languages like formal languages has to do with the origins of their formal machinery, as already outlined.  It was developed for a specific purpose within mathematical logic (formalizing formation rules and inference rules in a fully general way), and it is perfectly suited to the description of the invented languages for logic, metalogic, and computer programming.  But it is important that there are ways of making grammars mathematically explicit that are quite distinct from the generative one.  Chomsky has sometimes confusingly denied this point, claiming (as in, e.g.,\ \citealt{Chomsky66Topics}:\,12) that for him the term ``\isi{generative grammar}'' means nothing more than ``explicit account of sound/meaning correspondences'', but this does not square at all with his actual usage (\citealt{Ney93} argues this point at length). \citet{PullScho01LACL} use the term ``generative-enumerative syntax'' to stress that the referent is syntax formalized in terms of nondeterministic random enumerators; their paper discusses certain types of explicit grammar that are not generative in this sense.

The non-generative mode of formalizing grammars that Pullum and Scholz discuss uses model theory rather than rewriting systems to formalize syntactic description. Grammatical rules are taken to be constraints on the structure sentences, in a straightforward and informally comprehensible sense: the constraints in a model-theoretic grammar for English would say things that for convenience we can readily paraphrase in English. A few examples:

\begin{itemize} 
\item[--] A preposition phrase has a preposition as head. 
\item[--] A lexical head is the initial subconstituent of its parent. 
\item[--] A pronoun subject of a finite clause takes its nominative case form. 
\end{itemize} 

Such constraints can be stated more precisely as formul{\ae} of a logic; a grammar can be defined as a finite set of such formul{\ae}; structures of sentences can be taken to be the models for the interpretation of that logic; and grammaticality can be reconstructed as satisfaction of the constraints in the grammar, in the model-theoretic sense.

For additional concreteness, we can look very briefly at the syntax of preposition phrases (\textsc{pp}s) in English.  A typical old-fashioned \isi{generative grammar} would include a phrase structure rule like this:

\ea  \textsc{pp} \quad $\rightarrow$ \quad \textsc{p} \quad \textsc{np}
\z

Under its standard interpretation this licenses derivational steps in which the symbol ``\textsc{pp}'' is replaced by the sequence ``\textsc{p}~~\textsc{np}'', and derivatively licenses the building of (part of) a tree diagram that looks like this:

\ea
\Tree [.PP [ [.P ] [.NP ]]]
\z

We might easily think that the rule entails that prepositions always have \textsc{np} complements. It does not. There could be (and in fact for English there will need to be) other rules in the grammar saying things like this:

\ea
 \ea  \textsc{pp} \quad $\rightarrow$ \quad \textsc{p} \quad \textsc{pp} 
 \ex  \textsc{pp} \quad $\rightarrow$ \quad \textsc{p} \quad Clause 
\z
\z

So some \textsc{pp}s will not have \textsc{np} right branches. And we might easily think that the rule at least says that those prepositions that do take \textsc{np} complements precede their \textsc{np} complements. But it does not entail that either. There could be another rule in the grammar saying this (where $\varepsilon$ is a symbol representing the null string):

\ea  \textsc{pp} \quad $\rightarrow \quad \varepsilon$ 
\z

In that case there might be no prepositions appearing in the language at all. Or there could be a transformational rule like this:

\ea
\begin{tabular}[t]{ccccccccccccccc} \textsc{x} & - & \textsc{p} & - & \textsc{np} & - & \textsc{y} \\ 1 & - & 2 & - & 3  & - & 4 & $\Rightarrow$ & 1 & - & 3 & + & 2  & - & 4  
\end{tabular} 
\z

In that case prepositions would always be suffixed to their NP complements -- unless some other rule in the grammar tampered with things further. This is what I am referring to when I say that the grammar provides its definition holistically: in the same way that we are told that in the Brexit negotiations nothing is agreed until everything is agreed, with a \isi{generative grammar} we do not know anything about what any part of the grammar determines about any part of a sentence until we know what the entire grammar yields.

We might easily fall into the error of thinking that the rule ``\textsc{pp}~$\rightarrow$~\textsc{p}~~\textsc{np}'' does say that a \textsc{pp} always contains a \textsc{p}.  But it does not do that either. It does not guarantee anything about the interior of \textsc{pp}s. There could be another rule in the grammar saying ``\textsc{pp}~$\rightarrow$~\textsc{a}~\textsc{b}~\textsc{c}''.

Under the view I favour, the conditions on \textsc{pp}s would be stated directly as constraints -- and for concreteness we can take them to be constraints on the structure of trees.

Assume a predicate logic in which we quantify over nodes with variables $x, y, z\ldots$ and have a vocabulary of monadic predicates for category labels and binary relations for grammatical functions.  We write ``$\text{B}(x)$'' to mean ``node $x$ is labelled with category label \textsc{b}'' and ``$\text{F}(x,y)$'' to mean ``the \textsc{f} of node $x$ is node $y$'' (i.e., ``node $y$ bears the grammatical function \textsc{f} to its parent node $x$''). Then the constraint saying that \textsc{pp}s have \textsc{p} heads would be expressed precisely in this way:

\ea $\forall x [ \text{PP}(x) \rightarrow                \exists y [ \text{Head}(x,y) \wedge P(y) ] ]$ 
\z

And if we write $\text{Lexical}(x)$ to mean ``node $x$ bears a lexical category label'', ``$\text{Parent}(y,x)$'' to mean ``$x$ immediately dominates $y$'', and ``$x \prec y$'' to mean ``node $x$ is to the left of node $y$'', then the second constraint above, stating that lexical category nodes are initial in their phrases, can be stated like this:

\ea $\forall x, y [ \text{Head}(x,y) \wedge \text{Lexical}(y) \rightarrow \forall z [ \text{Parent}(z,x) \rightarrow x \prec z ] ]$ 
\z

Any set of trees characterized by a set of first-order logic formul{\ae} in this sort of way will be a regular set of trees (recognizable by a finite-state tree automaton) and its string yield will be a context-free stringset (these results are corollaries of theorems now found in textbooks of finite model theory like \citealt{EbbiFlum99} and \citealt{Libkin04}). By giving a finite set of first-order logic statements interpreted on tree models in this way, we explicitly characterize a set of trees and thereby a context-free set of strings.  We are in effect giving a formally explicit model for a context-free language without using a context-free \isi{generative grammar}.  I argue elsewhere \citep{Pullum13} that this yields significant advantages.

I point all this out merely in order to establish the point that \isi{normative} principles like those stated informally in English above can be made fully precise and become, without change, a formalized grammar with precise consequences, known parsability results, etc. I~am in no doubt that it is worth pursuing the goal of making the syntactic structural principles for human languages fully explicit -- formalizing them using the tools of logic and mathematics.

On the other hand, I do not believe that by formalizing some version of the grammar rules for a language we are thereby defining a hypothesis about the mind -- ultimately the brain structure -- of an idealized native speaker of that language. \citet{Kripke82} believes there are profound difficulties for this basically Chomskyan view, because there seems to be no way to identify the unique set of rules that guides a given speaker's grasp of their native language: indefinitely many grammars would account for all of the utterances they have produced (or judged acceptable) in the past, and nothing identifiable about the speaker in question can be said to fix the structure that will be revealed in further utterances as the speaker goes on.

I think \citet{Scholz90} was right to elaborate on {\Kripke}'s worry, and argue that we do not obtain an explanation of $S$'s linguistic capabilities simply by saying ``$S$ has a mentally inscribed representation of the \isi{generative grammar} $G$ in his brain''. And I think Chomsky is wrong in responding to {\Kripke} by denying that there is any \isi{normative} aspect to grammar (see \citealt{Chomsky86KL}:\,chap. 4).

What I am suggesting is that we are better placed to see how there can be a \isi{normative} conception of grammar that is not prey to {\Kripke}'s metphysical worries if we conceptualize grammar in model-theoretic terms, as approximate compliance with certain structural constraints on the form of sentences.

It might also assuage the worries expressed by Riemer (\hyperref[chap:riemer]{Chapter 9}, this volume) concerning a kind of authoritarianism that he sees as stemming from the ideas of \isi{generative grammar}. Imagining that there is some unique generative mechanism that is the mental and ultimately neurophysiological reality underlying the capacity to use English, and teaching students about it as if it were unique, he feels, is inimical to the idea that this complex world can be viewed from many divergent perspectives. It might even militate against our students feeling that they have the intellectual freedom to explore alternatives, and to encourage belief in an authority that could be ``argued to replicate and so to normalize, in the domain of education, the kinds of relations of social domination on which contemporary political orders rest'' (this volume, p.~\pageref{q:riemer:domination}).

The relevance of the model-theoretic approach to Riemer's problem is that it is by no means necessary that to adopt the ``\isi{unique form hypothesis}'' that he sees as implicit in standard \isi{generativism}. Indeed, I see absolutely no reason to believe that one unique, correct set of constraints defines English (or any other language), or that there is one correct way of formally expressing the content of such a set. What is needed for someone to be a competent user of English is not that they have some ideal and perfect constraint set (or generative system) neurochemically implanted in their brain, but merely that they have developed a set of constraints on grammatical form that, to a good approximation, leads them to structure their utterances in ways much like the way other speakers of English structure theirs. There will be indefinitely many ways to do this, and indefinitely many ways to represent formally what has been done. Let a hundred flowers blossom and a hundred schools of thought contend. I regard it as eminently plausible that among the vast population of native speakers of English, millions of slightly different systems of constraints contend. The reason this is not problematic is that all we need for linguistically expressed communication to be possible is substantial overlap in the consequences of the different systems. And the slight differences between the ways the constraints are defined and realized will of course be the seeds of future linguistic change.

One further thing I should say about \isi{normativity} concerns the notion of universal grammar. Here the issue of \isi{normativity} may not be relevant at all. Modern linguistics since \citet{Chomsky65} has stressed the goal of formulating a theory of \emph{universal} grammar (\textsc{ug}).  This theory has been taken to be not just a systematization of the facts that have been discovered to hold for all human languages, but a kind of ideal model of the human infant's capacity for learning languages.  And it is important that \textsc{ug} could in principle be an entirely non-\isi{normative} domain: it could be a description of a set of neurophysiologically instantiated devices or of a psychological organization.

I do not regard this as plausible or well supported, because of the lack of any account of mechanisms.  How does \textsc{ug} constrain the growth of grammars? We know how the curved horns of a ram grow (faster-growing cells on one side and slower-growing on the other), but nothing at all about how \textsc{ug} works or develops in the brain. And more recent work tells us nothing that helps (the literature on what some have been calling ``biolinguistics'' over the past decade seems to me increasingly to be parodying itself).

But we can set aside the issue of whether there might be a serious theory of the biological aspects of the human language acquisition capacity. I am concerned here simply with understanding grammars of individual languages. This will be necessary regardless of whether the human capacity to form and use them is constrained by a built-in \textsc{ug}.  It is by no means clear to me that any non-\isi{normative}, naturalistic, neurophysiological account of the parochial grammars of specific languages makes sense.

It is uncontroversial that the mentally inscribed grammar that Chomsky posits does not (i)~describe all the utterances that have occurred in the past, or (ii)~predict what utterances will occur in the future, or (iii)~identify the probabilities of occurrence for future utterances, or (iv)~make us do whatever it is we do when we produce an utterance or understand one.  Assuming that *\data{Fetch quickly it} is ungrammatical, for example, the correct statement of the grammatical rules of English does not imply (i)~that no one has ever said it, or (ii)~that no one will ever say it, or (iii)~that the probability of its being uttered is low, or (iv)~that we ought not to utter it.

The implication of a grammar defining something as ungrammatical is closer to being that no one \emph{should} say it if they want to be taken as speaking English as it is usually spoken.  What kind of ``should'' is that? Well, it should be clear enough that we are not talking about anything closely analogous to the moral sense of ``should''.  Moral philosophers standardly take morality to be universal.  Certainly for a moral realist (and moral realism is the metaethical view that I would subscribe to), torturing a child is not immoral just for certain people in certain circumstances though possibly moral in other cultures or under other circumstances; it is morally wrong for everyone.  Someone who disagrees is simply mistaken. Societies and cultures can evolve, and come to see that moral views they held earlier were mistaken,

But essentially none of the rules in the grammar of a particular language can be taken to hold universally. There are some 7,000 extant languages, differing quite radically in grammar as well as lexicon.  No one of them is more grammatically correct in its structures than any of the others. In matters of grammar we have to be radically relativist.

Morality also relates to behaviour in a way that has consequences for human actions.  From the fact that torturing a child is immoral it follows that we ought not to torture a child.  But from the fact that *\data{Fetch quickly it} is ungrammatical in English nothing at all follows concerning what anyone ought to do or not do.  Whether we ought to utter it will depend entirely on the circumstances.  During a minute of silence at a funeral, we ought not to utter it, or anything else.  But if we are playing a foreign character in a play and the script has your character saying *\data{Fetch quickly it}, then at the relevant point we ought to utter it.

This issue has sometimes been discussed in the philosophy literature, in the context of the \isi{normativity} of meaning.  Philosophers of language are in fact maddeningly uniform in their habit of talking only about reference of words -- as if all we ever do with our language were pronounce the word ``cat'' and successfully achieve reference thereby to a creature of the species \textit{Felis silvestris catus}, and as if that were the deepest and most interesting thing about language.  But even there, saying that \data{cat} refers to a certain animal species in the Felidae family does not mean that we ought to describe cats by using that word.  Even if we do want to refer to the creatures, we might want to use \data{moggy} or \data{pussy} or \data{foul razor-clawed mewing beast from hell}.  And we have no obligation to talk about cats or refer to them at all if we do not choose to.

My concern here is not with elementary lexical meaning but with syntax. However, it just as clear that no syntactic rule or constraint or principle conveys any obligation or presumption about how we should act.

For one thing, the constraints of syntax mostly involve categories and restrictions and classifications of which we have no conscious knowledge whatsoever, so typically we could never know whether we had violated them or not.  And almost everyone agrees that ``ought'' implies ``can'', of course: it is generally not taken as coherent to assert that we have a duty to do something that we are totally incapable of doing.

This point offers a clue to some understanding of what the prescriptivists are up to.  They peddle rules that, while not a correct reflection of the actual syntax of the language, are fairly easy for even linguistically unsophisticated people to check: never place an adverb between infinitival \data{to} and a plain-form verb (\data{to boldly go}), never end a sentence with a preposition (\data{What are you looking at?}), never begin a sentence with \data{however}, avoid the passive, etc. Such rules are hopeless as a guide to how to actually use the language like a normal person, but they have become cultural markers of attention to grammar. Most people are not aware of how inaccurate they are or how much they are ignored by truly accomplished writers, and it is moderately easy to identify violations.

Prescriptivists fail in some of their identifications: they mistake particles for prepositions, and mistake existentials or predicative adjective constructions for passives, and so on; but they \emph{think} they are correctly applying genuine rules, and they see a kind of quasi-moral force to the rules: they see the people who do not respect the rules as falling short of the standards of behaviour that society ought to maintain and enforce.  In other words, they see the rules of grammar as regulative.

Domains involving obligations on agents to act (or not act) in certain ways are not the only domain for \isi{normativity}.  Neither constitutive nor regulative rules of a game have any connection to anything about obligations.  We are not obliged to play the game at all, for one thing, so the rule defining a knight's move certainly does not say we should or should not move any particular knight.

Moreover, making an illegal move is not something we ``shouldn't do'' in absolute terms; whether we should do it depends on the circumstances. We might be playing against a sadistic jailer, with the life of a fellow prisoner forfeited if we lose, in which case we \emph{should} make the illegal move if we can get away with it and it will ensure a win.

\largerpage[1]What we should \emph{do}, if ``should'' has anything like the sense it has in connection with morality or similar kinds of obligation, has nothing whatever to do with the \isi{normative} force of linguistic rules.

I should also note that the rules of syntax do not have \isi{normative} force in any \emph{instrumental} way: it is not that they should be obeyed because bad things will happen if they are not obeyed, or because following them will enable one to get things that one needs. There is sometimes hint of this in naive talk about language and how we learn it: people talk about needing to make oneself understood in order to interact satisfactorily with other people. But in fact there is very little pressure to get things right grammatically.  There are people who live most of their lives speaking mostly in a language that is not their native language, and speaking it very badly. They still find ways to get what they want.

And perhaps the starkest and most obvious refutation of the instrumental view comes from looking at the actual facts of human infants' experience: while they are incapable of speech they are constantly looked after, and all their needs are met, but once they are four or five and can speak and understand they start being expected to do things that other people want them to do. The idea that we speak the way we do because we have to on instrumental grounds is nonsense.

\section{Understanding}
\label{sec:pullum:understanding}

The \isi{normative} rules of games and linguistic systems do not define anything as unethical, or contemptible, or inadvisable, or evil.  But I think what they do can be elucidated in terms of the very interesting work of Alan {\Millar} in \textit{Understanding People} (\citeyear{Millar04}), cited above. {\Millar} stresses the notion of a \emph{commitment} to follow rules: he separates the \emph{following} of rules from the \emph{commitment} to follow them.

It is important that there can be tacitly acknowledged rules that no one has set down in detail. Management of phone calls is an example. Nowhere is it set down that when a call is connected the recipient of the call is supposed to speak first, or that normally the maker of the call is supposed to instigate the ending of the call, or that the recipient speaks last (echoing the goodbye of the maker of the call). The fact that a certain set of rules is tacit does not preclude the existence of a practice based on them.

This is the potential answer to the problem with Crispin Wright's observation about effortless first-person authority: we do not appear to have that for syntax, in the general case.  We can have tacit command of a rule that one cannot state, and the fact that we cannot state it does not mean we cannot recognize departures from what it requires.

What the tacitly acknowledged rules of grammar do is to define certain ways of structuring expressions and associating them with meanings. {\Millar} put it this way in the handout for a talk at the University of Edinburgh in 2010:

\begin{quote}
The commitments incurred by participation in a practice have a closely analogous structure to those incurred by beliefs and intentions. Participating in a practice, G, incurs a commitment to following the rules of G.  To a first approximation that amounts to it being the case that one ought to avoid continuing to participate in G while not following the rules of G.  [\ldots\ T]here are two ways to discharge the commitment -- by withdrawing from the practice and by complying with the rules. \end{quote} 

Rules of grammar that define expressions as being structured in certain ways do not entail that we ought to structure our expressions in those ways.

We do incur a commitment to structure our expressions in the English way when we decide to speak English.  But incurring a commitment to do something does not entail that we ought to do it.  Sometimes the way to deal with a commitment that we incur by engaging in a certain practice is to cease engaging in the practice.

{\Millar} uses the example of offensive ethnic terms to illustrate the point that the existence of a practice says nothing about what we ought to do: \data{Chink} in English is an offensive epithet meaning ``Chinese person''. But the existence of a practice of saying \data{Chink} to mean ``Chinese person'' does not imply that anyone ought to use the term: most would say the opposite is true.

It is not by any means as easy to illustrate the same sort of thing in syntax, but I think it is possible.  Consider the fact that it is usual for the direct object to follow the verb in what the \emph{Cambridge Grammar of the English Language} \citep{HuddlestonePullum2008} calls canonical clauses:

\ea
\ea[]{The old tree shades the house.}
\ex[*]{The old tree the house shades.}
\z
\z

That rule codifies a certain practice, applying the generalization to an indefinitely large range of expressions, not just recording properties of previous utterances.  It has \isi{normative} force in that it defines it as syntactically incorrect to put the object before the verb.  If we do not put the object before the verb, other things being equal, we are not respecting the constraints of English syntax.  But it does not follow that anyone ought to place any direct object before any verb.  A person might have reason not to respect the constraints of English syntax.  For example, someone might be imitating the poetry of the eighteenth or nineteenth centuries, where the long-extinct order with object before verb was often employed.

One might well ask what point there could be to having rules that do not have to be obeyed.  This would be like having rules of the road that nobody has to follow.  The rules of the road at least have instrumental motivation relating to safety, but in the case of grammar we do not even have that.  {\Millar} (handout) in effect answers that question when he says:

\begin{quote}
On the view I am promoting it is our participation in practices and our ability to recognize what these require in particular situations that enables us both to make reasonable predictions about, and make sense of, what people think and do. The basic idea is simple. Because we know how people are supposed to act we can often make sense of how they do act and how they are likely to act. 
\end{quote} 
In talking about making sense {\Millar} is not necessarily to be interpreted as referring to the understanding of meaning, either semantic or pragmatic. The point has nothing inherently to do with the correct apprehension of literal meaning or perceiving the utterer's intended meaning, though it may apply to those accomplishments, and it may not even imply those abilities.  I am breaking with the usual practice among philosophers of language in wanting to talk about syntax.

The understanding I am alluding to is simply a matter of making sense of what is going on linguistically: what sort of system the interlocutor is using, what to expect, what to infer about the intent of a speaker or the likely form of further utterances.

Understanding of meaning is neither necessary nor sufficient for the syntactic ability I am talking about.  It is not necessary because it is often possible to grasp the structure of an uttered sentence without having the vaguest clue as to its meaning.  Faced with something like \data{I doubt whether that is not necessarily not untrue}, most people are aware that they have heard something grammatical but they cannot work out the truth conditions of a quadruple negation.  And we can immediately perceive the grammaticality of \data{Appearances are not deceptive, it only seems as if they are}; working out from its meaning whether it is sensible or contradictory is much harder.  But it is also not sufficient: being able to extract the meaning does not imply grasping the syntactic structure.  We often correctly identify intended meanings despite massive non-compliance with syntax: if a worried stranger says ``\data{Me need you help!  Me house go fire!}'' we would know what was meant.

However, it remains true that if we follow the practice of ordering words and structuring phrases and clauses in the English manner, and our interlocutor understands us to be following that practice, they can make sense of certain aspects of our intentions that otherwise would be inscrutable to them, and they can make much better guesses at why our linguistic behaviour is the way it is, and what to expect in future utterances.  They can assume that if we begin an utterance with an auxiliary verb and continue with a nominative pronoun (\data{Can we\ldots}; \data{Is he\ldots}; \data{Will they\ldots}) we are beginning a closed interrogative, and thus that we will probably continue with a verb phrase, and are probably about to ask a question with a yes/no answer, and are likely to have made the assumption that they know the answer to the question, and so on.

If we depart from the usual practices, that does not necessarily mean that we should not have, or that we have made a mistake.  If our interlocutor detects that we have departed from the usual grammatical practices, they can simply regard us as having ceased to operate in accord with the usual rules, and they can reason about why that might be and what they should assume from now on. A normal context is one where we appear to be respecting the same constraints on sentence structure as they would, and using words they have encountered before, so they assume we are using the same language, and they turn out to be right. But there are also anomalous contexts, such as one in which a native speaker who is required to draft a forced confession by the agents of a foreign dictatorship puts subtle syntactic mistakes into the text to signal that he is under duress, intending people back home to spot that he is not complying with the usual constraints.

There are any number of reasons why we might depart from normal syntax: we might be half asleep, drunk, delirious, brain-damaged, interrupted, distracted, foreign, playful, joking, impersonating someone. But if our interlocutor can see \emph{why} we have departed from the usual grammatical practices, they may be able to understand what is going on with us. Otherwise, we become mysterious. But it is of course our right to be mysterious if we want to: the fact that grammatical constraints have \isi{normative} force does not impose on us, even as a default, any obligation to obey.

\section{Conclusion}
\label{sec:pullum:conc}

What I have tried to do in this rather wide-ranging survey is to make the following points. I have stressed that the \isi{formalist movement} in logic had an important rationale: representing logical proof in a totally syntactic way so that the completeness and consistency of a logical language (the correspondence between its resources for proof and its efficacy in preserving truth under inference) could be mathematically verified.  It was in pursuit of this goal that Emil {\Post} invented the systems we now call generative grammars, the systems that were repurposed three decades later for use in linguistics.

Though talk of ``rules of grammar'' persisted in linguistics, even in Chomsky's work, it did not really make sense within a generative perspective (as Chomsky himself noticed): rewriting operations are \emph{not} rules in the sense that we can follow them, behave in accordance with them, be guided by them, or violate them.  Yet we do need a conception of rules with \isi{normative} force in the ordinary sense of rules that can be respected or violated, for the point of linguistic grammars is not to compactly represent a corpus or to characterize a mental organ, but to define well-formedness over an indefinitely large class of sentences.

The claim that rules of grammar have \isi{normative} force should not be confused with the bid to change people's linguistic habits and practices represented by the prescriptive grammar tradition. And it should also not be taken to be at odds with the empirical character of linguistics (there is a fact of the matter about whether English is a prepositional or a postpositional language), or to be in conflict with the goal of making the predictions of grammars fully clear and explicit, with help from tools from logic and mathematics.

I believe that generative grammars (the systems that {\Post} invented) were the wrong tool to pick, but there are alternatives.  Repurposing model theory for syntactic description, for example, is a better idea. Under that view grammars are finite sets of constraints on structure, and well-formedness is construed as model-theoretic satisfaction. An early work advocating this view was \citet{JohnPost80}. \citet{Rogers98} develops it in a much more technically sophisticated way, and uses the model-theoretic perspective to derive some fascinating insights concerning what generative grammars of early 1980s vintage actually claimed (they were in fact strongly equivalent to context-free phrase structure grammars). A small but growing minority of syntacticians have been further developing the idea that formal grammars could be developed along model-theoretic lines (see \citealt{PullScho01LACL} and \citealt{Pullum13} for discussion and references).

As to how formalized constraints on sentence structure can have \isi{normative} force without in any sense implying anything about what anyone should do, or implying any judgment on a person that they have done something wrong (or right), I think the work to turn to is that of \citet{Millar04}, who (as briefly recounted above) relates the \isi{normativity} of rules in certain kinds of systems not in what we ought to do but in what can make our words or actions more predictable to others. Weaving together the strands I have briefly surveyed here offers what I believe might be a productive line of work in the steadily unifying disciplines of philosophy, psychology, and linguistic science.

\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]

\end{document}
