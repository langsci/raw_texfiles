\documentclass[output=paper]{LSP/langsci} 
\author{Erich Steiner\affiliation{Universität des Saarlandes, Saarbrücken} 
}
\title{Methodological cross-fertilization: Empirical methodologies in (computational) linguistics and translation studies} 
\shorttitlerunninghead{Methodological cross-fertilization}
\abstract{Recent years have seen attempts at improving empirical methodologies in contrastive linguistics and in translation studies through interdisciplinary collaboration with multi-layer corpus architectures in computational linguistics. At the same time, explanatory background for empirical results is increasingly sought in more sophisticated models of language contact in typologically based contrastive linguistics on the one hand, and in language processing in situations of multilinguality, including translation, on the other. Three attempts are discussed to narrow the significant gap between the high level of abstraction of such models, and data provided through shallow analysis and annotation of electronic corpora. 

The first of these operationalizes the high level terms ``explicitness/explicitation'' in terms of lexicogrammatical data available in a contrastive corpus, treating them as dependent variables and attempting to explain their variation in terms of the independent variables controlled for in the corpus architecture. 

The second attempt starts from the same corpus architecture, yet includes annotations about textual cohesion in its operationalizations and develops increasingly fine-grained hypotheses to limit search space and variation between independent and dependent variables so as to get closer to causal explanations rather than explanations in terms of co-variation.

The third attempt intersects corpus data of the type outlined before with data from processing studies, aiming at an integration and mutual explanation of product and process data. Our focus here is on methodological issues involved in integrating data of such different types and granularity in an overall empirical research architecture.}
\ChapterDOI{10.5281/zenodo.1019691}
\maketitle

\begin{document}
% \todo{check hypothesis numbering}
%e.steiner@mx.uni-saarland.de

\section{Empirical methodologies: some issues to be addressed}\label{sec:steiner:1}
\largerpage
Recent years have seen a few, although still limited, attempts at improving empirical methodologies in contrastive linguistics and in translation studies through interdisciplinary collaboration with projects involving multi-layer corpus architectures as developed and refined in computational linguistics. These corpus architectures provide data enriched by a variety of techniques ranging from shallow to deep processing (\citealt{VelaEtAl2007}, \citealt{ČuloEtAl2008}, \citealt{TeichEtAl2008}, \citealt{Teich2010}). They allow the posing of linguistic questions as empirical questions even in areas which until recently were considered the province of hermeneutic debates supported by -- at best representative -- examples. If such data and their relationship to linguistic theorizing can be clarified, linguistics and translation studies can be made much more empirical than has hitherto been the case (cf. \citealt{Featherstone2009}; \citealt{ZfS2009}, \citealt{Hawkins2004} for critical debates in a wider linguistic context). 

As a necessary consequence of these developments, empirical methodologies have come under critical scrutiny leading to improved standards of data production, maintenance and analysis. At the same time, explanatory background for empirical results is increasingly sought in more sophisticated models of language contact in typologically based contrastive linguistics (e.g. \citealt{Thomason2001}, \citealt{Teich2003}, \citealt{Doherty2006}, \citealt{Fabricius-Hansen2008ed},
% \todo{is fabricius the edited volume or the introduction in that volume?}
 \citealt{Siemund2008}, \citealt{Steiner2008}, \citealt{Miestamo2008}, \citealt{Dunn2011}) on the one hand, and in language processing in situations of multilinguality, including translation, on the other (\citealt{AlvesEtAl2010}, \citealt{CarlEtAl2008}). The result of these developments is a conceptual and methodological gap between the necessarily high level of abstraction of models on the one hand, and the data provided through shallow (and cheap), or else deeper (and more expensive), analysis and annotation of electronic corpora on the other. It is not immediately obvious where and how stipulated abstract and general properties deriving from models of language variation, contact and change, such as \textit{complexity, explicitness, density, contrast, interference and shining-through etc.} show up in the data, and if so, which of the stipulated independent variables causes which (group of) properties to vary. This gap has to be narrowed through concerted efforts involving methodologies from computational linguistics, including machine translation, (contrastive) linguistics and translation studies, efforts yielding convincing operationalizations of the abstract properties involved. Abstract properties like \textit{complexity, explicitness, density, contrast, interference} and \textit{shining-through} can thus be linked to patterns in the data available as product data in corpora, or as process data in experimental processing studies.

Beyond this, and quite fundamentally, there is the question of ``representativeness'' of data: In what sense can we claim that our data, and how much of them, represent the phenomenon we are investigating, rather than some ad-hoc variation caused by any number and kind of independent variables outside the scope of our models? To take just one example, relative explicitness of textual encoding of meaning may be the result of different degrees of context dependence, of level of subject field expertise (of author and/or reader), of time-pressure during production, of the dialectics between economy vs. expressiveness, of the degree of training for the production of the register/genre at hand, of level of education, of formality, of the status of the text produced as an original or a translation etc. etc. If we are interested in the effects of one independent variable, say translation as a mode of text production, we must find ways of isolating it from the other potentially interfering variables. Otherwise, the effects found in our data may be said to derive from something else than the text production mode ``translating''. 

We shall discuss three test cases of work involving linguists, translation scholars and computational linguists (and marginally psycholinguists): one of them investigates a key notion of translation (\textit{explicitation}) using product-data, the other an under-researched area of language contact (\textit{borrowing and interference phenomena on the level of cohesion}), again using product-data from a corpus, and the third investigates key aspects of language processing during translation, thus focussing on process-data. The gap to be closed exists between the notions of \textit{explicitness/explicitation} and \textit{contact through cohesion} on the one hand, and the level of the available data on the other. If our models of translation, for example, stipulate that translated texts are more explicit than non-translated registerially-parallel (i.e. texts of the same register) original texts in the same language, and if we want to approach this assumption empirically, then we need to operationalize the notions of ``explicitness/explicitation'' with respect to the representational categories available in our data. If the categories in our data consist of

\begin{itemize}
\item 
lexical strings,
\item
annotation layers such as PoS, words, chunks, clauses, sentences, 
\item
statistics on relationships between these, 
\item
alignment phenomena between relevant units in originals and translations such as \textit{crossing lines,} and \textit{empty links}
\end{itemize}

we need to define, or rather operationalize, the notions of explicitness\slash ex\-pli\-ci\-tation in terms of these categories, and we need to do so in a theoretically motivated way. Of the categories of data just mentioned the first three should be self-explanatory. \textit{Crossing lines} as alignment phenomena occur when between aligned source-target translation units the source-target link crosses a unit boundary (non-local translations as in the translation of a syntactic subject into an object, or as the translation of a raising-verb into an adverbial). An \textit{empty link} occurs whenever one of the source-target nodes in a translation relationship is empty at a given level of representation. 

Seen relative to existing approaches, we are attempting to synthesize individual parameters of language comparison and language contact into more general dependent variables (\textit{explicitness, cohesion}), and we suggest operationalizations in such a way as to enable empirical corpus-based (and ultimately also experimental) investigations. We shall also try to isolate causally related independent variables for the variation observed (\sectref{sec:steiner:2}). Another attempt at narrowing our search space is the formulation of increasingly fine-grained hypotheses on corpus data as illustrated in \sectref{sec:steiner:3}. This should allow us to make our observations more precise, and also to systematically reflect textual cohesion, rather than lexis and grammar only. However, this further attempt in itself does not yet solve the problem of uniquely identifying causes and effects. To that end, we shall briefly discuss an attempt at intersecting corpus data with data from processing experiments, in order to find evidence for relationships stipulated by our models of language production, and of translation more specifically (\sectref{sec:steiner:4}). Finally, an attempt is made to identify achievements as well as persistent methodological weaknesses, and implications are identified for research methodologies.

\section{Explicitness of encoding, operationalization in terms of corpus-data and the task of isolating independent variables}\label{sec:steiner:2}

The first attempt \textit{CroCo}\footnote{Cf. \url{http://fr46.uni-saarland.de/croco/}; funded by DFG 2005--2009} departed from the assumption that translations as texts are characterized by the property of \textit{explicitness} relative to registerially parallel original texts within the same language. Elaborate tests were conducted on corpora of translations and registerially parallel texts in the target languages English and German. A further assumption was that this explicitness is due to the translation process, taking the form of \textit{explicitation} observable cross-linguistically between source and target text segments, so-called ``translation units''. Translation units were then searched for explicitation phenomena causing the observed differences in ``explicitness'' (cf. \tabref{tab:steiner:FrequenciesOfPOS} in \sectref{sec:steiner:4}). Register and language no doubt both play their parts as independent variables causing variation in explicitness, yet the assumption here was that the translation process plays its own theoretically motivated role in this configuration. The abstract notions of \textit{explicitness/explicitation} have their own history both in translation studies and linguistics, yet have only rarely been subjected to empirical studies (cf. \citealt{Englund-Dimitrova2005} and the literature cited therein) . 

The \textit{CroCo-corpus} is partitioned into 8 registers each in English and German (cf. \citealt{Hansen-SchirraEtAl2007}, \citealt{VelaEtAl2007}, \citealt{Steiner2008}), plus one cross-register reference corpus for English and German each. The sub-corpora were compiled using sampling techniques (\citealt{Biber1998}) and annotated for PoS , morphology, chunks, syntactic functions, clauses and sentences (cf. \citealt{ČuloEtAl2008} for an overview of the tools used). The sub-corpora of original and translated texts can be compared along all of the annotation layers, including combinations of them, both within and across English and German. A second and important source of data were alignments between originals and their translations on all of the levels annotated (i.e. word, chunk, clause, sentence cf. \citealt{ČuloEtAl2011}). \figref{fig:steiner:BidirectionalTranslationCorpus} shows the corpus structure.


\begin{figure}
\includegraphics[width=.5\textwidth]{figures/SteinerF1.png}
\caption{Bidirectional Translation Corpus (from \citealt[ch.2]{Hansen-SchirraEtAlfc})}
\label{fig:steiner:BidirectionalTranslationCorpus}
\end{figure} 

The notions of \textit{explicitness }and\textit{ explicitation} were then given a careful operationalization (cf. \tabref{tab:steiner:SummaryOfShallowStatistics} for ``shallow'' annotation layers) in terms of the types of information contained in the different configurations of relevant sub-corpora (cf. \figref{fig:steiner:BidirectionalTranslationCorpus}). It was then possible to show \textit{whether} and \textit{to what extent} the phenomenon of ``explicitness'' obtained for any of the sub-corpora compared to the others. 

\tabref{tab:steiner:SummaryOfShallowStatistics} uses as features low level data in the form of lexical density (LD), type-token-ratio (TTR) and part-of-speech tagging (PoS). The contrasts C1-n in the second column refer to contrasts between sub-corpora (reference corpora (ER, GR), corpora of originals (EO, GO), translation corpora (ETrans, GTrans), and register specific corpora within originals and translations as listed in footnote 2. In the third column, we list which indicator(s) in terms of the low-level data we believe to be indicative of which phenomena, and in the fourth column we posit explanations in terms of our three independent variables language, register, and status of a corpus as representing originals or translations. 

The independent variables \textit{language system, register }and\textit{ translation} can be reasonably isolated and related to the observed effects in the data. Remaining questions about representativeness of the sub-corpora can to some extent be approached with future improvements in sampling techniques and corpus size. There is the remaining question of the extent to which our corpora, especially the translation corpora, represent ``competent/standard/evaluated'' translations, rather than data full of opportunistic errors and mistakes. Doherty (cf. e.g. \citeyear[11ff]{Doherty2002}; \citeyear[1ff and 159ff]{Doherty2006}) strictly defends exclusively ``evaluated/controlled data'' as relevant for empirical work. As far as this methodological claim is concerned, we would defend the acceptance of texts as relevant data as long as they have been published as ``translations'', our main argument being that judgements about what counts as more or less competent language use are subject to a set of by now well-documented problems in language production generally (cf. e.g. \citealt{Haider2009}), and in evaluations of translations in particular \citep{House2001}. What our translational corpora do represent, we would claim, is the language produced in situations culturally accepted as ``translating'', which is not at all the same as holding that all these translations are ``good'' in the sense of being optimized solutions to the general problem called ``translation''. Furthermore, if the majority of the translated texts in the corpus can be shown to exhibit the property of ``explicitness'' relative to original texts, then this property is established as a distinguishing property of this subcorpus -- even if in separate evaluations of these translations it can be shown that they are sub-optimal.

\begin{table}
\resizebox{\textwidth}{!}{
\begin{tabular}{llll}
\lsptoprule
{Features} & {Contrast (C1-n)} & {Phenomenon: Indicator} & {Explanation}\\
\midrule
Lexical Density & C1 (Reference & - Experiential explicitness: LD & Language\\
(LD), Type-Token- & Corpora ER vs. GR) & (E{\textgreater}G) &  System\\
Ratio (TTR), Parts- & {} & - Strength of lexical cohesion & {}\\
of-Speech & {} &  other than repetition: TTR (G{\textgreater}E) & {}\\
proportionalities & {} & - experiential and referential & {} \\
(PoS) & {} & density: PoS (G{\textgreater}E in nominal & {} \\
{} & {} & orientation) & {}\\
\tablevspace\tablevspace
PoS & C2.2 (8 Registers & - Experiential density: nominal & Register,\\
proportionalities, & within languages E & orientation & Language\\
reflecting ``nominal & and G) & {} & {}\\
orientation'' & {} & {} & {} \\    
{} & {} & {English:} \textit{TOU {\textgreater} SHARE {\textgreater} WEB {\textgreater}} & {}\\
{} & {} & \textit{ESSAY {\textgreater} INSTR {\textgreater} SPEECH {\textgreater}} & {}\\
{} & {} & POPSCI{\textgreater} FICTION & {}\\
{} & {} & {German: }\textit{TOU {\textgreater} WEB {\textgreater} SHARE {\textgreater}} & {} \\
{} & {} & \textit{ESSAY {\textgreater} INSTR {\textgreater} SPEECH {\textgreater}} & {}\\
{} & {} & \textit{POPSCI {\textgreater} FICTION} & {}\\
\tablevspace\tablevspace
LD, TTR, PoS & {} & - referential and experiential & {}\\
(Nominal & {} & density: spread of language- & {}\\
Orientation) & {} & internal variation (G{\textgreater}E for TTR & {}\\
{} & {} & and nominal orientation; E{\textgreater}G for & {}\\
{} & {} & LD) & {}\\
{} & C2.1 (EO vs. GO by & - experiential and referential & Register\\
{} & register, with & density: LD, TTR, PoS & {}\\
{} & ER/GR differences & {} & {}\\
{} & factored out) & {} & {}\\
\tablevspace\tablevspace
LD, TTR, PoS & C3 (Translations vs.& Experiential explicitness: (LD) & Translation\\
{} & originals within a & (ORI{\textgreater}TRANS) & Process, De-\\
{} & language and within & - lexical variation: TTR & Metaphoriza-\\
{} & a register) & (ORI{\textgreater}TRANS) & tion\\
{} & {} & - referential density: nominality & {}\\ 
{} & {} & (ORI{\textgreater}TRANS, with exceptions) & {}\\ 
\lspbottomrule
\end{tabular}
}
\caption{Summary of ``shallow'' statistics used as operationalizations for ``explicitness'' (cf. \citealt[ch.14]{Hansen-SchirraEtAlfc})}
\label{tab:steiner:SummaryOfShallowStatistics}
\end{table}

However, even if it can be argued that a CroCo-type architecture allows systematic studies of co-variation between variables, and even if we make a case for its ``translations'' to represent relevant data, we have to admit of a significant methodological problem: the third one of our variables, \textit{translation}, if interpreted as \textit{translation process}, is inherently complex and at present still insufficiently-understood (cf. also \citealt{Becher2010}). Not only does it share all the complexities of monolingual text production, but it is text production under the additional constraints of a source text, plus usually the constraints of a professionally defined situation of production. This methodological problem can be systematically addressed by subjecting the notion of \textit{translation process} to a more detailed analysis and by testing its effects in experimental processing studies involving the cumulation and intersecting of data from key-stroke logging, eye-tracking and post-hoc protocols (cf. \citealt{AlvesEtAl2010}, see also \sectref{sec:steiner:4} below). Arguing on the basis of the results of \textit{CroCo}, we therefore feel justified in claiming that translated texts are characterized by some property, such as explicitness, and that the reason is not either the language, or the register, as these were controlled for separately. However, we are not able to convincingly show \textit{which aspect} of the translation process is related to precisely which sub-aspect of overall explicitness/explicitation. And finally, it cannot be excluded categorically that two variables, say \textit{translation} (independent) and \textit{explicitness} (dependent) co-vary, but with the causing factor being located outside our model and ultimately causing the co-variation. 

As a first evaluation of the \textit{CroCo}{}-line of research, we would argue that the general corpus-architecture and the data processing employed can be trusted to yield more and also methodologically refined results of the type indicated here, if it is used in replications of our study. But we need improvements in the areas of \textit{modelling} (internally over-complex variables, representativeness of data), \textit{operationalization} of the models in terms of linguistics features, and in \textit{processing techniques} for corpus data (processing pipelines, evaluation and significance of findings) and even more urgently for experimental data to be discussed in section 4 (amount and naturalness of data, experimental design). It is, for example, far from clear which of the product-based frequencies obtained from our corpora are the result of precisely which of the processes observed in eye-tracking or key-stroke logging experiments. There are at present no models known to us which would reliably relate corpus data to data from experiments, at least in translation studies (for a general critique of experimental data and its relationship to linguistic models cf. \citealt{Schlesewsky2009}). Improvements in modelling can be expected from translation studies and/or psycholinguistics, better operationalizations should come out of (contrastive) linguistics, and improved processing techniques are under development in computational linguistics. The task at hand now, it seems, is to improve methodologically guided communication between the relevant research communities. 

\section{Contrasting cohesive patterns in English and German: the role of hypotheses for interpreting corpus data and the challenge of identifying contact phenomena}\label{sec:steiner:3}

Our second attempt starts from the same corpus architecture as the one sketched above, yet includes annotations about textual cohesion in its operationalizations and develops increasingly fine-grained hypotheses to limit search space and variation between independent and dependent variables so as to get closer to causal explanations rather than explanations in terms of co-variation only. GECCo \footnote{\url{http://fr46.uni-saarland.de/gecco/GECCo/Home.html}; currently running and funded by DFG since 2011} sets out from the diagnosis that our current knowledge about English-German contrasts in cohesion is still weak. For contrastive grammar, we have reasonably comprehensive system-based accounts \citep{Hawkins1986,König2007}, yet these are not backed-up by empirical validation. Doherty's work (\citeyear{Doherty2002,Doherty2006}), which we have found very significant in its addressing phenomena of grammar, information structure and some aspects of cohesion, tests what she calls ``stylistic'' intuitions of competent native speakers and translators (\citeyear[11]{Doherty2002}), based on principles of optimal integration of local textual parts into their relevant discourse context (discourse appropriate translations, \citealt[1ff]{Doherty2006}). Unfortunately, her test environment is not very controlled and not critically assessed from a methodological point of view (cf. \citealt[ix]{Doherty2006}). Even so, she provides important intuitive and theoretically well-motivated insights into translation. Her overriding goal, however, of testing (previously trained) intuitions, rather than linguistic production and product as such, makes her work methodologically problematic as an empirical investigation. 

For cohesion, not even a system-based comparison is available, much less an empirical foundation for such a comparison. The tracing of contact phenomena on the level of cohesion is therefore necessarily still in its infancy (but cf. \citealt{Hansen-SchirraEtAl2007} for an early attempt; \citealt[a,b]{Kunzfc}). Substantial advances in technologies using multi-layer annotated electronic corpora for text-based investigations of phenomena of cohesion hold the promise of placing constrastive accounts on an empirical basis, and beyond this comparison also allow us to trace contact phenomena in suitably configured corpora. A multi-layer representation is used, approaching tree-bank functionality and including aligned data for English and German translations in both directions as a crucial empirical base, with the exception of the spoken subcorpora. Extensive frequency information about cohesive configurations is incorporated into what is essentially an extension and reconfiguration of the \textit{CroCo-corpus} referred to above, tied to varieties or registers of the language concerned, and this time notably including spoken sub-corpora (cf. \tabref{tab:steiner:GECCo}). 

\begin{table}
\begin{tabularx}{\textwidth}{XCC}
\lsptoprule
 & {German subcorpora} & {English subcorpora} \\
 \midrule
 & {spoken} & \\
  \midrule
 {comparable} & {original} & {original}\\
& {} & ELISA\\
& BACKBONE-DE & BACKBONE-EN\\
& GECCo spoken collection & MICASE\\
{parallel} & {translated?} & {translated?}\\
 \midrule
& {} {written} & {}\\
 \midrule
{comparable} & {original} & {original}\\
& CroCo-GO & CroCo-EO\\
{parallel} & {original\ translated} & 
{original\ translated}\\
& CroCo-GO\ CroCo-GTrans & CroCo-EO\ ETrans\\
\lspbottomrule
\end{tabularx}
\caption{GECCo corpus structure including spoken registers (cf. \citealt{Amoia2011})}
\label{tab:steiner:GECCo}
\end{table}

The CroCo corpus, partitioned into 4X8 plus two reference corpora, was restructured into 4 subcorpora (GO, EO, GTrans, ETrans) with the registers no longer saved as separate sub-corpora, but as structural attributes of the 4 sub-corpora. For the spoken registers, not contained in the earlier CroCo corpus, the \textit{GECCo-corpus} does not include translations, as these registers are usually not translated. As data for the contrastive work, though, they are sufficient. The new structure allows simpler and faster query in the CQP. Searches in the corpus can still be conducted within a single register or in all registers at the same time. This modified corpus structure implements some improved processing techniques of the type mentioned as desiderata in \sectref{sec:steiner:2} above (cf. \citealt{Amoia2011}). 


In terms of overall explanations for the data thus obtained, one of the interesting questions is that of whether contrastive properties of cohesion in the two languages point into the same direction as some assumed generalizations in contrastive grammar (directness of mapping from semantics to grammar (G{\textgreater}E), different tolerance of various forms of ``ellipsis'' (E{\textgreater}G), more explicit encoding in one of the languages in the clause (G{\textgreater}E), possibly the opposite tendency in the verb phrase (E{\textgreater}G), etc.), or whether cohesion serves as a dialectic counterpart, distributing constraints not in the same direction as in grammar, but possibly in the opposite one. In the terms of \citet[44ff]{Hawkins2004}, we are ultimately interested in how the two languages cue ``processing enrichment'' through their different systemic options of cohesion, and ultimately also in whether or not the enrichment, and thus interpretation of discourse units, is differently affected. A further interesting object of investigation are the properties of cohesive (referential and/or lexical) chains in terms of frequency, length, distance between elements, number and kind of entailments triggered through sense relations in and between lexical chains etc.), which hitherto have hardly been accessible to empirical investigations (but cf. \citealt{Hansen-SchirraEtAl2007} forthcoming for early thoughts along these lines) .

Our corpus-linguistic analysis includes the identification of various types of cohesive devices (\textit{reference, substitution, ellipsis, coherence relations, lexical cohesion}; for some important modelling background cf. \citealt{Halliday1976}; \citealt[524ff]{Halliday2004}) and their lexicogrammatical realizations, the linguistic expressions to which they connect (the antecedents), as well as the nature of the semantic ties established and properties of the cohesive chains where appropriate. Including translations in the analysis should provide evidence for analogies between cohesive devices in the two languages, but also show areas where one-to-one equivalents are not preferred, or even non-existent. 

The currently existing annotation requires an expansion in terms of additional layers of annotation, which are currently under construction. For instance, particular cohesive devices establishing \textit{reference} or \textit{substitution} can be investigated on the part-of-speech level. Other types such as\textit{ conjunctions} can be identified when examining the part-of-speech as well as the chunk level. For the investigation of \textit{ellipsis} combined queries into different layers of annotation can be employed. For the analysis of nominal, verbal or clausal ellipsis the current annotation is too shallow and does not permit a fine-grained differentiation of types of linguistic devices. Thus, more specific cohesive categories have to be developed and annotated. 

In order to narrow the gap between the concept of \textit{contact through} cohesion and the level of our data, a structured grid of hypotheses is specified for empirical analysis as a testing ground for 


\begin{itemize}
\item
contrasts in the uses of \textit{similar }systemic resources (e.g. the definite article in German vs. English, or the dependent variable in {(H1)} below)
\item
contrasts in the use of \textit{different} systemic resources for similar cohesive functions/purposes (e.g. substitution vs. reference through personal pronouns vs. lexical cohesion for the function of co-reference in German vs. English)
\item
traces of language contact due to different usages in contact vs. non-con\-tact varieties (categorical and/or in terms of frequency in comparisons of translated vs. original text of the same register within English or German). 
\end{itemize}

Note that the formulation of hypotheses as such is not a new development in our context (cf. \citealt[141ff]{Steiner1991}; \citealt[143ff]{Teich2003}; \citealt[127ff]{Hansen2003}; \citealt[89ff]{Neumann2008}), and is, of course, standard practice in many strands of empirical work. What we are using them here for in particular is the motivated narrowing down of search space in our data for the specific purposes of our investigation. 

Examples of some hypotheses are:
\begin{exe}\exi{(H1)} \label{hyp:steiner:1}
3\textsuperscript{rd} person singular neuter pronouns vs. masculine and feminine pronouns (frequency E(nglish){\textgreater}G(erman) for originals (contrast)), in terms of PoS overall and proportionally within pronouns. 
\z

Cf. \REF{ex:steiner:1} and \REF{ex:steiner:2} for examples from our corpus:

\ea\label{ex:steiner:1}
\textit{Eine verantwortungsbewusste Politik kann diesen Prozess, der zudem von objektiven Faktoren determiniert wird, nicht nur flankieren. \textbf{Sie} muss \textbf{ihn} vielmehr formen.}
\z

\ea\label{ex:steiner:2}
\textit{A responsible policy can not only accompany this process, which is additionally determined by objective factors, \textbf{it} must moreover shape \textbf{it}.}
\z

Preliminary tests on {(H1)} have been run and are relatively straightforward to carry out with lexical search on our lemmatized sub-corpora. Initial results (cf. \citealt{Kunzfc}) indicate higher overall frequency as predicted, yet sensitive to register, and even unconditioned higher frequency for cohesive vs. non-cohesive usage E{\textgreater}G %(cf. Hypothesis 4).
(cf. {(H4)})
Interpretation is less clear, because the observed differences may be due to, at least, the predominance of grammatical vs. natural gender in co-reference for 3\textsuperscript{rd} person singular pronouns in German, the possible preference in German for demonstrative reference over simple personal or possessive reference 
%(cf. Hypothesis 4) 
(cf. {(H4)}), the different degrees of availability of lexical cohesion as an alternative to pronominal reference between the languages etc. So, while {(H1)} narrows the search space for findings, it does not in itself lead us unambiguously from the observation of co-variation to causal explanation. 

\begin{exe}\exi{(H2)}\label{hyp:steiner:2} 
GO{\textgreater}ETrans(lations){\textgreater}EO(riginals) in locally non-ambiguous 3\textsuperscript{rd} person reference within their register.
\z

A German -- English contrastive pair of (constructed) texts is given in \REF{ex:steiner:3} and \REF{ex:steiner:4} below:

\ea\label{ex:steiner:3}
\textit{Mein Freund machte seinen Abschluss, besorgte sich einen Kredit und gründete seine erste Firma. Er/ sie/ es/ der/ die/ das/ dies/ diese(r,s)/ letztere(r/s), der Versuch/ daraus \ldots wurde ihm zum / entwickelte sich ein Verhängnis.}
\z

\ea\label{ex:steiner:4}
\textit{My friend got his degree, obtained a loan and founded his first business. It/ this/ that/ out of this, the attempt developed (into) a disaster.} 
\z

The underlying assumption here is that English translations (versions of \REF{ex:steiner:4}) from German (versions of \REF{ex:steiner:3}) show less local ambiguity in local antecedent -- pro-form relationships than English originals (not exemplified above), inheriting this (hypothesized) property from their German originals. ``Local'' here needs to be operationalized into ``between adjacent clauses'' or some such measure. It can then be tested, if ambiguity is taken to mean ``number of possible antecedents for each relevant pro-form''. Our assumptions here are triggered by, once more, the existence of grammatical gender in German, as well as by the higher usage of alternative and less ambiguous forms instead of \textit{es} in German (cf. {(H4)}). These findings, if corroborated, should include fewer possible antecedents for German ``er/sie/es'' than for English ``he/she/it'', but certainly fewer possible antecedents for the alternative (demonstrative/ adverbial/ fully lexical) cohesive alternatives. We are referring here to the systemically conditioned availability in German of the demonstrative article, as well as ``pronominal adverbs'', both of which have a function of narrowing the range of plausible antecedent phrases for their occurrences if compared with personal or possessive pronouns, providing a motivation for our hypothesis (cf. \citealt{Kunzfc}: \sectref{sec:steiner:4}). As far as the cost of this analysis is concerned, we have to trace the possible antecedent -- pro-form relationships within a local domain, which as such is possible on the basis of PoS annotations, combined with chunk and clause annotation. Open questions, however, arise out of the fact that co-reference relationships need not be local in the sense just introduced, thus making our measure of ``ambiguity'', and in that sense ``complexity'', one of local structure of the encoding, rather than an overall textual measure. Nor can we directly infer processing complexity from this local encoding complexity -- which has to be taken for granted for any product- rather than process-based work in isolation. Local encoding ambiguities will, in fact, often be tolerated by language producers and processors in the interest of more global efficiency (\citealt[47f]{Hawkins2004}).

\begin{exe}\exi{(H3)}\label{hyp:steiner:3}
ETrans-T(arget)T(ext){\textgreater}GO(riginals)-S(ource)T(exts) in explicitated 3\textsuperscript{rd} person reference through use of fully-lexical TT-equivalent of pronominal source.
\z


The assumption here is again that German co-reference chains in originals are locally, i.e. between adjacent members of a chain, less ambiguous than in English originals. If this is the case, then one strategy for an English translation would be to use lexically-headed phrases, possibly combined with pre-modifying demonstrative/ deictic material, to achieve a similar effect as their German source text originals. {(H3)}
%Hypothesis 3 
refers to one aspect of %Hypothesis 2
{(H2)}, the two are thus not strictly independent. In order to obtain the relevant data, we have to retrieve co-referential chains from texts, which at this stage can only be obtained from costly hand-coded small corpora. We also consider chunk-by-chunk alignments between translationally related ST-TT units, which is why we are currently exploring improvements through increased use of tools for lexical chaining. %Hypothesis 3 
{(H3)} would again successfully limit our search space, however on somewhat costly data, and with a somewhat indirect link to relevant assumptions.

\begin{exe}\exi{(H4)}\label{hyp:steiner:4} 
EO{\textgreater}GO in cohesive usage of \textit{it vs. es} (because of alternative usage in German of demonstratives of various sorts and pronominal adverbs) between matching registers in original texts, measured both in terms of PoS overall and as proportion of cohesive vs. non-cohesive usage of \textit{it}.
\z

{(H4)} shares some of its background assumptions with {(H1)}, but in this case we would focus on the use of \textit{it/ es }in cohesive vs. non-cohesive usage. The production of the data is not trivial, though. Our annotation needs to cover grammatically triggered usages of 3\textsuperscript{rd} person singular pronoun \textit{it/es}, because these need to be classified into one relevant sub-class, which would then leave the relevant co-referential and thus cohesive complement class. Again, given the data can be produced at reasonable cost, the hypothesis would successfully limit the search space, even though the results obtained could be partly due to other interferring factors -- though not register or the translation vs. original status, as these are being kept constant. 

\begin{exe}\exi{(H5)}\label{hyp:steiner:5} 
In terms of the phenomena tested in H1 -- H4, we predict that in a comparison of originals and translations (in this case within the same language and register), the translations will diverge from the originals in the direction of their source language. 
\z

The background for this explanation is an assumed interference, or rather, shining-through effect (cf. \citealt{Teich2003}). As some initial findings indicate (cf. \citealt{Kunzfc}: \sectref{sec:steiner:4}), this is largely, but, dependent on register, not always borne out. Here it will be interesting to trace explanations for why register appears to be an influential variable on some element of the translation process.

Further hypotheses are developed for \textit{comparisons of vagueness/ ambiguity of reference and scope}. Differences can be expected here deriving from usage of different lexicogrammatical realizations of some constant cohesive relationship, or even from different cohesive relationships altogether. An example would be the contrastive use of a generic full lexical phrase vs. a definite phrase vs. a phrase pre-modified through a determiner (possessive vs. deixis vs. demonstrative) vs. a phrase headed by a pro-form (demonstrative vs. pronoun) as tested on aligned ST-TT pairs. The interest would not be in the phenomenon as such, which has been researched under ``accessibility rankings'' (e.g. \citealt{Ariel1990}, \citealt[45]{Hawkins2004}), but in the different kinds of \textit{ambiguity} and/ or \textit{vagueness} associated with each case in interpretation/ enrichment. In general, we would predict that a) translations are less ambiguous and vague than their originals in SL-TL configurations (explicitation through translation), but also b) that they diverge from their original registerially-parallel counterparts in the direction of the respective source language (interference, shining-through).

A final type of hypothesis makes reference to contrastive register-specificity of cohesive configurations, and again their behaviour under contrast vs. contact conditions. For example, German written as opposed to spoken registers may be characterized by dense lexical chains with relatively low lexical repetition, whereas this distinction may be much smaller and involving more repetition for English. For translations from one of these languages into the respective other, we would then predict an interference-like ``shining-through'' effect (cf. \citealt{Teich2003}) of source registers onto their target corpora. These configurations will be operationalized as length of lexical or referential chains, density of chains, number of chains per text sample, frequency, length, distance between elements, number and kind of entailments triggered through sense relations in and between lexical chains\footnote{I am grateful to Marilisa Amoia for emphasizing the importance and accessibility of such relationships to me in recent discussions. } etc. On the basis of WORDNET-type taxonomic classifications, we are investigating different levels of abstraction/ generality in chain progression language internally, but also between aligned lexical translation units. Assuming that it is a frequent translational strategy to resort to a superordinate term as a lexical equivalent in cases of lexical gaps or simply lack of knowledge, one might hypothesize greater generality in translations over originals. On the other hand, if contrastive registers of originals show different degrees of implicitness, possibly realized as higher generality in English of lexically realized concepts, as a register feature, as is sometimes hypothesized in comparisons of English and German texts, this might interfere with translational effects. Add to this the increased reliance of English on ``general nouns'' as a means of lexical cohesion (\citealt{Schmid2000}, \citealt{Mahlberg2005}), and we have grounds for separately exploring lexical generality as a register feature in originals, and decreasing generality relative to originals in both directions. 

Another assumption on which one could base hypotheses about lexical cohesion would be that more lay-type registers, rather than expert-type registers, use topological , and often polyphyletic (non-strict inheritance), classification systems rather than typological monophyletic (strict inheritance) ones (cf. \citealt[23ff]{Halliday1993}). With the help of WORDNET-based tools for lexical analysis, we can operationalize the concepts of \textit{typology vs. topology} and of \textit{monophyletic vs. polyphyletic} or else \textit{historical vs. genetic}, or \textit{hyponymy vs. meronymy} into lexically-implied sense relationships between elements of lexical chains between registers within and across languages, and between originals and translations. Note that this does not only apply to nouns and their derived adjectives, but also to preferred semantic verb classes: the often observed preference of \textit{relational} vs. \textit{action} verbs in English over German texts may contribute to generality and thus implicitness of the vocabulary used in lexical chains. 

At this early stage of the GECCo-project, we would hypothesize shining-\linebreak through effects for ST-TT configurations, and for density of chains only a possibly increasing effect of the translation process as such. We need to be aware, though, that the frequency data that can be obtained through work of the type described here is valid and interesting in research on text production in general, whether in monolingual or multilingual contexts, and is furthermore only possible through the joining of efforts from (contrastive) linguistics, translation studies, and computational linguistics. 

Where in our research methodology can we trace \textit{contact} phenomena, rather than just \textit{contrasts} in terms of categories and frequencies? In short, where we compare originals of the same register, including the register-neutral reference corpora, across languages, we obtain cohesive contrasts. Where we compare originals and translations within the same language and the same register, any resulting differences would seem to be due to either interference, or else ``normalization'' in the sense of ``hyper-adaptation to target-language norms''. In a weak sense, these are contact phenomena. One possible causal source of these phenomena would then be the translation process, involving some form of ``borrowing'' (\citealt[70ff]{Thomason2001} and earlier). Our research architecture is sensitive not only to classical forms of borrowing, but characteristically to shifting frequencies (i.e. over- or underuse relative to the norm established by the same register in the ``originals'' corpus) below the threshold of structural or lexical borrowing. The translation process in a narrower sense is not the only possible source of contact phenomena in our architecture. The cause of variation could, in fact, be any other component of the contact situation, as long as it impinges on the translation process in a wider sense. In order to make our notion of ``translation'' more precise, we need to appeal to process studies as shown in the following section.

\section{Improving corpus architectures and relating data in corpora to data from processing experiments against relevant models}\label{sec:steiner:4}

The third attempt intersects corpus data of the type outlined before with data from processing studies, aiming at an integration and mutual explanation of product and process data. Our focus here is on methodological issues involved in integrating data of such different types and granularity in an overall empirical research architecture. We shall start, though, with a few more general requirements on empirical work of the type discussed here, before concentrating on intersecting different types of data with relevant models.

There is an overall ongoing challenge in research attempts of the type discussed here: The researcher needs to be constantly aware of the cut-off point between very costly ``deep'' (and to some extent less reliable) annotation, and more ``shallow'' (and to some extent more reliable) annotation, the latter of which leaves a substantial gap between data and interpretation. Linguistically ``deep'' annotations, notwithstanding their disadvantages in terms of cost of production and in terms of reliability, have a clearer relationship to highly general models of language processing, whereas the cheaper and often more reliable surface annotations yield data in a very indirect and at worst spurious relationship to more ambitious and general modeling. Our annotation layers in \textit{CroCo} (cf. \sectref{sec:steiner:2}), for example, involve lexico-grammatical information, some of it shallow and low-cost (part-of-speech-tagging, type-token-ratio, lexical density), some other annotations deeper and involving heavy checking of (semi-)automatic annotations (chunking, clause analysis, and levels of alignments), and some layers even involving annotation by hand requiring monitoring of inter-coder-consistency. Even more challenging in our follow-up project GECCo (cf. \sectref{sec:steiner:3}), annotations involve those above plus yet more expensive annotations: referential indexing, annotating proform -- antecedent configurations, chaining of referential and lexical chains. It is obvious that ways need to be found of producing these with acceptable costs and of sufficient quality, something which cannot be regarded as solved on anything but a small scale. Improved contacts between researchers in translation studies, contrastive linguistics and computational linguistics in particular are essential to make any progress here so as to improve mutual understanding of the issues involved, as well as of the possibilities and limitations of computational technologies available currently.

The question also needs to be raised of how research architectures can be made more standardized than hitherto, allowing independent repetition and (dis-) confirmation of findings. \citet[176ff]{Schlesewsky2009} demands this for experimental data, yet the same is obviously true for corpus data. Relevant research communities need to more systematically share data and replicate each other's findings in order to arrive at methodological standards comparable to those in the more established empirical research fields. Something like ``multicentric studies/ trials'' may become possible for some research questions, and possibly most urgently in experimental, rather than corpus-based, studies. 

As we have implied in some passages here, and elsewhere (cf. \citealt{AlvesEtAl2010}), corpora, processing pipelines and evaluated results from corpus-based studies can be used stand-alone as sources of data to check on hypotheses of the types mentioned above. However, they will usually allow the discovery of co-variation of independent and dependent variables only, rather than a necessarily causal relationship. Even if we manage to align source-target units pair-wise within the same register and for only one hypothesis, thus excluding all but one independent variable, we may at best suspect a causal relationship. There is always in principle the possibility that our two variables in independent-dependent pairings co-vary because of some other variable outside our research design, a danger which is more or less plausible, depending on how good our model is. Graded predictions fare somewhat better than categorical predictions, as formulated e.g. in \citet[31ff]{Hawkins2004}, yet the basic methodological problem remains, at least as long as the data used are restricted to corpus i.e. product data.

Which brings us to our final point: in order to have a chance of explaining any findings we may have, we need a model, and if at all possible a model predicting the relevant behavior of our variables. The model and its derived hypotheses need to be precise enough to be falsifiable on our data. This is not always the case in (psycho-) linguistic studies generally (cf. \citealt[170ff]{Schlesewsky2009}), and very hardly at this point in translation studies. And finally, we need to relate corpus data to behavioural data in the widest sense (eye tracking, key-stroke logging, think-aloud protocols, production time or reaction time studies, EEG studies, FMRI, generally to psycholinguistic and even neurophysiological data) to pave the way towards more principled explanations of the results obtained in corpus studies. This is not because psycholinguistic and neurophysiological data show us the ``working of the mind'' directly, but rather because they provide additional, and in some cases possibly more direct windows into the mind, even though the latter is not directly observable. Provided, that is, that we have models of translation, language contact etc. which make predictions for the data that we have. 



\begin{table}[b]
\begin{tabularx}{\textwidth}{XSS}
\lsptoprule
 {Type of shift} & {E-G} & {G-E}\\
 \midrule
verb-noun & 24.31 & 16.98\\
verb-adjective & 11.69 & 02.80\\
verb-adverb & 06.95 & 00.25\\
adjective-noun & 17.43 & 09.48\\
adjective-verb & 01.84 & 09.92\\
adjective-adverb & 01.42 & 11.58\\
noun-adjective & 13.89 & 21.63\\
noun-verb & 05.74 & 16.98\\
noun-adverb & 03.40 & 01.08\\
adverb-adjective & 10.06 & 01.34\\
adverb-noun & 03.05 & 01.59\\
adverb-verb & 00.21 & 06.36\\
\lspbottomrule
\end{tabularx}
\caption{Frequencies of PoS-shifts (\%) (\citealt[116]{AlvesEtAl2010})}
\label{tab:steiner:FrequenciesOfPOS}
\end{table}


\tabref{tab:steiner:SummaryOfShallowStatistics} above shows data and interpretations from intra-lingual comparisons and inter-lingual comparisons, yet at that stage without any ``parallel'' corpora, i.e. source-unit into target-units mappings. Assume now that we have such additional data as shown in \tabref{tab:steiner:FrequenciesOfPOS} (PoS-shifts in aligned translation units) and Figures \ref{fig:steiner:EyeFixations} -- \ref{fig:steiner:TranslationProcessData} below\footnote{Project ProBral, funded by DAAD and CAPES 2008-2011}.

The data shown in \tabref{tab:steiner:FrequenciesOfPOS} are frequencies of PoS-shifts in source-target word alignments (not restricted to the passage shown in Figures \ref{fig:steiner:EyeFixations} to \ref{fig:steiner:TranslationProcessData}), eye-fixations from a eye-tracking study (\figref{fig:steiner:EyeFixations}), key-stroke logging data from the same text passage in \figref{fig:steiner:DraftingRevision}, and process data in \figref{fig:steiner:TranslationProcessData} showing shifts in intermediate solutions from two subjects translating the passage shown in Figures \ref{fig:steiner:EyeFixations} and \ref{fig:steiner:DraftingRevision}. In order to interpret these data, we clearly need a type of modelling of the relevant linguistic processes (translation, language production) which makes predictions for these kinds of data. As the situation is currently, we may have models making predictions for the linguistic data, and existing models of translation procedures may even make predictions about shifts as shown in \tabref{tab:steiner:FrequenciesOfPOS}. Yet our models are still too unspecific -- and models about a different domain -- to make predictions about eye movements and key-stroke loggings directly. The links between cognitive processes in translations and those kinds of data are quite indirect and probably much more prone to interference by other factors, than the purely linguistic data are. 

  


\begin{figure}[t]
\includegraphics[width=.9\textwidth]{figures/SteinerF3.png}
\caption{Eye fixations by S2 while deciding to us a noun or a verb for the translation of sich widersprechen in the drafting phase \citep[134]{AlvesEtAl2010}}
\label{fig:steiner:EyeFixations}
\end{figure} 
 

\newcommand{\stern}{{\color{red!90!black}$\bigstar$}}
\newcommand{\raute}{{\color{blue}$\blacklozenge$}}
\newcommand{\xpfeil}{\mbox{\color{blue}$<\hspace*{-.5em}\raisebox{0mm}{$\times$}\hspace*{-1em}\sqsupset$}} 
\newcommand{\lpfeil}{{\color{blue}\textleftarrow}}
\newcommand{\rpfeil}{{\color{blue}\textrightarrow}}
\newcommand{\Rpfeil}{{\color{blue}$\Rightarrow$}}
\newcommand{\maus}{$\bigcap$}

% \todo[inline]{please transform original F4 into text with \stern \raute \xpfeil \lpfeil{} \rpfeil \maus  with \texttt{typewriter {\color{red}sha}pe}}
% \todo{do headings properly}

 
\clearpage 

\begin{figure}
\adjustbox{minipage=\textwidth}{
Drafting phase \\
\stern \stern \stern \stern \texttt{We} \raute \texttt{are} \raute \texttt{conc} \xpfeil \texttt{vinced} \stern \raute {\color{red}[\stern\texttt{15.701}]} \texttt{that} \raute \stern \stern \stern \texttt{success} \stern \texttt{ful} \raute {\color{red}[} {\color{red}\stern\texttt{44.623}]} \texttt{do} \raute \texttt{not} \raute \texttt{contract} \xpfeil \xpfeil \texttt{dict} \stern \stern \stern \texttt{ion} \stern \lpfeil \lpfeil \lpfeil \lpfeil \lpfeil  \lpfeil \lpfeil \\
\lpfeil \lpfeil \lpfeil \lpfeil \lpfeil \lpfeil \lpfeil \stern \xpfeil \xpfeil \xpfeil \xpfeil \xpfeil \xpfeil \texttt{are} \raute \texttt{no} \lpfeil \lpfeil \lpfeil \lpfeil \lpfeil \lpfeil {\color{red}[\stern\texttt{49.640}]} \\  \texttt{leader} \xpfeil \xpfeil \xpfeil \xpfeil \xpfeil \xpfeil \stern \texttt{management} \raute \stern \stern \texttt{and} \raute \texttt{social} \raute {\color{red}[\stern\texttt{01:17.774}]} \texttt{resp} \stern \texttt{onsibilit} \stern \texttt{y} \raute \stern \Rpfeil . \raute \\

\medskip
Revision phase \\
\texttt{[} \maus \texttt{]} \stern \stern \stern \stern \stern \texttt{[} \maus \texttt{]} \texttt{t} \rpfeil  {\color{blue}[\texttt{ShftCtrl}\rpfeil]} \stern \texttt{in} \raute \texttt{conc} \xpfeil \texttt{flict} {\color{red}[\stern \texttt{10.846}]} \texttt{[} \maus \texttt{]} \stern \texttt{contradictory} {\color{red}[\stern\texttt{26.575}]} \texttt{[} \maus \texttt{]}
}
 \caption{Translation process data by S2 in the drafting and in the revision phase (from \citealt[131]{AlvesEtAl2010})}
 \label{fig:steiner:DraftingRevision}
\end{figure}

\begin{figure}
\includegraphics[width=.93\textwidth]{figures/SteinerF5.png}
\caption{Translation process data showing shifts in intermediate solutions}
\label{fig:steiner:TranslationProcessData}
\end{figure} 

\clearpage

As an illustration of the kind of hypothesis we would suggest here, look at Hypothesis {(H6)} below:

\begin{exe}\exi{(H6)}\label{hyp:steiner:6} 
We assume that in producing a given translation unit for a trigger source-text unit, a highly metaphorized (nominalized) passage in comparison to an experientially equal less metaphorized source passage will
\z

\begin{enumerate}
\item
trigger a higher number of attempted intermediate word-alignments before the final solution is produced,
\item 
trigger more and/or longer eye fixations on the problematic unit
\item 
trigger longer pause units and more attempts plus more revisions in the key stroke units for that passage.
\end{enumerate}

We also predict that the effects are negatively correlated to training of subjects and to length of time given for the task, but positively to direction of translation (into foreign vs. into native language). We furthermore predict a scale of relative strength of these variables training {\textgreater} length of time {\textgreater} direction of translation to be mirrored in relative frequencies of 1. to 3. above. 


\section{Conclusion and outlook}\label{sec:steiner:5} 

The significant properties of hypotheses such as our illustrative {(H6)} above are that it makes predictions for all of our strands of data and that it is based on a \textit{ranking} of independent variables as to strength of effect. We would thus also be looking at \textit{graded effects} in the data, rather than just on yes/no-effects. But note at the same time that in order to derive hypotheses such as {(H6)} above, we need models (of the translation process in this case) making predictions in terms of our data. And this is an area where conceptual work needs to be invested: existing models of translation are not fine-grained enough to make this sort of prediction at the moment, so these models need to be developed before studies using combinations of data from corpora and processing data can achieve the effects which they deserve. We are not claiming here that the problems involved are insurmountable, but rather that they are quite general to empirical language studies, and that we should improve communication across relevant research communities to find solutions. Empirical methodologies in contrastive linguistics and translations studies stand a lot to gain from such developments by being able to become more truly ``empirical''. The relevant sub-fields of computational linguistics, on their part, will find much-needed applications for (partly) existing solutions in search of relevant problems, but may even derive intelligent new solutions. 

\section*{Abbreviations}   
\begin{tabularx}{.45\textwidth}{>{\scshape}lQ}
tou & Tourism text\\
share & Shareholder letter\\
web & Website\\
essay & Essay\\
instr & Instructional text\\
\end{tabularx}
\begin{tabularx}{.45\textwidth}{>{\scshape}lQ}
speech & Speech\\
popsci & Popular Science\\ 
fiction & Fiction\\
ori & Original\\
trans & Translation\\ 
\end{tabularx}

\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]

\end{document}