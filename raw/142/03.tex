\chapter{Lessons we still have to learn}\label{chap3}

I do not want to fall into the temptation of considering a field like theoretical linguistics that has traditionally focused on the (cognitive) phenotype as ``quaint, old-fashioned, or incapable of deep insight''. I borrow this phrase from \cite{jarrett2018and}, who use it to say that now that we are living in the Age of the Genome, it is all too easy to characterize in this (negative) way fields of biological research that analyze the phenotype. Although I have come to question many assumptions made in the linguistic tradition I grew up in, I think there is a lot of value to that work, and lots of insights worth integrating.\footnote{Most of the problems I use here as illustrations are drawn from the realm of syntax. I do so largely because this has traditionally been the domain over which species-specificity has been defined, especially in the linguistics tradition I am most familiar with. This is not to say that this is the only interesting area of research. Semantics and pragmatics are other domains of great relevance, but here I feel significant progress has already been made, pointing to substantial evolutionary continuity. I personally found the treatments in \cite{hurford2007origins,moore2018gricean2,moore2018gricean,krupenye2019theory} very compelling.}

Here is a key result worth preserving: the attention to what \cite{marr1982vision} called the ``computational'' level, and in particular, the type of formal characterization that was at the center of the earliest work in generative grammar, and that gave rise to what is informally known as the ``Chomsky hierarchy'' of formal languages, now perhaps more familiar to students in computer science than in linguistics. \cite{chomsky1956three,Chomsky1957} demonstrated that words in natural languages are not arranged like beads on a string, and from there went on to discuss the relevance of considerations pertaining to ``context-free'' and ``context-sensitive'' grammars and associated memory structures to capture patterns of dependencies attested in language (for a detailed and accessible retelling of these central arguments, see \cite{lasnik2000syntactic}).\footnote{Without getting into technicalities of formal language theory, I take it to be absolutely essential to recognize the need for dependencies that go beyond the range of adjacent elements, and recognize hierarchically organized expressions forming both nested and crossing dependencies; that is, dependencies of the ``if \ldots then'' sort, and those dependencies underlying the organization of the English verbal complex (see \cite{lasnik2000syntactic} for extensive discussion): \textit{John and Sally (may) (have) (been) run(ning)}.} I agree with \cite{o2005using,fitch2014toward} that this early work contains deep, long-lasting results, with the additional advantage of being ``theory-free'' in the sense that (unlike more recent results) it does not depend on any ``technical'' intricacy or jargon that is peculiar to a particular linguistic framework or tradition.\footnote{As a reviewer points out, this is not to say that the Chomsky hierarchy is the only game in town and free of problems (for useful discussion, see \cite{rogers2011aural}). My point here is a modest one: the Chomsky hierarchy is a useful tool to express explicitly essential points of convergence across linguistic frameworks.} As such, these are results that have the potential to translate more easily across fields, and can readily inform research beyond the narrow confines of linguistics departments.

As a matter of fact, Chomsky's early characterization of linguistic rules and representations, refined over the years, in particular thanks to the insights of Aravind Joshi \citep{joshi1985tree}, constitutes an under-appreciated ``consensus'' that Ed Stabler has done well to highlight in recent years \citep{stabler2011computational,stabler2013epicenter}. As Stabler states, over the years a ``substantial, non-trivial consensus about the nature of linguistic structure has emerged'' over a wide range of theoretical approaches. Though often hidden behind jargon, numerous  independently-proposed grammar formalisms (Stabler lists at least six of them) converge on a claim that goes back to \cite{joshi1985tree}: natural languages are both strongly and weakly mildly context-sensitive. That is to say, mildly context-sensitive grammars ``can both define the sentences of human languages (weak adequacy) and also provide the structures of those languages (strong adequacy)''. To put it in other words, the range of dependencies that are attested in natural languages is constrained in a way that, following Chomsky's early description in \textit{Syntactic Structures}, requires computational ``power'' (specific memory structures/representations) beyond the scope of finite-state automata and also beyond that of a class of ``push-down automata'' (associated with ``context-free'' grammars).

\section{Choosing among grammar formalisms}

This foundational result, whose robustness Stabler is right to emphasize, allows me to question a well-known statement made by Ray Jackendoff to the effect that ``one's theory of language evolution depends on one's theory of language'' \citep{jackendoff2010your}. At first sight, Jackendoff's statement sounds like a truism, and as such is uncontroversial (in the same way, one would think, that one's view of language depends on one's view of evolution). But hidden behind this statement is a presupposition that the well-known and much-advertized fragmentation of the field of theoretical linguistics into fiercely contested traditions is highly relevant for thinking about Darwin's problem. In other words, it is claimed that one must choose one's theoretical allegiance carefully because doing so makes quite different predictions about the evolutionary narrative one will tell. I think this is a massive overstatement. I am not questioning that different theoretical frameworks make different predictions about specific language data, but I am not convinced they make radically different testable predictions about evolutionary questions (more concerned with general ``design features'' of language, following Hockett's influential guidelines; \cite{hockett1960origin}). As Stabler stresses, many grammar formalisms converge (unfortunately, in a ``hidden'', tacit fashion) on an abstract, computational characterization of human languages, and it is ultimately that point of convergence, that ``epicenter of linguistic behavior'', as \cite{stabler2013epicenter} calls it, borrowing a phrase by Tom Bever, that we want to ask evolutionary questions about. Put differently, there may well be far less disagreement about one's theory of language, and as such far more consensus about what it is that is to be captured when one theorizes about language evolution.

In particular, I think it is critical to realize that many linguists' theoretical considerations (what is the ``right'' analysis for indirect questions, or interjections, or infixation, etc.) do not bear on answerable evolutionary questions. I doubt we will ever be in a position to reconstruct with any certainty the specific grammatical constructions attested at a particular stage of, say, proto-language. In the absence of actual data, this strikes me as hopeless, as \cite{lewontin1998evolution} would say. Instead, the focus should be on the range of grammatical constructions made possible at various stages of evolution, and there, Stabler's hidden consensus is extremely relevant.

What I think is needed to inform one's theory of language evolution is less focus on one's theory of language (where we can rely on the hidden consensus), and far more focus on how one's theory of language is integrated with other levels of analysis linking the genotype and the (computational/behavioral) phenotype: the neural predictions one's theory of language makes are of far greater importance for one's theory of language evolution. And here too, contrary to what Jackendoff's statement under discussion may suggest, there are far fewer options available than one might think; in this case, because few theoretical linguists engage with the interdisciplinary task of constructing linking hypotheses across levels of analysis, keeping the notorious slash between mind and brain, and hiding behind statements like ``we know so little/nothing about the brain''. Accordingly, I suggest we rephrase Jackendoff's statement as follows: One's theory of language evolution depends on one's linking hypotheses across levels of analysis.

This way of phrasing things has the advantage of bringing into focus a vitally important task for linguists and non-linguists alike: the importance of bridging the gap between mind and brain. I indeed believe that the strength of one's evolutionary narrative depends on
how often the brain is alluded to in it (hence my preference for the use of the term ``language-ready \textit{brain}''). This is most clearly the case when one tries to exploit the expanding dataset generated by paleogeneticists: this is a fantastically rich resource of new data for linguists and cognitive scientists, but one that is silent about cognitive issues in the absence of well-articulated, patiently developed, experimentally tested conjunctions of hypotheses from genes to proteins to cells to circuits, the dynamics of these neural circuits and eventually to cognitive processes and representations. It is in this context that the insistence on tackling the ``mapping problem'' \citep{fisher2015translating,poeppel2012maps} across all these levels is most acute. As correctly stated in \cite{hagoort2018prerequisites}, there is a prerequisite that must be fulfilled in order to present a solid evolutionary narrative on the human language-ready brain: one must first be explicit about the neurobiology at stake.

This, of course, does not mean that we have to understand everything about ``language in the brain'' (``Broca's problem'', as one might call it) before embarking on Darwin's problem. Inquiry can proceed in parallel (I think it must, for evolutionary considerations will inform neurolinguistics), but neurobiological considerations have epistemological priority for the same reason Humboldt's problem (`what is knowledge of language?') has priority over the ultimately more fundamental problem of language acquisition (Plato's problem), as Chomsky already made clear in his review of Skinner's book \textit{Verbal Behavior} \citep{chomsky1959chomsky} and again in \cite{chomsky1986knowledge}.

Incidentally, the very same detailed mapping between mind and brain is also clearly needed to address Darwin's problem's close cousin: Plato's problem. As developmental psycholinguist Evan Kidd put it in the context of language acquisition research,\footnote{\url{https://knowablemagazine.org/article/mind/2020/how-babies-learn-language}} ``the frontier of the field will be the integration of neuroscience because, ultimately, it is our brains that are learning language. Understanding that process may well bring us closer to more psychologically plausible theories of how language is not only learned but also represented in the brain''.

Tinbergen would be pleased to see how the strength of the field of cognitive biology of language depends on how answers to all his ``why-questions'' are integrated.

\section{How to think about evolutionary novelties}

Apart from learning to work across levels of analysis and explanatory dimensions, there is a second lesson I think researchers in language evolution need to assimilate. It pertains to what we take to be ``novel'' or ``special'' about human language.

I will organize the discussion in this section around quotes from an influential paper that contributed to the revival of evolutionary considerations in some corners of linguistics, and that is concerned with the notion of evolutionary novelty \citep{hauser2002faculty}.\footnote{I am here drawing on ideas and selected quotes from \cite{hauser2002faculty} that I first discussed in \cite{boeckx2013biolinguistics}. While the main message may not be news to biologists, I feel it is one that many linguists still need to assimilate. For a complementary perspective, see \cite{fujita2016certain}.}

As is well-known, \cite{hauser2002faculty} introduced a distinction between the Faculty of Language in the Broad Sense (FLB) and the Faculty of Language in the Narrow Sense (FLN) to invite ``[l]inguists and biologists, along with researchers in the relevant branches of psychology and anthropology, [to] move beyond unproductive theoretical debate to a more collaborative, empirically focused and comparative research program''. At bottom, the authors issue a renewed call for a robust comparative basis to study human cognition. They focus on the number of properties (traits, mechanisms, etc.) that human language shares with what can be found in other species, and designate these properties as ``FLB''. But they stress that ``[s]omething about the faculty of language must be unique in order to explain the differences between humans and the other animals'', which they label as ``FLN''.

Thanks to the emergence of compelling cases for deep homology (e.g., the convergence found among vocal learning animals (e.g., \cite{pfenning2014convergent}), I sense that linguists are now quite comfortable with the claim that some aspects of our linguistic capacity are shared with other species. But although FLB was introduced to facilitate 
``productive discussion of language evolution'', the whole FLB/FLN distinction may not have been the most felicitous way of redirecting attention to the need for a comparative, as opposed to a contrastive, approach to the study of human language evolution. The reason for this is the way the FLB/FLN discussion was framed: it may have helped perpetuate the idea that next to shared aspects (FLB), there are properties of our language faculty that are ``special'', ``unique'', or ``species-specific'' (FLN).

As  \cite{jackendoff2005nature} were quick to point out, ``the Narrow/Broad dichotomy\dots makes space only for completely novel capacities and for capacities taken intact from nonlinguistic and nonhuman capacities, omitting capacities that may have been substantially modified in the course of human evolution''. The dichotomous way of framing the question using terms like FLN or FLB makes it hard to explore the many capacities that were gradually and substantially modified in the course of human evolution. Are these  to be included in FLB or in FLN? Take the contribution of the much-discussed \textit{FOXP2} gene in the context of language. While highly conserved across species, the gene is known to harbor two key mutations that are not found in our closest living relatives (chimpanzees) \citep{enard2002molecular}. Say, for the sake of the argument, that we succeed in establishing that these mutations contributed to our linguistic phenotype (not implausible in light of \cite{enard2009humanized}). Would this be about FLB, or FLN? FLN, it seems to me, but not necessarily according to Hauser, Chomsky, and Fitch, who take the highly conserved nature of the gene to automatically make it part of FLB \citep{fitch2005evolution}.

At the heart of the FLN/FLB distinction is the attempt on the part of Hauser, Chomsky, and Fitch to grapple with the problem of innovation and novelty in biology. While biologists have made great progress over the past century and a half in understanding how existing traits diversify, much more modest progress has been made in understanding how novel traits come into being in the first place. To remedy this explanatory deficit, some biologists have first attempted to define what counts as a novelty (see, e.g., \cite{muller2005innovation}). In this context, it is worth noting the similarity between how \cite{fitch2005evolution} define FLN (``that which is specific to language and unique to humans'') with the definition put forth in \cite{muller1991novelty}: ``a structure that is neither homologous to any structure in the ancestral species nor homologous to any other structure of the same organism''.

When one turns to the relevant biology literature, one finds a consensus regarding how such novel structures arise \citep{moczek2008origins,linz2019origins,prud2011body}: phenotypic novelty is largely reorganizational. In other words, novelty arises from the combination of generic mechanisms, whose collective effects give rise to what appears to be \textit{de novo} characters.

Interestingly, the possibility of emergent novelty is alluded to in \cite{fitch2005evolution}: ``Something about the faculty of language must be unique in order to explain the differences between humans and the other animals -- if only the particular combination of mechanisms in FLB''. But this seems to me to put in jeopardy the very FLN/FLB distinction.

\largerpage[-1]
The possibility of emergent, reorganizational novelty is present in some of Chomsky’s own works \citep{chomsky2005rules,chomsky2000new,chomsky2004beyond}, but is always put into question as soon as it is considered.\footnote{I agree with a reviewer that the dismissal of this emergentist possibility is not unrelated to the failure on the part of many linguists to appreciate the structuring role of ``developmental noise''. For relevant discussion, see \cite{lewontin2001triple,mitchell2018innate}.} Consider this quote from \cite{chomsky2005rules}: ``Now a question that could be asked is whether whatever is innate about language is specific to the language faculty or whether it is just some combination of the other aspects of the mind. That is an empirical question and there is no reason to be dogmatic about it; you look and you see. What we seem to find is that it is specific.'' This is unfortunate, for Chomsky’s stance (which essentially boils down to the way in which the FLN/FLB distinction has been understood) indeed ``mak[es] some hypotheses — in our view the most plausible ones — impossible to state'' \citep{jackendoff2005nature}: the FLN/FLB distinction, which is designed to separate the old from the new, focuses on component parts, and as such makes it difficult to understand the new as a collection of the old (the system as a whole). Perhaps for this reason, one finds in Fitch's more recent writings passages like this: ``What all of these examples make clear is that the distinction between general and linguistically specialized mechanisms is hard to draw, even in those cases where the mechanisms themselves seem fairly clearly defined. Most areas of language are not, and will not soon be, so clearly defined, and thus the distinction itself is of little use in furthering our understanding of the mechanisms''
\citep{fitch2011unity}.

I take Fitch's statement to mean that the FLN/FLB distinction, in the hands of linguists, turned out not to be so useful after all. I side with \cite{bloomfield2011birds}, according to whom, ``[p]erhaps this is a good time to reconsider whether attempting to distinguish between qualitative and quantitative differences is helpful if the quantitative advantage is vast.''

It is indeed puzzling that so many researchers still cling to the FLN/FLB distinction when the rationale behind the distinction given at the outset of \cite{hauser2002faculty} is to reject the notion of the faculty of language as a monolithic object: how can we identify if some mechanism is ``specific to language'' if ``language'' itself is not a well-defined, unique object?\footnote{Perhaps it is for this reason that the phrase
``unique/specific to language'' does not appear in the original 2002 paper, but only in \cite{fitch2005evolution} who take it from \cite{pinker2005faculty}, where FLN is characterized in those terms for the first time (contrary to the literature that routinely attributes it to \cite{hauser2002faculty}).}

As discussed in \cite{theofanopoulou2015cognitive} the problematic nature of the FLN/FLB distinction is actually part of a larger problem concerning the construction of cognitive phylogenies (see also \cite{martins2020vocal}). Although motivated by evolutionary considerations and an attempt to identify shared characteristics, accounts like \cite{fitch2010social} reveal assumptions that are at odds with the ``entangled'' nature of evolutionary products. Trying to map cognitive traits onto cladograms is treating these traits as encapsulated (Fodorian) modules \citep{fodor1983modularity}, which they are clearly not. Defined as modules, cognitive traits retain a certain contrastive character: the defining feature of \textit{x} (a species/cognitive trait) will be what is not shared. But what makes traits distinct, or species-specific, is the way their ingredients have come together in the course of evolution. The fascination with ``brand-new properties'' is largely due to the almost exclusive focus on the phenotypic level. But as soon as one asks how that phenotypic trait is implemented in the brain, how the neural circuit responsible for it is genetically encoded, one runs into the tinkering nature of evolution, and the massively generic nature of elementary operations below the phenotypic level.

Alternative approaches, such as the concept of reconstructing the evolutionary trajectory of behavior in terms of ``phylogenetic refinement'' \citep{cisek2019resynthesizing}, focused on neurophysiological mechanisms rather than ``definitions of putative functions inherited from psychological traditions'', strike me as far more productive. Certainly, the preceding paragraphs help cast further doubts on narratives focused on ``component parts'', like Berwick and Chomsky's about a syntactic operation like ``Merge'' being \textit{the} basic property that adds content to FLN \citep{berwick2016only}.

\section{More continuity}

To conclude this chapter, I present an argument for looking for pervasive continuity for each and every aspect of the human language faculty.

The case study I will sketch in this section is rooted in a long-standing idea within generative linguistics, and has even featured in a \textit{Science} article \citep{heinz2011sentence}. It is the idea that phonology and syntax (or sound patterns and sentence patterns) are quite different \citep{bromberger1989phonology}; specifically, it is about the claim that human syntax requires more powerful computational resources, not attested in other species. An aspect of this idea has been used productively to argue for what \cite{fitch2018animals} has called ``phonological continuity'' across species---a hypothesis that goes back to  \cite{samuels2011phonological,samuels2015can}, where a compelling case for the presence of virtually all basic ingredients of human phonological rules and representations in non-linguistic creatures is presented.

The case for continuity in phonology exploits the well-established idea that, computationally speaking, phonological processes can be captured by finite-state machinery \citep{karttunen1993finite}, i.e.,
computational resources that are accessible to (many) other species. In Fitch's own words: ``humans share the processing capabilities required to deal with regular-level sequential processing, and thus phonology, with other animals, and these shared capabilities are implemented in homologous neural processing algorithms and circuitry'' \citep{fitch2018animals}.

The flip side of this claim is also something that Fitch has capitalized on: from the statement above it follows that other components of human language that require richer computational resources may constitute examples of evolutionary discontinuity. The prime candidate here is of course natural language syntax, where dependencies require more elaborate memory systems, as \cite{chomsky1956three,Chomsky1957} established.
Indeed, claims that the sort of computations underlying human language syntax are beyond the reach of other species are frequently made in review articles (e.g., \cite{berwick2011songs}). It has been called the ``syntax barrier'', or the ``supra-regular boundary'' \citep{fitch2014toward,fitch2018bio}. Even when it looks like monkeys can break this barrier \citep{jiang2018production}, the discontinuity hypothesis is stated in terms of a much higher \textit{propensity} by humans to build mental hierarchies beyond the reach of finite-state machines (Fitch's Dendrophilia hypothesis; \cite{fitch2014toward,fitch2018bio}).

Here I would like to argue that in fact one can adopt a perspective where syntax and phonology, and as a result, humans and other animals, exhibit a higher degree of continuity. Sure, they are not identical, but above and beyond the modifications, there is descent, as Darwin would have put it.

My starting point is an observation I made in collaboration with Juan Uriagereka \citep{boeckx2011biolinguistics}: when we look at the space available in the ``Chomsky hierarchy'' of formal languages, and we focus on the portions of it occupied by natural language patterns, it is clear that these patterns are sparsely, as opposed to densely, distributed. The clearest example of this comes from Joshi's observation already mentioned in this chapter that some key grammatical dependencies in language are of the mildly context-sensitive type, they are, as it were, right at the border between context-free and context-sensitive systems. They occupy the lowest rank of the context-sensitive family. Uriagereka and I pointed out that something similar happens at the levels of finite-state and context-free systems: natural language patterns are among the simplest within each class. Why should this be?

I believe that part of the answer can be found in recent work by computational linguist Thomas Graf, whose conclusions I will summarize briefly here (see, e.g., \cite{graf2014dependencies,graf2015cognitive,graf2020curbing}). Graf's central thesis is that the differences between phonology and syntax are a bit of an illusion. They exist under one perspective, but, importantly for our purposes, dissolve from another perspective. Specifically, the idea that sound patterns are computationally simpler or less demanding (in a quantifiable sense) than sentence patterns (sub-regular vs. supra-regular) relies on the characterization of language patterns as sets of strings. Graf's major point is that if we step away from this characterization and take into account other data structures (e.g., trees, well-known to syntacticians), then the difference between phonology and syntax is far less spectacular. Graf points out that recognizing
the set of licit trees (to be exact, Derivation Trees, following the Minimalist Grammar formalism of \cite{stabler2011computational}) can be accomplished by exploiting the resources of a finite-state automaton. In effect, this simple relativization of data structure (string vs. tree) makes the computational resources for phonology and syntax identical. This is indeed Graf's thesis: 
no language components require computational resources that fall outside the sub-regular domain. Building on work on phonological patterns by Heinz and collaborators (e.g., \cite{heinz2018computational}), Graf suggests that this characterization can be further refined: nearly all of these patterns, with principled exceptions, fall into the simple, ``strictly local'' layer of the ``sub-regular'' domain.\footnote{In addition to its relevance for evolutionary considerations, Graf's thesis also has important learnability considerations, bearing directly on some of the problems for the usefulness of the Chomsky hierarchy pointed out in \cite{rogers2011aural}. For relevant discussion, see \cite{rawski}.} For the sake of completeness I should mention that Graf, following Heinz, argues
that some phonological patterns, e.g., harmony processes, are best characterized as being ``tierwise strictly local'' (requiring representations well-known from autosegmental phonology), and so do syntactic processes such as movement dependencies. Thus, the phonology/syntax parallelism may well be preserved all the way.

For my present purposes, Graf's thesis is very significant: it removes yet another layer of species (or cognitive) discontinuity. If phonology and syntax make use of the same computational resources, and if there is phonological continuity, then there is also syntactic continuity. This of course does not mean that there won't be differences. Phonology is not syntax. Strings and trees are different data structures. But behind these differences, there is a deeply conserved computational architecture that is very simple (and not unique to humans). The differences are variations on a (simple computational, subregular) theme. 

Graf's logic achieves two important things for evolutionary studies: First, it illustrates how computational considerations going back to the earliest work in modern linguistics can help identify underlying parallelisms that would be obscured by jargon specific to syntax or phonology in this case. Second, it shows that these distinctions made in the standard formulation of the ``Chomsky hierarchy'' can be studied in a way that abolishes discontinuity across cognitive domains, or species (i.e., it can be studied in a comparative, as opposed to contrastive, mode). As such it can provide operational definitions of linguistic abilities ideally suited for comparative testing in a neurobiological context (see, e.g., \cite{wilson2017conserved}).

When focusing on the phylogeny of syntactic dependencies, it is likely that we will find deeper roots \citep{petkov2012pursuit,watson2020nonadjacent,girard2021chimpanzees}, but also gradual changes pertaining to data structure: hierarchical trees, which I think predated the emergence of \textit{sapiens}, and maybe some autosegmental/tier additions that may be species-specific additions, giving rise to mildly context-sensitive patterns when defined over strings \citep{malassis2020baboons}. We should welcome such a sharp departure from non-uniformity that all too often still dominates in theoretical linguistics and that was pretty much the ``only game in town'' not so long ago \citep{anderson2004doctor}. It's descent with modification all the way down.






