\chapter{Defining and solving Language Games} \label{ch:defining and solving language games}
So far I have assumed that the locutionary Semantic Constraint and conventional meanings in particular were given and available in the computation of the referential meanings of an utterance. This was the natural task of micro-semantics and I have shown how Equilibrium Semantics provides a more or less complete account of it and, indeed, of the whole process of communication in the small. But micro-semantics does not by itself explain how conventional meanings emerge in the first place nor how these meanings change over time. Both the origin and dynamics of conventional meaning must themselves reside in communication as there is no other possible source. Indeed, the Semantic, Syntactic, and Phonetic Constraints that are used in the Generation and Interpretation Games are themselves the products of communication. This is not to deny the initial endowment of capacities people are born with, only to highlight the role communication plays in activating these capacities.

I sketched a skeletal framework for macro-semantics in \sectref{sec:macro-semantics} and in this chapter I will give it some flesh. As I said earlier, my goal is only to offer a toy model of both origins and change that reveals the essence of these processes. I will abstract from syntax and phonetics altogether and look only at imaginary single-word sentences in studying the origins of conventional meaning. I will also make other simplifying assumptions. Once this core model of a Language Game is developed, it will be relatively easy to see how it might be extended to accommodate more realistic scenarios. I will do the same sort of thing with semantic change, but before I get to it, I will briefly discuss \citet{grice:umsmwm}, \citet{lewis:c}, \citet{skyrms:s}, and \citet{tomasello:ohc} on origins and compare them with Language Games. A byproduct of this network of Communication Games is that it shows a way to think about convention more broadly and realistically than \citet{lewis:c} did.

I remind the reader that my approach is analogous to the idea of general equilibrium in economics where one has multiple interacting markets in society. In our situation, there are multiple interlocking conversations in society and they lead to what I call a meaningful equilibrium.\is{equilibrium!meaningful|(} I start with a particularly simple Language Game and then progressively generalize it.


\section{A simple Language Game} \label{sec:a simple language game}

Recall that the locutionary Semantic Constraint has the general form $\omega \longrightarrow P \longrightarrow$\hspace{-1.1em}\raisebox{1ex}{\scriptsize $u$}\hspace{.55em} $ \sigma$ where the first map is called the conventional map and the second the referential map. That is, the locutionary Semantic Constraint consists of the Conventional and Referential sub-Constraints. The property $P$ is the conventional meaning and the infon $\sigma$ is the referential meaning.\footnote{I have deliberately used $P$ instead of the concept $C$ to keep things simple.}

To start with, assume there are just two properties $P_1$ and $P_2$ of interest in a community of three agents ${\cal A}_1$, ${\cal A}_2$, and ${\cal A}_3$. These two properties yield the two contents $\sigma_1$ and $\sigma_2$. That is, the referential map $P_1 \longrightarrow$\hspace{-1.1em}\raisebox{1ex}{\scriptsize $u$}\hspace{.55em} $ \sigma_1$ and $P_2 \longrightarrow$\hspace{-1.1em}\raisebox{1ex}{\scriptsize $u$}\hspace{.55em} $ \sigma_2$ is taken as given. In these one-to-one correspondences, $u$ is just a parameter and takes on different values, say, $u_1$ and $u_2$, for different utterance situations. For example, $P_1$ could be the property \emph{apple} and $P_2$  the property \emph{banana} and $\sigma_1$, $\sigma_2$ could be a particular apple and banana in the environment relative to the corresponding utterance situation in which the respective property is used by the agents.

Where do these referential maps come from? They are purely ontological and are naturally given as part of the individuation of reality. I had discussed such ontological transforms in \emph{Language and Equilibrium} (\citeyear[Section~2.4]{parikh:le}) and they need not detain us here. But, just as an example, the \emph{instantiation} map is as follows: $ind(P,s) = \soa{(\ x \mid s \vDash\ \soa{P;\ x}\ )} = \soa{a}$ where $s$ is the situation relative to which a property $P$ is instantiated in the individual $a$.\footnote{See \sectref{sec:information}.} In this map there is an infon embedded inside another infon and it is read as ``the individual $x$ such that $x$ has the property $P$ in situation $s$.'' The use of the variable $x$ indicates that the result of this operation need not be unique. It is via instantiation that one goes from the property \emph{apple} to a particular apple in some situation. I am taking such referential/ontological maps as given but in general such metaphysical individuation of reality and the emergence of conventional meanings occur side by side in a larger process I have called Equilibrium Metaphysics in the book just cited.

Assuming the Referential sub-Constraint, a language $\cal L$ is required in order to communicate contents such as $\sigma_1$ and $\sigma_2$. This is because the properties $P_1$ and $P_2$ cannot be used directly as they are abstract. So an agent has to use a property \emph{by} using a word, something that can be physically produced. Assume $\cal L$ is made up of just two single-word sentences $\omega_1$ and $\omega_2$.\footnote{For those puzzled by such single-word sentences, think of \citegen{wittgenstein:pi} ``Slab!'' or \citegen{quine:wo} ``Lo! A rabbit.''} Initially, at time $t = 0$ as it were, neither word is conventionally attached to either property. There is no conventional map such as, for example, $\omega_1 \longrightarrow P_1$ and $\omega_2 \longrightarrow P_2$ or $\omega_1 \longrightarrow P_2$ and $\omega_2 \longrightarrow P_1$. Assuming this pairing emerges in a one-to-one way, these are the only four correspondences possible.

In this very simple scenario, there are exactly two words, two conventional meanings, and two referential meanings. Why are both conventional and referential meanings required? Why isn't it possible to go directly from word to object? This was discussed in my previous book and in \sectref{sec:classic example} and will be fully addressed in \chapref{ch:centrality of communication}, but the short answer is twofold: both are required to avoid \citegen{frege:sr} problem of informative identities such as ``Hesperus is Phosphorus'' and to account for the full range of meaning, especially as encountered in the case of noun phrases involving names, descriptions, and generalized quantifiers.

For semiosis to occur, one of the two conventional maps above must result from the communicative exchanges in the community, either $\omega_1 \longrightarrow P_1$ and $\omega_2 \longrightarrow P_2$ or $\omega_1 \longrightarrow P_2$ and $\omega_2 \longrightarrow P_1$. Suppose ${\cal A}_1$ talks to ${\cal A}_2$ and to ${\cal A}_3$ via two separate utterances. The notation I have used so far for a Communication Game is $\Gamma_u$. This was convenient when there were just two interlocutors $\cal A$ and $\cal B$ and just one utterance situation $u$ being considered. Now there are multiple agents and multiple utterance situations so it is better to drop the parameter $u$, keeping in mind that all communication always occurs inside a particular utterance situation, and to add superscripts for the two agents. So the Communication Game between ${\cal A}_1$ and ${\cal A}_2$ would be called $\Gamma^{12}$ or $\Gamma^{21}$, both being the same game, the first symbol representing the game from ${\cal A}_1$'s point of view and the second symbol representing the game from ${\cal A}_2$'s point of view. Similarly, the Communication Game between ${\cal A}_1$ and ${\cal A}_3$ would be $\Gamma^{13} = \Gamma^{31}$. For our purposes it does not matter who the speaker is and who the addressee is. Both agents in both Communication Games can assume either role. This situation can be represented visually as shown in Figure~\ref{fig:language game}.


\begin{figure}[h]
\begin{center}
\begin{picture}(30,50)(45,60)
%-------------------------------
%NODES
\put(54,72){\circle*{3}}
\put(162,72){\circle*{3}}
\put(-54,72){\circle*{3}}
\put(54,72){\line(1,0){108}}
\put(54,72){\line(-1,0){108}}


\put(54,81){\makebox(0,0){${\cal A}_1$}}
\put(162,81){\makebox(0,0){${\cal A}_2$}}
\put(-54,81){\makebox(0,0){${\cal A}_3$}}


\put(108,81){\makebox(0,0){$\Gamma^{12} = \Gamma^{21}$}}
\put(0,81){\makebox(0,0){$\Gamma^{13} = \Gamma^{31}$}}

\end{picture}

\caption{A simple Language Game $\Gamma$} \label{fig:language game}
\end{center}
\end{figure}


The figure depicts a \emph{connected} graph with the vertices labeled by the three agents and the edges labeled by the corresponding Communication Games.\footnote{A graph here is a set of vertices representing the agents in the community together with undirected edges linking them which represent Communication Games between them. The edges are undirected because it does not matter who the speaker is and who the addressee is. A connected graph is one in which there is a path from any vertex to any other vertex.} Such an object consisting of a network of agents and Communication Games between them is a \emph{Language Game}. Its symbol is just $\Gamma$ (or $\Gamma(n)$ for $n$ agents). Importantly, Language Games are almost never common knowledge; only their component Communication Games may be. This fact will figure in \chapref{ch:convention} when we discuss convention.

In general, there will be many more agents and many more Communication Games as well as many more words, conventional meanings, and referential meanings. When conventional meanings emerge through such a Language Game, the community of agents will become a \emph{linguistic} community. It is possible in principle to envisage a larger graph encompassing the entire population of the planet that comprises many linguistic communities with some agents being members of more than one such community. Such a graph may not be connected. As populations move about, coming closer or drawing apart either spatially or culturally, the links among them will change and so will their languages. 

As words do not have conventional meanings initially, the Communication Games in Figure~\ref{fig:language game} will have a special form which I now specify. Consider $\Gamma^{12} = (SG^{12}, CSG^{12}, GG^{12}, UG^{12})$, the four component games. Here the superscripts can be ordered as $12$ or $21$ as both pairs of corresponding games are the same. When considering a Communication Game as part of a wider Language Game, it is convenient to abstract from the details of the various Setting Games and Content Selection Games. Alternatively, we can just assume that $SG^{12}$ induces the trivial $CSG^{12}$ shown in Figure~\ref{fig:CS12}.


\begin{figure}[h] 
\input{figures/pix5content12.tex}
\caption{Content Selection Game $CSG^{12}$}
\label{fig:CS12}
\end{figure}

I have assumed that ${\cal A}_1$ is the content selector and, later, speaker but it could easily have been ${\cal A}_2$. And I have assumed that the content he wishes to convey is $\sigma_1$ rather than $\sigma_2$ just to keep the indices aligned. The upshot of this is that ${\cal A}_1$ chooses to convey $\sigma_1$ to ${\cal A}_2$ in order to get her to do some action $a_1$. This could easily have been assumed at the outset but I want to emphasize that the whole Communication Game with all its component games is involved in the Language Game.

Now we come to $GG^{12}$, the Generation Game. Since ${\cal A}_1$ has chosen to convey $\sigma_1$ -- that is his only option in the trivial $CSG^{12}$ -- he has also implicitly selected $P_1$ in virtue of the Referential sub-Constraint $P_1 \stackrel{u}\longrightarrow \sigma_1$. At this stage, neither $\omega_1$ nor $\omega_2$ are attached to either conventional meaning $P_1$ or $P_2$ so ${\cal A}_1$ has to consider both words as possible carriers of the intended meaning. Keep in mind that if, say, $\omega_1$ had been conventionally connected to $P_1$ rather than $\omega_2$, the shape of $GG^{12}$ would also have been trivial, essentially $\omega_1$ as his choice followed by $\sigma_1$ as her choice, mediated by $P_1$ via an already existing locutionary Semantic Constraint. But part of the difficulty the agents face at the dawn of (linguistic) semiosis is that they have to not only choose a linguistic expression and a corresponding referential meaning via a conventional meaning, they also have to connect the linguistic expression with a conventional meaning in the first place. This leads to a slightly more complicated game of partial information as shown in Figure~\ref{fig:g12g21}.\footnote{The shape of the game tree is identical to the shapes of the Content Selection Games in Figure~\ref{fig:vague CS} in \sectref{sec:back to communication} and in Figure~\ref{fig:content game} in Section~\ref{sec:a complete example}. The latter games, however, are completely different as they involve the selection of contents and responses and the former involves the selection of utterances and meanings.}


\begin{figure}[h] 
\input{figures/pixconventionalmeaning.tex} 
\caption{Partial information game $g^{12} = g^{21}$} \label{fig:g12g21}
\end{figure}


%\begin{figure}[htbp]
%\begin{center}
%\begin{picture}(150,125)(45,18)
%%-------------------------------
%%NODES
%\put(54,72){\circle*{6}}
%
%\put(54,72){\input{figures/unit2}}
%\put(54,18){\input{figures/unit2}}
%
%\put(108,99){\circle*{3}}
%\put(108,45){\circle*{3}}
%\put(54,72){\vector(2,1){54}}
%\put(54,72){\vector(2,-1){54}}
%
%%\put(108,72){\oval(15,84)}
%
%
%\put(54,81){\makebox(0,0){$s_1$}}
%%\put(108,108){\makebox(0,0){$t$}}
%%\put(108,54){\makebox(0,0){$t'$}}
%
%\put(180,118.5){\makebox(0,0){$a,a$}}
%\put(180,82.5){\makebox(0,0){$c,c$}}
%\put(180,64.5){\makebox(0,0){$a,a$}}
%\put(180,28.5){\makebox(0,0){$c,c$}}
%
%\put(75,94.5){\makebox(0,0){$\omega_1,P_1$}}
%\put(87,66){\makebox(0,0){$\omega_2,P_1$}}
%\put(135,115.5){\makebox(0,0){$\sigma_1,P_1$}}
%\put(135,97.5){\makebox(0,0){$\sigma_2,P_2$}}
%\put(135,61.5){\makebox(0,0){$\sigma_1,P_1$}}
%\put(135,43.5){\makebox(0,0){$\sigma_2,P_2$}}
%
%\end{picture}
%
%\caption{Partial information game $g^{12} = g^{21}$} \label{fig:g12g21}
%\end{center}
%\end{figure}


In this game, ${\cal A}_1$ wants to convey $\sigma_1$ by using $P_1$ in the situation $s_1$. He not only makes a choice between $\omega_1$ and $\omega_2$ but also chooses $P_1$ in $s_1$, thereby connecting one of the words with $P_1$. And ${\cal A}_2$ has to choose not only between $\sigma_1$ and $\sigma_2$ but also between $P_1$ and $P_2$ because she does not know whether he is conveying $P_1$ or $P_2$. This compels ${\cal A}_1$ to consider an alternative situation $s_2$ in which he has to choose between attaching $\omega_1$ and $\omega_2$ to $P_2$ and ${\cal A}_2$ has the same choices as before. Also, since $\sigma_1$ and $P_1$ are linked and $\sigma_2$ and $P_2$ are linked via the referential map,  ${\cal A}_2$'s choice is really between $(\sigma_1,P_1)$ and $(\sigma_2,P_2)$. The payoffs have been made symmetric to keep things simple and we have $a > c$ because $(\sigma_1,P_1)$ is the correct interpretation in $s_1$ and $(\sigma_2,P_2)$ is the correct interpretation in $s_2$.

There is no question of anything else in $GG^{12}$ as there are no other linguistic expressions available that might lead to alternative locutionary games. And there are no other games of partial information either, because the utterance consists of just one word, either $\omega_1$ or $\omega_2$. It could be said that $g^{12} = g^{21}$ is a semantic game and there must be a corresponding syntactic game for both words and this is true but I will omit it for now as we are dealing just with single words. So $GG^{12}$ contains just $g^{12}$ and nothing else.\footnote{The form of $GG^{12}$ is actually similar to the form of the Generation Games we saw in Figure~\ref{fig:GG} and Sections~\ref{sec:generation game} and \ref{sec:solving generation games}. The speaker has a choice between $\omega_1$ and $\omega_2$ because both can express $\sigma_1$ via $P_1$ and both words lead to the same game $g^{12}$. The difference occurs in the possible different costs of $\omega_1$ and $\omega_2$ and therefore in the net values received by the speaker with each choice. But I have deliberately abstracted from such complications because they obscure the essence of Language Games.}

It should be obvious that the Interpretation Game $UG^{12}$ also contains just $g^{12}$ and nothing else. So we have completed the description of $\Gamma^{12}$.

There are two Nash equilibria in $g^{12}$, one corresponding to ${\cal A}_1$ choosing $(\omega_1,\allowbreak P_1)$ in $s_1$ and $(\omega_2,P_2)$ in $s_2$ and ${\cal A}_2$ choosing $(\sigma_1,P_1)$ at the (upper) intermediate node on the right side of the figure and $(\sigma_2,P_2)$ at the (lower) intermediate node on the left side of the figure, and the other corresponding to ${\cal A}_1$ choosing $(\omega_2,P_1)$ in $s_1$ and $(\omega_1,P_2)$ in $s_2$ and ${\cal A}_2$ choosing $(\sigma_1,P_1)$ at the (upper) intermediate node on the left side of the figure and $(\sigma_2,P_2)$ at the (lower) intermediate node on the right side of the figure. In plain English, ${\cal A}_1$ can optimally attach either word to $P_1$ and the other word to $P_2$ and ${\cal A}_2$ must always choose based on whether $P_1$ 
is being conveyed or $P_2$. Because the payoffs are symmetrically chosen, both equilibria are equivalent. In other words, either $\omega_1$ or $\omega_2$ could attach to $P_1$ as one would expect in the absence of any differential costs of the two words. Note that since $s_1$ is actual and $s_2$ is counterfactual, only the relevant part of either equilibrium corresponding to $s_1$ will be played.

%There is one somewhat subtle element that needs to be considered. How does ${\cal A}_2$ know that her choice of $(\sigma_1,P_1)$ leads to a payoff of $a$ and not $c$? And vice versa? All that she can observe is the word uttered. But neither word is attached to $P_1$ at the outset so she cannot use the locutionary Semantic Constraint to generate the possible interpretations.
%
%There are two alternative ways of viewing this situation. One is to say that there is a process of trial and error and feedback through which ${\cal A}_2$ learns the right payoffs. That is, the Communication Game $\Gamma^{12}$ is repeated until ${\cal A}_2$ gets it right. Certainly this must happen in some cases. For example, if ${\cal A}_1$ is conveying something about a particular apple rather than a particular banana but ${\cal A}_2$ thinks the opposite then she will fail in her understanding and therefore in her response to ${\cal A}_1$ in $CSG^{12}$. But this failure is an opportunity to learn the right payoff and therefore the intended meaning of the utterance.
%
%Another possibility is to look historically at what actually happened in semiosis. Gestures preceded verbal language and it is likely that early humans both pointed at objects in their visual environment and uttered accompanying grunts. In other words, language could build on gesture and early attempts at verbal communication were probably overdetermined: the addressee could infer the correct verbal payoffs from what the speaker was pointing at. Thus, much learning must also have happened in this one-shot way, instantaneously. Of course, pointing and other gestures also involve solving games but here context and salience play some role in eliminating most possibilities. There is also an element of naturalism as opposed to conventionalism in gestural language just as there is with visual language that is absent in verbal language. So semiosis uses quasi-naturalistic symbols to ascend to purely conventional symbols.
%
%There is no reason to choose between these two ways, either repeated games with learning or overdetermination via gestures, as both must have occurred with different parts of language. But the result is that the right payoffs were and are assigned to each interpretation.
%
%A third approach is to use the signaling games of \citet{lewis:c} where all the possible initial situations (or states as they are called in that literature) are included in the game. In the case of $g^{12}$ this would mean adding another initial situation $s_{1'}$ where the speaker wants to convey $\sigma_2$ by using $P_2$. Since there are just these two possible contents -- in the relevant utterance situation -- that agents may want to convey, there are just two possible initial states. The reason this approach works is that when one lists all possible contents relative to the utterance situation then the addressee can assign payoffs appropriately for each possible initial situation. The difficulty arises only when there is just one initial situation because then there is no way to tell what the right payoffs are.
%
%Even though this ploy works \emph{mathematically}, it is not empirically acceptable. It is fine when there are just two possible contents and therefore two possible states, but what happens in a real life setting when there can be an infinity of possible contents? Even if the number is finite but very large it would go beyond the limited capacities of finite agents. It just is not reasonable to expand the game to too many initial situations.
%
%But, a reader may rightly ask, isn't this kind of exhaustive listing exactly what I did in the partial information games of Chapter~2? Yes, but in that setting, I was assuming the locutionary Semantic Constraint and there are only a relatively small number of possible conventional meanings of any word. So what I did there was to exhaustively list all the possible conventional meanings of a word and all the possible referential meanings issuing from those conventional meanings which became the possible interpretations in the locutionary Flow Constraint. This is why there was no problem of guessing payoffs that arose there. As I pointed out in \sectref{sec:psycholinguistics}, this seems to be compatible with the psycholinguistic evidence as well. 
%
%Besides, I had added that the partial rationality of agents implies that they do not always consider \emph{all} the possible conventional meanings and that is one of the ways in which miscommunication occurs. A particular conventional meaning (and, consequently, referential meaning) may just not strike an agent. This is the reason why there are speaker games and addressee games and the two can differ in general.
%
%The key difference between Lewis's exhaustive listing of states and my exhaustive listing of initial situations is that Lewis is considering a potentially unrestricted set of possible contents and I am considering a very restricted set of possible contents generated by the locutionary Semantic Constraint. In other words, the strategies are similar but they are being used in very different settings, one where conventional meanings have not yet been fixed and the other where conventional meanings have been fixed.
%
%So Lewis's signaling games cannot be employed and, since ${\cal A}_1$ actually wants to convey $\sigma_1$ in $s_1$, no other content and initial situation need be entertained in $g^{12}$. This keeps the size of the game manageable for finite agents. But this means that ${\cal A}_2$ needs to learn her payoffs either via repetition or overdetermination. There is no free lunch.

This completes my discussion of $\Gamma^{12}$ which brings us to $\Gamma^{13} = (SG^{13}, CSG^{13},\allowbreak GG^{13}, UG^{13})$, the Communication Game between ${\cal A}_1$ and ${\cal A}_3$. In this game, too, it can be said that $SG^{13}$ induces a trivial $CSG^{13}$ which is similar to $CSG^{12}$ except that $\sigma_1$ is replaced by $\sigma_2$ and $a_1$ by some other action $a_2$. It is not necessary that the two Content Selection Games differ in this way as ${\cal A}_1$ may have the same exchange with both the other agents. But in that case the conventional meanings corresponding to the language $\cal L$ consisting of the two words would only be partially learned, which is an important case I will return to later. For now, assume that the exchanges involve different contents so that the whole language may be learned.

$CSG^{13}$ then leads to the same partial information game shown in Figure~\ref{fig:g12g21} that is also a part of $GG^{13}$ and $UG^{13}$ but this time the situation $s_2$ is actual and not $s_1$ so the two partial information games are not, strictly speaking, identical. It is called $g^{13}$ or $g^{31}$. Again, there are two symmetric Nash equilibria as before but only the parts corresponding to $s_2$ rather than $s_1$ get played.


%\begin{figure}[htbp]
%\begin{center}
%\begin{picture}(150,125)(45,18)
%%-------------------------------
%%NODES
%\put(54,72){\circle*{6}}
%
%\put(54,72){\input{figures/unit2}}
%\put(54,18){\input{figures/unit2}}
%
%\put(108,99){\circle*{3}}
%\put(108,45){\circle*{3}}
%\put(54,72){\vector(2,1){54}}
%\put(54,72){\vector(2,-1){54}}
%
%%\put(108,72){\oval(15,84)}
%
%
%\put(54,81){\makebox(0,0){$s_2$}}
%%\put(108,108){\makebox(0,0){$t$}}
%%\put(108,54){\makebox(0,0){$t'$}}
%
%\put(180,118.5){\makebox(0,0){$c,c$}}
%\put(180,82.5){\makebox(0,0){$a,a$}}
%\put(180,64.5){\makebox(0,0){$c,c$}}
%\put(180,28.5){\makebox(0,0){$a,a$}}
%
%\put(75,94.5){\makebox(0,0){$\omega_1,P_2$}}
%\put(87,66){\makebox(0,0){$\omega_2,P_2$}}
%\put(135,115.5){\makebox(0,0){$\sigma_1,P_1$}}
%\put(135,97.5){\makebox(0,0){$\sigma_2,P_2$}}
%\put(135,61.5){\makebox(0,0){$\sigma_1,P_1$}}
%\put(135,43.5){\makebox(0,0){$\sigma_2,P_2$}}
%
%\end{picture}
%
%\caption{Partial information game $g^{13} = g^{31}$} \label{fig:g13g31}
%\end{center}
%\end{figure}


%In this game, which works analogously to the earlier partial information game, ${\cal A}_1$ wants to convey $\sigma_2$ by using $P_2$ in the situation $s_2$. Again, he has to make a choice between $\omega_1$ and $\omega_2$ but he also chooses $P_2$, thereby connecting one of the words with it. Correspondingly, ${\cal A}_3$ has to choose not only between $\sigma_1$ and $\sigma_2$ but also between $P_1$ and $P_2$, and so her choice is between $(\sigma_1,P_1)$ and $(\sigma_2,P_2)$ as happened earlier. The payoffs have been made symmetric again and this time $(\sigma_2,P_2)$ is the correct interpretation in both the upper and lower intermediate nodes in the game tree where ${\cal A}_3$ has to make a decision. Again there are two Nash equilibria as before.

I had said earlier that it does not matter which agent is the speaker and which the addressee in these two Communication Games and I have taken ${\cal A}_1$ to be the speaker in both for now. I will soon account for the other possibilities.

If $\Gamma^{12}$ and $\Gamma^{13}$ and in particular $g^{12}$ and $g^{13}$ were played completely independently of each other, then the same word could end up attaching to both conventional meanings, $P_1$ and $P_2$. But the agents are at least partly rational and this suffices for ${\cal A}_1$ who is involved in both sets of games making his choices so as not to overload any word as that would be suboptimal and would lead to inefficiencies in future exchanges. So if he chooses to connect $\omega_1$ with $P_1$, then he simultaneously chooses to connect $\omega_2$ with $P_2$, and vice versa. In this way, he makes the best use of the linguistic resources available to him. This is nothing but the Consistency Condition across utterances mentioned in \sectref{sec:macro-semantics} in its simplest form. This condition is not something externally or arbitrarily imposed on agents, it is just a product of rationality because future ambiguity incurs an avoidable processing cost.

Thus, the way ${\cal A}_1$ solves $g^{12}$ and $g^{13}$ and, therefore, $\Gamma^{12}$ and $\Gamma^{13}$ is by choosing \emph{complementary} Nash equilibria in these games in keeping with the Consistency Condition. Out of the four possibilities $\{(\omega_1,P_1), (\omega_2,P_2)\}$, $\{(\omega_1,P_2), (\omega_2,P_1)\}$, $\{(\omega_1,P_1), (\omega_1,P_2)\}$, and $\{(\omega_2,P_1), (\omega_2,P_2)\}$, the last two are eliminated. This still leaves two equilibria on the speaker's side. On the addressees' side, it is clear that the first game yields $(\sigma_1,P_1)$ and the second game yields $(\sigma_2,P_2)$.

How does the speaker choose between the two Nash equilibria that still remain? It is only when $\omega_1$ and $\omega_2$ have identical or inversely proportional costs and expected frequencies of use that the two equilibria will be equivalent. In all other cases, there will be a natural bias in payoffs that will favor one equilibrium over another.\footnote{It is worth referring back to footnote~\ref{foot:Pareto criterion} in \sectref{sec:solving locutionary global games} at this stage.} For the relatively rare cases when they are equivalent, there are three ways to break the symmetry. 

One purely internal way is to introduce some dynamics into the space of strategies as, for example, \citet{skyrms:s} does. This is generally unsatisfactory for reasons discussed in \sectref{sec:comparisons}.

A second is to say that there is a short process of trial and error and feedback through which ${\cal A}_2$ learns ${\cal A}_1$'s choice of equilibrium which is determined by the flip of a fair coin. That is, the Communication Game $\Gamma^{12}$ is repeated until ${\cal A}_2$ gets it right in a few steps. Certainly this must happen in some cases. For example, if ${\cal A}_1$ is conveying something about a particular apple rather than a particular banana but ${\cal A}_2$ thinks the opposite, then she will fail in her understanding and therefore in her response to ${\cal A}_1$ in $CSG^{12}$. But this failure is an opportunity to learn the right equilibrium and therefore the intended meaning of the utterance.

The third possibility is to look historically at what must actually have happened in many situations. Gestures preceded verbal language and it is likely that early humans both pointed at objects in their visual environment and uttered accompanying grunts. In other words, language could build on gesture and early attempts at verbal communication were probably overdetermined: the addressee could infer the correct verbal response from what the speaker was pointing at. Thus, much learning must also have happened in this one-shot way, instantaneously. Of course, pointing and other gestures also involve solving games but here context and salience play some role in eliminating most possibilities. There is also an element of naturalism as opposed to conventionalism in gestural language, just as there is with visual language, that is absent in verbal language. So semiosis uses quasi-naturalistic symbols to ascend to purely conventional symbols.

Let us assume that ${\cal A}_1$ chooses $\{(\omega_1,P_1), (\omega_2,P_2)\}$ rather than $\{(\omega_1,P_2),\linebreak (\omega_2,P_1)\}$. And we already know that ${\cal A}_2$ chooses $(\sigma_1,P_1)$ in $g^{12}$ and ${\cal A}_3$ chooses $(\sigma_2,P_2)$ in $g^{13}$. Note that the presence of the two conventional meanings in the addressees' choices tells one that ${\cal A}_2$ has associated $\omega_1$ with $P_1$ and ${\cal A}_3$ has associated $\omega_2$ with $P_2$.

This implies that the entire Language Game $\Gamma$ has been solved with two results. One is that the words $\omega_1$ and $\omega_2$ acquire the conventional meanings $P_1$ and $P_2$, respectively. The other is that the individual communications get across the referential meanings $\sigma_1$ and $\sigma_2$. So there are both private and public successes. The so-called meaningful equilibrium of $\Gamma$ can be formally expressed as $\{[(\omega_1,P_1), (\sigma_1,P_1)]; [(\omega_2,P_2), (\sigma_2,P_2)]\}$.

Now, ${\cal A}_1$ has participated in both games and so he knows the whole meaningful equilibrium. But ${\cal A}_2$ has learned only that $\omega_1 \longrightarrow P_1$ and ${\cal A}_3$ has learned only that $\omega_2 \longrightarrow P_2$. Each of the addressees has encountered only part of the language and thus shares in only part of the success. In the special case where both addressees know there are only two words and conventional meanings in the language, they could infer the other connection as a possibility but in more realistic situations this cannot be done. They will learn only that part of the language that they experience. This is quite reasonable. If someone hasn't come across the word \Expression{sesquipedalian}, they may not know its conventional meaning and be unable to use it.

This more or less winds up the description of $\Gamma$ and how it is solved via the Consistency Condition being added to the Nash criterion. It results in the Conventional sub-Constraint being established and through it the locutionary Semantic Constraint because the Referential sub-Constraint is given.

One matter that remains is that I made ${\cal A}_1$ the speaker in both the games. What if he had been the addressee in, say, $g^{13}$? How would ${\cal A}_3$ know which equilibrium to discard as she plays just that one game? She cannot choose arbitrarily as ${\cal A}_1$ may already have made his choice in the other game $g^{12}$ which could conflict with her choice. In such cases what happens is that ${\cal A}_1$, being a player in both games, gives ${\cal A}_3$ feedback via repetition or gestural overdetermination that a certain word has already been used for another conventional meaning. So it does not matter who the speaker is and who the addressee is, in general. All permutations are allowed.

This may strike the purist as unsatisfactory as they may want something fully internal to the setup. But my goal is empirical adequacy and in actual fact such original acts of semiosis are not likely to happen in a pristine way but in a somewhat messy back-and-forth, repetitive, and overdetermined way. Also, there are likely to be a few more Communication Games with the same two words among the three members of our tiny community before the conventional meanings are solidly cemented. But the simple Language Game I have presented captures the heart of the matter. And it is easy to create more complex Language Games even with just three members because we can add multiple links between any two members as they can talk to each other multiple times.\footnote{Such a graph with multiple possible links between any two vertices is called a \emph{multigraph}.}

Indeed, it is almost as easy to generalize the foregoing to $n$ agents connected by multiple Communication Games in a new Language Game $\Gamma$ together with $m$ words, $m$ conventional meanings, and $m$ referential meanings of interest to the community. Let $S^\star_{ij}$ be the equilibrium strategy of agent ${\cal A}_i$ in game $g^{ij}$ (or, equivalently and a little loosely, in $\Gamma^{ij}$) which is a part of $\Gamma$. This could be a speaker strategy or an addressee strategy and there will be as many such strategies as there are games ${\cal A}_i$ plays with the other agents in the community. ${\cal A}_j$ will naturally play the same game and in that case it will be called $g^{ji}$ and the corresponding equilibrium strategy will be $S^\star_{ji}$. If ${\cal A}_i$ is the speaker then ${\cal A}_j$ will be the addressee and if ${\cal A}_j$ is the speaker then ${\cal A}_i$ will be the addressee. Keep in mind that both the speaker strategy and the addressee strategy \emph{in the initial situations that are actual} are ordered pairs, either $(\omega_{l'},P_l)$ or $(\sigma_l,P_l)$ with $l = 1, 2, \ldots, m$ and $l' = 1, 2, \ldots, m$.

The first index $i$ in the strategy $S^\star_{ij}$ goes from $1$ to $n$ because the Language Game is a connected graph\footnote{For notational reasons, it is easier to stick to a graph than a multigraph. But the definition and theorem below carry over to multigraph Language Games without any difficulty.} and each of the $n$ agents participates in at least one game. The second index $j$ belongs to an index set $K_i \subset \{1, 2, \ldots, n\}$ because the agent ${\cal A}_i$ may not talk to every other agent and also not to himself. So there will be index sets $K_1$, $K_2$, \ldots, $K_n$ for each of the $n$ agents. For example, ${\cal A}_1$ may talk to ${\cal A}_4$ and ${\cal A}_7$ via Communication Games $\Gamma^{14}$ and $\Gamma^{17}$ and to no one else in which case $K_1 = \{4,7\}$. And so on, for each of the $n$ agents. 

Lastly, let the intention to convey a particular content $(\sigma_l,P_l)$ in $g^{ij}$ be denoted by $\hbox{intention}^{ij}_l$. A meaningful equilibrium may now be defined as follows.

\begin{definition}

A Language Game $\Gamma(n) \equiv \Gamma$ involving $m$ words, $m$ conventional meanings, and $m$ referential meanings relative to appropriate utterance situations is given. Let $S^\star_{ij}$ be a component strategy of $g^{ij}$ in $\Gamma$ with $i = 1, 2, \ldots, n$ and $j \in K_i$ where $K_i\subset \{1, 2, \ldots, n\}$ is an index set containing indices $j$ corresponding to each game $\Gamma^{ij}$ in $\Gamma$. Then $S^\star_{ij}$ is a component of a meaningful equilibrium of $\Gamma$ if and only if

\begin{enumerate}

\item Nash Condition: $S^\star_{ij}$ is a component of a Nash equilibrium of $g^{ij}$, either the speaker component or the addressee component depending on whether ${\cal A}_i$ is the speaker or addressee of $g^{ij}$.

\item Consistency Condition: $\hbox{intention}^{ij}_l \neq \hbox{intention}^{ik}_{l'}$ if and only if $1^{st}(S^\star_{ij}) \neq 1^{st}(S^\star_{ik})$ and $2^{nd}(S^\star_{ij}) \neq 2^{nd}(S^\star_{ik})$ for all $k \in K_i$.\footnote{Here, $1^{st}(a,b) = a$ and $2^{nd}(a,b) = b$, that is, $1^{st}$ picks out the first component of an ordered pair and $2^{nd}$ picks out the second component.}

\end{enumerate}

\label{def:me}
\end{definition}

The first condition just says that every meaningful strategy must be part of a Nash equilibrium in the relevant component partial information game. The second condition says that the same speaker strategy or the same addressee strategy can be employed in two different component games an agent plays if and only if the intentions in those component games are the same. This is somewhat compactly expressed and I urge the reader to work out what happens when $S^\star_{ij}$ and $S^\star_{ik}$ are both speaker strategies or both addressee strategies or one is a speaker strategy and the other is an addressee strategy. All four cases are covered by the above statement of the Consistency Condition.

Since the number of words, conventional meanings, and referential meanings are all $m$, all Definition~\ref{def:me} says is that the set of words is mapped via a bijection onto the set of conventional meanings in equilibrium. There are $m!$ such permutations if the payoffs are all symmetric as they were in Figure~\ref{fig:g12g21} and any one set of meaningful equilibrium strategies can then be selected.

Since such a bijection always exists, and since Nash equilibria always exist, a meaningful equilibrium always exists, and we can record this important fact below.

\begin{theorem}

A meaningful equilibrium always exists in a Language Game $\Gamma(n)$ involving $m$ words, $m$ conventional meanings, and $m$ referential meanings relative to appropriate utterance situations.

\label{thm:me}
\end{theorem}

Thus, the foregoing explains with respect to a relatively simple Language\linebreak Game how words acquire conventional meanings and, therefore, how semiosis gets off the ground. If required, it may be assumed that such pairings are further cemented by repeated use via $\Gamma$ played at subsequent times. 

Combined with the analyses of Parts~\ref{part:III} and \ref{part:IV}, this completes the reduction of meaning, how both conventional meanings and referential meanings attach to utterances, assuming nothing more than the partial rationality of finite agents. In this sense, the main problem of semantics identified in \sectref{sec:semantics} -- how language acquires meaning -- has been solved at an abstract level. It required first solving the problem of communication in the small -- micro-semantics -- assuming conventional meaning was fixed, and then solving the problem of communication in the large -- macro-semantics -- allowing conventional meaning to vary. This top-down way of approaching the main problem of semantics via communication provides a fairly tight constraint on how more detailed accounts of particular expressions in language are constructed. As I said in \sectref{sec:classic example}, it brings a \emph{uniformity} to the many subquestions of semantics.

Though the model actually constructed is very simple, it has elements of \citet{peirce:ws}, \citet{saussure:clg, saussure:cgl}, and the later \citet{wittgenstein:pi}, the first because it shows how the connections between words and conventional meanings are arbitrary, the second because it shows they are simultaneous and systemic, and the third because it shows how all meaning arises ultimately from use or communication.

\section{Some generalizations} \label{sec:some generalizations}

I now briefly describe some generalizations mentioned in \sectref{sec:macro-semantics}. 

The most obvious one is to allow lexical ambiguity by making the number of conventional and referential meanings of interest $m'$ with $m' > m$. Since the number of words $m$ would be less than the number of conventional meanings $m'$, some or all of the words would have to carry more than one meaning. This is relatively straightforward to do by simply adding more initial situations to the games of partial information such as $g^{12}$ and $g^{13}$ and making the game trees a little more complex. For example, if $m = 2$ and $m' = 3$ then in the simple Language Game with $n = 3$ agents we would add one initial situation to both $g^{12}$ and $g^{13}$, each corresponding to some third conventional meaning $P_3$ and referential meaning $\sigma_3$, and add more branches to each situation. This slightly complicates the definition of a meaningful equilibrium and makes it a little harder to see its simple essence so I do not pursue it here.

In actual fact, the number of possible conventional meanings, that is, properties, is infinite. So, in this case, one has to first identify a finite number of conventional meanings that have the highest frequency of use and then follow the procedure above.\footnote{Incidentally, this double fact, of identifying a finite number of possibilities for conventional meanings based on frequency of use, and then identifying the equilibrium of the relevant finite game, seems to solve \ia{Wittgenstein, Ludwig@Wittgenstein, Ludwig}Wittgenstein’s skeptical paradox (of plus and quus) as discussed by \citeauthor{kripke:w} (\citeyear{kripke:w}).\ia{Kripke, Saul A.@Kripke, Saul A.}} This can be done in one fell swoop by simply assuming anticipated frequencies abstractly and selecting some finite number of conventional meanings at the start, or it can be done via a dynamic Language Game played at discrete times $t = 0, 1, 2, \ldots\ $. The latter is naturally more difficult to work out and involves semantic change, something I will consider in \chapref{ch:semantic change}.

It is also not too hard to extend the utterances to full multiple-word sentences and allow a pre-given grammar. All it involves is adding syntactic partial information games to the semantic ones we already have. This is notationally cumbersome but not conceptually so, especially for relatively simple sentence structures, and, again, the definition of a meaningful equilibrium has to be modified to include the syntactic games.

Something much more adventurous is also conceivable. It is possible not only to generate the (locutionary) Semantic Constraint but also the Syntactic Constraint from nothing but rationality. That is, we can show how sentences acquire syntax and how a language acquires a grammar. This could result in contradicting Chomsky's account of a universal grammar that is innate, something that is being questioned anyway.

%While the consensus after Chomsky seems to be that a universal grammar is innate or was acquired millennia ago, there is no particular obstacle to connecting sentences to syntax \emph{without} the prior presence of a grammar. In other words, it is possible not only to generate the (locutionary) Semantic Constraint but also the Syntactic Constraint, that is, a grammar from nothing but rationality. In order to fit the data, however, a partial approach involving an innate universal grammar whose parameters get set by external stimuli can also be attempted.

A similar strategy can be deployed with phonetics but as I have not said much in detail about phonetic games I just mention the possibility. The reason why all three Constraints -- Semantic, Syntactic, and Phonetic -- can be handled in analogous ways is that, as I've said earlier, they \emph{are} analogous. All involve attaching semantic, syntactic, or phonetic values or contents to words and then allowing such couplings to be disambiguated in identical ways via the Flow Constraint.

This indicates that really only the Flow Constraint corresponding to partial rationality is fundamental. The other three are a consequence of communication viewed macro-semantically. I haven't said how the Referential sub-Constraint of the Semantic Constraint might be derived. Suffice it to say here that it is a result of rationality and ontology. I discuss it partly in \emph{Language and Equilibrium} (\citeyear[Section~2.4]{parikh:le}). Thus, the partial rationality of agents engaged in communication is responsible for all of semiosis and communication itself arises from the needs and goals of agents.

Vagueness \is{vagueness}is harder to handle because different agents will come to attach slightly different concepts to words as discussed in \chapref{ch:vagueness}. This means conventional meanings are not fully shared and it requires relaxing the Consistency Condition slightly and allowing an extended locutionary Semantic Constraint to operate. Modulation, implicature, and free enrichment are also not easy to incorporate. It might be reasonably conjectured that early communication with new words is free of such complications. The referential map is more complex than I have assumed because a conventional meaning can lead to more than one referential meaning (e.g.\ for words like \Expression{the} and \Expression{every}). This flexibility also needs to be accommodated and the map derived from first principles as mentioned above. Conflict can also be introduced into the relevant Content Selection Games of each Communication Game in $\Gamma$. Since conflict usually does not enter into the corresponding partial information games, this does not pose insuperable problems. Finally, just as it is possible to derive grammar from rationality in principle, it should also be possible to pull rational ontologies out of seemingly thin air.

These last few extensions are undoubtedly more challenging. In any case, once such models have been elaborated it should be possible to apply them empirically as suggested in \sectref{sec:macro-semantics}.\is{equilibrium!meaningful|)}


\section{Comparisons}\label{sec:comparisons}

\citet{grice:umsmwm} sketches an account of word meaning on the basis of an agent having a procedure in his repertoire to convey a certain meaning. It has the following drawbacks:

\begin{enumerate}
\item It leaves out the element of choice that inevitably arises when deciding which word to pair with which meaning. It focuses on a single word-object pair and so leaves out the key to symbolic meaning that \citet{deacon:ss} emphasizes in the first part of his book: that symbolization is \emph{systemic} and the link between word and referent is mediated by the link between word and word. This, incidentally, is exactly what happens in a Language Game via the bijection referred to in \sectref{sec:a simple language game}.

\item It fails to show how individual decisions result in a group conventional meaning. Though \citet[127]{grice:landc} does say that each member of the group acts in a certain way on the condition that other members do likewise, he does not address how such a circular situation might get off the ground. In a much later publication, \citet[290--297]{grice:mr} constructs a kind of mythical scenario for how nonnatural meaning might emerge from natural meaning. As I wrote in \chapref{ch:Grice}, because he lacked the apparatus of game theory, he could not quite envisage how such nonnatural pairings might arise naturally through rational choice although they may build on quasi-natural gestural pairings.

\item A less significant missing element is the two-tier system of conventional and referential meaning as opposed to simple word-referent connections. This could presumably be added to Grice's account without too much difficulty.
\end{enumerate}

\citet{lewis:c} is able to overcome the first shortcoming above with his invention of signaling games. But, short of everyone playing one large communal game rather than participating in a network of two-person games, he shows no mechanism to go from micro-semantics to macro-semantics, from individual to group. What is missing is the idea of multiple, interlocking games in society that enable the micro level to be linked with the macro level. This is what makes Lewis's notion of convention too strict as I will argue in the next chapter. And the third flaw mentioned above is also present.

Because there are perfectly symmetrical multiple equilibria in Lewis's signaling games, \citet{skyrms:s} adds adaptive dynamics of a special sort (e.g.\ replicator dynamics, reinforcement learning) to break the symmetries to realize unique equilibria in most cases given arbitrary starting points in the space of possible strategies. This conclusion does not appear to be so remarkable because rather special sorts of dynamics are assumed without much argument. In a lucid paper, \citet{huttegger:eem}\footnote{See also the references therein.} suggests that such dynamics may be supported by assuming that agents imitate one another though this argument is made in a very general way and is also rather vague: why would one kind of dynamics be copied rather than another and how does the process get off the ground? It is not enough to say that success is copied because it may be difficult for the agents to gauge what is successful in many circumstances. Also, the method seems to allow only the simplest cases to be considered where there may be two words and two meanings or possibly $m$ words and $m$ meanings. Apart from the fact that such dynamical movements in strategy space may take a relatively long time (e.g.\ a thousand iterations) to converge to an equilibrium implying nonequilibrium behavior for an excessive duration, the approach is implemented at such a high level of abstraction that it is unrealistic and unclear how to get to more empirically interesting results.

Skyrms's model is behaviorist and makes the intentions and intelligence of human agents superfluous. That the same framework applies to both bees and humans is a flaw, not a virtue, because something important has been left out and the special nature of human conventional and referential meaning is lost. Surely humans do not need to blunder about a thousand times the way bees may have to in order to reach an equilibrium.

He misses the role of gestures that are likely to have accompanied speech at an early stage of evolution. As I explained, linguistic meaning follows gestural meaning temporally, the latter being much easier to establish, and overdetermination enables one-shot pairings to occur quite often. This kind of bootstrapping allows agents to avoid potentially questionable and extended dynamics to reach equilibrium.

In trying to avoid assumptions like common knowledge and salience that\linebreak Lewis made, Skyrms seems to ignore the role of the situation in which communication occurs. Early signals were likely meant to identify salient objects in the environment such as food or predators and these could have been steppingstones to later signals where such salience was not available. And, as I have argued, more complex communication often relies on aspects of the situation that are invisible such as shared background information.
 
He also focuses exclusively on pure coordination. As Gintis\ia{Gintis, Herbert@Gintis, Herbert} points out in his review of Skyrms's book:\footnote{\url{https://www.amazon.com/gp/customer-reviews/R1TCQS6NFB5M3K/}}

\begin{quote}

As it turns out, there are very few zero-sum games in reality, and there are equally few pure coordination games. Thus Skyrms misses most of the real action, which lies in understanding the conditions under which signaling equilibria emerge even when agents do not have identical interests. This is most acute in human societies in which individuals routinely stand to gain from transmitting untruthful messages. But it is equally important in animal societies in which males gain from transmitting incorrect messages concerning their reproductive fitness to females, who use such information to guide their choice of mates. There is a brilliant literature on this topic, based on contributions by Ronald Fisher, Michael Spence, Amos Zahavi, and Alan Grafen, but this literature is not touched upon in Skyrms's exposition.

\end{quote}

As I said in the previous section, conflict can be introduced in Content Selection Games and partial information games if required more or less painlessly and so Language Games can readily handle both coordination and conflict.

Lastly, Skyrms does not address the issue of the need for a network of interconnected games. This suggests that he operates within the same ambit as Lewis as far as linking the micro and macro levels is concerned.

As an aside, I point out that Skyrms generalizes the notion of semantic content by making it probabilistic but my notion is more general yet because there are multiple distributions in a content as discussed in Sections~\ref{sec:how to think about content} and \ref{sec:eliminating possible indirect contents}. Also, Skyrms's content is holistic and does not allow for partial propositions (e.g.\ the content of an uttered noun phrase). Lastly, it is not Austinian as it does not incorporate the described situation or what he calls the state. As a result it is not fine-grained enough and succumbs to logical omniscience and to the Fregean problem of informative identities.

Thus, my basic verdict is that signaling games in the Lewis-Skyrms tradition are on the right track but do not go far enough toward explaining the origins of meaning.

\citegen{tomasello:ohc} account is more satisfying, both theoretically and empirically. He emphasizes the precedence of gestures as compared with words and uses the term ``piggybacking'' to describe what I have called overdetermination. His argument about so-called recursive \ia{Grice, Paul@Grice, Paul} Gricean ``mindreading'' is less convincing and I have argued in earlier chapters that it is generally not required. He too underscores the situated and intelligent nature of communication as I have throughout this book and in my earlier publications, but because he does not avail of game-theoretic ideas, he misses the possibility of arguing for his conclusions in greater detail and with greater rigor through the use of mathematical models.














