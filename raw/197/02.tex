\chapter{Overview of the oscillators/energy levels framework}
The o/el framework and conventional frameworks offer very different conceptualizations of “syntax”. In conventional approaches, a syntax “module” builds “structures” of “objects” which map both to speech motor output and to a meaning representation. This modular approach separates syntax from the phenomena that are most directly important for communication: movements/sensory experiences (a.k.a. the \isi{sensorimotor} interface, \isi{phonological} form) and meaning experiences (a.k.a. the conceptual-intentional interface, logical form). The modular interface view encourages us to see syntax as independent from meaning and independent from movement/sensation. We should reject this way of thinking. Syntax should not be understood as a module, but as a generic term for mechanisms which organize meaning and \isi{sensorimotor} experience. Experiences are highly ordered states, and syntax is a mechanism for creating order. 

  The o/el framework rejects modules and instead embraces the notion of a \textit{system}. A system is a portion of the universe associated with some partially predictable mapping from an initial state to a subsequent state. In the o/el conception there are many systems, of two fundamental types: concept systems and syntactic systems. Unlike the weak, unidirectional interfaces of modules, o/el systems may have strong, bidirectional interactions. Even more importantly, o/el systems do not operate on structures of “objects”. Instead, concept systems and syntactic systems have states and exert forces on each other. Below we develop a detailed picture of these systems and their interactions. 

  Another important way in which the current approach differs from conventional ones is that we attempt to motivate the framework with inferences based on knowledge of neural population dynamics. The o/el framework is derived from a microscopic conceptualization in which population coding and interpopulation connectivity play major roles in determining behavioral patterns. There are many ways in which our derivation of a macroscopic analysis relies on incomplete information and unsubstantiated assumptions regarding the microscale; I accept the possibility that invalidation of the microscale assumptions may compromise the framework. However, it is also possible that revisions to the microscale assumptions may lead to modest revisions or elaborations, without necessarily invalidating the framework.

\section{Concept systems}

How do complex patterns of thought arise in the brain? For example, consider the sentence \textit{Al drinks coffee}. In the conventional metaphor, a phrase is a “structure of objects” that arises from the merger of smaller objects. These objects -- words and phrases, i.e. “linguistic units” -- are also the sort of objects that can be “containers”. Thus words contain meanings and phrases contain words. Connected object representations as below use \isi{vertical orientation} and connection schemas to encode these containment relations, of the sort shown in {\figref{fig:2:1}}.

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img8.png}
\caption{Linguistic units as connected objects.}
\label{fig:2:1}
\end{figure}
 

  Are schemas of this sort useful? Imagine a scenario in which you engage two thought patterns in succession. First, you engage the pattern \textit{Al drinks coffee}. Next, you engage an alternative pattern, \textit{Bo drinks tea}. Then, you return your attention to \textit{Al drinks coffee.} Then, back again to \textit{Bo drinks tea}. And so on… What would we expect to observe in the brain in this scenario? 

  The connected objects schema is not well suited for addressing this question. Because the sentences are conceptualized as structures of objects, we can ask: “where do the objects come from?”, “how do they become connected or combined?”, and “what happens to them when we switch to a different pattern?” Do the objects get destroyed? Do they move somewhere? Do they vanish, are they hidden? Do the objects ever change over time? Where are these objects located in space? Etc.

  The essence of the problem is that conventional approaches force us to think of linguistic units \textit{as objects}. To construct abstract understandings of phenomena, we often use the \textsc{abstractions-}\textsc{are}\textsc{{}-objects} metaphor (e.g. \textit{put your feelings aside}, \textit{tear an argument into pieces}, \textit{build a new life}). But regardless of how familiar it is and how intuitive it seems, the \textsc{units}\textsc{{}-are-}\textsc{objects} metaphor is not necessarily a useful conceptualization of language. 

  In the o/el framework, linguistic units are \textit{not} objects. They are not the sorts of things that contain meaning, and are not the sorts of things that can be connected. They do not occupy space, they do not have orientational relations. The o/el framework rejects all entailments of the \textsc{units}\textsc{{}-are-}\textsc{objects} metaphor. Instead, we adopt an alternative in which meanings are experiences, experiences are trajectories of system states in a \isi{state space}, and various forces influence those trajectories. Our task then becomes construction of a \isi{state space}, analysis of state trajectories, and determination of forces. Because meaning experiences are trajectories, meanings are inherently temporal. 

\subsection{Concept-specific populations of neurons}

To develop an intuition for the \textsc{meanings}\textsc{{}-are-}\textsc{trajectories} metaphor we consider a simple utterance, \textit{Al drinks coffee.} We pose the following question: \textit{physically}, in space and in time, what happens when a \isi{speaker} produces this utterance? Let's suppose that in some brief period of time preceding the utterance, in the brain of the \isi{speaker}, there is a population of excitatory cortical neurons which in a statistical sense\footnote{To assess this empirically, we would want fine spatial and temporal resolution of electrochemical gradients, neurotransmitter concentrations, and synaptic connectivities, along with information regarding \isi{articulatory} movements, vocal tract geometry, and acoustic signals. Whether this can be accomplished with current technology is beside the point: we can imagine associating populations of neurons with concepts in this way. Note that we require no assumptions about the uniqueness or overlap of the populations at this point. The idea that spiking in distributed populations of neurons (or assemblies, ensembles, etc.) may correspond to things like concepts or words is a fairly old one; see for example (\citealt{Abeles2012,Braitenberg1978,Hebb1949,pulvermuller1999}).} is associated with concepts that contribute to the relevant experience of meaning. For exposition, we identify those concepts as [Al], [drinks], and [coffee]. Furthermore, suppose that we can differentiate the population into an [Al] subpopulation, a [drinks] subpopulation, and a [coffee] subpopulation. Thus each concept is associated with a population of many neurons. No strong assumptions are necessary regarding the temporal permanence, spatial distributions, sizes, or independence of these concept populations.

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img9.png}
\caption{Conceptual systems as distributed populations of neurons. Conceptual systems differ in their projections to and from sensory systems, motor systems, and other systems.}
\label{fig:2:2}
\end{figure}
 

  The picture in {\figref{fig:2:2}} shows populations that are \textit{distributed}: concept populations are associated with neurons in multiple areas of the brain, rather than just a single area. Moreover, the “meanings” of concepts are qualia which we assume to be determined by patterns of synaptic projection to and from sensory systems, motor systems, and other concept populations. For example, our experience of \textsc{coffeeness} is a consequence of signals to and from the peripheral sensory systems which provide information regarding taste, odor, appearance, temperature, etc. of coffee, as well as the motor systems used to pour coffee, drink it, brew it, and also other concepts which relate to \textit{coffee}: beans, mugs, caffeine, etc. There is no essential meaning of \textit{coffee} because the pattern of projection varies over time within an individual and varies in space, i.e. between individuals.
\subsection{Phase transitions to collective oscillation}

Before a \isi{speaker} experiences a meaning associated with [Al], [drink], and [coffee], the neurons of each of these concept populations must undergo a phase transition from an \textit{inactive} regime, in which action potentials are sparse in time and relatively uncorrelated, to an \textit{active} regime, in which action potentials are frequent and highly correlated. We conjecture that integrating action potentials for each population on an appropriate timescale results in an oscillatory spike-rate,\footnote{There is plenty of evidence that \isi{oscillation} plays an important role in the nervous system, and that neural populations exhibit oscillatory patterns of spiking (\citealt{AverbeckEtAl2003,Buzsaki2006,BuzsákiDraguhn2004,CanoltyKnight2010,EngelEtAl2001,Fuster2001,GerstnerKistler2002,Izhikevich2006,Izhikevich2007,IzhikevichEdelman2008,Klimesch1999}). However, because concept populations are hypothetical, the assumption that these populations oscillate is a conjecture.} as shown in {\figref{fig:2:3}}. We conceptualize this phenomenon as the emergence of a macroscopic \textit{collective oscillation} \citep{AcebrónEtAl2005,BreakspearEtAl2010,HongStrogatz2011,Kelso1997,SchonerKelso1988,Strogatz2000,Winfree2001}. There are many possible causes of these phase transitions, but let us imagine for concreteness that the \isi{speaker} sees a man named \textit{Al} and a dark liquid falling into his mouth from a cup he holds. We infer that this peripheral sensory information, through a chain of interactions, causes the relevant concept populations to undergo transitions to the collective \isi{oscillation} regime.


  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img10.png}
\caption{Spike raster and short-time integration of spike rate as concept populations transition to a regime of collective oscillation.}
\label{fig:2:3}
\end{figure}
 

  The transition to collective \isi{oscillation} is a localized emergence of a state that is highly ordered in space and time. The microscopic \isi{state space} has many dimensions. There are numerous degrees of freedom: membrane voltages, ion channel states, neurotransmitter concentrations, etc., of all of the neurons in the relevant populations. In contrast, the macroscopic pattern of \isi{oscillation}, which we obtain by integrating strategically over the microscopic variables, represents a drastic reduction in the volume of this \isi{state space}, and is far more practical as an analytic tool. The transient oscillations have only several degrees of freedom: \isi{phase angle} (θ), angular velocity ($\dot{\theta}$, i.e. instantaneous frequency), and radial amplitude ($r$).

\subsection{Concept populations as systems with surroundings} 

To be explicit, we model each concept population as a \textit{concept system} with a time-varying state vector. Interactions between systems are forces, which depend on system states. Moreover, each system has a \isi{surroundings}. These constructs -- \textit{systems}, \textit{states}, \textit{forces}, and \textit{surroundings} -- are derived from our microscopic population model. Systems are macro-scale models of populations. System states derive from integrating over population microstates. Forces between systems derive from integrating over the influences of synaptic projections from neurons in one population to another. The \isi{surroundings} derives from integrating microscale influences, the origins of which we do not differentiate as systems.\footnote{The \isi{surroundings} is where we locate our ignorance in a given analysis. We can always improve our analyses by constructing new systems from the \isi{surroundings}, but so doing, the analyses become more complex. We often refer to the influence of the \isi{surroundings}, and this should be viewed as a strategy of simplification.} These constructs are illustrated in {\figref{fig:2:4}}.

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img11.png}
\caption{The universe is partitioned into systems and a surroundings.}
\label{fig:2:4}
\end{figure}
 

\subsection{System state variables: Excitation and phase}

To construct a change rule for system states, we must define the \isi{state space}. To do this, we reconceptualize the spike-rate of each population, i.e. a time-integration of action potentials, as a macroscopic \textit{order parameter}, $A$. The \isi{order parameter} $A$ is the deviation of the \isi{spike rate} from a reference value associated with the inactive regime. Furthermore, we conjecture that when a system activates, variation in the \isi{order parameter} has two components: an \isi{oscillation} component $x_{\text{osc}}$, and an excitation component x\textsubscript{exc}, whose sum is the \isi{order parameter}, i.e. $A$ = x\textsubscript{exc} + $x_{\text{osc}}$. We then approximate the \isi{oscillation} component as a harmonic \isi{oscillation} with time-varying amplitude and \isi{phase angle}, i.e. $x_{\text{osc}} = r(t) \cos θ(t)$.The phase variable θ of a system is taken to be 2π{}-periodic, evolves according to an intrinsic system frequency $f_0$, and is influenced by forces from other systems and the \isi{surroundings}. The radial amplitude of $x_{\text{osc}}$ is assumed to be proportional to the excitation component of the system, i.e. $r \propto x_{\text{exc}}$. This analysis of $A$ is schematized in {\figref{fig:2:5}}.  

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img12.png}
\caption{The system order parameter has two components: an oscillation component: $x_{\text{osc}}$, and an excitation component: x\textsubscript{exc}.}
\label{fig:2:5}
\end{figure}
 

  For exposition, we rename the excitation component x\textsubscript{exc} as ``e" and refer to \isi{phase angle} (θ) simply as \textit{phase}. We make a heuristic simplification by assuming the dynamics of e and θ are separable due to differences in relevant timescales. This stipulated separation entails that there is a fast timescale τ\textsubscript{e} such that changes in e occur over intervals τ\textsubscript{e}, and that τ\textsubscript{e} is much smaller than $τ_{θ} = 1/f$, the period of the \isi{oscillation}. Hence in our analyses of the dynamics of e and θ, intermittent abrupt changes in e are assumed not to interact directly with θ. Furthermore, the \isi{intrinsic frequency} $f_0$ is considered to be slowly-varying on utterance timescales, and for many purposes can be conceptualized as a fixed parameter. 

  Given the above construction, the \isi{state space} for one \isi{concept system} is the union of subspaces for e and θ. We do not attempt to provide a more detailed derivation of these variables and their separation from a microscopic, population-scale model. Nonetheless, we speculate that \isi{oscillation} arises from intra-pop\-u\-la\-tion synaptic interactions, intrinsic neuronal dynamics, cortical microcircuit structure, and coupling between neurons and the extracellular medium; excitation relates more directly to the number of neurons which participate in a population \isi{oscillation}.  

\subsection{Meaning experiences are trajectories in state space}

The utterance \textit{Al drinks coffee} does not “have” a meaning. An utterance can only \textit{have} a meaning if we presuppose that meanings are objects contained in words. We reject these object and containment metaphors. Instead, meanings are experiences which correspond to trajectories in \isi{concept system} e,θ space. The conventional object and o/el trajectory metaphors are contrasted in {\figref{fig:2:6}}. In o/el terms, a \isi{meaning experience} associated with a single concept arises when two conditions are met: (i) a stable periodic trajectory occurs in the θ subspace associated with a \isi{concept system}, for an interval of time on the order of $τ_{θ}$, and (ii) the excitation of the \isi{concept system} exceeds a threshold value λ\textsubscript{e}. When e > λ\textsubscript{e} we refer to the system as \textit{excited}; when $0 < e < λ_e$, we refer to the system as \textit{active}; when $e = 0$ we refer to the system as \textit{inactive}. The phase θ of an inactive system is undefined, because the inactive state does not exhibit a collective \isi{oscillation}. 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img13.png}
\caption{The conventional metaphor in which meanings are connected objects vs. an alternative metaphor in which meanings are trajectories in a state space.}
\label{fig:2:6}
\end{figure}
 
  The e,θ \isi{state space} is 2-di\-men\-sional for one \isi{concept system}, and $2n$-di\-men\-sional for $n$ concept systems. Moreover, when $n$ concept systems are excited, a \isi{relational meaning} experience associated with those systems is a stable periodic orbit in the $n${}-di\-men\-sional θ subspace. Typically we are interested in meaning experiences associated with systems whose e > λ\textsubscript{e}, i.e. excited systems. We will sometimes refer to these as \textit{attended} meanings, because we imagine that the relevant concept systems have e values which are sufficient to support conscious attention to a \isi{meaning experience}. In contrast, subconscious experience of meaning occurs via active, unexcited systems.

  Note that e and θ state variables are analytical constructs which we can attempt to derive from a higher-di\-men\-sional microscale \isi{state space}. This derivation procedure uses methods of projection and integration in order to reduce dimensionality. Accordingly, the \isi{state space} is always constructed \textit{ad hoc} to accommodate the systems which we consider relevant for a given analysis.\footnote{For example, we are currently ignoring the fact that \textit{drinks} is associated with concept systems which are grammatical in nature, such as person, number, and \isi{tense}. In later analyses we explicitly construct such systems, but for the current purpose of introducing the o/el framework, we keep things simple.} The \isi{state space} construction procedure is (i) stipulate a set of concept systems; (ii) construct a space with e and θ dimensions for each system; (iii) construct the union of these spaces by combining them orthogonally. The picture to have in mind is shown in {\figref{fig:2:7}}. 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img14.png}
\caption{Projection and integration operations can be applied to a microscale state space in order to derive a macroscale state space, which consists of excitation and phase subspaces.}
\label{fig:2:7}
\end{figure}
 

  The \isi{state space} is neither permanent nor a physical space. It is a heuristic tool that we construct strategically to meet the needs of a given analysis. Describing [Al], [drinks], and [coffee] with orthogonal excitation and phase variables is useful because it provides a coarse model of the much higher-di\-men\-sional states of neural populations. Conceptualizing meaning experiences as trajectories in e,θ space opens up a new approach to reasoning about linguistic phenomena.

\subsection{Relational meaning experiences are relative phase-con\-fi\-gu\-ra\-tions}

Individual concept meaning experiences rarely occur in isolation. The production of \textit{Al drinks coffee} is associated with simultaneous excitation of concepts [Al], [drinks], and [coffee]. Yet simultaneity of excitation is not sufficient for understanding the \textit{relational} character of meaning experiences. This is obvious from consideration of utterances such as \textit{Al likes Bo} and \textit{Bo likes Al}, where the same concepts are excited and yet different relational meanings are experienced. Since we do not experience both of these relational meanings simultaneously, there must be a mechanism which distinguishes system states in which [Al] and [Bo] have different relations to [likes]. Moreover, this mechanism should also govern the relations of [Al] and [coffee] to [drinks] in \textit{Al drinks coffee}, as well as any arbitrary relations of this sort. 

  To that end we propose a \textit{principle of relational meaning}: \isi{relational meaning} experiences are stable relative phase-configurations. Recall that \isi{relative phase} ϕ is defined as the antisymmetric difference of phases, i.e. ϕ\textsubscript{ij} = θ\textsubscript{i} \textminus{} θ\textsubscript{j} = \textminus{}ϕ\textsubscript{ji}. For exposition we often refer to ϕ without indices and interpret this as the absolute value of \isi{relative phase}, i.e. {\textbar}ϕ{\textbar} = {\textbar}θ\textsubscript{i} – θ\textsubscript{j}{\textbar}. Furthermore, we pursue a strong hypothesis that all \isi{relational meaning} experiences are associated with a stable state in which ϕ ${\approx}$ 0 or π, which we call \textit{in-phase} and \textit{anti-phase}, or +ϕ and −ϕ-configurations, respectively. More precisely, for any pair of concept systems $i$ and $j$, a \isi{relational meaning} experience occurs when both systems are excited \{e\textsubscript{i}, e\textsubscript{j}\} > λ\textsubscript{e} and have a stable \isi{relative phase} such that {\textbar}ϕ\textsubscript{ij}{\textbar} ${\approx}$ \{0 or π\} and dϕ\textsubscript{ij}/dt ${\approx}$ 0. Specifically, we hypothesize that in-phase-configurations ($ϕ \approx 0$) are associated with agent-action relations, e.g. [Al][drinks], and that anti-phase-configurations (ϕ ${\approx}$ π) are associated with patient-action relations, e.g. [drinks][coffee]. These basic hypotheses are summarized in {\tabref{tab:1:1}}. Many additional ϕ-relation hypotheses are developed subsequently.

  \begin{table}
\begin{tabularx}{\textwidth}{XXll}
  \lsptoprule
  \multicolumn{2}{c}{\textbf{conceptual systems}} & 
  \textbf{semantic relations} & 
  \textbf{$\phi$ configurations}\\
  \midrule{}
  [Al] & [drink] & agent-action & in-phase: ϕ ${\approx}$ 0\\{}
  [coffee] & [drink] & patient-action & anti-phase: ϕ ${\approx}$ π\\
  \lspbottomrule
  \end{tabularx}
\caption{Hypothesized relative phase-configurations for agent and patient semantic relations.}
\label{tab:1:1}
\end{table}

  The ϕ-configurational basis for differences in \isi{relational meaning} between [Al]\linebreak\relax[drinks] and [coffee][drinks] is illustrated in {\figref{fig:2:8}}. Crucially, [Al][drinks] and [coffee][drinks] ϕ-configurations remain constant despite the fact that all three θ variables are changing. Constant ϕ, when stable over time periods on the order of τ\textsubscript{θ}, gives rise to the experience of \isi{relational meaning} between systems, as long as those systems are excited. Note that ϕ-configurations are periodic trajectories in θ space, but we can also construct a ϕ space in which a stable ϕ-configuration is a point. Moreover, because θ dimensions are circular, wrapping around the interval [0, 2π], ϕ patterns can also be represented as a static phase difference on a unit circle, as in {\figref{fig:2:8}}. In such representations we choose some system as a reference, and the phase angles of all other systems are shown relative to the phase of the reference system. For visual clarity, we depict systems with a +ϕ configuration as having a small ϕ separation, as with [Al] and [drinks] in {\figref{fig:2:8}}.

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img15.png}
\caption{The agent-action relation corresponds to ϕ = 0. The patient-action relation corresponds to ϕ = π.}
\label{fig:2:8}
\end{figure}
 

  The principle of \isi{relational meaning} requires \isi{relational meaning} experiences to be \textit{stable} ϕ-configurations. Here \textit{stable} means that ϕ, when perturbed, returns to an equilibrium value (0 or π) on a timescale which is substantially smaller than τ\textsubscript{θ}. Fluctuations constantly perturb θ variables of systems and hence perturb ϕ. A \isi{stabilizing mechanism} is thus required to force ϕ back an equilibrium value. 

  What is the \isi{stabilizing mechanism}? Our \isi{microscopic model} suggests that synaptic projections between concept systems and other systems could accomplish this stabilization. By integrating over interpopulation synaptic projections we can derive macroscopic \textit{coupling forces}, which serve to stabilize ϕ. However, if these forces act directly between concept systems, there is a problem.

\subsection{Direct interactions between conceptual systems are unlearnable and inflexible} 

Let's imagine that a direct interaction between [Al] and [drinks] concept systems were indeed responsible for stabilizing their ϕ-configuration. On the basis of our \isi{microscale conception}, such interactions must be learned: macroscopic forces are derived from synaptic weights (i.e. efficacy of neurotransmitter release/uptake), connectivity patterns, etc. between populations. Learning is an evolution of these variables on supra-utterance timescales. Moreover, the interaction, if a stabilizing one, would need to be fairly strong, otherwise moderate perturbations would overcome the equilibration forces. 

  There are two problems with the direct coupling scenario. The first involves \isi{learnability}. Two different types of interactions between [Al] and [drinks] would need to be learned: in-phase and anti-phase interactions for the agent and patient roles, respectively. Moreover, these two types of interaction would need to be learned for all \textit{pairs} of concepts: for $n$ concept systems there are $2n^2$ interactions. The second problem involves flexibility. If the learned stabilizing interactions are too strong, then there is a danger that excitation of one \isi{concept system} will always cause other concept systems that it interacts with to become excited. For example, imagine that when [Al] becomes excited, direct interaction forces excite [drinks] and [coffee] as well. This is a problem if one wants to experience the meaning of \textit{Al eats granola}, for example. With direct interactions between concept systems, system trajectories would be prone to seizures in which all concept systems become excited. The solutions to the flexibility and \isi{learnability} problems are provided by syntactic systems.

\section{Syntactic systems}

Syntactic systems are the primary mechanism for stabilizing ϕ-configurations of concept systems. There are two basic aspects of this mechanism. First, concept systems \isi{resonate} with syntactic systems through mutual positive feedback. We refer to this as \textit{resonance} because syntactic systems have strong, asymmetric interactions with concept systems. Second, syntactic systems couple strongly to other syntactic systems. Hence syntactic systems can organize and stabilize ϕ-configurations between concept systems, without requiring strong direct coupling between concept systems. Syntactic systems provide an \textit{indirect}, \textit{flexible} mechanism for stabilizing \isi{relational meaning}, one which does not rely on learning direct interactions between concepts. Henceforth we abbreviate concept systems as \textit{c-systems}, and syntactic systems as \textit{s-systems}. 

\subsection{Microscopic conception of syntactic and conceptual systems}

Both c- and s-systems have e and θ state variables, and these are derived in the same way from a \isi{microscale conceptualization} of populations, as shown in {\figref{fig:2:9}}. But the microscopic pictures of c-systems and s-systems differ in some important ways which help resolve the \isi{learnability} and flexibility problems. First, for concepts we imagine a large, distributed population of neurons. Each individual c-system is a subpopulation of this full population, and despite substantial overlap of these subpopulations, c-systems can be distinguished from each other on the basis of their interactions with other systems and the \isi{sensorimotor} \isi{surroundings}. On the \isi{macroscale}, the primary mechanism of learning is not “adding” new c-systems, but rather differentiating and blending existing c-systems. On the microscale this entails that new sub-populations are not “created”, but rather new patterns of interaction arise with the \isi{sensorimotor} \isi{surroundings} and other conceptual systems. Interactions between c-systems are presumed to be relatively weak: c-systems can activate other c-systems (this is often called \textit{priming}), but typically do not excite other c-systems.

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img16.png}
\caption{Microscale conception of interactions between conceptual system populations and syntactic system populations.}
\label{fig:2:9}
\end{figure}
 

  In contrast to the full population of c-systems, the full population of s-systems is spatially localized, possibly in inferior frontal gyrus, or in basal ganglia-thal\-a\-mo\-cor\-ti\-cal circuits. Individual s-systems, i.e. subpopulations of the full s-system population, overlap to lesser degree with each other than c-systems do, and interact more strongly because of their spatial localization. 

  The c- and s-system populations project to one another, and under certain conditions c-system populations may \isi{resonate} with s-system populations, a phenomena we refer to as \textit{cs-resonance}. We assume that the capability for cs-resonance is phylogenetic, but in development, different c-systems become preferentially biased to \isi{resonate} with different types of s-systems. Furthermore, we speculate that the effects of general learning mechanisms (e.g. Hebbian spike-timing dependent synaptic plasticity), when integrated on supra-utterance timescales, differentiate the full syntactic population into various s-system subpopulations. Biases for in-phase and anti-phase coupling interactions between s-system populations are learned in this manner, giving rise to a grammar of ϕ-coupling.

\subsection{Conceptual systems resonate with syntactic systems}

The cs-resonance mechanism can be understood as follows. First, forces from the \isi{surroundings} activate a c-system and a corresponding s-system. These systems begin to \isi{resonate} weakly, via a positive mutual feedback interaction. Microscopically, the positive feedback resonance mechanism derives from integrating the effects of excitatory-to-excitatory interpopulation projections between an s-system and c-system. Because these projections are excitatory, resonating c- and s-systems always have an in-phase ϕ-relation. 

  Recall that activation implies a collective \isi{oscillation}, but not stability of $\dot{\theta}$ and not necessarily an e value sufficient for a \isi{meaning experience}. In general many c-systems may be active and may compete for resonance with a given s-system; \isi{surroundings} forces influence this competition as well. The competition from other c-systems and \isi{surroundings} forces can potentially destabilize a newly formed resonance between c- and s-systems. We thus imagine a \isi{pre-stable phase} of production in which interaction between a c-system and s-system may or may not lead to a strong cs-resonance. If positive feedback between the c- and s-system is sufficiently strong relative to destabilizing forces, the c- and s- system abruptly become \textit{excited}, which entails that the s-system e value exceeds a threshold, as shown below. The e value of the c-system also increases, but for reasons that become clear we need make no specific assumptions about c-system e values relative to other c- or s-systems. We henceforth refer to a pair of resonating c- and s-systems (whether excited or merely active) as a \textit{cs-system}, or simply a \textit{system}. In the {\figref{fig:2:10}}, the c-system [coffee] resonates with the s-system \{N\}, and this gives rise to a stable, excited cs-system.

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img17.png}
\caption{Resonance between an s-system and a c-system results in both systems transitioning to an excited state.}
\label{fig:2:10}
\end{figure}
 
  A key diagnostic of cs-system excitation is intrapopulation and interpopulation \isi{spectral coherence}, a concept which we develop in more detail later on. Moreover, the stabilization of ϕ entails an augmentation of e. The excitation threshold λ\textsubscript{e} plays an important role in a variety of analyses we develop subsequently. When a cs-system has below-threshold excitation (i.e. the system is active but not excited), the system cannot participate in a stable ϕ-configuration with other cs-systems and hence cannot evoke an attended \isi{relational meaning} experience. In general, we imagine that there are many active but unexcited cs-systems, before and during production. Thus, in the production of an utterance such as \textit{Al drinks coffee}, the excitation of [Al], [drinks], and [coffee] is merely the tip of an iceberg: a large amount of subthreshold activity occurs below the surface.

\subsection{Coupling force types and valence} 

To classify interactions between systems, we distinguish two types of coupling and two valences of coupling. Relative phase coupling (ϕ-coupling) is an interaction that depends on \isi{relative phase} ϕ and influences $\dot{θ}$. The {\figref{fig:2:11}} shows the phases of two systems on a \isi{phase circle}, which is the space of possible phases. The effects of the \isi{relative phase} (ϕ) \isi{coupling force} are shown by the arrows: an attractive ϕ-force drives θ variables (which are also rotating counterclockwise) toward one another, resulting in a decrease in ϕ; a repulsive ϕ-force drives θ variables away from one another, resulting in an increase in ϕ. The \isi{coupling force} is associated with a periodic sinusoidal \isi{potential function} V(ϕ), such that  $F(\phi)=-dV(\phi)/d\phi$. The effect of the force on ϕ is analogous to a ball rolling down a hill while submerged in a viscous fluid: the force causes ϕ to change until it reaches the stable equilibrium of 0 (attractive force, see {\figref{fig:2:11}}) or ±π (repulsive force, see {\figref{fig:2:12}}), where it stops. Because θ is a periodic variable, it is convenient to map ϕ to the interval [−π,+π]. 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img18.png}
\caption{Attractive ϕ-coupling involves a force which drives a pair of systems to have minimally different phases.}
\label{fig:2:11}
\end{figure}
 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img19.png}
\caption{Repulsive ϕ-coupling involves a force which drives a pair of systems to have maximally different phases.}
\label{fig:2:12}
\end{figure}
 

  The other type of force is excitation coupling (e-coupling). Excitation coupling is an interaction which depends on and influences e variables. An excitatory e-\isi{coupling force} results in each system increasing the e value of the other, and an inhibitory e-\isi{coupling force} results in the each system decreasing the e value of the other, as shown in {\figref{fig:2:13}}. We do not specify a functional form for this force, as its role in the current framework is not well developed and is generally subsumed under other mechanisms.

  Both ϕ-coupling and e-coupling forces can have positive [+] or negative [−] valence, as schematized below. An attractive (+ϕ) force causes the θ of systems to become more proximal and a repulsive (−ϕ) force causes θ to become more distal. An excitatory (+e) force causes e values to increase, and an inhibitory (−e) force causes e values to decrease. 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img20.png}
\caption{ϕ-coupling and e-coupling can have [+] or [−] valence.}
\label{fig:2:13}
\end{figure}
 

  Equations (1) and (2) below show the roles of ϕ and e forces in influencing how θ and e variables change in time. The total ϕ and e forces a system experiences are sums over forces from pairwise interactions with other systems, plus forces from the \isi{surroundings}, $S$. These forces have coupling strengths/susceptibilities $\Phi$ and $\varepsilon $, respectively. The ϕ-force from $S$ is assumed to be negligible, because the \isi{surroundings} are too large to exhibit a collective \isi{oscillation}. However, the \isi{surroundings} can exert non-negligible e forces. The term f\textsubscript{i} is an \isi{intrinsic frequency} of the system (angular velocity $ω = 2πf$), representing population-internal forces which promote collective \isi{oscillation}. The operator  $\widehat{E}[\overrightarrow{\theta} ,\overrightarrow{e}\,]$ is a placeholder for mechanisms of e-organization, and we develop these in detail later on.

\eabox{$$
{\dot{{\theta}} }_{i}={2{\pi f}}_{i}+{F}_{{\phi S}}\left(S,{\theta} _{i}\right)+\sum _{j}{{{\Phi} _{{ij}}F}_{\phi} \left({\phi} _{{ij}},{e}_{i},{e}_{j}\right)}
$$}

\eabox{$$
{\dot{{e}}}_{i}=\text{Ê}\left[\overrightarrow{{\theta}} ,\overrightarrow{{e}}\,\right]+{F}_{{eS}}\left(S,{e}_{i}\right)+\sum _{j}{{{\varepsilon} _{{ij}}F}_{e}\left({\phi} _{{ij}},{e}_{i},{e}_{j}\right)}
$$}

  Some properties of ϕ- and e-coupling can be derived from our \isi{microscale conceptualization}. For one, the valences of ϕ and e forces (i.e. the signs of elements of matrices $\Phi$ and $\varepsilon $) are correlated: attractive and mutually excitatory coupling tend to co-occur, and repulsive and mutually inhibitory coupling tend to co-occur. The basis for this correlation is the association of [+] valence forces with predominantly excitatory post-synaptic targets of interpopulation synapses, and conversely the association of [−] valence forces with predominantly inhibitory neurons as post-synaptic targets. These microscale patterns are illustrated in {\figref{fig:2:14}}. When the excitatory neurons in population A project primarily to excitatory neurons in population B, the effect of spikes of neurons in A is to attract θ\textsubscript{B} to θ\textsubscript{A} and augment e\textsubscript{B}; when excitatory neurons in B project primarily to inhibitory neurons in B, their effect is to repel θ\textsubscript{B} from θ\textsubscript{A} and diminish e\textsubscript{B}. 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img21.png}
\caption{Positive valence coupling derives from a predominance of excitatory-to-excitatory projections between two populations. Negative valence coupling derives from a predominance of excitatory-to-inhibitory projections.}
\label{fig:2:14}
\end{figure}
 

  The correlation of $\Phi$ and $\varepsilon $ and valence implies that ϕ and e forces depend on both ϕ and e values of systems. However, we offer no specific form for the ϕ-e interaction here because it would be too speculative. Nonetheless, our hypothesis that \isi{relational meaning} experiences require the relevant cs-systems to be in an \isi{excited state} accords with the hypothesis that ϕ-coupling forces are modulated by e values: the ϕ forces exerted by unexcited systems are too weak to stabilize ϕ-configurations, while systems with above-threshold e values can exert ϕ forces on one another that are sufficiently strong to induce a high degree of cs-system \isi{coherence}.

  The ϕ- and e-\isi{coupling force} matrices $\Phi$ and $\varepsilon $ are also sign-symmetric. The basis for this is the intuition that Hebbian learning between bidirectionally coupled populations would be unstable on long timescales, if the valences of interactions between those populations were asymmetric. For instance, imagine a population A that is +ϕ coupled to population B, while B is −ϕ coupled to A. Spike-timing dependent learning would strengthen synapses which promote attraction of θ\textsubscript{B} to θ\textsubscript{A}, but also strengthen synapses which promote repulsion of θ\textsubscript{A} from θ\textsubscript{B}, leading to an unstable interaction in which A chases B while B runs away. Thus valence-symmetry is expected for any pair of coupled systems. In contrast, there is no reason to expect a high degree of correlation in pairwise coupling \textit{strength} for either ϕ- or e-coupling forces. These strengths are derived from synaptic efficacy and numbers of synapses (or synaptic density, i.e. average number of synapses per neuron). To summarize, the elements of $\Phi$ are correlated in sign and magnitude with those of $\varepsilon $, and within each matrix there is sign symmetry but not a high degree of correlation.

\subsection{The syntactic mechanism for organizing relational meaning}

With the conceptual tools outlined above we can construct a new understanding of the flexible emergence of \isi{relational meaning} experiences. The key idea is that stable, invariant ϕ-configurations between c-systems are created indirectly through their resonances with strongly coupled s-systems. The coupling structure and \isi{phase circle} representations for two example-configurations are schematized in {\figref{fig:2:15}}. The [Al][drinks] +ϕ-configuration obtains because the c-system [Al] resonates with the s-system \{+N\}, the c-system [drinks] resonates with the s-system \{V\}, and the s-systems \{+N\} and \{V\} are strongly +ϕ coupled. Likewise, [coffee] resonates with \{−N\}, and \{V\} and \{−N\} are strongly −ϕ coupled.

  Although ϕ-configurations can be decomposed into pairwise relations, multiple ϕ-configurations which obtain simultaneously will often be shown by projecting them onto the same \isi{relative phase} axis, as in {\figref{fig:2:15}}. Furthermore, because the hypothesized mechanism for stabilizing ϕ-configurations is strong ϕ-coupling between s-systems, the \isi{phase circle} representation generally implies coupling between s-systems only; ϕ-configurations between c-systems are an indirect consequence of strong \isi{s-system coupling}. We nonetheless sometimes label c-systems on the \isi{phase circle} for convenience. Because a ϕ-configuration of c-systems entails the same-configuration between the s-systems which \isi{resonate} with those c-systems, we think of a ϕ-configuration as a configuration of a cs-system. 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img22.png}
\caption{A phase circle representation in which multiple ϕ-configurations are depicted.}
\label{fig:2:15}
\end{figure}
 

  Importantly, a ϕ pattern alone is not sufficient for a \isi{relational meaning} experience. In addition, the pattern must be stationary in a local epoch of time. For a pattern to be stationary, there must be a \isi{stabilizing mechanism}, and s-systems provide this mechanism. Recall the dynamic equation for θ. In general, the intrinsic frequencies $f_i$ of any two systems are not the same and fluctuations in \isi{surroundings} forces constantly perturb their phase velocities $\dot{\theta}_i$. In the absence of coupling forces, \isi{intrinsic frequency} differences and \isi{surroundings} perturbations cause ϕ to drift. In contrast, with the strong coupling of cs-resonances, c-system and s-system phase velocities $\dot{\theta}_i$ equalize to a compromise $\dot{\theta}_i$, the value of which depends on the relative strengths of the forces and the intrinsic frequencies. This will hold as long as the coupling forces -- which act to equalize \isi{phase velocity} -- are strong compared to the perturbing forces. Thus given sufficiently strong coupling forces, a ϕ-configuration will remain stable.

\subsection{Interference and differentiation}

The preceding analyses distinguished between \{+N\} and \{−N\}. Why do we need to make this distinction, and how can it be understood on the microscale? The distinction between \{+N\} and \{−N\} systems (and on the microscale, \{+N\} and \{−N\} populations) is necessary because of \textit{interference}. Imagine that there is just a single, undifferentiated \{N\} population. For an utterance like \textit{Al drinks coffee}, both [Al] and [coffee] \isi{resonate} with \{N\}, and [drinks] resonates with \{V\}. According to the \isi{relational meaning} hypotheses presented earlier, [Al]\{N\} and [coffee]\{N\} should obtain +ϕ and −ϕ-configurations with [drinks]\{V\}, respectively. These conditions are incompatible: it is not stable for \{N\} to be simultaneously +ϕ and −ϕ coupled to \{V\}.

  How does the nervous system resolve this dilemma? A crucial constraint on any solution is that populations cannot be created or added (without incurring the \isi{multiplicity problem}). We cannot simply posit that there is a second \{N\} population, independent of the original one. Instead, we imagine that there is one single \{N\} population, and that speakers learn to differentiate that population into \{+N\} and \{−N\} subpopulations, which are biased to +ϕ and −ϕ couple to \{V\}, respectively.

  A consequence of differentiation is that subpopulations can \textit{interfere} with one another, and if the \isi{interference} is too strong, the collective oscillations of those subpopulations become unstable. This can happen for two reasons. First, when a population is differentiated, the resulting subpopulations are smaller than the original population. The interaction forces exerted by the subpopulations on other systems become smaller, and the subpopulations themselves become more susceptible to forces from other systems and the \isi{surroundings}. This can result in instability. Second, differentiated systems are not entirely independent: the corresponding subpopulations will typically overlap. The repeated differentiation of a finite population eventually results in instability, because the resulting subpopulations have greater degrees of proportional overlap with one another. \textit{Proportional overlap} is defined here as the ratio of neurons which are in both populations to the total number of neurons. The \{N\} > \{+N\}/\{−N\} differentiation provides two \{N\} populations which are quite stable when simultaneously excited, but when we differentiate one of these subpopulations further, stability may be threatened. The loss of stability from differentiation has important consequences, which we examine in later chapters.

\section{Selection and quantal organization of excitation}

Whereas the principle of \isi{relational meaning} involves organization of \isi{relative phase} (ϕ), the principle of \isi{quantal excitation} involves organization of excitation (e). The movements associated with the production of speech arise from an organized, ordered selection of systems, determined by their relative excitation. Selection is a mechanism in which supra-threshold excitation of systems induces excitation of gestural/motor systems. Here we propose a \textit{principle of quantal excitation}: syntactic systems are organized and re-organized in a \isi{quantal relative excitation} potential. This organization results in the ordered selection of motor behaviors associated with language.

\subsection{The quantal relative excitation potential}

The principle of \isi{quantal excitation} is based on a conjecture that there exists a mechanism which organizes the relative excitation of s-systems into quasi-discrete, or \textit{quantal} excitation levels. We identify this mechanism with a \textit{stabilizing regime} of the \isi{excitation operator} Ê in the dynamical equation for e. The \isi{stabilizing regime} of Ê is one in which e states are mapped to themselves, and thus relative e values remain constant. The \isi{stabilizing regime} of Ê is associated with a conservative \isi{excitation potential}, V(e), as shown in {\figref{fig:2:16}} for utterances \textit{Al sleeps} and \textit{Al drinks cold coffee}. 

  Observe in the examples that there are large differences in potential energy between excitation levels. The potential barriers between excitation levels entail forces which stabilize s-system e, thereby preventing e values from increasing to a higher level. The force that each system experiences is –dV(e)/de, i.e. the opposite of the derivative of the potential. The conception of force and potential energy here derives from an analogy to conservative forces, but we do not actually require a conserved quantity. Furthermore, we imagine these forces to be stationary only for a local period of time, i.e. a single epoch of e-organization during which Ê is in the \isi{stabilizing regime}. 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img23.png}
\caption{Examples of quantal excitation potentials and associated forces which prevent the increase of excitation.}
\label{fig:2:16}
\end{figure}
 

  Two levels of the potential representation have a special  interpretation. The lowest level of the potential is the \textit{ground level}, and systems on this level are by definition in an active, unexcited state. Ground state systems have at least the minimal e value required for collective \isi{oscillation}, but do not have sufficient e to participate in a stable ϕ-configuration. There are no “systems” below the ground level, because a system by definition is a population which exhibits collective \isi{oscillation}. We distinguish \textit{ground level} systems from \textit{excited} systems, which have sufficient excitation to participate in stable ϕ-configurations. The highest level of the potential is called the \textit{selection level}, and systems on this level have sufficient excitation to induce the selection of gestural/motor systems which are associated with a c-system. The four main classes of excitation states are summarized in {\tabref{tab:1:2}}.

\begin{table}
\begin{tabularx}{\textwidth}{lQ}
\lsptoprule
State & Description\\
\midrule 
inactive & system is undefined. no collective \isi{oscillation}.\\
ground-level (active) & system is active, but not in an \isi{excited state}.

collective \isi{oscillation} and minimal cs-resonance are unstable.\\
above-ground (excited) & system is excited but not selected.

stable, strong cs-resonance; can participate in stable ϕ-configuration.\\
selection-level & system is excited and selected.

gates open for simulation or execution of associated gestural/motor systems.\\
\lspbottomrule
\end{tabularx}
\caption{Four classes of system excitation.}\label{tab:1:2}
\end{table}
 
 We have not addressed the question of how the quantal character of the relative \isi{excitation potential} can be derived from a \isi{microscale model}. Presumably, quantal e-organization manifests partly from e-coupling interactions between s-systems, and we note that the effects of the potential are reminiscent of normalization mechanisms associated with on-center/off-surround fields \citep{Grossberg1978,Grossberg1987}. However, a detailed understanding of this mechanism has not yet been developed, and the quantal potential must currently be viewed as a phenomenological approximation with primarily heuristic value. Because of this, there is no reason to commit to a particular shape of the potential, and one can imagine a number of alternatives, examples of which are shown in {\figref{fig:2:17}}.

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img24.png}
\caption{Alternative representations of the excitation potential.}
\label{fig:2:17}
\end{figure}
 

  Although the particular form of the \isi{potential function} is not so important, its quantal nature is paramount, because the effect of the potential must be to stabilize a pattern of relative excitation which enforces mutual exclusivity of selection. Hence, when [Al]\{+N\} is selected, [drink]\{V\} and [coffee]\{−N\} are not selected, and so on. 

  There are several points to emphasize regarding the e-potential representations. First, as explained above, these representations are schematic and imply transiently discretized patterns of relative excitation; they do not imply specific values or specific relative magnitudes. Second, e-potentials govern the e values of s-systems, not c-systems. In cs-resonances, c-system e values are correlated with s-system e values, but the correlation is not exact. We nonetheless often label c-systems in e-potentials, for convenience, and often refer to cs-systems in this context. 

  Third, intermediate levels of an e-potential \textit{never exist independently of the systems which occupy them}. The potential is conceptualized as an emergent phenomenon associated with interactions between s-systems, and as such it is not sensible to imagine an “\isi{unoccupied level}”. (The ground and selection levels are exceptions, for reasons we discuss later.) The potential levels are \textit{not} locations in space, and the systems are \textit{not} objects which occupy locations. Instead, the quantal potential is understood as a pattern of organization that is created by a combination of local interactions between s-systems and a general purpose ordering mechanism which operates on relative e values. Rather than saying that systems \textit{occupy} levels, it is more precise to say that interactions between s-systems bring about the conditions for the stabilization of their relative excitation.

\subsection{Canonical reorganization}

While the \isi{stabilizing regime} of Ê enforces an approximate temporal invariance on e, a \isi{reorganization regime} of Ê causes intermittent, abrupt changes in e. These changes map e-configurations to e-configurations in predictable ways. Reorganization mappings cause changes in e-configuration which are discontinuities on the ϕ-timescale. We refer to the stable periods of time between these discontinuities as e-epochs. We are interested here in the various forms that reorganization mappings can take, and in what aspects of the system state they might depend on. In general, reorganization operations could depend on all θ and e variables of all active and excited systems -- i.e. the full system state. However, we can infer that some information is typically not relevant to the mapping. 

  The default mechanism for ordering the selection of systems is the \isi{canonical reorganization} mapping, Ê\textsuperscript{cr}. θ/ϕ information is irrelevant for \isi{canonical reorganization}. The operation  can be understood as follows, using the utterance \textit{Al drinks coffee} as an example. First, assume the initial condition in epoch (e1), an e-configuration which is stabilized by the stabilization regime Ê. In epoch (e1), [Al]\{N\} has selection-level excitation, and this drives the excitation of motoric/gestural systems associated with [Al]. Feedback resulting directly or indirectly from motoric/gestural excitation eventually causes a transition to the canonical \isi{reorganization regime}. The canonical re-organization mapping Ê\textsuperscript{cr}\linebreak causes an abrupt change from epoch (e1) to epoch (e2), in which the selection-level system is \textit{demoted} to the lowest \isi{excited state}, and all other excited systems are \textit{promoted} one level. Ê\textsuperscript{cr} applies to transitions from (e2) to (e3) and from (e3) to (e4) as well. Note that Ê\textsuperscript{cr} produces a cycle when iterated: e\textsubscript{1}, e\textsubscript{2}, … e\textsubscript{n}, e\textsubscript{1}… 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img25.png}
\caption{The canonical reorganization operation demotes selected systems and promotes non-selected systems.}
\label{fig:2:18}
\end{figure}
 

  In {\figref{fig:2:18}} we show a more compact representation in which e-organization state vectors,  $\widetilde{{e}}$, are operated upon element-wise by reorganization vectors. In an e-organization state vector, systems are assigned to vector dimensions in order of their relative excitation. The figure above shows e-organization state vectors from a series of epochs. Each  $\widetilde{{e}}$ is operated upon by the canonical \isi{reorganization operator} Ê\textsuperscript{cr}. The arrows in each element of Ê\textsuperscript{cr} indicate which basic operation (promotion or demotion) applies to the corresponding element of  $\widetilde{{e}}$. In this context, \isi{canonical reorganization} can be understood as demotion of the most highly excited system to the lowest above-ground level, and promotion of all other systems by one level. For convenience, the excitation and selection thresholds are shown by dashed and solid red lines, respectively.

  There are a couple alternative formal approaches to representing reorganization mappings. One is to define a relative \isi{quantal excitation} state vector  $\overrightarrow{{e}}$, where each dimension corresponds to a different excited cs-system. The value in a dimension is an integer from 1 to $n$, where $n$ is the number of s-systems which occupy distinct e-levels, and the value corresponds to excitation rank order of the corresponding system. The \isi{canonical reorganization} mapping in this scheme is given in (3).

\eabox{$$
{\text{Ê}}^{{cr}}\left(\overrightarrow{{e}}\,\right):e\rightarrow \left[e\bmod n\right]+1
$$}

  Another formalization uses a cyclic permutation matrix. In this case we define the e-state as a binary matrix Ë, as shown in (4), where each column corresponds to a level of the e-potential and each row to a system (so, a value of 1 in row $n$, column $m$, entails that system $n$ occupies excitation level $m$). Repeated action of the permutation matrix Ê on Ë results in a return to the initial pattern. 

\ea
$$
\text{Ê}^{{cr}}=
\left[
  \begin{matrix}
  0 & 0 & 1\\
  1 & 0 & 0\\
  0 & 1 & 0
  \end{matrix}
\right],
{\text{Ë}}_{1}=
\left[
  \begin{matrix}
  1 & 0 & 0\\
  0 & 1 & 0\\
  0 & 0 & 1
  \end{matrix}
\right]
$$
\z


  The e-state vector and matrix representations are somewhat less general than the e-organization representation, for reasons that will become clear later. In contrast, the e-organization representation has greater flexibility and we make extensive use of it. The \isi{canonical reorganization} is a useful construct because many of the phenomena we are interested in can be analyzed in relation to the canonical mapping.

  Although we do not attempt to model the internal dynamics of the reorganization process, we imagine promotion and demotion as brief periods of relatively strong excitatory and inhibitory forces. The picture we have in mind is in {\figref{fig:2:19}}. In the stable epoch (e1), the augmentation forces on [drinks]\{V\} and [coffee]\{N\} are not sufficient to promote these systems. But feedback regarding the selection of [Al]\{N\} in (e1) induces a transition to the \isi{reorganization regime}, in which there is a strong suppressive force on [Al]\{N\}, along with strong forces which augment the excitation of other systems. This may occur in combination with a reduction of the sizes of barriers in the potential. 

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img26.png}
\caption{Canonical reorganization involves a rapid change of system excitation states.}
\label{fig:2:19}
\end{figure}
 

  The overall effect of the reorganization is that the e value of the selected system, [Al]\{+N\}, decreases and the e values of other excited systems, [drinks]\{V\} and [coffee]\{−N\}, increase. We assume that Ê returns to the stabilization regime when a new system surpasses a \isi{selection threshold}, i.e. when [drinks]\{V\} is selected, resulting in the stable epoch (e2). It is important to emphasize that because systems are not objects, there is no sense in which there is a collision between objects. We never worry about lines crossing or objects occupying the same space in o/el representations.

\subsection{The combined picture: Two conceptions of time}

The o/el framework provides two conceptual models of the temporality of speech, one suited for reasoning about \isi{relational meaning} experiences, the other suited for reasoning about action ordering. As shown in {\figref{fig:2:20}}, a \isi{production trajectory} begins with the activation of cs-systems. A stable ϕ-configuration of excited systems then emerges in conjunction with an initial e-configuration, as a result of an \isi{initial organization operator}, Ê\textsuperscript{io}. (We examine mechanisms of initial organization in a subsequent chapter.) The e-configuration is then iteratively reorganized, while the ϕ-configuration remains constant. Consequently, we see that ϕ variables have a fixed point attractor throughout the trajectory (θ variables have a periodic attractor), while e variables exhibit intermittent discontinuous changes. The steady state periods between reorganizations are e-epochs. The conceptual models of time we have constructed help distinguish between the ϕ-epoch timescale on which ϕ-configurations are stable (i.e. a \isi{relational meaning} experience is invariant) and the e-epoch timescale on which e-configurations are stable.

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img27.png}
\caption{The ϕ-configuration can be constant while the e-configuration is reorganized.}
\label{fig:2:20}
\end{figure}
 

  In conventional approaches, there are diverse perspectives on how \isi{linearization} (selection/ordering) and structure building/variable binding (\is{relational meaning}relational\linebreak meaning) interact, but these are generally understood to create and operate on structures of connected objects. The o/el model provides an alternative framework for thinking about the interaction between \isi{relational meaning} and temporal order, one specific to ϕ-organization, the other specific to e-organization. Because ϕ-epochs tend to span multiple e-epochs, it is not easy, nor even useful to combine them into a single space for visualization. One approach would be to map relative excitation to oscillator amplitude, in which case we can visualize the temporal evolution as in {\figref{fig:2:21}}.

  
\begin{figure}
\includegraphics[width=\textwidth]{figures/Tilsen-img28.png}
\caption{A corkscrew representation in which both excitation and phase are represented over time is difficult to interpret.}
\label{fig:2:21}
\end{figure}
 

  This corkscrew visualization is too cluttered to be of much use, so instead we often juxtapose e-potential and ϕ{}-circle representations. These representations are analytical tools which encourage us to think differently about speech. Word order, instead of being a \isi{spatial arrangement} of objects, is understood as a discontinuous trajectory in excitation space. Meaning relations, instead of being connections between objects, are experiences of stable relative phases of system oscillations.

\section{Conventions and terminology}
\begin{tabularx}{\textwidth}{@{}lQ@{}}
$A$ &  order parameter of a system\\
$S$ &  surroundings\\
θ, r &  phase, radial amplitude of oscillatory component of order parameter\\
e &  excitation component of {order parameter}\\
ϕ &  relative phase\\
c-system &  concept system, written in square brackets, e.g. [coffee], [drink]\\
s-system &  syntactic system, written in curly brackets, e.g. \{−N\}, \{V\}\\
cs-system &  pair of resonating c- and s-systems, e.g. [drink]\{V\}, [coffee]\{−N\}\\
cs-systems in a stable-configuration &  {\textbar}drink coffee{\textbar}\\
Utterances &  written italicized text, e.g. \textit{Al drinks coffee}\\
+ϕ-coupling/configuration &  in-phase (attractive, proximal) coupling/configuration\\
−ϕ-coupling/configuration &  anti-phase (repulsive, distal) coupling/configuration\\
+e-coupling &  excitatory e-coupling\\
−e-coupling &  inhibitory e-coupling\\
$\widehat {{E}}$ &  e-organization operator
\end{tabularx}\is{order parameter}\is{surroundings}\is{relative phase}\is{concept system}\is{syntactic system}
