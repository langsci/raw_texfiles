\chapter{Discussion \& conclusions}\label{ch:Summary}\largerpage[2]
\section{Of words, phonemes and allophones}
The aim of \chapref{ch:Phoneme-Split} was to develop an explanatory
model of a specific type of sound change: \isi{phoneme split}, or \isi{phoneme}
genesis. Yet, in the course of developing that model, the change being
modeled itself underwent a certain kind of transformation. When \isi{phoneme}
split was first introduced in \sectref{sec:Actuation-1} it was
described as \isi{allophone} becoming \isi{phoneme}. The implication, particularly
in the case of \isi{vowel nasalization}, was that a completely new \isi{phoneme}
category had to be created, something that had not been previously
modeled. The classical representations for the \isi{synchronic} and diachronic
rules are given below.
\begin{covexample}
\label{allophonic rule}$/V/\rightarrow[\tilde{V}]/$\_\_$N$
\end{covexample}
\begin{covsubexamples}
\item \label{split-1}$/VN/>/\tilde{V}/$
\item \label{split-2}$[\tilde{V}]>/\tilde{V}/$
\end{covsubexamples}
In scenario (\ref{split-1}), the loss of the nasal context \emph{(N)}
is the precipitating event, critical to the emergence of the \isi{phoneme}.
In scenario (\ref{split-2}) the loss of the nasal context is irrelevant;
the \isi{phoneme} arises through some other mechanism.

Immediately, the \isi{actuation} problem arises – the problem of determining
why \isi{phoneme split} sometimes happens and sometimes does not (\citealt{Labov1968}).
If the conditioning context can be lost without \isi{phoneme genesis}, then
it cannot be the loss alone that creates the \isi{phoneme} (\ref{split-1}).
But if the loss of context is irrelevant and coincidental, then contextually
predictable phonemes are possible and we have no way to determine, or predict, the status of such sounds (\ref{split-2}).

The solution to this impasse suggested by the Multiple-Parse
Model is that phonemes are nothing other than hypotheses made by individual
listener/speakers about how to break up word-level units, hypotheses
that may change from moment to moment and from token to token. Once
such a hypothesis is made it acquires its own representational reality
– at least for that listener/speaker. Because \isi{allophonic} relationships only
exist as corollaries of a given phonemic analysis, they are automatically
generated under one hypothesis, and automatically missing under the
other. 

However, even under the “\isi{allophonic}” analysis, allophones never
actually surface in this model. The process that generates what linguists
would label as an \isi{allophone} does not occur at the same representational
level as the \isi{phoneme}; it occurs in the region shared between two adjacent
phonemes.\footnote{Incidentally, this reveals another hidden assumption of the generative
notation: the fact that coarticulation appears to only affect one
of the segments involved. Nasalization occurs on the vowel, but vocalization
should also occur on the nasal. This \isi{bias} is most likely based in
\isi{perception}, but articulation-wise, the \isi{allophonic} relationship may
be relatively symmetric.} It is predictable in the sense that \isi{nasality} is predictable when
the \isi{velum} is lowered. But it is meaningless to talk about bigram predictability
– the predictability of \isi{vowel nasality} from the subsequent nasal –
because the listener does not hear a sequence of phones. Under one
\isi{parsing} of the input, \isi{nasality} will be attributed to \isi{gestural}
overlap between adjacent phonemes, under another it will be attributed
to \isi{gestural} overlap within a single \isi{phoneme}. In either case it will
be entirely predictable. 

The sound change in question, therefore, does not actually involve
the generation of a new \isi{phoneme} category. If I assume that all possible
hypotheses are entertained for all ambiguous inputs, then all phonemes
exist at all times, and it is only their probabilities that might
change over time.\footnote{This does not preclude the merger of \isi{phonetic} values in the pronunciation
of two sounds that were previously distinct (e.g. the so-called \textsc{pin\slash pen}
merger in certain dialects of American English).} This reframing avoids the representational paradoxes discussed
earlier. Actuation now pertains to factors that affect the probability
distribution over the hypothesis space. Such factors are likely to
be numerous, and undoubtedly include aspects of language processing
not explored here. In the same vein, the \isi{vowel nasalization} model
is not to be taken as applicable to all types of sound change, nor
even as a model of all aspects of \isi{vowel nasalization} change. In
the next two sections some other factors are briefly discussed, along
with possible extensions of the current work.

\section{Additional implications \& future work}

The Multiple-Parse Model is a model of the internal dynamics of a
single word category in isolation. In this model the assumption is
that \isi{sub-lexical} categories are derived from words, rather than the
other way around (see, e.g. \citealp{beckman2000ontogeny}). Once
such categories arise, however, they are expected to exert influence
in the other direction (English orthography is likely to produce a
similar effect). Even without the influence of explicit \isi{phoneme} categories,
we expect word-level representations to be linked in some way that
reflects their similarity to each other. Therefore, the evolution of a given
word cannot truly occur in isolation. 

Sound change is typically taken to refer to change at the \isi{phoneme} level.
In the Multiple-Parse Model change is taken to occur at a less abstract
level: \isi{sub-lexical}, but specific to an individual word. I assume that
a generalization stage is necessary, likely requiring multiple, semi-independent
changes at the word level.\footnote{Not to mention the spread of change to all members of a speech community, which I assume is another necessary stage of change, but well beyond the scope of the present work.} The dynamics of such a model are not trivial, and require, among
other constraints, that the phoneme-to-word feedback \isi{bias} be strong
enough to allow generalization to occur across all words containing
that \isi{phoneme}, but not so strong as to prevent changes at the level
of the individual word. 

One interesting consequence of adopting the position that word categories
precede \isi{phoneme} categories, is that \isi{phonetic} regularities must begin
as \noun{states} (stored \isi{articulatory} variables), rather than \noun{processes}
(the result of combining two or more linguistic units), in the infant
learner. Processes are potentially inferred gradually, over sufficient
amounts of variable data (e.g. \citealp{goodman1997inseparability}),
but individual \noun{state} representations might persist, as \noun{process}
ones do in the simulations of the previous section. 

The opposite course of development might be expected to occur in the
domain of morphology, where explicit concatenation requires a \noun{process}
model, but \noun{state} analyses become available over time. In fact,
the “competition” between the Analysis-1 \isi{parse} and the Analysis-2 \isi{parse} bears a high degree of similarity to dual-route theories of
morphology (e.g. \citealt{caramazza1988lexical,frauenfelder1992constraining}).
Classically, transparent morphological alternations are assumed to
be rule-based, analogously to \isi{allophonic} alternations. However, it
is evident from the historical record that morphological affixations
that were once productive can fall out of use, resulting in a few
artifactual forms that are unlikely to be decomposed into their constituents
by modern speakers. Additionally, some highly frequent forms, although
transparently decomposable, may behave as though they have unique
lexical entries (e.g. \citealt{baayen1997singulars}.\footnote{See \citealt{levelt1999theory}
for a review of frequency-based storage, and \citealt{burani1987representation,baayen1993frequency}
for further discussion of factors affecting morphological storage.})
This parallelism does not seem to be coincidental, and is especially
relevant to \isi{allophonic} alternations that occur precisely at morpheme
boundaries.

Morphophonological alternations are, in fact, often taken to comprise
the best evidence of an active \isi{phonological} rule. This is because
the morphological process involved is assumed to be productive. That
is, it is assumed to be a \noun{process}. Yet, the change that led
to the \isi{phonological} alternation may only have come about due to representations
becoming more \noun{state}-like, as is implied by the behavior of
the Multiple-Parse Model. If this is on the right track, then truly
\isi{phonological}, truly productive alternations may only arise when \noun{state}
and \noun{process} representations are balanced in such a way as to
preserve this tension. Determining the necessary conditions for this
to happen presents an interesting area for future research.

\section{Types of sound change}

In the modeling of sound change, the term “\isi{phonetic} \isi{bias}” seems
to have been used as a cover term to refer to phonetically-based sound
change of more or less any kind. Thus it has been (or can be) applied
to word reduction, vowel \isi{lengthening}, \isi{vowel nasalization}, and nasal
\isi{place assimilation} (or loss), among others. However, there is no \emph{a priori}
reason to expect all phonetically-based sound change to operate in
the same way. And part of an ultimate theory of sound change
will include a taxonomy both of the source of a given change, as well
as its \isi{actuation} mechanism. 

The Multiple-Parse Model of \isi{vowel nasalization} presented in \chapref{ch:Phoneme-Split} is based on the hypothesis that coarticulatory \isi{nasalization}
is \emph{not} best analyzed as a \isi{phonetic} \isi{bias}; that is, as a constant
pressure acting in a fixed direction. Instead, the source of \isi{nasalization}
is taken to be an inherent property of motor planning involving the
temporal overlap between adjacent \isi{articulatory} gestures. Synchronically,
overlap degree is assumed to vary as a function of \isi{speaking rate},
among potentially many factors, all of them contributing to a stable
distribution with a certain degree of variance. In the implemented
model, a change in the \isi{resting activation} of a word-level category
acts to shift both the absolute durations of the \isi{articulatory} parameters,
as well as the proportion of overlap. Words become shorter, with a
higher degree of overlap, as \isi{resting activation} increases. This follows
from the assumption that activation level directly affects not only
the speed with which words are accessed and initiated, but also the
speed at which articulation unfolds. The utility of this model is
only as good as this assumption, and will need to be revised if our
understanding of the \isi{frequency} effect changes.\footnote{The correlation between \isi{speaking rate} and degree of coarticulation,
as well as the correlation between word-\isi{frequency} and degree of coarticulation,
appear to be quite robust. It is less clear, however, what the exact
mechanism is that mediates between activation level and degree of
coarticulation. Without this link, we run the risk of modeling an
epiphenomenon, rather than the phenomenon itself.} However, \isi{actuation} is achievable by any mechanism that can shift
the overlap distribution as a whole.

The Multiple-Parse Model, of course, is meant to be not just a model
of \isi{vowel nasalization}, but of all linguistic phenomena that are functionally
equivalent to \isi{vowel nasalization}. Establishing this class is not trivial,
and I will only hypothesize here that phenomena involving \isi{articulatory}
overlap, \isi{articulatory} blending, and \isi{articulatory} masking will generally
be possible to model in this way. True \isi{phonetic} biases can also be
incorporated into the general model. Consonants occurring before other
consonants (rather than vowels) can be considered to be in a perceptually
disadvantaged position. This is especially true for stops, since most
of the cues to their identity actually occur in the transitions to
a following vowel (e.g. \citealt{liberman1954role}), but likely
holds to some extent for most consonants. Articulatorily, the \isi{velum}
gesture attributed to the nasal in a word like \textit{camp} will be
overlapped to some extent not only with the preceding vowel, but also
with the following consonant. The overlap with the preceding vowel
is highly audible, while the overlap with the following stop is much
less so, due to the complete closure in the oral cavity. The stop
context, relative to a vowel context (such as in the word \textit{camo}),
can be thought of as biasing for nasal deletion (or a nasal vowel).
This can be implemented as a factor that raises the probability of
the single-segment \isi{parse}.\footnote{In fact, the word-final context modeled in \chapref{ch:Phoneme-Split}
does not constitute a homogeneous \isi{phonetic} environment. Unless the
\isi{target} word is in absolute phrase-final position it will be followed
by another word, beginning with either a consonant or a vowel. Because
the two different possibilities consist of different perceptual environments,
segment loss might only occur in the former, resulting in a type of
liaison (e.g. \citealt{Tranel1981}). There is also some evidence
to suggest that changes restricted to specific words can be attributed
to their historically higher occurrence rates in the perceptually
disadvantaged environment (e.g. \citealt{brown2012discourse}).}

Velar \isi{palatalization} was briefly discussed in \sectref{sec:Competing-targets}
as an example of gesture blending. Faster productions will result
in more overlap between consonant and vowel, which should merge the
two gestures more completely, as well as render the combined \isi{production}
shorter. Both \isi{phonetic} properties should lead to an increase in the
probability of the single-segment analysis. The many different ways
in which \isi{palatalization} can be realized in different languages (e.g.
{k\textgreater t͡ʃ}, {k\textgreater kʲ}, {kj\textgreater kʲ},
etc.) suggests a number of possible influencing factors, as well
as an inherently larger space of possible parses. One such \isi{parse} results
in a two-segment analysis, with an intermediate tongue position for
the consonantal gesture (see \figref{fig:/k=0002B2+i/}); another
results in a single-segment analysis with a complex two-\isi{target} gesture
(see \figref{fig:Palatalizationc}). Perceptual asymmetries have
been found with respect to the rate of misidentification of {[}ki{]}
sequences in noise and fast speech (as {[}ti{]} and {[t͡ʃi]},
most commonly) suggesting that \isi{phonetic} \isi{bias} plays a role in this
change (\citealt{Guion1998,Chang2001}).

In contrast, the phenomenon of vowel \isi{lengthening} (\sectref{subsec:Model-2:-Lengthening})
does not appear to be the direct result of overlap, blending, or masking.
There is, however, no consensus in the literature regarding the \isi{phonetic}
source of this effect. In fact, there is not even agreement about
whether the process is one of \isi{lengthening} before \isi{voiced} obstruents,
or shortening before \isi{voiceless} ones (\citealt{gimson1970introduction,wells1982accents}).
Of the hypotheses proposed, most have an \isi{articulatory} basis (e.g.
\citealt{belasco1958variations,delattre1962some,chen1970vowel,lisker1974explaining,Klatt1976,moreton2004realization,schwartz2010phonology}),
but auditory/perceptual accounts have been offered as well (e.g.
\citealt{lisker1957closure,javkin1977phonetic,Kluender1988}). None
of these have been firmly established empirically, and strong arguments
have been made against many of them. Without some idea of what the
mechanism for the actual increase (or decrease) in \isi{length} is, it is
not possible to produce an insightful model. Work in progress suggests,
in fact, that the apparent \isi{lengthening} effect may be epiphenomenal:
the result of partial temporal compensation, resulting from an upper
limit on the duration of \isi{voiced} obstruents \citep{MorleySmith}.
If this is correct then it suggests another type of \isi{misparsing}
that can occur when multiple sources affect the same \isi{phonetic} dimension
in roughly the same way. In the case of vowel \isi{length}, contributing
sources include phrase-final \isi{lengthening}, \isi{lengthening} due to slowed
\isi{speaking rate}, and greater \isi{length} due to an inherently longer vowel,
creating ambiguity as to how the observed duration should be attributed.\footnote{This theory requires that some type of \isi{normalization} be carried out
– even if it is just a comparison between neighboring segments. If
pure duration is the dimension of contrast, then it is hard to see
how segments could be classified as ``long'' or ``short'' unless speaking
rate, at minimum, is taken into account.}

Other kinds of change, such as transphonologization, or chain shifting,
suggest yet other potential sources, but it is beyond the scope
of this book to speculate about their exact nature. However, given
a hypothesis regarding the source of the phenomenon and the representational
level at which it acts, it is possible to create an implemented model.
Such a model may, or may not, bear much resemblance to those proposed
in this work, yet the basic questions about the relationship between
theory and model, and between model and implementation will remain
the same. 

\section{Summary \& conclusions}

Computational models allow us to run experiments with language that
are not possible in the real world, such as those at the timescale
of diachronic change. They present a powerful and useful tool for making
explicit tests of our current theories. Computational models can be
used to establish existence proofs, demonstrating that it is possible
to solve a problem in a particular way. On the flip side, modeling requires extensive simplification of the complex factors
at play in language use and comprehension and there is never any
guarantee that the simplifications have not altered the problem to
the point that the results no longer shed light on the phenomenon
of interest. Implemented models are often tailored to specific problems,
and may prove to be inconsistent with other known aspects of language.
In order to get a model to run, there are various implementational
choices that must be made, choices that may, in fact, contain hidden
theoretical assumptions. Thus, the interpretation of modeling results,
just like the interpretation of the more traditional type of experimental
results, must include serious consideration of potential confounds. 

The purpose of the present work has been to bring the theoretical
issues to the fore via explicit links between different implementational
approaches and the types of representational structures they embody.
In this way a number of representational inconsistencies, or paradoxes,
were uncovered. The more transparent of these were the cases in which
tokens were assigned two different underlying representations, or
where an explicitly separate (i.e. stored) category was also subject
to a process, giving the phenomenon a hybrid \noun{state-process}
status. In fact, there may be a paradox lurking in applying a process
(e.g. knowing that tokens should be lengthened in a particular context)
but failing to account for the effects of that process (\isi{lengthening})
when adding the produced token back to the perceptual \isi{exemplar} cloud. 

Two apparent successes of the basic iterative \isi{exemplar} model – accounting
for frequency-based \isi{lenition}, and \isi{phonetic} similarity effects – were
called into question. \sectref{subsec:Model-1:-Context-Free}
demonstrated that, depending on the specifics of how word \isi{frequency}
is represented, successive reduction of tokens does not necessarily
produce the observed negative correlation between \isi{frequency} and word
\isi{length}. Retention of fine-grained \isi{phonetic} detail (without retention
of \isi{production} context) was shown to actually disrupt predictable \isi{phonetic}
\isi{allophony}. Depending on other representational decisions, the result
was either a single variant that occurred in all contexts, multiple
variants that occurred unpredictably, or a continuously moving \isi{target}
(\sectref{sec:Context-Dependent-Iterativity}). 

Developing \isi{exemplar} models that make the right kinds of predictions
requires some force for constraining the powerful \isi{iterativity} mechanism
of the per\-cep\-tion-\isi{production} feedback loop. This is often accomplished
in practice by filtering out tokens that fall between two existing,
contrastive categories. But in the absence of contrast, something
else is required to keep the categories bounded. There seems to be
a common misconception that \isi{exemplar} models do not require underlying
representations, or targets. But models may in fact implement what
amounts functionally to a soft \isi{target}, or attractor, even if it is
not identified as such (\sectref{subsec:Soft-Targets}). Such
a \isi{target} may, in fact, be necessary to produce bounded behavior.

Furthermore, the standard assumption of an identity mapping between
\isi{perception} and \isi{production} obscures the complexity of the speech processing
problem. In fact, differences between what speakers intend to produce,
and what listeners perceive, are likely to play a large role in diachronic
change that arises from \isi{synchronic variation}, the very thing these
models are trying to explain. Nor is it the case that iterative application
of an articulation-based \isi{bias} (such as anticipatory feature spread)
can be assumed to lead to cumulativity on the acoustic side (Chapter
\ref{ch:Perception-Production}). To produce the type of gradual
increase that is desired, a change in the relative timing of articulators
may be required. The explanation as to why such a change would occur
is the answer to the \isi{actuation} question itself.

A proposal was offered in \chapref{ch:Phoneme-Split} for one
way to account for \isi{phoneme genesis} arising from \isi{allophonic} split.
The model was designed in a way that prioritized representational
consistency, capturing both change and stability, and implementing
a plausible mechanism for change at both local and global scales.
The “correct” \isi{sub-lexical} representations were not assumed,
and therefore, neither was the \isi{allophonic} rule (or \isi{production} \isi{bias}).
Instead, the equivalent of an \isi{articulatory} representation was decided
independently for each token. Feedback occurred in the dependence
of the \isi{parsing} probabilities on the values of the \isi{articulatory} parameters.
In the reported simulations there was only one choice to be made by
the speaker/listener, whether to store or generate the degree of overlap
between the two \isi{articulatory} gestures. Stability was achieved by a
general-purpose force (\isi{speaking rate}), acting bi-directionally, to
both lengthen and shorten tokens. Different stable states resulted
from different \isi{resting activation} levels, which affected the rapidity
with which the words were produced. This result hinged on two properties
of the model: the dependence of the \isi{speaking rate} effect on word duration
(longer tokens were lengthened more than shorter tokens for the same
decrease in rate), and the implementation of \isi{resting activation} as
a shift in the mean of the \isi{speaking rate} distribution. Numerous other
implementational choices are possible, but only a small fraction of
them lead to a theoretically coherent, cognitively plausible, empirically
adequate outcome. Thus, the existence proof embodied in the Multiple-Parse
Model has merit in and of itself. The results also raise the possibility
that certain consistently intractable problems in the study of change
and \isi{actuation} may be artifacts of the overt and covert assumptions
of the traditional notational system.

On the one hand, the work in this book represents relatively minor
variations on existing proposals and models: the basic \isi{exemplar} architecture
in which \isi{sub-phonemic} detail is retained; the role of word \isi{frequency}
in sound change; ambiguity in surface forms as the driver of variation;
etc. Its primary innovation may be in bringing together and explicitly
implementing those elements. Yet, the result is a radical re-conceptualization
of basic \isi{phonological} tenets. I have suggested that 1) \isi{phoneme split} is neither \isi{phoneme}
creation, nor \isi{allophone} loss, 2) neither \isi{allophonic} rules
nor phonemic inventories actually exist as traditionally described, and 3) \isi{phonological}
rules as we typically understand them may only arise under restricted
conditions, requiring morphological antecedents and a more explicit
stage of learner generalization. This conceptual shift was largely
a consequence of forcing diachronic and \isi{synchronic} representations
to match, revealing that questions about how sound categories change
are really questions about what sound categories are – how they are
mentally represented –, and that neither question can be adequately
answered without the other.
