\chapter{\label{chap:Appendix A}Model parameters: Chapters 1--4}

For each of the basic \isi{exemplar} models for which simulations were run,
the following parameter values were used:

\begin{table}[H]\footnotesize
\caption{Simulation parameter values}
\begin{tabular}{lS[table-format=1.1]cS[table-format=1.1]S[table-format=1.1]S[table-format=1.1]cc}
\lsptoprule
 & {$\varepsilon$} & {$\sigma_{\text{error}}$} & $\alpha$ & {p} & {$\beta$} & $\sigma$ & {N}\tabularnewline
\midrule
Baseline Model (\chapref{ch:The-Exemplar-Model}) & .3 & $\sigma$ & .1 & – & – & 2 & –\tabularnewline
Model 1: Context-Free (\sectref{subsec:Model-1:-Context-Free}) & .3 & $\sigma$ & .5$\sigma$ & – & – & 2 & –\tabularnewline
Model 2: Context-Dep. (Gradient) (\sectref{subsec:Phrase-Final Lengthening}) & .3 & $\sigma$ & .1 & .25 & – & 2 & –\tabularnewline
Model 3: Context-Dep. (Discrete) (\sectref{subsec:Model-3:-Categorical}) & .3 & $\sigma$ & – & .5 & – & 2 & –\tabularnewline
Soft-Target Model (\sectref{subsec:Soft-Targets}) & .3 & $\sigma$ & .1 & .25 & .6 & 2 & 50\tabularnewline
\lspbottomrule
\end{tabular}

\end{table}


\chapter{\label{chap:Appendix B}The frequency effect}

This material is supplemental to Chapters \ref{subsec:Model-1:-Context-Free}
and \ref{subsec:Word-Frequency} of the main text.

The \isi{iterative model} implies that the \isi{frequency} effect must arise in
the lifetime of the speaker, and only after they have had sufficient
exposure to a given (high \isi{frequency}) category. This may happen very
quickly. However, the less time it takes, the more opportunities there
will be for lower-\isi{frequency} categories to “catch up”. Therefore,
in order to give the best chance to the basic model, I will assume
the largest possible time period in which the effect could arise:
the age of the experimental population for which \isi{frequency} effects
are found. As the pool of participants for psychology and linguistics
experiments is most often university undergraduates, I will take
20 years to be the maximum amount of time necessary to produce a reduction
in duration comparable to what has been reported in the literature.

I don't know how many model iterations correspond to 20 years. But
I will define the number of productions during this time, for a word
of \isi{frequency} \emph{f}, as $n_{f}$, and the proportion by which it
is reduced, as $\delta_{n_{f}}$, from an initial average duration
of $\overline{d_{0}}$. This period of time will be called an epoch
(\emph{e}). 
\begin{equation}
\overline{d_{n_{f}}}=\overline{d_{0}}-\delta_{n_{f}}\overline{d_{0}}\label{eq:epoch-dur}
\end{equation}
To simplify the problem, I will consider a scenario in which there
is only a single token belonging to each category, located at the
category mean, which is replaced, each time \isi{production} occurs, by
a token reduced by a fixed proportion of the current duration. With
this simplification all categories will reduce faster, since it is
always the most reduced token that is chosen in \isi{production}. However,
since all measures are comparisons between categories of different
frequencies (rather than absolute values), this should not affect
the result. Low and high \isi{frequency} categories are also of exactly
the same size token-wise in this simplified scenario, and only update-rate
differentiates them. Equalizing low- and high -\isi{frequency} categories
in this way does affect the outcome, as we saw in \sectref{subsec:Model-1:-Context-Free},
but it advantages the basic model by ensuring that higher-\isi{frequency}
words are always shorter than lower-\isi{frequency} ones.

In the simplified scenario, each generation is exponentially more
reduced than the last. From Eq (\ref{eq:linear bias}): $x_{o(+n)}=x_{o}\left(1-\alpha\right)^{n}$,
I can derive Eq. (\ref{eq:Reduction}), which expresses the duration,
after 1 epoch, for a word category of \isi{frequency} \emph{f}, and an
initial average duration of $\overline{d_{0}}$. Rewriting Eq. (\ref{eq:linear bias})
in terms of these variables:
\begin{equation}
\overline{d_{n_{f}}}=\overline{d_{0}}(1-\alpha)^{n_{f}}
\end{equation}
Substituting in from Eq. (\ref{eq:epoch-dur}):
\begin{equation}
\overline{d_{0}}-\delta_{n_{f}}\overline{d_{0}}=\overline{d_{0}}(1-\alpha)^{n_{f}}
\end{equation}
And, 
\begin{equation}
\delta_{n_{f}}=1-(1-\alpha)^{n_{f}}\label{eq:Reduction}
\end{equation}

We don't know what the amount of reduction over 1 epoch is. But we
do have an idea of the size of the \isi{frequency} effect: word duration
as a function of \isi{frequency} (log \isi{frequency} is typically what is plotted
in order to make the \isi{frequency} distribution closer to Normal, see
e.g. \citealt{gahl2012reduce}). If I assume a linear relation between
word duration and log \isi{frequency}, then for each unit change in log
\isi{frequency}, the difference in word duration should be equal to a constant
value (\emph{b}). Thus, the predicted difference in duration between
a low \isi{frequency} and high \isi{frequency} word is related to the difference
in frequencies by the following formula: 
\begin{equation}
\frac{\Delta d_{e}}{\log(f_{L})-\log(f_{H})}=b\label{eq:log-linear}
\end{equation}
If speakers/listeners begin at birth with equal experience of all words
– meaning, none – then the differences in duration that accrue over
the course of an epoch will be due entirely to the amount of reduction
that occurs over that epoch. By the time that one epoch has passed,
the higher \isi{frequency} word of any pair will have reduced more than
its counterpart. Assuming that the two words in question are otherwise
identical, for our purposes, that they have the same original duration,
then the difference in absolute duration at that time will be given
by:
\begin{equation}
\Delta d_{e}=\delta_{n_{L}}-\delta_{n_{H}}\label{eq:duration-diff}
\end{equation}
Combining (\ref{eq:log-linear}) and (\ref{eq:duration-diff}),
\begin{equation}
\delta_{n_{L}}-\delta_{n_{H}}=b\left[\log\left(\frac{f_{L}}{f_{H}}\right)\right]
\end{equation}
Substituting in Eq. (\ref{eq:Reduction}):
\begin{equation}
1-(1-\alpha)^{n_{L}}-[1-(1-\alpha)^{n_{H}}]=b\left[\log\left(\frac{f_{L}}{f_{H}}\right)\right]
\end{equation}
Simplifying:
\begin{equation}
(1-\alpha)^{n_{H}}-(1-\alpha)^{n_{L}}=b\left[\log\left(\frac{f_{L}}{f_{H}}\right)\right]\label{eq:reduction-to-freq}
\end{equation}

The higher the \isi{frequency} of a given word, the more times it should
be produced within a given time period. And if reduction is proportional
to the log \isi{frequency}, with every \isi{production} resulting in a given amount
of reduction, then the number of productions should also be proportional
to log \isi{frequency}. 
\begin{equation}
n_{f}=\rlog(f)\label{eq:n-productions}
\end{equation}
Substituting (\ref{eq:n-productions}) into (\ref{eq:reduction-to-freq}):
\begin{equation}
(1-\alpha)^{\rlog(f_{H})}-(1-\alpha)^{\rlog(f_{L})}=b\left[\log\left(\frac{f_{L}}{f_{H}}\right)\right]\label{eq:boundary cond}
\end{equation}
Assuming that it is possible to find values for $\alpha$ and \emph{r}
that satisfy Eq. (\ref{eq:boundary cond}) for all frequencies, the
additional reduction that will occur over the lifetime of the speaker
can then be determined. 

If 1 epoch corresponds to about 20 years, then there will be about
4 over the lifetime of an individual. If I assume a constant rate
of \isi{production} for each category proportional to its \isi{frequency}, then
lifetime (\emph{E}) average reduction is given by $\delta_{E_{f}}=1-(1-\alpha)^{4n_{f}}$,
which can be rewritten as:
\begin{equation}
\delta_{E_{f}}=1-(1-\alpha)^{4\rlog(f)}
\end{equation}

With the necessary constants, I can now determine the difference
in reduction between the same two word categories after 4 epochs.
If I assume that there exists a floor beyond which words cannot reduce
further, then I will need to determine if any words are predicted
to reach floor in the lifetime of the speaker, and what effect that
will have on the behavior of the \isi{frequency} dependence – either entirely
neutralizing the duration difference between certain words, or decreasing
that difference to some extent. 

The exact predictions of the linearly biased \isi{frequency} model will
depend on a host of implementational details. As already discussed
in the text, the choice of whether lower-\isi{frequency} categories should
have proportionally fewer tokens than higher-\isi{frequency} categories
will affect the outcome. Other parameters that have the potential
to alter the outcome include whether or not each individual experience
is automatically added to memory – or only a certain minimum number,
or some average of recent experience – and how quickly older memories
\isi{decay}, being replaced by new experiences. It may be possible, if unlikely,
that at least one set of parameter values exists that will prevent
any words reaching floor within the lifetime of the speaker. However,
under any parameter settings, all words are predicted to continue
reducing over the lifetime of the speaker. This prediction is empirically
testable.

\chapter{\label{chap:Appendix C}Derivation of State Model}

This material is supplemental to \sectref{subsec:Lengthening-as-State}
of the main text.

For the Pure State Model (G), with 2-targets, each sub-category is
subject to two forces: \isi{entrenchment}, and \isi{inertia}. Under the simplifying
assumption that each sub-category can be treated as a Normal distribution
with constant variance, the \isi{equilibrium} locations of the sub-category
means can be derived in the following way. At \isi{equilibrium} the \isi{entrenchment}
force is balanced by the \isi{inertial} force due to each sub-category's
attractor. The location of the sub-category mean is the location at
which the displacement that would occur due to the \isi{entrenchment} force
is exactly counteracted by the displacement that would occur due
to the \isi{inertia} force. For the non-biased sub-category this \isi{equilibrium}
occurs under the following conditions:
\begin{equation}
\beta\left(\overline{x_{E}^{NB}}-N\right)=\varepsilon\left(\overline{x_{E}}-\overline{x_{E}^{NB}}\right)\label{eq:xNB-state}
\end{equation}
For the biased sub-category, \isi{equilibrium} occurs when:
\begin{equation}
\alpha\left(\overline{x_{E}^{B}}-L\right)=\varepsilon\left(\overline{x_{E}}-\overline{x_{E}^{B}}\right)\label{eq:xB-State}
\end{equation}
Because the \isi{entrenchment} force depends on the global mean, so too
do the two \isi{equilibrium} equations. In turn, the global mean can be
expressed as a function of the sub-category means (where the proportion
of biased tokens is given by \emph{p}): 
\begin{equation}
\overline{x_{E}}=(1-p)\overline{x_{E}^{NB}}+p\overline{x_{E}^{B}}\label{eq:weighted-means}
\end{equation}
With three equations, we can solve for the three distribution means.
Solving for $\overline{x_{E}^{NB}}$ in Eq. (\ref{eq:xNB-state}):
\begin{equation}
\overline{x_{E}^{NB}}=\frac{\beta N+\varepsilon\overline{x_{E}}}{\beta+\varepsilon}
\end{equation}
Solving for $\overline{x_{E}^{B}}$ in Eq. (\ref{eq:xB-State}):
\begin{equation}
\overline{x_{E}^{B}}=\frac{\alpha L+\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}
\end{equation}
Substituting these two values into Eq. (\ref{eq:weighted-means}):
\begin{equation}
\overline{x_{E}}=(1-p)\frac{\beta N+\varepsilon\overline{x_{E}}}{\beta+\varepsilon}+p\frac{\alpha L+\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}
\end{equation}
Solving for $\overline{x_{E}}$ as a function of \emph{p, }and collecting
terms:
\begin{equation}
\overline{x_{E}}=\frac{(1-p)\beta N}{\beta+\varepsilon}+\frac{(1-p)\varepsilon\overline{x_{E}}}{\beta+\varepsilon}+\frac{p\alpha L}{\alpha+\varepsilon}+\frac{p\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}
\end{equation}

\begin{equation}
\overline{x_{E}}-\frac{(1-p)\varepsilon\overline{x_{E}}}{\beta+\varepsilon}-\frac{p\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}=\frac{(1-p)\beta N}{\beta+\varepsilon}+\frac{p\alpha L}{\alpha+\varepsilon}
\end{equation}

\begin{equation}
\begin{aligned}[t]
&\frac{\overline{x_{E}}(\beta+\varepsilon)(\alpha+\varepsilon)-(\alpha+\varepsilon)(1-p)\varepsilon\overline{x_{E}}-(\beta+\varepsilon)p\varepsilon\overline{x_{E}}}{(\beta+\varepsilon)(\alpha+\varepsilon)}\\
= &\frac{(1-p)\beta N}{\beta+\varepsilon}+\frac{p\alpha L}{\alpha+\varepsilon}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}[t]
& \frac{\overline{x_{E}}(\beta+\varepsilon)(\alpha+\varepsilon)-(\alpha+\varepsilon)(1-p)\varepsilon\overline{x_{E}}-(\beta+\varepsilon)p\varepsilon\overline{x_{E}}}{(\beta+\varepsilon)(\alpha+\varepsilon)}\\
= & \frac{(1-p)\beta N(\alpha+\varepsilon)+p\alpha L(\beta+\varepsilon)}{(\alpha+\varepsilon)(\beta+\varepsilon)}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}[t]
& \overline{x_{E}}\left[(\beta+\varepsilon)(\alpha+\varepsilon)-(\alpha+\varepsilon)(1-p)\varepsilon-(\beta+\varepsilon)p\varepsilon\right]\\
= & (1-p)\beta N(\alpha+\varepsilon)+p\alpha L(\beta+\varepsilon)
\end{aligned}
\end{equation}

\begin{equation}
\overline{x_{E}}=\frac{(1-p)\beta N(\alpha+\varepsilon)+p\alpha L(\beta+\varepsilon)}{(\beta+\varepsilon)(\alpha+\varepsilon)-(\alpha+\varepsilon)(1-p)\varepsilon-(\beta+\varepsilon)p\varepsilon}\label{eq:Model G-eq}
\end{equation}\pagebreak

\noindent Eq. (\ref{eq:Model G-eq}) is a complex function of $\alpha,\beta,\varepsilon,N,L$,
and \emph{p}, the derivative of which is not trivially calculated.
For known values of $\alpha,\beta,\varepsilon,N$, and \emph{L} ,
$\overline{x_{E}}(p)$ can be determined exactly. The general behavior
of this function, however, can be understood via the following chain
of reasoning. 

For a given $p=p_{i}$ (for $p_{i}<1$), the \isi{equilibrium} location
of the global mean can be found using Eq. (\ref{eq:Model G-eq}).
Now imagine that \emph{p} increases from $p_{i}$ to $p_{j}$. This will
result in the global mean moving closer to the biased sub-category
(Eq. (\ref{eq:weighted-means})). A change in the global mean will
cause a change in the \isi{entrenchment} force for both sub-categories.
It will increase for the non-biased sub-category, which is now farther
from the global mean; and it will decrease in exactly the same degree
for the biased sub-category, which is now closer to the global mean.

Because \isi{inertia} does not depend on \emph{p}, the lefthand sides of
Eqs. (\ref{eq:xNB-state}) and (\ref{eq:xB-State}) will remain constant.
Thus, the non-biased sub-category will shift in the direction of the
mean – rightward – as a result of the increase in \emph{p}. The decrease
in the \isi{entrenchment} force on the biased sub-category, conversely,
will cause a shift away from the mean, and towards the attractor at
\emph{L}. This is also a rightward shift, however. The net effect
will be to perturb the sub-categories from their former \isi{equilibrium}
locations to points farther to the right, and closer to \emph{L}.
As \emph{p} increases, $\overline{x_{E}}$ will always increase (as
long as both sub-categories are located between \emph{N} and \emph{L}).

The distance between the means of the two sub-categories can also
be written as a function of \emph{p}. Once \isi{equilibrium} has been reached,
the separation can be derived from Eqs. (\ref{eq:xNB-state}) and
(\ref{eq:xB-State}):
\begin{equation}
\Delta\overline{x_{E}}\equiv\overline{x_{E}^{B}}-\overline{x_{E}^{NB}}=\frac{\alpha L+\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}-\frac{\beta N+\varepsilon\overline{x_{E}}}{\beta+\varepsilon}
\end{equation}
Collecting terms and simplifying:

\begin{equation}
=\frac{\alpha L}{\alpha+\varepsilon}-\frac{\beta N}{\beta+\varepsilon}+\frac{\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}-\frac{\varepsilon\overline{x_{E}}}{\beta+\varepsilon}
\end{equation}

\begin{equation}
=\frac{\alpha L}{\alpha+\varepsilon}-\frac{\beta N}{\beta+\varepsilon}+\varepsilon\overline{x_{E}}\left[\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}\right]
\end{equation}

\pagebreak\noindent The change in sub-category separation as a function of changing \emph{p}
is thus given by:
\begin{equation}
\frac{\partial\Delta\overline{x_{E}}}{\partial p}=\frac{\partial\overline{x_{E}}}{\partial p}\varepsilon\left[\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}\right]\label{eq:Model G-sep}
\end{equation}
In order to determine $\frac{\partial\Delta\overline{x_{E}}}{\partial p}$,
we must be able to calculate $\frac{\partial\overline{x_{E}}}{\partial p}$.
For the special case in which all forces have the same strength ($\alpha=\beta=\varepsilon$),
it is straightforward to calculate the derivative of Eq. (\ref{eq:Model G-eq}):
\begin{equation}
\overline{x_{E}}=\frac{2\alpha^{2}N+p(2\alpha^{2}L-2\alpha^{2}N)}{4\alpha^{2}-2\alpha^{2}}
\end{equation}
Collecting terms and simplifying:
\begin{equation}
\overline{x_{E}}=\frac{2\alpha^{2}[N+pL-pN]}{2\alpha^{2}[2-1]}
\end{equation}

\begin{equation}
\overline{x_{E}}=N+p(L-N)\label{eq: State-special case}
\end{equation}
This gives the expected behavior; for $p=0$, there is only the non-biased
distribution, which is stable at \emph{N}, and for $p=1$, there is
only the biased distribution, which is stable at \emph{L}. For equal
numbers of biased and non-biased variants, each sub-category stabilizes
at the same distance from its attractor, and the global mean is halfway
between the two. The change in the global category mean as a function
of \emph{p} is a positive, fixed value: $L-N$, the derivative of
(\ref{eq: State-special case}). Plugging this value for ${\partial\overline{x_{E}}}/{\partial p}$
into Eq. (\ref{eq:Model G-sep}) gives:
\begin{equation}
\frac{\partial\Delta\overline{x_{E}}}{\partial p}=(L-N)\alpha\left[\frac{1}{2\alpha}-\frac{1}{2\alpha}\right]=0\label{eq:separation-special case}
\end{equation}
Thus, while the overall category mean gets larger as \emph{p} increases,
the separation between the categories remains constant. 

In the general case, the separation between the two sub-categories
will show different behavior for different parameter values. Because
${\partial\overline{x_{E}}}/{\partial p}>0$, the sign of ${\partial\Delta\overline{x_{E}}}/{\partial p}$
depends on the $\varepsilon[{1}/({\alpha+\varepsilon})-{1}/({\beta+\varepsilon})]$
term. When $\alpha<\beta$, the separation increases with increasing
\emph{p}. This follows from the fact that ${\partial\Delta\overline{x_{E}}}/{\partial p}$
is positive only when $\varepsilon[{1}/({\alpha+\varepsilon})-{1}/({\beta+\varepsilon})]>0$.
For $\varepsilon[{1}/({\alpha+\varepsilon})-{1}/({\beta+\varepsilon})]$
to be greater than zero it must be the case that ${1}/({\alpha+\varepsilon})>{1}/({\beta+\varepsilon})$.
This, in turn, requires that $\alpha<\beta$. By the same reasoning,
the separation decreases as a function of increasing \emph{p} when
$\alpha>\beta$. Finally, the separation remains constant when $\alpha=\beta$,
because this entails that $\varepsilon[{1}/({\alpha+\varepsilon})-{1}/({\beta+\varepsilon})]=0$,
verifying the result in Eq. (\ref{eq:separation-special case}).

\chapter{\label{chap:Appendix D}Derivation of Process Model}

This material is supplemental to \sectref{subsec:Model-B:-Lengthening}
of the main text.

For the Pure Process Model, there is a single category, and all tokens
are subject to the same \isi{inertial} force, in proportion to their distance
from the single attractor at \emph{N}. Additionally, a proportion
\emph{p} of randomly selected tokens undergo a \isi{lengthening} process,
moving away from the rest of the distribution during \isi{production}. The
simplifying assumption, that each sub-distribution can be treated
as a Normal distribution with constant variance, is adopted. To derive
the model behavior I will look at the contribution of the different
forces in stages. This derivation references the stages depicted in
Figure \ref{fig:Derivation}.

First I apply the \isi{lengthening} process, at time $t$, to tokens drawn
from a distribution with a global mean of $\overline{x_{t}}$. These
tokens are simultaneously subjected to an \isi{inertial} force. Eq. (\ref{eq:Model-B B-Prime})
gives the mean of the biased sub-distribution at time \emph{t,}
\begin{equation}
\overline{x_{t}^{B}}^{\prime}=\overline{x_{t}}(1+\alpha)+\beta(N-\overline{x_{t}})\label{eq:Model-B B-Prime}
\end{equation}
and Eq. (\ref{eq:Model-B NB-Prime}) gives the means of the non-biased
sub-distribution at time \emph{t}.
\begin{equation}
\overline{x_{t}^{NB}}^{\prime}=\overline{x_{t}}+\beta(N-\overline{x_{t}})\label{eq:Model-B NB-Prime}
\end{equation}
On average, a proportion \emph{p} of the distribution will be lengthened,
thus the location of the global mean, after \isi{lengthening} and \isi{inertia}
apply, can be expressed as
\begin{equation}
\overline{x_{t}}^{\prime}=(1-p)\overline{x_{t}^{NB}}^{\prime}+p\overline{x_{t}^{B}}^{\prime}\label{eq:Model-B weight mean}
\end{equation}
Entrenchment must also be applied in order to determine the final
outcome, but \isi{entrenchment} does not affect the location of the global
mean, only the locations of the sub-distribution means, and their
separation. To see this, I can compare the global mean before and
after \isi{entrenchment} applies. After \isi{entrenchment}, the means of each
sub-distribution are given by:

\begin{equation}
\overline{x_{t}^{B}}^{\prime\prime}=\overline{x_{t}^{B}}^{\prime}-\varepsilon\left(\overline{x_{t}}^{\prime}-\overline{x_{t}^{B}}^{\prime}\right)
\end{equation}
\begin{equation}
\overline{x_{t}^{NB}}^{\prime\prime}=\overline{x_{t}^{NB}}^{\prime}-\varepsilon\left(\overline{x_{t}}^{\prime}-\overline{x_{t}^{NB}}^{\prime}\right)
\end{equation}
Substituting into Eq. (\ref{eq:Model-B weight mean}), gives
\begin{equation}
\overline{x}_{t}^{''}=(1-p)\left[\overline{x_{t}^{NB}}^{\prime}-\varepsilon\left(\overline{x_{t}}^{\prime}-\overline{x_{t}^{NB}}^{\prime}\right)\right]+p\left[\overline{x_{t}^{B}}^{\prime}-\varepsilon\left(\overline{x}_{t}^{\prime}-\overline{x_{t}^{B}}^{\prime}\right)\right]
\end{equation}
Simplifying and collecting terms:
\begin{equation}
=\overline{x_{t}}^{\prime}-\varepsilon(1-p)\left(\overline{x_{t}}^{\prime}-\overline{x_{t}^{NB}}^{\prime}\right)-p\varepsilon\left(\overline{x_{t}}^{\prime}-\overline{x_{t}^{B}}^{\prime}\right)
\end{equation}

\begin{equation}
=\overline{x_{t}}^{\prime}+\varepsilon(1-p)\overline{x_{t}^{NB}}^{\prime}-\left[\varepsilon(1-p)+p\varepsilon\right]\overline{x_{t}}^{\prime}+p\varepsilon\overline{x_{t}^{B}}^{\prime}
\end{equation}

\begin{equation}
=\overline{x_{t}}^{\prime}-\varepsilon\overline{x_{t}}^{\prime}+\varepsilon\left[(1-p)\overline{x_{t}^{NB}}^{\prime}+p\overline{x_{t}^{B}}^{\prime}\right]
\end{equation}
The term $(1-p)\overline{x_{t}^{NB}}^{\prime}+p\overline{x_{t}^{B}}^{\prime}$
is equivalent to $\overline{x_{t}}^{\prime}$ by Eq. (\ref{eq:Model-B weight mean}).
Therefore
\begin{equation}
\overline{x_{t}}^{\prime\prime}=\overline{x_{t}}^{\prime}-\varepsilon\overline{x_{t}}^{\prime}+\varepsilon\overline{x_{t}}^{\prime}=\overline{x_{t}}^{\prime}
\end{equation}
Because it does not depend on \isi{entrenchment}, the global mean at \isi{equilibrium}
can be determined directly from (\ref{eq:Model-B B-Prime}) and (\ref{eq:Model-B NB-Prime}).
Equilibrium occurs when the two sub-distributions are also at \isi{equilibrium},
and the global mean stops changing: $\overline{x_{E}}=\overline{x_{E}}^{\prime}$,
$\overline{x_{E}^{NB}}^{\prime}=\overline{x_{E}^{NB}}$, and $\overline{x_{E}^{B}}^{\prime}=\overline{x_{E}^{B}}$.
Therefore,
\begin{equation}
\overline{x_{E}}=(1-p)\overline{x_{E}^{NB}}^{\prime}+p\overline{x_{E}^{B}}^{\prime}
\end{equation}
Substituting in Eqs. (\ref{eq:Model-B B-Prime}) and (\ref{eq:Model-B NB-Prime}):
\begin{equation}
\overline{x_{E}}=[\overline{x_{E}}+\beta(N-\overline{x_{E}})]-p[\overline{x_{E}}+\beta(N-\overline{x_{E}})]+p[\overline{x_{E}}+\overline{x_{E}}\alpha+\beta(N-\overline{x_{E}})]
\end{equation}
Simplifying and collecting terms:
\begin{equation}
\overline{x_{E}}=\overline{x_{E}}+\beta(N-\overline{x_{E}})+p\overline{x_{E}}\alpha-p[\overline{x_{E}}+\beta(N-\overline{x_{E}})]+p[\overline{x_{E}}-\beta(N-\overline{x_{E}})]
\end{equation}

\begin{equation}
\overline{x_{E}}=\overline{x_{E}}+\beta(N-\overline{x_{E}})+p\overline{x_{E}}\alpha
\end{equation}

\begin{equation}
\overline{x_{E}}=\overline{x_{E}}(1-\beta+p\alpha)+\beta N
\end{equation}

\begin{equation}
\overline{x_{E}}(1-1+\beta-p\alpha)=\beta N
\end{equation}

\begin{equation}
\overline{x_{E}}=\frac{\beta N}{\beta-p\alpha}\label{eq:equ-mean-state}
\end{equation}
For the case when $p\alpha<\beta$, the denominator in (\ref{eq:equ-mean-state})
is positive. As \emph{p} increases (but $p\alpha$ remains smaller
than $\beta$), the denominator decreases, and the global mean increases.
As $p\alpha$ approaches $\beta$, the global mean goes to infinity;
\isi{lengthening} is unbounded. For $p\alpha>\beta$ the only stable point
is negative, and thus there is no well-defined \isi{equilibrium}. The \noun{process}
model is thus only stable if the \isi{lengthening} strength is not too great,
and the percentage of biasing contexts is not too large. 

To calculate the dependence of the sub-distribution separation on
\emph{p,} the effect of \isi{entrenchment} must be included. The \isi{equilibrium}
separation is defined as: 
\begin{equation}
\Delta\overline{x_{E}}^{\prime\prime}\equiv\overline{x_{E}^{B}}^{\prime\prime}-\overline{x_{E}^{NB}}^{\prime\prime}
\end{equation}
And 
\begin{equation}
\overline{x_{E}^{B}}^{\prime\prime}=\overline{x_{E}^{B}}^{\prime}+\varepsilon\left(\overline{x_{E}}^{\prime}-\overline{x_{E}^{B}}^{\prime}\right)
\end{equation}
\begin{equation}
\overline{x_{E}^{NB}}^{\prime\prime}=\overline{x_{E}^{NB}}^{\prime}+\varepsilon\left(\overline{x_{E}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}\right)
\end{equation}
Therefore, 
\begin{equation}
\overline{x_{E}^{B}}^{\prime\prime}-\overline{x_{E}^{NB}}^{\prime\prime}=\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}+\varepsilon\left(\overline{x_{E}}^{\prime}-\overline{x_{E}^{B}}^{\prime}\right)-\varepsilon\left(\overline{x_{E}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}\right)
\end{equation}
Collecting terms:
\begin{equation}
=\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}-\varepsilon\left(\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}\right)
\end{equation}
and
\begin{equation}
\Delta\overline{x_{E}}^{\prime\prime}=(1-\varepsilon)\left(\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}\right)
\end{equation}
The observed separation at \isi{equilibrium} depends on the separation due
to prior model forces. From Eqs. (\ref{eq:Model-B B-Prime}) and (\ref{eq:Model-B NB-Prime}), 
\begin{equation}
\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}=\overline{x_{E}}(1+\alpha)+\beta(N-\overline{x_{E}})-[\overline{x_{E}}+\beta(N-\overline{x_{E}})]
\end{equation}
This reduces to $\alpha\overline{x_{E}}$. Note that this is exactly
the amount that biased tokens are shifted away from the mean at \isi{equilibrium}.
Because this is a \noun{process} model, the separation created by
the \isi{lengthening} \isi{bias} only exists transiently, and it is not possible
for any specific subset of tokens to continue to increase their separation
from the rest of the distribution. Therefore, the prior separation
between the sub-distribution means is always given by the \isi{lengthening}
\isi{bias} applied to that mean. And the total separation, by 
\begin{equation}
\Delta\overline{x_{E}}=(1-\epsilon)(\alpha\overline{x_{E}}).\label{eq:Cat Sep-1}
\end{equation}
In the stable parameter range, where $\overline{x_{E}}$ increases
as \emph{p} increases, the separation of the sub-distributions also
increases, but more slowly, by a factor of $\alpha(1-\epsilon)$.

\chapter{\label{chap:Appendix E}Nasalization model parameters}

The following parameters were identical for the two models:
\begin{itemize}
\item The \isi{entrenchment} strength is set to $\varepsilon=0.2$
\item The \isi{production} error on each \isi{articulatory} dimension is drawn from
the distribution $\mathcal{\mathscr{N}}\left(0,0.25\sigma_{x^{Z}}\right)$,
where $\sigma_{x^{Z}}$ indicates the standard deviation of the current
distribution of stored tokens on dimension $x^{Z}$
\item Speaking Rate:
\begin{itemize}
\item Expansion force (\emph{E}) is a random variable distributed according
to $\mathcal{\mathscr{N}\left(\mathrm{0,0.25}\right)}$. 
\item The \isi{speaking rate} transformation lengthens or shortens a given duration
parameter, according to the following dependence on \emph{E:}
\begin{equation}
x_{i}^{O^{\prime}}=\frac{2x_{i}^{O}}{(1+e^{k_{O}E})}\label{eq:Speaking rate transform-1}
\end{equation}
\begin{equation}
x_{i}^{V^{\prime}}=\frac{2x_{i}^{V}}{(1+e^{-k_{V}E})}\label{eq:Speaking rate transform-1-1}
\end{equation}
\begin{equation}
x_{i}^{N^\prime}=\frac{2x_{i}^{N}}{(1+e^{-k_{N}E})}\label{eq:Speaking rate transform-1-1-1}
\end{equation}
\end{itemize}
For these simulations all gestures are set to the same elasticity
($k_{O}=k_{N}=k_{V}=1$). 
\item Model outputs are reported after 10,000 iterations
\item $x_{i}^{O}$ is never allowed to fall below 0, or to exceed the shorter
of the two values $(x_{i}^{N},x_{i}^{V})$
\item The duration of $x_{i}^{V}$ is never allowed to fall below 50 ms,
or to exceed 600 ms
\item The duration of $x_{i}^{N}$ is never allowed to fall below 25 ms,
or to exceed 500 ms
\end{itemize}

\section{No-Phoneme Model}
The \isi{fluency} attractor affects overlap duration according to the following
formula:
\begin{equation}
x_{i}^{O^{\prime}}=x_{i}^{O}+\beta(T-x_{i}^{O})\label{eq:Frequency attractor-1}
\end{equation}
The \isi{target} overlap duration for these simulations is set at $T=x_{i}^{N}$.
$\beta$ parameterizes \isi{frequency} on a scale between 0 and 1. 

\section{Multiple-Parse Model}
\begin{itemize}
\item Resting activation acts as a perturbation to the expansion force,
\emph{E}. The mean of the expansion function is shifted ${1}/{4}$
of a standard deviation for each unit of \emph{f}, where \emph{f} parameterizes
\isi{frequency}:
\end{itemize}
\[
\overline{E}^{\prime}=\overline{E}-f(0.25\sigma_{E})
\]

\begin{itemize}
\item The overlap duration for Analysis 2 tokens is a random variable distributed
according to $\mathcal{\mathscr{N}}\left(0.25\overline{x^{N}},\sigma_{x^{N}}\right)$
\item The probability of Analysis 1 is given by:
\begin{equation}
P(a=1)=Ae^{-b(1-Q)}-C\label{eq:segmentation-1-1}
\end{equation}
where $Q={x_{i}^{O}}/{x_{i}^{N}}$\emph{ }. For all simulations,
the constants are set to: $A=1$, $b=2$, and $C=0$.
\end{itemize}
