\documentclass[output=paper,
modfonts
]{LSP/langsci}


%\input{localpackages.tex}
%\input{localcommands.tex}


\ChapterDOI{10.5281/zenodo.495447}

\title{How to wake up irregular (and speechless)}
\author{Charles Yang\affiliation{University of Pennsylvania}}



% \chapterDOI{} %will be filled in at production
% \epigram{}


\abstract{%
I suggest that morphological defectiveness arises when the learner fails to discover a productive/default process in a morphological category. The detection of productivity, or lack thereof, can be accomplished by the Tolerance Principle, a simple mathematical model of language learning and generalization. In this paper, I show that the absence of \emph{*amn’t}, the negative contracted form of \emph{am}, in most English dialects can be predicted on purely numerical basis. Implications for language acquisition, variation, and change are also discussed. 
}

\begin{document}
\maketitle

\section{From Irregular Verbs to  Productivity}
In my first \isi{linguistics} talk, which was also my job interview at Yale,
I proposed that \ili{English} irregular\is{irregularity} \isi{past tense} is not learned by forming
associations between the \isi{stem} and the inflected form,  contrary to the
dominant view in the psychological study of language
\citep{Rumelhart1986a, Pinker1999}. Rather, irregular \isi{past tense} is
generated by \textit{morpholexical} rules. These rules do not generalize beyond
a fixed list but are rules nevertheless, in the sense that they take
the \isi{stem} (e.g., \textit{think}) as the input and generate an output
(e.g., \textit{thought}), the inflection,\is{inflection} via a
computational process of structural change (e.g.,
``Rime~$\rightarrow$~ /ɔt/'').   I was approaching the
problem  as a computer scientist: rules are most 
naturally  realized as a list of if-then statements, 
for regulars and irregulars\is{irregularity}
alike, which turns out to be the approach taken 
throughout the 
history of \isi{linguistics} 
\citep{Bloch1947, SPE, Halle1993} including Steve's own work
(\citeyear{Anderson1973a, Anderson1992}). There is in fact
developmental evidence for the rule-based approach when I reanalyzed 
the \isi{past tense} \isi{acquisition} data purportedly confirming
the associationist account \citep{KLNL}.

I supposed Steve was at least somewhat persuaded by the argument; a
few months later I got the job.  But he did   
wonder aloud after the talk, with a quizzical frown-cum-smile that
only he can manage: ``But how does  a rule wake up in the morning and
decide to be irregular?''\is{irregularity}  

Indeed. Since words do not wear tags of (ir)\isi{regularity}, any
morphological\is{morphology} theory that recognizes \isi{regularity} and ir\isi{regularity},
which is pretty much everything on the
market, must say something about how a rule or process wakes up to be
irregular.\is{irregularity} In fact, 
theories that reject  such a categorical distinction
\citep[e.g.,][]{Hay2003, McClelland2002}  ought to be off
market. Children's morphological \isi{productivity} is strongly discrete; see
\citealt[][Chapter 2]{POP}  for a cross-linguistic review. Their errors
are almost exclusively over-regularizations of productive rules. This
is quite well known thanks to the \isi{past tense} debate: for example, the
\isi{past tense} of \textit{hold}  sometimes surfaces as \textit{holded}, with the ``-d'' rule
\citep{Marcus1992}.   What is not widely known and even
less appreciated is the near total absence of over-\textit{irregularization}\is{irregularity} errors, despite frequent anecdotes to the
contrary (e.g., \textit{bite-bote}, \textit{wipe-wope}, \textit{think-thunk},
etc.; \citealt{Bowerman1982, Bybee1985, Pinker1999}). These errors
are sufficiently rare, occurring in  about 0.2\% of \ili{English}-learning children's
\isi{past tense} use,  that \citet{Xu1995} dub them ``weird \isi{past tense}
errors''. Not  a single instance of \textit{bote},
\textit{wope}, \textit{thunk}, or many conceivable \isi{analogical} patterns
can be found in the millions of child \ili{English} words in the public
domain \citep{CHILDES}. The distinction between regular and irregular\is{irregularity}
rules was in fact observed in Berko's \citeyearpar{Berko1958} original Wug test. While  children 
were quite happy to add ``-d'' to novel verbs such as \textit{rick} and
\textit{spow}, only one out of eighty six subjects irregularized \textit{bing} and
\textit{gling}, although adults are often prone to form irregular
analogies\is{analogical} in an experimental setting.\footnote{This suggests
  that the Wug test and similar methods such as rating have
 task-specific complications and should not be taken as a direct
 reflection of an   individual's morphological\is{morphology} knowledge; see
 \citealt{Schutze2005} and \citealt{POP} for   discussion.}  

So  Steve's question sent me on
a long quest. To maintain that both regulars and irregulars\is{irregularity} are
computed by rules, I needed a story of how children separate out productive
and  unproductive rules so precisely and effortlessly.
Although a solution was worked 
out shortly after \citep{Yang2002}, it took me many years 
 to fully recognize the \isi{scope} of the \isi{productivity} problem -- one
of the ``central mysteries'' in \isi{morphology}
\citep[35]{Aronoff1976} -- and the challenges it poses. 

At a
first glance, it 
doesn't seem difficult to give an answer for  \ili{English} \isi{past tense}. The
rule ``add -d'' covers most verb types in the language and can thus be
deemed regular,  
as   ``statistical predominance'' has always been regarded as the hallmark of
the default  \citep[e.g.,][14]{Nida1949}. But this is surely
too simplistic when crosslinguistic and psychological factors
are taken into account.  More 
concretely, at least four empirical problems, each of which is
illustrated with a familiar example in \reff{four},  fall under the
\isi{productivity} problem. 
\begin{exe} \ex \label{four}
\begin{xlist} 
\ex \ili{English} \isi{past tense}: That a default rule is learned abruptly and
results in overregularization,  after 
a protracted stage of rote memorization (\citealt{Marcus1992, KLNL}). 
\ex \ili{English} stress:\is{stress} That the grammar of  \ili{English} \isi{stress} \citep{SPE,
  Hayes1982, Halle1987}  is not trochaic with a list of lexical
exceptions\is{exception} despite an overwhelming majority of \ili{English}
words bearing \isi{stress} on the first syllable  \citep{Cutler1987,
  LY2013}.  
\ex German noun plurals: That a suffix\is{suffixation} (``-s'') can be the productive
default despite coverage of fewer nouns than any of its four
competitors
 \citep{Clahsen1992, Wiese1996}. 
\ex \ili{Russian} gaps: That morphological categories needn't and sometimes
do not have a default, as illustrated by the missing inflections of
certain \ili{Russian} verbs in the 1st person singular non-past \citep{Halle1973a}. 
\end{xlist} \end{exe}


In \citet{POP}, I propose a model of \isi{productivity}, the 
\isi{Tolerance Principle}, which provides a unified solution for 
the problems in \reff{four}, as well as 
similar problems that involve inductive learning in phonology,\is{phonology} syntax,
and language change. In this paper, I revisit Steve's question which,
in a significant way, drove this project forward. My focus is on 
a topic that has featured prominently in Steve's recent research:
morphological gaps and the nature of \isi{defectiveness} in word formation
\citep[e.g.,][]{Anderson2008, 
  Anderson2010b}.   

\section{The Tolerance Principle}\label{sec:yang:2}
The development of the \isi{Tolerance Principle} started as a purely formal
conjecture: How 
would one represent a rule ($R$) and the
exceptions\is{exception} of that rule (e.g., a set of words $w_1$, $w_2$, ...,
$w_n$)? If one is committed to a mechanistic account of the
matter -- like a computer programmer, for instance -- perhaps
the only way to encode rules and exceptions\is{exception} is 
through a set of conditional statements: 
\begin{exe}\label{elsewhere} \ex

{ If} $w=w_1$ { Then} ...

{ If} $w=w_2$ { Then} ...

...

{ If} $w=w_e$ { Then} ...

Apply $R$

\end{exe}
\noindent This of course immediately recalls the Elsewhere Condition,
ever present in \isi{linguistics} since P{\={a}}\d{n}ini
\citep{Anderson1969,  Aronoff1976, Kiparsky1973b, Halle1993}.
In particular,  the data structure in \reff{elsewhere} entails that in
 order for a (productive) rule to apply to a word, the system must
 scan through a list  to ensure that it is \textit{not} one of the
 exceptions\is{exception} ($w_1$, $w_2$, ...,
$w_e$).

There is something perverse about \reff{elsewhere}. For
example, to produce \textit{walked}, one  must scan through the irregular
verbs to make sure that \textit{walk} is not found on the list. But a
moment of reflection suggests that the Elsewhere Condition makes perfect
sense. The alternative to listing the irregulars\is{irregularity} would have to be
listing the regulars. One can imagine assigning each regular verb a
flag,  which immediately triggers the application of the ``add -d'' rule. But
that would imply that the morphological status of  \textit{every} word
 must be committed to special memory;  the irregulars\is{irregularity} as well, since
 they are by  definition  unpredictable.  Perhaps even more
 surprisingly, there is broad 
 behavioral  support for  the 
irregulars-first-regulars-later\is{irregularity} representation of rules; see
\citealt[][Chapter 3]{POP} for review. The psycholinguistic evidence
comes from real-time processing of words and morphology.\is{morphology} When 
irregulars\is{irregularity} and regulars are suitably matched for various factors
(e.g., \isi{stem} and surface frequency)\is{frequency} that affect the speed of
processing, irregulars\is{irregularity} are recognized and produced significantly \emph{faster} than regulars -- which is consistent with the 
algorithmic interpretation of the Elsewhere Condition as a
computational model of 
language processing. 

From \reff{elsewhere}, then, we can develop  an empirically motivated
cost-benefit calculus for the price of exceptions.\is{exception}  Specifically, 
words that fall under a productive rule must ``wait'' for the
exceptions\is{exception} to be processed first: the
more exceptions\is{exception} there are, the longer the rule will have to wait.
Under very general assumptions about word frequencies,\is{frequency} we can prove:

\begin{exe} \ex \label{TP}
\textit{Tolerance Principle} 

Suppose a rule  $R$ is applicable to $N$ items in a learner's
vocabulary, of which $e$ are exceptions\is{exception} that do not follow $R$. The
sufficient and necessary condition for the \isi{productivity} of $R$ is:  

\[ e \leq \theta_N\,\,\,\mbox{where}\,\,\, \theta_N := \frac{N}{\ln N} \]

\end{exe}
The \isi{Tolerance Principle}  requires two \isi{input} values,
$N$ and $e$, and returns the \isi{productivity} status of a rule.  Its
application  requires a well-defined rule such that $N$ and $e$ can be
measured, by the child learner during \isi{language acquisition} and by the
researcher when studying linguistic \isi{productivity}. To learn the 
structural description of a rule, typically in the form of ``X
$\longrightarrow$ Y'', one will need to invoke  inductive learning
models such as  those studied in artificial intelligence, cognitive
science, and indeed \isi{linguistics} \citep[e.g.,][]{LSLT}. Almost all inductive
models  form generalizations over specific learning instances and try
to discover the  shared characteristics of individual elements associated with a shared
pattern.  For example, suppose two good baseball hitters can be
described with feature\is{features} bundles  [+red cap, +black shirt, +long socks]
and [+red cap, +black shirt, +short socks]. The rule 
``[+red cap, +black shirt] 
$\longrightarrow$ good hitter'' will follow, as the shared \isi{features}
(cap, shirt) are retained and the conflicting feature (sock) is
neutralized. Obviously, the application of inductive learning 
 must  encode the structural constraints\is{constraint} on
the human language faculty and other cognitive systems  implicated
in \isi{language acquisition} \citep{ASPECTS}. While it is clear that the
properties  of human language are far from arbitrary, it
remains an open question to what extent they reflect a
unique system of \isi{Universal Grammar} (e.g., \isi{Merge};
\citealt{Berwick2016}) or  general principles of  cognition and 
learning that  show continuities with other domains and species;
see \citealt{Yang2004, Chomsky2005, Yang2017} for general discussions.  





Table \ref{t:tolerance} provides some sample values of $N$ and the
associate threshold value $\theta_N$.  

\begin{table}[h!]
\begin{center}
\caption{The tolerance threshold for rules of varying sizes}
\label{t:tolerance} 
\begin{tabular}{rrc}
\lsptoprule
$N$    & $\theta_{N}$  & \%\\  
\midrule
    10 & 4     & 40.0\\
    20 & 7     & 35.0\\
    50 & 13    & 26.0\\
   100 & 22    & 22.0 \\
   200 & 38    & 19.0 \\
   500 & 80    & 16.0\\
 1,000 & 145   & 14.5\\
 5,000 & 587   & 11.7\\ 
 \lspbottomrule
\end{tabular}
\end{center}
\end{table}

The apparently, and perhaps  surprisingly, low threshold has
interesting implications for \isi{language acquisition}. Most 
importantly, it suggests that all things being equal,  smaller
vocabulary (smaller values of $N$) can tolerate relatively more
exceptions.\is{exception} That is, productive rules are \textit{more} detectable
when  learners have \textit{less} experience with a language, especially
when they have a small \isi{lexicon} that only consists of relatively high
\isi{frequency} words.  This may  explain children's
remarkably early command of the main ingredients of language
\citep{Yang2013}, as well as the reason  why maturational constraints\is{constraint}
may aid rather than hamper \isi{language acquisition} \citep{Newport1990};
see \citealt[][Chapter 7]{POP} for extensive discussion. 

The \isi{Tolerance Principle} has
proved highly effective. In  \citet{POP}, it was applied almost 100
times, making accurate \isi{productivity} predictions across many languages and
domains using only \isi{corpus} statistics. Furthermore,
experimental studies in collaboration with Kathryn Schuler
and Elissa Newport have found near categorical confirmation for the
\isi{Tolerance Principle} in artificial language studies with young children
\citep{Schuler2016}. 
Some of these robust results are unexpected. 
This is because the \isi{derivation} in  \reff{TP} makes use of
numerical approximations that only hold when $N$ is large. In
the empirical case studies, however, the value of $N$ is often very
modest (e.g., 8 or 9 in the artificial language studies) as it refers to the number 
 distinct lexical items in a morphological category. 
For the moment, I put these questions aside and 
return to  the problems in \reff{four}: the low threshold of exceptions\is{exception} provides just the right approach to the
\isi{productivity} problem across languages.  

Consider first the \isi{acquisition} of \ili{English} \isi{past tense}.  Through an
inductive process illustrated earlier, 
the phonological\is{phonology} diversity of the regulars will quickly
establish that any verb can take the ``{-d}'' suffix.\is{suffixation} Its \isi{productivity}
will be determined by the total number of verbs ($N$) and the
irregulars\is{irregularity} ($e$) in the learner's vocabulary.  The same consideration
applies to the irregular\is{irregularity} rules. For instance, the seven irregular verbs
\textit{bring}, \textit{buy}, \textit{catch}, \textit{fight}, \textit{seek}, \textit{teach}, and \textit{think}  all follow the \isi{stem} change ``ought''. Such a mixed bag of
phonological shapes will also yield an all-inclusive rule, as
shown by computational implementations \citep{Yip1998a}. But the ``ought''
 rule will fare terribly. It only works for 
seven items, with hundreds and thousands of exceptions,\is{exception}  far
exceeding the tolerance threshold. As a result, the rule 
will be relegated to
\isi{lexicalization}.  Other irregular\is{irregularity} patterns can be analyzed similarly:
as I show elsewhere \citep[][Chapter 4]{POP}, they all
wake up nonproductive in the morning, thereby accounting for the near
total absence of over-irregularization\is{irregularity} errors \citep{Xu1995}.

\newpage 
Following the same logic, we can see that the emergence of the ``-d''
rule will require a long period of gestation. Although children  can quickly
induce its structural description -- using no more than a few dozen verbs
\citep[again][]{Yip1998a} -- their early verbs will contain 
many irregulars.\is{irregularity}  Of the top
200 verbs inflected in the \isi{past tense} \citep{CHILDES}, 76 are
irregulars.\is{irregularity} Because  $\theta_{200}$ is only 37, 
it follows that children who know some 200 most frequent verbs cannot
recognize  the \isi{productivity} of ``-d'' despite its ``statistical
predominance''. During this period of time, even though verbs may be
produced with the ``-d'' suffix, they are in 
effect irregular:\is{irregularity} the suffix\is{suffixation} has no \isi{productivity} and does not
extend beyond a fixed list rote-learned from the input.\is{input} The telltale
evidence for \isi{productivity} comes from the first attested overregularization
errors \citep{Marcus1992}. For individual learners with reasonably
complete records of language development,  the \isi{Tolerance Principle} can
help us understand  why the regular rule becomes productive at that
exact moment it did. For example, ``Adam'', the poster child of \ili{English} past
tense research \citep{Pinker1999}, produced his first
over-regularization error at 2;11: ``What dat feeled like?'' In the
transcript of almost a year prior to that point, not a single
irregular\is{irregularity} verb \isi{past tense} was used incorrectly. It must be 
that by 2;11, Adam had acquired a sufficiently large number of
regulars to overwhelm the irregulars.\is{irregularity} To test this prediction, I
extracted every verb \isi{stem} in Adam's speech until 2;11. There are $N =
300$ verbs in all, out of which $e = 57$ are irregulars.\is{irregularity} This is very
close to the predicted $\theta_{300} = 53$, and the small discrepancy
may be due to the under-sampling of the regulars,  which tend to be
less frequent and thus  more likely missing  from the corpus.\is{corpus} The critical
point to note here is that Adam apparently needed 
a filibuster-proof majority of regular verbs to acquire the ``-d''
rule:  this is strongly consistent with the predictions of the
\isi{Tolerance Principle} as illustrated in Table \ref{t:tolerance}. 

The problems of \ili{English} \isi{stress} and German plurals in \reff{four} are
similar. In the \ili{English} case,  the assignment of \isi{stress} to the first
syllable may be transiently productive when the child has a very small
vocabulary \citep{Kehoe1997a, LY2013}. But it will fail to clear the tolerance
threshold when the vocabulary reaches a modest size:  even 85\% of
coverage is not sufficient for larger values of $N$ (e.g., 5000;  Table 
\ref{t:tolerance}). In the German case, none of the five \isi{plural}
suffixes\is{suffixation} can tolerate the other four as exceptions,\is{exception} not least the
``-s'' suffix,  which covers the smallest set. In both cases, the
learner will carry out \textit{recursive} 
applications of the \isi{Tolerance Principle}. When  no rule emerges as
productive over the totality of a lexical set, the learner will
subdivide it along some linguistic dimension, presumably making use of 
constraints\is{constraint} on language and other cognitive systems,  and attempt
to discover productive rules within. Such a move, while more complex, 
is always more likely to yield productive rules: again, smaller $N$'s
that result from subdividing the \isi{lexicon} tolerate a relatively higher
proportion of exceptions\is{exception} than larger $N$'s.  For the \isi{acquisition} of
stress,\is{stress} dividing words into nouns and verbs and taking the syllabic\is{syllable}
weight into account, as prescribed by all modern metrical theories, 
lead to productive rules of \isi{stress} assignment, an outcome that accords
well with both structural and behavioral findings 
\citep{Ladefoged1968, Baker1976, Kelly1992, Guion2003}. The study by
\citet{LY2013} also reveals important differences between theories of
\isi{stress} in their statistical
coverage  of the \ili{English} lexicon:\is{lexicon} while all theories handle a great
majority of \ili{English} words, only the theory of \citealt{Halle1998}
clears the tolerance threshold of exceptions.\is{exception}  For the \isi{acquisition} of German
plurals, the move is to subdivide the nouns by grammatical gender as
well as phonological conditions, similar to certain theoretical
approaches to German morphology \citep[e.g.,][]{Wiese1996}. The \textit{-s} suffix indeed survives as the default because the other suffixes\is{suffixation}
are productive with more restrictive domains of nouns.  

The emergence of  morphological gaps is a logical outcome of the Tolerance
Principle, which constitutes the topic of the present study. When a
rule wakes up irregular,\is{irregularity} the learner must learn, from positive
evidence, the inflected form for each word. Failing to hear a particular inflected form  
will render the speaker  speechless when that
form is needed. 


\section{Why Am+Not ≠ Amn't?}
\subsection{Conditions on Gaps}
Many current theories of \isi{morphology}, including \isi{Distributed Morphology}
\citep[for which see][]{Halle1993}, Optimality Theory \citep{OT}, Dual-Route Morphology
\citep{Pinker1999, Clahsen1999}, Network Morphology \citep{Brown2012},
Paradigm\is{paradigm} Function Morphology \citep{Stump2001b} and others, invoke the
notion of competition, which by design results in a default or winning
form (at least in the inflectional\is{inflection} domain). This architectural feature
of the theories is
inherently incompatible with the  existence of morphological gaps,
which are  quite widespread across languages \citep{Baerman2010a}. 
The Tolerance based approach, while also 
competition based  (through the Elsewhere
Condition), does not stipulate a default or productive rule as a 
primitive in the theoretical machinery. Rather,  the presence or absence
of a productive rule is the outcome of 
\isi{language acquisition}, to be determined by children
through the composition of the linguistic data. 
 More specifically, the Tolerance
Principle provides the following corollary \citep[142]{POP}:  
\begin{exe} \ex \label{gaptheorem} \textit{Conditions on gaps}

Consider a morphological category $C$ with $S$ alternations,\is{alternation} each
affecting $N_i$ lexical items ($1\leq i \leq S$),  and $\sum_i N_i =
N$. Gaps arise in $C$ only if: 

 \[\forall i, 1\leq i\leq S, \sum_{j\neq i} N_j > \theta_N \]
\end{exe}  
\noindent That is, none of the alternations\is{alternation} ($S_i$) in $N$ are
sufficiently numerous to tolerate all the rest ($\sum_{j\neq i} N_j$) as
exceptions:\is{exception}  no productive \isi{alternation}  will be identified. The
speaker must hear the morphological realization of every word in $C$;
if any is to slip through the cracks, a defective gap appears.  I
should note that in the conception and application of the Tolerance
Principle, the terms such as 
``category'' and ``alternation''\is{alternation} are meant to be
general and not restricted to \isi{morphology} per se. For instance,
``category'' can be interpreted as any well-defined 
structural class with a finite number of  elements (phonemes,\is{phoneme} words,
\isi{morphosyntactic} structures, the directionality of a finite number of
functional heads, etc.), and ``alternation''\is{alternation} can be understood as any
outcome of a computational process defined over such a class. The Tolerance
Principle asserts that in order for a productive pattern to emerge,
one of the alternations\is{alternation} must be statistically dominant. Elsewhere 
 I have studied  several well-known gaps in \ili{English},
Polish, \ili{Spanish}, and \ili{Russian} \citep[][Chapter 5]{POP}. Their presence is
predictable entirely on numerical ground, requiring nothing more than
tallying up the counts of the lexical items subject to each
alternation. \is{alternation}
In what follows, I  provide a \isi{Tolerance Principle} account of 
another much-studied instance of a defective paradigm. 

\subsection{The Statistics of  N't Gaps}

In many dialects 
of \ili{English}, \emph{n't} is not permitted to contract onto 
auxiliary verbs such as \emph{am} and \emph{may}, as seen in the unavailability of, for
example, ``*I amn't tired'' and ``*You mayn't do that''
\citep[e.g.,][]{Anderwald2003, 
  Bresnan2001HB, Broadbent2009, Frampton2001,Hudson2000,Zwicky1983a}. 
Following \citet{Zwicky1983a}, I will  assume that the contracted
negative \emph{n't}  is an inflectional\is{inflection} affix.\is{affix} The question 
is why \emph{n't} cannot attach to all auxiliary verbs residing
in the Tense\is{tense} node.
From the perspective of the \isi{Tolerance Principle}, the emergence of gaps
must result from a critical mass of  exceptions\is{exception} to the contraction
process.   

Let us consider the behavior of the auxiliary hosts for \emph{n't}. 
\citet[][ p508]{Zwicky1983a}  provide  a near comprehensive list,
which I revise with some additional information in Table 
\ref{t:amnt}. 

\begin{table}[h!] 
\begin{center}
\caption{The morphophonological \isi{alternation} of n't
  contraction}
\begin{tabularx}{\textwidth}{Xr X rr }
\lsptoprule
\multicolumn{2}{c}{aux+not}  &  \multicolumn{2}{c}{n't contraction}  &   (\%)  \\ 
\midrule
could [kud] & 45,256 & [kudn̩t]  & 106,123 &
                                                          \hspace{0.15cm} 70.104 \\ 
did [dɪd]  & 128,432 & [dɪdn̩t] & 342,202 & 72.711 \\
does [dʌz] &72,194& [dʌzn̩t] & 164,922 & 69.553\\
had [hæd] &27,410  & [hædn̩t] & 46,987 & 63.157\\
has [hæz] & 28,529  & [hæzn̩t] & 29,578 &
                                                               50.255\\
has [hæz] & 28,529  & {\textbf{[eɪnt]}} &749 &
                                                               1.273\\
have [hæv] & 24,957& [hævn̩t] & 45,849 &
                                                              63.868\\
have [hæv] & 24,957& {\textbf{[eɪnt]}} & 981 &
                                                              1.367\\
is  [ɪz] &  189,538& [izn̩t] & 100,164 & 34.275\\
is  [ɪz] &  189,538& {\textbf{[eɪnt]}} & 2,537 & 0.868\\
might  [maɪt] & 14,780 & [maɪtn̩t] &
                                                                      78 & 0.525\\
must  [mʌst] & 4,156 & {\textbf{[mʌsn̩t]}} &  917 & 18.076\\
need  [nid]  & 3,705 & [nidn̩t] & 1,235 & 25.000\\
ought  [ɔt] & 1,031  & [ɔtn̩t] & 66 & 6.016 \\
should  [ʃud] & 20,577  & [ʃudn̩t] & 25,576 &
  55.416\\
was  [wʌz] &97,457  & [wʌzn̩t] & 141,384 & 59.196\\
would [wud] &46,205 & [wudn̩t] & 85,853 & 65.012 \\
 
\midrule 

am  [æm] & 10,258 & ∅ & 5& 0.041  \\
am [æm] & 10,258  & {\textbf{[eɪnt]}} & 2,046 & 16.622 \\ 
are  [ar] &89,083 &  [arnt]  & 50,137 & 35.602\\
are  [ar] &89,083 &  {\textbf{[eɪnt]}}  & 1,073 & 0.765\\
can  [kæn]  & 75,531 &{\textbf{[kænt]}}  & 201,060 & 72.692\\
dare  [dɛər]& 320 & % [d\textipa{E}\textipa{@}rn̩t] 
		∅			& 25 & 7.246 \\
do [du] & 81,074 & {\textbf{[dont]}} & 654,576 & 88.979\\
may  [meɪ] & 36,195 & ∅ & 12 & 0.033\\
shall [ʃæl]& 1,271 &   ∅ & 123 & 8.824 \\
were  [wr̩]& 41,224 & [wr̩nt] & 35,120 & 46.002  \\
will  [wɪl] & 39,068 & {\textbf{[wont]}} & 86,158 & 68.802 \\ 
\lspbottomrule
\end{tabularx} \label{t:amnt}\end{center}\end{table}

Table \ref{t:amnt} provides the frequencies\is{frequency} of the auxiliary verbs and their
negation in both uncontracted and contracted forms in
the 520-million-word Corpus\is{corpus} of Contemporary American \ili{English} (COCA;
\citealt{COCA}).  Given the heterogeneity of the textual sources, a
handful tokens of \textit{amn't}~ and \textit{mayn't}~ can be found albeit at very
low frequencies.\is{frequency} The \textit{n't}-contracted forms of \textit{  shall} and 
\textit{dare} -- \textit{shan't} and \textit{daren't} -- are also impossible for
most American \ili{English} speakers but are included here for
completeness. Although \textit{shan't} is often perceived  as a
stereotypically British \ili{English} feature, it 
seems to be vanishing across the pond as well. In  a 6.6-million-word
\isi{corpus} of British \ili{English} \citep{CHILDES}, not a single instance of
\textit{shan't} is found. And its \isi{frequency} of usage has been in a steady
decline since 1800, the beginning date of the Google Books  Corpus.\is{corpus} As
of \textit{  daren't}, the OED does not provide any citation and it has always
been very rare throughout the period of the Google Books Corpus.\is{corpus} These
gapped forms are marked by \font\msbm = msbm10 at 11pt
\hbox{\msbm \char 63}.

The prescriptively maligned \textit{ain't} ([eɪnt]), however, 
is robustly attested for \textit{am}, \textit{are}, \textit{is}, \textit{have},
and \textit{has} in COCA as well as a six-million-word \isi{corpus} of
child-directed American \ili{English} \citep{CHILDES}.
% \footnote{For many 
%  speakers, 
%  there is another negative contracted form for \textit{am}, namely,
%   \textit{aren't}, especially in tag questions (``I am a good runner,
%   aren't I''). Formally, \textit{aren't} has the same effect on the
% contraction system  as \textit{ain't}: it is not predictable from
%   the auxiliary \textit{am}, thereby constituting an exception to any
%   potential productive process. As will be explained later, it is
%   likely that the presence of forms such as \textit{ain't} and {\textit{%     aren't}, in an indirect way, gave rise to the \textit{amn't}
%   gap. \label{aren't}}
Since the phonological form of
[eɪnt] is unpredictable from the auxiliary host, it is boldfaced in
Table \ref{t:amnt} to mark its idiosyncrasy,  along with a few other
exceptions  to which I return later.  Note that
 the \isi{frequency} estimates of the \textit{ain't} forms are 
approximate.  First, I only counted strings where \textit{ain't} is
immediately preceded by a pronoun -- the majority case, but
sentences with a lexical subject (e.g., ``Kids ain't ready'') are not
included. Second, because both \textit{be} and \textit{have} can take on \textit{ain't}, the counts 
for the auxiliaries are parceled out by extrapolating from the
frequencies\is{frequency} of the regularly contracted \textit{
  n't} forms.\footnote{Here I gloss over the fact that there are \ili{English}
  dialects in which \textit{ain't} is also an alternative form of
  negative contraction for \textit{do}, \textit{does}, and \textit{did}
  \citep[e.g.,][]{Labov1968, Weldon1994}. It would be difficult to
  estimate their frequencies\is{frequency} but formally, this use of \textit{ain't}
 serves to create additional (unpredictable) exceptions to the contraction
  process which, as we discuss below, contributes to the breakdown of
  \isi{productivity} and  the emergence of  gaps. } For
instance, there are 2,054 instances of ``you/they 
ain't'': the ``share'' for \textit{are} is based on the count of ``aren't''
(50,137) relative to  ``haven't'' (45,849). This amounts to 52.2\% of 2,054,
or 1,073, as recorded in the Table. Finally, the estimate of \textit{ain't} as the contraction of \textit{am}~+~\textit{n't} cannot follow a
similar process because, of course, \textit{amn't} is 
gapped. I thus allocated roughly 75\% of the ``I ain't'' counts,
which is the share of ``I am not'' out of the total of ``I am not''
and ``I have not'', to the
contraction of \textit{am not}.
For these five auxiliaries that can be realized as \textit{ain't}, the
percentage of the contracted  forms are based on the sum of
uncontracted, \textit{n't}-contracted, and 
\textit{ain't}-contracted forms. More precise  estimates are
certainly possible but as  we will see, the exact frequencies\is{frequency} are not
especially important for our 
purposes: it is more pertinent to approximate a ``typical'' \ili{English}
speaker's experience with these forms. Roughly, we would like to know
whether an \ili{English} speaker will have encountered a specific
\isi{phonological word} at all, by using some independently motivated
\isi{frequency} threshold (e.g., once per million; \citealt{Nagy1984}): it is evident
that the \isi{frequency} of \textit{ain't} is sufficiently high  for this
threshold  despite our rough estimates. 


A tempting approach to gaps is to appeal to indirect negative evidence
\citep{LGB, Pinker1989}. A strong version takes the shape of \textit{lexical conservatism}: do not use a form unless it is explicitly
attested. This recalls Halle's [-Lexical Insertion] treatment of gaps 
in his classic paper (\citeyear{Halle1973a}) and can be found in recent
works as well \citep[e.g.,][]{Pertsova2005, Steriade1997, Rice2005b, Wolf2009}. A
weak version makes use of \isi{frequency} information. For instance, if \textit{amn't} were  possible, language learners would have surely 
heard it in the input,\is{input} especially since \textit{am} is highly
frequent and would have had plenty of opportunities to undergo \textit{n't} contraction. Its  conspicuous absence, then, would  provide
evidence for its 
ungrammaticality \citep[e.g.,][]{Daland2007, Sims2006, Baerman2008,
  Albright2009b}. 

Traditional \isi{acquisition} research has always viewed indirect negative
evidence with strong suspicion 
\citep{Berwick1985, Osherson1986, 
  Pinker1989}. Research on the \textit{amn't} gap \citep[e.g.][]{Hudson2000} has also
questioned its usefulness. However, with the recent rise of probabilistic
approaches to \isi{language acquisition} especially  Bayesian
models of inference,  the field has seen a revival of indirect negative evidence.
If the conception of learning is a zero-sum -- or more
precisely,  one-sum\,---\ game which  assigns a
probabilistic distribution over all linguistic forms,  the unattested will
necessarily lose out to the  attested, at least in most
probabilistic models of language learning. 
 A thorough assessment of indirect negative evidence within a
 probabilistic framework 
is beyond the \isi{scope} of the present
paper; see \citealt{Niyogi2006, Yang2015aa, Yang2017}. But 
a careful statistical examination of gaps serves to 
reveal its deficiencies.   Note that the  question is not whether
indirect negative  
evidence can account for \textit{some} missing forms: 
the absence of \textit{amn't} is indeed unexpected under any reasonable
formulation. The real challenge is to
ensure that indirect negative evidence will pick out \textit{only} the
gapped forms but nothing else, while keeping in mind that 
morphological \isi{inflection} is generally not gapped but fully
productive, readily extending to novel items.


Two observations can be made about the \isi{frequency} statistics in Table
\ref{t:amnt}, which suggest that  indirect negative evidence is unlikely to
succeed.   First  the \textit{n't} forms of several auxiliaries such as \textit{might} and \textit{need} are in fact quite rare. They appear
considerably less frequently than  once per million,  which is
generally regarded as the minimum  
threshold to guarantee exposure for most \ili{English} speakers
\citep{Nagy1984}. In the  six-million-word \isi{corpus} of child-directed
American \ili{English} \citep{CHILDES}, \textit{mightn't} appears only once,
\textit{needn't} appears only twice, and \textit{
  mustn't} does not appear at all. In the other words, these \textit{n't}
forms may be so rare 
that they are in effect absent for many children
\citep{Hart1995}. Lexical conservatism thus will not 
distinguish them 
from the truly gapped \textit{amn't}, \textit{mayn't}, \textit{daren't}, and \textit{shan't}, the last of which is in fact more
frequently attested in COCA. Second, consider a statistical interpretation of
indirect negative evidence. The last column of Table
\ref{t:amnt} provides the percentage of the 
\textit{n't} contraction out of all negated forms. 
An auxiliary with  an unusually low
ratio may mean  that it has performed below expectation and 
could be a clue for its \isi{defectiveness}. However, the statistics in Table
1 suggest otherwise. It is true that \textit{amn't} and \textit{mayn't} have very low ratios:  this fact alone is not remarkable
because these are indeed gaps. But exactly how low should a
ratio be for the learner to regard a contracted form to be
defective? On the one hand, we have \textit{mightn't} and \textit{oughtn't}
at 0.525\% and 6.016\%, and these are not defective. On the
other hand, we have \textit{
  daren't} and \textit{shan't} at 7.246\% and 8.824\%, but these in fact are
 defective.  There doesn't appear to
be a threshold of \isi{frequency} or probability that can unambiguously
distinguish gapped from ungapped items. 

\subsection{ N't Contraction in Language Development and Change}
Let's see how the \isi{Tolerance Principle} provides an account of the \textit{amn't} gaps. The simplest approach is to consider all the auxiliary verbs
and their \textit{n't} contractions collectively as  a homogeneous set. Using the
once-per-million threshold as a reasonable approximation of a typical
American \ili{English} speaker's vocabulary, and taking the size of the
Corpus\is{corpus} of Contemporary American \ili{English} (520 million words) into account, there are
18 auxiliaries with reliably attested \textit{n't} forms. The four
gapped forms are all below this threshold and are thus excluded from
consideration. It is important to 
clarify that, unlike various forms of lexical conservatism and
indirect negative evidence discussed earlier, we do 
not regard the absence of these forms as evidence for their
\isi{defectiveness}. Rather, the learner's task is  to deduce, on the
basis of the 18 well-attested forms, including \textit{am}$\sim$\textit{ain't},
that  \textit{n't} contraction is 
not a productive pattern in the \ili{English} auxiliary system. 

This is quite easily accomplished. Of the 18 auxiliaries,
\textit{n't} is realized as follows:
\begin{exe}  \ex \begin{xlist}
\item {[n̩t]}: could,
  did, does, had, need, should, was, would (8)
\item   {[eɪnt]} in variation
with either [nt] or [n̩t]:  have, has, is, are (4)
\item {[nt]}: can, were (2)
\item idiosyncratic vowel change: do, will (2)
\item {[eɪnt]}: am (1)
\item {[n̩t]}  but
  idiosyncratically deletes [t] in the auxiliary (see \citealt[508--509]{Zwicky1983a}   for discussion): must (1)
\end{xlist} \end{exe}
For any of these alternations\is{alternation} to be productive, it must have no more
than $\theta_{18} = 6$ exceptions.\is{exception} The most promising
[n̩t], which applies to 8 auxiliaries and thus has 10
exceptions,\is{exception} is a long way off. Even if we are to include the
[n̩t]-taking \textit{have}, \textit{has}, and \textit{is} and
ignore the unpredictable variant [eɪnt] form, the rule ``nt
$\longrightarrow$ n̩t'' still falls short of
\isi{productivity}. Thus, the 
learner will be able to conclude, from the Conditions on Gaps
\reff{gaptheorem}, that \textit{n't} contraction is not a productive
process for \ili{English} auxiliaries and must be learned lexically. If \textit{amn't} fails to appear in the input,\is{input} it will be absent. Only after
the learner has already concluded that a category does not have a
productive rule can they start to regard the absence of evidence as
evidence of absence. 


The preceding analysis, while correctly identifies the \textit{n't} gaps,
has some inadequacies.  For one thing, based on the 18 contracted
forms, the primary evidence for \isi{language acquisition}, learners would
also identify \textit{mustn't} and \textit{oughtn't} as gapped as they fall
below the minimum \isi{frequency} of once per million. 
This is not necessarily a fatal shortcoming: 
\textit{mustn't} and \textit{oughtn't} are still 
considerably more frequent than \textit{amn't} and \textit{mayn't}, the two
genuinely  gapped forms, and children may acquire them in later stages of
\isi{acquisition}.  But more significantly, as Steve pointed out 
to me in a personal communication (unrelated to the current
celebratory volume), the preceding brute-force approach 
misses an important structural generalization. Table
\ref{t:amnt} is divided into two halves on Steve's advice.  As he
insightfully observes,  none of the auxiliaries that ends in an
obstruent is gapped; these are listed in the top portion of the
Table. By contrast, gaps are only found in the 
auxiliaries that do not end in an obstruent, which are listed in the bottom
portion of the Table. 

If we carry out a Tolerance analysis along the feature\is{features}
[$\pm$obstruent], a much more elegant and interesting pattern
emerges. For the 12 [+obstruent] auxiliaries, only four have
exceptions\is{exception} -- \textit{has}, \textit{have}, \textit{  is}, and \textit{must} -- just below $\theta_{12}=4$. Thus, \ili{English} learners can
identify a productive rule:  

\begin{exe} \ex \label{nt-rule}

nt $\longrightarrow$
n̩t~/~[+obstruent]~\#~\underline{\hspace{0.5cm}} \end{exe}

\noindent This immediately accounts for the  fact that speakers generally
accept the forms \textit{mightn't} and \textit{oughtn't} despite their very low
frequencies\is{frequency} (well below once per million): these two
auxiliaries, of course, follow the structural description of
\reff{nt-rule}. By contrast, \textit{amn't}, \textit{mayn't}, 
\textit{daren't}, and \textit{shan't}, some of which 
appear more frequently than \textit{mightn't} and \textit{oughtn't}, are
generally rejected because they fail to meet the structural
descriptions of the productive rule in \reff{nt-rule}. 

Consider now the six  [-obstruent] auxiliaries in the bottom portion of Table
\ref{t:amnt}. Here  \textit{am} and \textit{are} have [eɪnt],
\textit{can} and \textit{were} add [nt], and \textit{do} and \textit{will} have
idiosyncratic vowel changes. Since the Tolerance threshold $\theta_6 =
3$, no distinct pattern will  be identified as productive:
\isi{lexicalization} is required 
and gaps are predicted for \textit{mayn't}, \textit{daren't}, \textit{shalln't}, and of
course \textit{amn't}.  

 The calculation here is very delicate but it is
interesting to push the \isi{Tolerance Principle} to the limit. 
What if the
child has not learned \textit{ain't} as the \textit{n't}-contracted form for
\textit{am} and \textit{are}? Although \textit{ain't} forms are quite robustly
attested in COCA as well as in child-directed \ili{English}, they are still
strongly dialectal and are, at least in the input to some children,  less frequent
than the ``regular'' forms such as \textit{
  aren't}, \textit{isn't}, \textit{haven't}, and \textit{hasn't}. If so, a
child during an early stage of \isi{acquisition} may 
in effect have only five [-obstruent] auxiliaries and their contracted
forms to learn from: namely, \textit{  are}, \textit{can}, \textit{
  do}, \textit{were}, and \textit{will}. Here the statistically dominant
pattern of
``nt~$\longrightarrow$~[nt]~/~[-obstruent]~\#~\underline{\hspace{0.5cm}}''
does reach \isi{productivity}: the two idiosyncratic exceptions\is{exception} of \textit{do} and
\textit{will} fall below the threshold of $\theta_{5} = 3$, and 
\textit{n't} contraction is predicted to be transiently productive! 

Bill Labov (personal communication) distinctly recalls being a young
\textit{amn't} speaker only to exit that stage at a later time.
Indeed, we can find attested examples in American \ili{English}-learning children's
speech.  
The three examples in \reff{amnt} are taken from the CHILDES
database \citep{CHILDES}: \is{corpus}

\begin{exe} \ex \label{amnt}
\begin{xlist} 
\item I amn't a dad. (Kate/Kim, 3;6: Sawyer Corpus 3-12-92.cha) \label{katekim}
\item I'm doing this puzzle well, amn't I? (Mark, 3;11: MacWhinney
  Corpus 67b1.cha)
\item Amn't I clever? (Mark, 3;11: MacWhinney Corpus 67b2.cha)
\end{xlist} \end{exe}

The reader is encouraged to listen the
  audio recordings of the examples in  \reff{amnt} in the CHILDES
  database. The first child's 
identity is unclear due to discrepancies in transcription. The examples
from Mark can be heard as the investigator's  exact
revoicing (Brian MacWhinney, personal communication). Although three
examples seem quite rare, it is worth noting that almost all \textit{am}'s are contracted onto the pronoun (i.e., \emph{I'm not}). Of the one million American \ili{English} child utterances, there are only 42
full forms of \textit{am} followed by negation (i.e., \emph{I am not}),
which makes the three \textit{amn't} errors not so negligible.
Of course, everyone eventually hears `I ain't': from pop songs on
radio if not from the immediate family and friends. Thus,
 \textit{amn't} will disappear according to the Tolerance-based
analysis, for \textit{ain't} introduces an additional \isi{exception} which leads to
the breakdown of \isi{productivity} for the [-obstruent] class.

Corroborative  evidence for the (transient) \isi{productivity} of \textit{n't} contraction can also be found in other auxiliaries. 
To my great
surprise, there are  numerous instances of \textit{willn't} as the negative
contracted form of \textit{will} and \textit{whyn't} for `why
  don't/didn't' in the speech of  many parent-child dyads, apparently
  all from   the New 
England region.  Other than enriching the empirical data on
contraction, \textit{willn't} and \textit{whyn't} do not tell us much about the
\isi{productivity} of \textit{n't} contraction or its \isi{acquisition}: if parents
use them frequently, and they do,  children will follow. Nevertheless,
\textit{willn't} can also be found in the spontaneous speech of children who
are not from the New England region:\footnote{Brain MacWhinney (personal
  communication) confirmed
  that the only time he or his wife ever used \textit{willn't} was  when
transcribing   Ross's speech. }

\begin{exe} \ex \label{willnt}
\begin{xlist} 
\item No we willn't. \label{rosswill} (Ross 2;9, Colorado, MacWhinney Corpus 26b2.cha)
\item Oh it willn't fit in there  (Marie 6;6, Ontario, Evans Corpus dyad07.cha)
\item He willn't be a good boy (Jared 6;7,  Ontario, Evans Corpus dyad19.cha)
\end{xlist} \end{exe}
Perhaps most strikingly is an utterance produced by Sarah, a child
from the Harvard studies \citep{Brown1973}:\footnote{The contraction of \textit{n't} onto the main verb as in \reff{sarah} was 
attested in the history of \ili{English}: see \citealt{Brainerd1989} for
 \textit{  caren't} (`don't care') and \citealt{Jespersen1917} for \textit{bettern't}, \textit{usen't}, and indeed \textit{whyn't}.\label{caren't}}

\begin{exe} \ex \label{sarah}
And the reindeer saidn't. \end{exe}

\noindent Taken together, the examples in \reff{amnt}, \reff{willnt}, and
\reff{sarah} suggest that  \textit{n't}
contraction is at least transiently productive for some \ili{English}-learning children. 


% 04:40 PM|~/Downloads/MacWhinney]cat 0*cha 1*cha 2*cha  > young.cha
% [04:40 PM|~/Downloads/MacWhinney]grep '*CHI:" young.cha | grep "couldn't"
% > bash: unexpected EOF while looking for matching `"'
% bash: syntax error: unexpected end of file
% [04:41 PM|~/Downloads/MacWhinney]grep '*CHI:" young.cha | grep "couldn\'t"
% > bash: unexpected EOF while looking for matching `"'
% bash: syntax error: unexpected end of file
% [04:41 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "couldn't"
% *CHI:	I bumped my head on the side of the bed and couldn't get up in the
% [04:41 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "couldn't" | wc
%        1      17      73
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "didn't" | wc
%       19     179    1032
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "hadn't" | wc
%        0       0       0
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "hasn't" | wc
%        0       0       0
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "haven't" | wc
%        1       8      47
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "is't" | wc
%        0       0       0
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "needn't" | wc
%        0       0       0
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "shouldn't" | wc
%        0       0       0
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "wasn't" | wc
%        0       0       0
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "wouldn't" | wc
%        2      19     116
% [04:42 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "aren't" | wc
%        2      17      92
% [04:43 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "can't" | wc
%       42     359    2100
% [04:43 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "don't" | wc
%      139    1151    6824
% [04:43 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "won't" | wc
%       22     204    1162
% [04:43 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "weren't" | wc
%        0       0       0
% [04:43 PM|~/Downloads/MacWhinney]nln 8
% 8	5	3
% [04:43 PM|~/Downloads/MacWhinney]grep "*CHI:" young.cha | grep "isn't" | wc
%        2      17     111

Ross's \textit{willn't} presents an especially interesting opportunity
for studying the \isi{productivity} of \textit{n't} 
contraction. The CHILDES \isi{corpus} 
 contains a relatively extensive record of Ross's longitudinal
 language development. We can then study his auxiliaries and
 contractions, and subject his individual grammar to the kind of fine-grained analysis of Adam's \isi{past tense} (\sectref{sec:yang:2}). By the
 time Ross produced \emph{No we willn't}, he had used 9 \textit{n't}-contracted auxiliaries:
\begin{exe} \ex \label{Ross}
\begin{xlist} 
\item couldn't, didn't, haven't, isn't, wouldn't \label{rossa}
\item aren't, can't, don't, won't \label{rossb} \end{xlist} \end{exe}

If Ross had not started partitioning the auxiliaries by the
[$\pm$obstruent] feature,\is{features} the $N=9$ examples in \reff{Ross} supports
the productive use of \textit{n't} contraction because the four examples
in \reff{rossb} are below the number of tolerable exceptions\is{exception}
($\theta_9 = 4.2$). The 5/4 split between rule-governed and
exceptional\is{exception} items is exactly the stimuli used in the artificial
language study \citep{Schuler2016} where children nearly categorically
generalized the rule. If he failed to distinguish the syllabic [n̩] in
\reff{rossa} and the nonsyllabic [n] in \textit{aren't} and \textit{can't}
in \reff{rossb}, it would have been even easier for \textit{n't}
contraction to reach \isi{productivity}.  Thus, Ross's productive use of \textit{n't}
contraction in \reff{willnt} is  predicted by the Tolerance
Principle. 

The naturalistic evidence from child language  is admittedly thin, but it suggests that
the emergence of the \textit{amn't} and other gaps in 
the auxiliary system may be due to the use of \textit{ain't}. Again, the
gaps would not be the result of mutual exclusivity: there are \isi{doublets}
such as \textit{haven't}$\sim$\textit{ain't} etc. so \textit{amn't} and
\textit{ain't} could have coexisted side by side. Gaps arise/arose
because the form of \textit{  ain't} weakens the numerical advantage of
\textit{n't} contraction, pushing it below the Tolerance threshold. 

Finally, a little historical detective work bolsters our treatment of 
the \textit{amn't} gap.\footnote{I am grateful to Anthony Warner for pointing
  out the important study of \citealt{Brainerd1989}.} 
According to \citet[117]{Jespersen1917}, ``the contracted forms seem to
have come into use in speech, though not yet in writing, about the
year 1600.'' The change appears to have originated in non-standard
speech before spreading to mainstream usage.  Subsequent scholarship, however, places the date to a
somewhat later time (e.g., 1630, \citealt[181]{Brainerd1989}; see
also \citealt[208--209]{Warner1993}).
Pursuing the results from the Tolerance-based analysis, we can make
two observations.   

First, it is likely that \textit{n't}-contraction was at one point
productive, which seems especially effective for the [+obstruent]
auxiliaries; see also \reff{sarah} and fn \ref{caren't}.
Brainerd's study finds  that \textit{didn't}, \textit{hadn't}, \textit{shouldn't}, and \textit{wouldn't} appeared from 1670s, soon after the
\textit{n't} contraction appeared in the \ili{English} language. These were
followed  by \textit{couldn't}, \textit{mightn't}, \textit{needn't}, and \textit{mustn't} in the 18th century, and the last to join the group was
\textit{oughtn't} in the 19th century, first attested in Dicken's 1836
\textit{The Village 
  Coquette}. 
  Thus  speakers at  that time  must have formed a  productive
  contraction rule  for [+obstruent] auxiliaries, perhaps like the one
  given in \reff{nt-rule}.  
Following this line of reasoning, we  make the prediction, admittedly
one that is difficult to test, that if a new [+obstruent] auxiliary is
to appear in the language, it will be immediately eligible for \textit{n't} contraction.  

Second, and  in contrast to the [+obstruent] class that
had been expanding the number of \textit{n't} contractible auxiliaries,
the [-obstruent] class has been 
steadily losing members.  
Interestingly, the [-obstruent] auxiliaries were quite systematically
available for \textit{n't} contraction by the end of the 17th century
\citep{Brainerd1989}.   
Of special interest are of course those that were \textit{n't} contracted
in the past but are presently gapped. According to Brainerd's study, 
the first
instance of \textit{shan't} appeared in 1631, \textit{mayn't} in  1674, \textit{daren't} in 1701: all three are now gapped. 
 The very fact that they fall out of usage
points to the non-\isi{productivity} of  \textit{  n't} contraction for these
[-obstruent] auxiliaries: 
in general, a productive rule would have gained rather than lost
members. 

How, we wonder, did the [-obstruent] class lose its \isi{productivity}? Much
more detailed historical investigation will be needed but an
interesting  hypothesis can be offered as follows. The historical
development of \textit{n't} contraction may mirror the trajectory of
\isi{language acquisition} by children; that is, ontogeny may recapitulate
phylogeny. Our discussion of children's \textit{n't} contraction in
modern American \ili{English} suggests that the use of \textit{ain't} for \textit{am not}, which children probably acquire later during \isi{acquisition},
increases the number of exceptions\is{exception} for the contraction 
process. It is conceivable that the emergence  of
\textit{ain't}, an unpredictably  contracted form of \textit{am not},  
was also the culprit for the breakdown of \isi{productivity}. 

Historically,  \textit{an't}/\textit{a'nt} surfaced as the contracted form
of \textit{am not} between 1673
and 1690. But by the early 1700s, \textit{an't}/\textit{a'nt} began to be
used for both \textit{am not} and \textit{are not}
\citep[186]{Brainerd1989}. Whatever the phonological cause for this convergence,
or how/when \textit{ain't} joined the fray, the effect is that \textit{am not} no
longer had a predictable form of contraction. If our analysis of
children's \textit{amn't} and \textit{willn't} is correct, then we would
find \textit{amn't} and \textit{ain't} to be in complementary distribution: 
If a dialect does not allow \textit{ain't} for \textit{am not}, \textit{amn't}
would be possible; otherwise \textit{amn't} would be gapped. 

The most direct evidence for this suggestion comes from the 
dialectal distribution of \textit{amn't}, and its correlation with \textit{ain't}. 
The OED notes that \textit{amn't} is present in ``nonstandard'' American \ili{English} and various northern
parts of the UK. There is little to suggest that \textit{amn't} is
possible in American \ili{English} at all;  all the five occurrences in COCA
come from Scottish and Irish writers.\footnote{ The \isi{corpus} of child-directed
American-\ili{English},  surprisingly, contains one instance 
of \textit{amn't}: ``I am stirring, amn't I?'' It was produced by Colin Fraser,
on staff in Roger Brown's Harvard study of \isi{language acquisition}
\citeyearpar{Brown1973a}. A little Internet research reveals that Fraser, later a
Cambridge scholar with a few psychology textbooks to his credit, is a
native of Aberdeen.} It is 
remarkable, then,  that 
 Scotland and Ireland have ``traditionally completely \textit{ain't}-free dialects'' \citep[520]{Anderwald2003a}: it is
precisely in these regions where \textit{amn't} is robustly attested,
both in the century-old \textit{The \ili{English}
  Dialect Dictionary} \citep{Wright1898} and in recent dialect
surveys of \ili{English} \citep{Anderwald2003}.\footnote{I would like to thank Gary Thoms for
  discussion for the distribution of \textit{amn't} in Scottish \ili{English}.}


Before I conclude this section, it is important to clarify the \isi{scope} of the present
analysis. The \isi{Tolerance Principle}, through  Conditions on
Gaps \reff{gaptheorem}, can identify defective morphological
category where gaps \textit{may} emerge. Such categories are defined 
 by the structural descriptions of rules. It does not predict, at
 least synchronically, 
which items within these categories will be gapped. That issue,  in my view,
is completely a matter of usage frequency:\is{frequency} if the inflected form of an
item in a defective category is used very  rarely or not at all, it will be gapped.  Of
course, it is also possible that no gaps are found in a defective
morphological category, if 
all items happen to be inflected\is{inflection} sufficiently frequently. In that
case, however, we do predict that if a novel item 
 matches the structural description of a defective category, the speaker
will be at a loss to produce an inflected form.  Thus, the emergence
of gaps, just as the calibration of \isi{productivity},  is 
determined by the composition of the \isi{input} data. 
Finally, the preliminary work on the history of \textit{n't} contraction
suggests that the Tolerance 
Principle can be applied to the study of language change. It makes
concrete predictions about \isi{productivity} -- the rules that could 
gain new members, and the rules that could  only lose existing
members -- as long as the relevant values of $N$ and $e$ from
historical data can be reliably estimated. The reader is referred to
\citealt{POP} for a case study of  the so-called
  dative sickness in \ili{Icelandic} morphosyntax.

\section{Gaps in  I-language}
Halle's classic paper (\citeyear{Halle1973a}) contains the much
criticized proposal that gaps are caused by the [+Lexical Insertion]
feature\is{features} associated with certain forms. As noted earlier, this kind of
lexical conservatism is difficult to reconcile with the unbounded
generativity of word formation, and similar approaches using indirect
negative evidence are also unlikely to succeed. But in a footnote
of that very paper, Halle proposes an alternative approach which he
himself regards as equivalent but has almost never been discussed by other
researchers:

\begin{quote}
The proposal just sketched might be modified somewhat as regards the
treatment of words formed by rules that traditionally have been called
``nonproductive''. One might propose that all words formed by
non-productive rules are marked by these rules as [-Lexical
Insertion]. The smaller subset of actually occurring words formed by 
such rules would then be listed in the filter with the feature\is{features}
[+Lexical Insertion].  ... In other words, it is assumed that words
generated by a productive process are all actually occurring and that
only exceptionally may a word of this type be ruled out of the
language. On the other hand, words generated by a nonproductive rule
are assumed not to be occurring except under special circumstances. In
this fashion we might capture the difference between productive and
nonproductive formations (5). 
\end{quote}

\citet{Hetzron1975}, while arguing against Halle's [+Lexical
Insertion] proposal,  makes essentially the same suggestion. Rules
are either productive or \isi{lexicalized}, and gaps arise in the
unproductive corners of the grammar. His conception of gaps can be
strongly identified with the Elsewhere Condition, a critical component
of the present theory:  
\begin{quote}
The speaker must use ready-made material only for ``exceptional''\is{exception} forms,
while everywhere else he could very well ``invoke the word formation
component''. Technically, this can be represented by a disjunctive 
set of rules where idiosyncratic or ``exceptional''\is{exception} formations are
listed with as much explicitness as necessary, while the general word
formation rules would appear afterward, with the power to apply 
``to the rest'' (871). 
\end{quote}

That is, gaps arise when \isi{productivity} fails. The problem of gaps thus
reduces to the problem of \isi{productivity}. 
Some subsequent proposals have adopted
a similar approach \citep{Albright2009b, Baronian2005, Hudson2000,
  Maiden2010, Pullum1977, Sims2006}, including Steve's own account
(\citeyear{Anderson2010b}): gaps  result from conflicting
 forces in word formation such that the
output form becomes unpredictable and thus unrealized. The Tolerance
Principle provides a precise solution of what makes a rule
productive, and its application to gaps reinforces the general
position  that gaps and \isi{productivity} are two sides of the same coin.

The \isi{Tolerance Principle} is a provable  consequence of the
Elsewhere Condition and follows from the general principle of efficient
computation: the child prefers \textit{faster} grammars, a ``third
factor'' in  language design par excellence \citep{Chomsky2005}.   In
fact, a stronger 
claim can be made in favor of such an analytical approach. I  submit
that a descriptive analysis of languages, however 
 typologically complete or methodologically  sophisticated, cannot 
in principle provide the right solution for \isi{productivity}.  First, as
noted earlier, the categorical nature of children's morphological\is{morphology}
\isi{acquisition} suggests that \isi{productivity} must be demarcated by a discrete
threshold \citep[see also][36]{Aronoff1976}. But note that such  a
threshold is  empirically \textit{undiscoverable}. Productive 
 processes will  lie above the threshold and
unproductive processes will lie below, but with arbitrary ``distance''
from it in both cases. Thus, the threshold 
cannot be regressed out of the data. Second, while linguists now have
an ever expanding arsenal of investigative tools to study
\isi{productivity}, ranging from the Wug test to fMRI to Big Data, the
psychological grammar is developed without supervision in a matter of
few years; these new
empirical methods presently are at best a description of the speaker's
grammatical knowledge and not yet learning models that account for how such knowledge is acquired. 
 Finally, even if
we were to discover the threshold of \isi{productivity} through a
statistical analysis -- e.g., a productive
rule must hold for at least 85\% of eligible words -- it would
still remain mysterious why the critical value is exactly what it is,
rather than 80\% or 90\%.   

In other words, an I-language approach to \isi{productivity} is needed, one
which builds exclusively on the inherent constraints\is{constraint} on language and
cognition that all  children have access to, with deductively
established properties that  must hold universally across languages.  
The study of language as a part of human biology, I believe,
is an approach that Steve endorses and pursues  \citep{Anderson2002}, which
can be seen in his writings on  \isi{morphology} and related issues 
\citep{Anderson2010Ex, Anderson2015}.  

\enlargethispage*{1cm}Finally, a personal note. It is no exaggeration to say that I owe my
professional career to Steve. He 
managed to create a position for
me at Yale, which  kept me close 
to my young family and thus \isi{linguistics}, and further away from the
seductive fortunes in  the tech sector. It was also Steve who taught
me, more effectively than anyone, the difference between linguistic  
evidence and rhetoric. It has been a privilege to learn from him.  To figure  out  how to wake up irregular took over 15 years; the 
answer, I hope, is to  his satisfaction. It may once again win 
me a spot, this time in the Linguistic Club of
Ashville, North Carolina. 



\section*{Acknowledgements}
I would like to thank Steve Anderson, Bill
    Labov, Brian MacWhinney,  Gary Thoms, Anthony Warner, and an
    anonymous reviewer for helpful discussions and comments.

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
}

\end{document}
