\chapter{Results and discussion} \label{chapter6}
The present chapter presents and discusses the results of the main experiment conducted following the methodology illustrated in \chapref{chapter5}. The results of the pilot study were already presented in \sectref{results_PS}. The data analysis is illustrated in \sectref{results_MS}. In \sectref{discussion}, the results are interpreted and discussed against the hypotheses formulated in \sectref{hypotheses}. \sectref{limitations} addresses the limitations of the study.

\section{Results} \label{results_MS}
The present section illustrates the results of the analysis of the data collected during the main experiment. The results are presented for each dimension analysed in the main study and explored through the measures introduced in \sectref{measures}. For an overview of the measures explored in the experiment, the reader may refer to Table \ref{tab:metrics_overview} in \sectref{approach}.

\subsection{Performance measures: Analysis} \label{performanceresults}
The first type of data analysed comprises the two product-related performance measures selected for the experiment (see \sectref{performancem}): terminological accuracy and errors and omissions.
\subsubsection{Terminological accuracy} \label{accuracy}
The first metric analysed was the degree of terminological accuracy achieved for the PDF, the CAI, and the simASR condition. A total of 108 terms (36 for each speech) were evaluated by the author and by an experienced conference interpreter trainer working in the language pair English-German. The evaluation was conducted for all nine participants, for a total of 972 evaluations.

Before conducting the evaluation on the whole data set, I verified the adequacy of the grading scale by calculating Cohen's $\kappa$ value on a sample of 22 terms (20\% of all stimuli, which is a recommended amount in literature. See \citealt[326]{mellinger_quantitative_2017}). Cohen's $\kappa$ provides an indication of the level of agreement of two raters on the categories used for the evaluation. The maximum value achievable is 1, which indicates perfect agreement, while values below 0.6 are often considered problematic. This measure is more reliable and indicative of agreement than a mere calculation of the percentage of identical evaluations by two raters. The inter-annotator agreement calculated on the sample was substantial at 0.70 (see \citealt{landis_measurement_1977}). This confirmed the adequacy of the definitions chosen to describe the three categories in the grading scale. The evaluation was then completed for the remaining stimuli and the percentage of terms for each category (0, 1, 2) was calculated for each subject. The percentages thus calculated were averaged between the two raters. The main discrepancies in the coding by the two raters was due to mispronunciations of the target terms, for which the first rater was more benevolent and assigned a value of 2 if the terms used were the ones presented in the glossary, while the second rater was stricter and usually assigned a code of 1 or 0. On the whole sample, the average intercoder agreement for the accuracy evaluation was nonetheless substantial ($\kappa = 0.77$).
\subsubsubsection{Accuracy of terms searched} \label{accuracy_searched}
In this category, I considered the terminological accuracy achieved when terminology equivalents were suggested on screen, either automatically (simASR) or after a glossary query (PDF and CAI). This analysis should provide a first insight into how much the interaction with the tool interferes with the interpreting process and into how often a glossary query is successful and corresponds with an accurate rendition. If accuracy scores are high, this may provide a first indication that the integration of digital support into the SI process is feasible. This hypothesis requires, however, also the examination of errors and omissions co-occurring with searches in the glossary (see \sectref{errors_omissions}). In order to determine the effect of the tool on the degree of terminological accuracy achieved, the next category of terms (searched/found) will also be analysed.

For each participant and each tool, the bar charts below (Figures \ref{fig:accuracy_PDF}--\ref{fig:accuracy_ASR}) illustrate the distribution of the three levels of terminological accuracy (0, 1, 2) for the terms searched in the glossary.

%\todo{need data for figures}
\begin{figure}[p]
%\includegraphics[width=0.9\linewidth]{images/Accuracy_PDF.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=4cm,
  ymin = 0,
  ymax = 100,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Accuracy (\%)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend style={at={(0.5,-0.5)},anchor=north},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,71.88)
  (2,86.96)
  (3,66.67)
  (4,73.91)
  (5,94.00)
  (6,73.53)
  (7,86.36)
  (8,70.59)
  (9,78.13)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,3.13)
  (2,6.52)
  (3,5.00)
  (4,15.22)
  (5,0.00)
  (6,8.82)
  (7,1.52)
  (8,5.88)
  (9,4.69)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,25.00)
  (2,6.52)
  (3,28.33)
  (4,10.87)
  (5,6.00)
  (6,17.65)
  (7,12.12)
  (8,23.53)
  (9,17.19)
  };
  \legend{Rank 2,Rank 1,Rank 0}
  \end{axis}
\end{tikzpicture}
\caption[Accuracy scores of terms searched (PDF)]{Accuracy scores for terms searched in the PDF condition}
\label{fig:accuracy_PDF}
\end{figure}


\begin{figure}[p]
%\includegraphics[width=0.9\linewidth]{images/Accuracy_CAI.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=4cm,
  ymin = 0,
  ymax = 100,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Accuracy (\%)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend style={at={(0.5,-0.5)},anchor=north},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,84.00)
  (2,83.82)
  (3,85.00)
  (4,83.93)
  (5,89.51)
  (6,86.53)
  (7,88.29)
  (8,86.07)
  (9,89.15)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,6.00)
  (2,7.35)
  (3,3.33)
  (4,8.93)
  (5,0.00)
  (6,3.03)
  (7,2.78)
  (8,1.67)
  (9,3.85)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,10.00)
  (2,8.82)
  (3,11.67)
  (4,7.14)
  (5,3.13)
  (6,6.06)
  (7,5.56)
  (8,8.33)
  (9,3.85)
  };
  \legend{Rank 2,Rank 1,Rank 0}
  \end{axis}
\end{tikzpicture}
\caption[Accuracy scores of terms searched (CAI)]{Accuracy scores for terms searched in the CAI condition}
\label{fig:accuracy_CAI}
\end{figure}

\begin{figure}[p]
%\includegraphics[width=0.9\linewidth]{images/Accuracy_ASR.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height = 4cm,
  ymin = 0,
  ymax = 100,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Accuracy (\%)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend style={at={(0.5,-0.5)},anchor=north},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,95.83)
  (2,98.61)
  (3,94.44)
  (4,93.06)
  (5,98.61)
  (6,95.83)
  (7,97.22)
  (8,95.83)
  (9,97.22)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,2.78)
  (2,0.00)
  (3,4.17)
  (4,5.56)
  (5,1.39)
  (6,4.17)
  (7,2.78)
  (8,0.00)
  (9,2.78)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,1.39)
  (2,1.39)
  (3,1.39)
  (4,1.39)
  (5,0.00)
  (6,0.00)
  (7,0.00)
  (8,4.17)
  (9,0.00)
  };
  \legend{Rank 2,Rank 1,Rank 0}
  \end{axis}
\end{tikzpicture}
\caption[Accuracy scores of terms searched (simASR)]{Accuracy scores for terms searched in the simASR condition\label{fig:accuracy_ASR}}
\end{figure}


As expected, the best performance was achieved with simASR support: on average, 96.3\% of terms received a score of 2, 2.62\% a score of 1 and only 1.08\% a score of 0. The aggregate mean value for the 2 and 1 scores was 98.92\%, which indicates on average a very high degree of accuracy in the term renditions. This is to be expected, as for the simASR condition all stimulus terms were also ``found'' by the simulated tool. After a glossary query, the CAI tool led to a slightly worse performance (86.26\% of terms obtained a score of 2 on average), followed by the PDF tool (78.00\% rated as close renditions, and a notable 16.36\% were either untranslated or mistranslated). Overall, the terminological accuracy achieved with digital support was quite high, also in the PDF condition.

With reference to within-subject differences, a first inspection of the data showed a more nuanced picture, with cases in which greater accuracy (i.e. a higher percentage of 2 scores for terms) was achieved in the PDF condition than in the CAI condition, and fewer omissions or unacceptable translations were recorded for the PDF glossary than for the CAI tool.

I proceeded to verify whether the differences observed on the raw data between the scores achieved with the PDF glossary, the CAI tool, and the ASR-CAI mock-up were significant.

Since each participant was tested multiple times on the three conditions, I conducted a Friedman test. Choosing a non-parametric test seems appropriate, as the sample tested was small and the normality of distribution could not be assumed for all conditions. Non-parametric tests were chosen for the analysis also for the other metrics analysed in the study.

A Friedman test was conducted on the rank-transformed data for each of the categories of the grading scale. Since each participant worked in three conditions, ranks ranged between 1 (lowest value) and 3 (highest value), with the appropriate half scores in the case of ties.

For the terms searched, the difference between tools was not significant for the category ``acceptable renditions'' (score = 1): $\chi^2(2) = 3.182, p = 0.149$. A significant effect of tool was found for the category ``close renditions'' (score = 2), $\chi^2(2) = 14.89, p = 0.001$, and for the category ``zero or unacceptable renditions'' (score = 0), $\chi^2(2) = 16.22, p < 0.001$.

Post-hoc Wilcoxon signed-ranks tests with a Bonferroni correction were conducted for the significant categories, to identify which differences had led to the significant result. More close renditions were observed for the simASR condition (Mdn = 95.83\%) than for the CAI condition (Mdn = 86.07\%) and for the PDF condition (Mdn = 73.91\%). This difference was statistically significant between the simASR tool and the PDF glossary ($p = 0.005$) and between the CAI tool and the simASR tool ($p = 0.005$). No statistically significant difference was observed between the PDF and the CAI condition ($p = 0.096$).

More 0 scores were assigned to terms searched in the PDF condition (Mdn = 17.19\%) than in the CAI condition (Mdn = 7.14\%) and in the simASR condition (Mdn = 1.39\%). The Wilcoxon signed-ranks tests showed a statistically significant difference between the simASR and the PDF condition ($p=0.004$) and between the simASR and the CAI condition ($p=0.004$). A statistically significant difference was also observed between the PDF and the CAI condition ($p = 0.02$).

\subsubsubsection{Accuracy of terms searched/found} \label{accuracy_found}
The observations made for the terms looked up were confirmed by the analysis of the accuracy scores for the terms searched/found in the glossary. In this case, the differences between the tools were smaller and the results overall encouraging, as the average percentage of terms rated as close renditions ranged from 91.35\% in the case of the PDF glossary to 96.30\% for the ASR-CAI mock-up.

As for within-subject differences, the trend observed for the terms queried also emerged at a first inspection of the data for the terms queried and found. Note that for the simASR condition, all terms queried were also ``found'' (the ASR-CAI mock-up did not contain recognition errors). However, the terms queried in the PDF glossary and with the CAI tool may not be found. Thus, a query may not lead to an accurately translated term. In order to explore this further, I repeated the Friedman test also for the terms searched/found. The test yielded similar results to those obtained for the terms searched. Significant differences between tools were found for the categories ``close rendition'' ($\chi^2(2) = 7.52, p = 0.023$) and ``zero\slash unacceptable rendition'' ($\chi^2(2) = 14.89, p = 0.001$).

Wilcoxon signed-ranks tests with a Bonferroni correction for repeated measures were conducted also for the terms searched/found in the glossary. After a Bonferroni correction (3 comparisons), statistically significant differences were found both for the category of close renditions and for the category of zero\slash unacceptable renditions. In the ``close renditions'' category, a statistically significant difference was found only between the simASR and the PDF condition ($p = 0.026$). For the ``zero\slash unacceptable renditions'', a statistically significant difference was found between the CAI and the PDF condition ($p = 0.005$) and the simASR and the PDF condition ($p = 0.005$).

\subsubsection{Errors and omissions} \label{errors_omissions}
The participants' interpretations were further analysed to explore how the interaction with the tools interfered with the subjects' interpreting performance on a global level. In this analysis I considered two types of issues: severe errors and complete omissions.

The following sections report the results of a series of tests conducted on the data. I first considered all sentences (without distinguishing between target and continuation sentences) and analysed the number of severe errors and complete omissions occurring after a query. I then explored where the majority of errors and omissions occurred, whether in the target sentence or in the continuation sentence. Finally, I analysed how the tool used influenced the prevalence of errors and omissions after a glossary query.

\subsubsubsection{Evaluation framework} \label{evaluation_errors}

As for the analysis of the terminological accuracy, the assessment framework for errors and omissions was validated before proceeding with the evaluation. The level of intercoder agreement was calculated on a sample of sentences. The evaluation task included two subtasks: the identification of target and continuation sentences that contained a severe error, and the identification of sentences omitted in their entirety. The evaluation was conducted by the same raters as in \sectref{accuracy}. The sentences to be evaluated were presented in an Excel table. The original sentence and the rendition for each participant were juxtaposed, with an additional column for comments. For the scope of the present study, the participants' renditions were presented only in written format (but see e.g. \citealt{montecchio_masterarbeit_maddalena_2021} for an evaluation including effects on fluency).

The average Cohen's $\kappa$ value was calculated for the first sample and was found to show moderate agreement ($\kappa = 0.50$). The inspection of the ratings assigned by the second evaluator showed that some sentences were marked as erroneous renditions, even though the only mistake consisted in the term used. At first, I had aimed to separate the evaluation of errors in the term renditions from the evaluation of the rest of the sentences. However, I realised that it would have been impossible to judge a sentence as ``not erroneous'' when the choice of term determined a complete upturning of the sentence meaning. Therefore, the evaluation task was defined as follows:

\begin{quote}
   ``Please highlight in yellow the sentences that have been interpreted incorrectly (= the sentence does not make sense OR its meaning is entirely or largely different when compared to the original)''.
\end{quote}

This led to a higher number of sentences coded as erroneous, but also to substantial agreement between the raters after the evaluation was repeated on the sample and on the rest of the data set ($\kappa = 0.66$).

The reliability of the yes/no scale for omissions was very high (mean $\kappa$ in the sample = 0.99), as was to be expected. The omitted sentences had been left blank in the evaluation table â€“ the task merely consisted in confirming that the content of the sentences had been omitted, and was formulated as follows: ``Please highlight in grey the sentences that were omitted completely (this might seem quite straightforward if the field is left blank (``/''), but there might be cases in which you feel that the ``T'' sentence already provides the information contained in the ``C'' sentence)''.

On the whole data set, the agreement between the two raters was almost perfect, apart from one case in which the second rater did not code the continuation sentence as omitted because the sentence of the ST consisted in a reformulation of the preceding target sentence.

\subsubsubsection{Global analysis} \label{globalanalysis}

To start, I calculated the percentage of severe errors and complete omissions coinciding with a stimulus query. The values were calculated for each rater and then averaged. For this first global analysis, I did not consider the effect of the tool on the participants' performance. Rather, the objective was to explore how much the interaction with the tool resulted in evident issues in the participants' renditions.

In the whole sample, on average, around one sixth of all sentences including a term query led to severe errors (Mdn = 16.91\%). Slightly more errors were committed in continuation sentences (Mdn = 16.67\%) than in target sentences (Mdn = 13.50\%), as shown by Figure \ref{fig:errors_TC}.

\begin{figure}
\includegraphics[width=\linewidth]{images/Errors_T or C.pdf}
\caption[Errors Target or Continuation]{Boxplot of the percentage of errors in target and continuation sentences after a query. On average, more severe errors were committed in continuation sentences.}
\label{fig:errors_TC}
\end{figure}

As for the number of omissions, a glossary query resulted in a complete omission of either the target or the continuation sentence (or both) in a limited number of cases (Mdn = 2.66\%). More continuation sentences were omitted than target sentences (Mdn = 5.32 \% vs. 1.05\%, see Figure \ref{fig:omissions_TC}).

\begin{figure}[H]\centering
\includegraphics[width=0.9\linewidth]{images/Omissions_T or C.pdf}
\caption[Omissions Target or Continuation]{Boxplot of the percentage of omissions in target and continuation sentences after a query. On average, more continuation sentences were omitted completely.}
\label{fig:omissions_TC}
\end{figure}

Despite the apparent differences observed when considering the sample average, the bar plot of the data (Figure \ref{fig:erroromiTC_all}) showing each participant's performance did not suggest a common trend as to whether more errors were committed in target or continuation sentences. To confirm the lack of difference, I conducted a Mann-Whitney $U$ test. As expected, the difference was not statistically significant ($U = 34.5, z = -0.53, p = 0.59$).\largerpage

The bar plot did however show that all participants omitted more continuation sentences than target sentences. I therefore proceeded to verify whether the difference was significant. A Mann-Whitney \textit{U} test confirmed that the difference was statistically significant ($U = 2.00, z = -3.41, p = 0.001$).

\begin{figure}
%\includegraphics[width=1\linewidth]{images/Barplot_EO_TC.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 25,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {\%},
  enlarge x limits = .07,
  bar width = 6,
%  nodes near coords,
  legend cell align=left,
  legend style={at={(0.5,-0.25)},anchor=north},
  legend columns={2},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,23.66)
  (2,18.48)
  (3,22.63)
  (4,12.79)
  (5,6.99)
  (6,17.16)
  (7,11.79)
  (8,13.50)
  (9,10.64)
  };
  \addplot+[lsMidDarkBlue]
  coordinates {
  (1,23.66)
  (2,23.91)
  (3,15.79)
  (4,24.42)
  (5,10.75)
  (6,16.67)
  (7,16.98)
  (8,8.50)
  (9,11.70)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,1.08)
  (2,0.00)
  (3,1.05)
  (4,1.16)
  (5,1.08)
  (6,0.00)
  (7,0.94)
  (8,2.00)
  (9,0.00)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,6.45)
  (2,5.43)
  (3,4.21)
  (4,2.33)
  (5,8.60)
  (6,1.96)
  (7,1.89)
  (8,5.50)
  (9,5.32)
  };
  \legend{Errors -- targent sentence,
  Errors -- continuation sentence,
  Omissions -- target sentence,
  Omissions -- continuation sentence}
  \end{axis}
\end{tikzpicture}
\caption[Percentage of errors and omissions in target and continuation sentences]{Percentage of errors and omissions in target and continuation sentences (Terms searched, mean value per participant)}
\label{fig:erroromiTC_all}
\end{figure}
%\todo{need data for figure}

\subsubsubsection{Effect of tool on errors and omissions: Terms searched} \label{tool_errors_omissions_S}
To verify how the individual tools affected the occurrence of severe errors and of complete omissions, I first analysed all cases in which a term query had been performed. As remarked for the analysis of the terminological accuracy (see \sectref{accuracy}), for the simASR condition, all terms searched were also ``found''.

\begin{figure}
\includegraphics[width=\linewidth]{images/BP_Errors_S_Tool.pdf}
\caption[Mean percentage of errors (Terms searched, by tool)]{Mean percentage of errors for terms searched (Terms searched, mean value by tool)}
\label{fig:BPErrSTool}
\end{figure}

As shown by Figure \ref{fig:BPErrSTool}, fewer severe errors were committed under the simASR condition (Mdn = 11.11\%). The worst performance was obtained with the PDF tool (Mdn = 19.85\%), while in the CAI condition a glossary query yielded more severe errors than in the simASR condition, but less than in the PDF condition (Mdn = 15.28\%).

\begin{figure}
\includegraphics[width=\linewidth]{images/BP_Omissions_S_Tool.pdf}
\caption[Mean percentage of omissions (Terms searched, by tool)]{Mean percentage of omissions for terms searched (Terms searched, mean value by tool)}
\label{fig:BPOmSTool}
\end{figure}

With reference to the number of sentences omitted completely due to the interaction with the tool (see Figure \ref{fig:BPOmSTool}), the simASR yielded the best results (Mdn = 0.00\% omitted sentences). Queries with the PDF glossary resulted in 4.55\% (Mdn) omitted sentences, while the CAI tool led to better results (Mdn = 2.00\%).

The bar plots in Figures \ref{fig:ErrSPar} and \ref{fig:OmSPar} show the differences for each participant both for severe errors and completely omitted sentences.

%\todo{need data for figures}
\begin{figure}
%\includegraphics[width=0.9\linewidth]{images/Errors_S_Tool.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 40,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Severe errors (\%)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend pos={north east},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,35.94)
  (2,17.05)
  (3,27.59)
  (4,21.59)
  (5,16.00)
  (6,18.18)
  (7,23.53)
  (8,19.85)
  (9,16.41)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,13.00)
  (2,22.79)
  (3,18.33)
  (4,19.64)
  (5,7.03)
  (6,21.97)
  (7,15.28)
  (8,4.17)
  (9,12.50)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,20.14)
  (2,22.22)
  (3,13.19)
  (4,15.97)
  (5,5.56)
  (6,11.11)
  (7,4.86)
  (8,8.33)
  (9,5.56)
  };
  \legend{PDF,CAI,simASR}
  \end{axis}
\end{tikzpicture}
\caption[Mean percentage of errors (Terms searched, by tool and participant]{Mean percentage of errors for terms searched (Terms searched, mean value by tool per participant)}
\label{fig:ErrSPar}
\end{figure}

\begin{figure}
%\includegraphics[width=0.9\linewidth]{images/Omissions_S_Tool.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 12,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Complete omissions (\%)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend pos={north west},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,7.81)
  (2,2.27)
  (3,5.17)
  (4,4.55)
  (5,4.00)
  (6,3.03)
  (7,2.94)
  (8,10.29)
  (9,4.69)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,2.00)
  (2,4.41)
  (3,3.33)
  (4,1.79)
  (5,4.69)
  (6,0.00)
  (7,1.39)
  (8,1.67)
  (9,3.85)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,1.39)
  (2,1.39)
  (3,0.00)
  (4,0.00)
  (5,5.56)
  (6,0.00)
  (7,0.00)
  (8,0.00)
  (9,0.00)
  };
  \legend{PDF,CAI,simASR}
  \end{axis}
\end{tikzpicture}
\caption[Mean percentage of omissions (Terms searched, by tool and participant]{Mean percentage of omissions for terms searched (Terms searched, mean value by tool per participant)}
\label{fig:OmSPar}
\end{figure}

To test whether the observed differences were significant, I conducted non-parametric Friedman's tests for the errors and omissions data. A non-parametric test was used since the omissions data were not normally distributed for the simASR condition and due to the limited sample size.

A significant difference was found for the effect of tools on the percentage of errors in the sentences for which a glossary query had been conducted ($\chi^2(2)=8.00, p=0.018$). The post-hoc Wilcoxon signed-ranks tests with a Bonferroni correction showed a statistically significant difference only between the simASR and the PDF condition ($p=0.014$).

A statistically significant difference between tools was also found for the percentage of omitted sentences after a glossary query ($\chi^2(2)=8.97, p=0.011$). The post-hoc Wilcoxon signed-ranks tests with a Bonferroni correction showed a significant difference between the simASR and the PDF condition ($p=0.010$).

\subsubsubsection{Effect of tool on errors and omissions: Terms searched/found} \label{tool_errors_omissions_F}
To further the analysis, I explored whether there were differences between the tools in the percentage of severe errors and complete omissions after successful queries (terms searched/found).

\begin{figure}
\includegraphics[width=\linewidth]{images/BP_Errors_SF_Tool.pdf}
\caption[Mean percentage of errors (Terms searched/found, by tool]{Mean percentage of errors for terms searched/found (Terms searched, mean value by tool)}
\label{fig:BPErrSFTool}
\end{figure}

For terms searched/found, the simASR results are the same as the ones reported in \sectref{tool_errors_omissions_S}. As shown by Figure \ref{fig:BPErrSFTool}, slightly more severe errors were committed under the CAI condition (Mdn = 13.97\%) than under the PDF condition (Mdn = 13.39\%).

\begin{figure}
\includegraphics[width=\linewidth]{images/BP_Omissions_SF_Tool.pdf}
\caption[Mean percentage of omissions (Terms searched/found, by tool]{Mean percentage of omissions for terms searched/found (Terms searched, mean value by tool)}
\label{fig:BPOmSFTool}
\end{figure}

As for the number of sentences omitted completely despite a successful term query (Figure \ref{fig:BPOmSFTool}), the CAI tool yielded better results (Mdn = 1.92\% omitted sentences) as compared to the PDF condition (3.70\%).

The bar plots in Figures \ref{fig:ErrSFPar} and \ref{fig:OmSFPar} show the differences for each participant both for severe errors and completely omitted sentences when the terms were searched/found in the glossaries.

%\todo{need data for figures}
\begin{figure}
%\includegraphics[width=0.9\linewidth]{images/Errors_SF_Tool.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 40,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Severe errors (\%)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend pos={north east},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,34.00)
  (2,11.25)
  (3,23.96)
  (4,26.39)
  (5,12.50)
  (6,12.04)
  (7,21.97)
  (8,12.04)
  (9,13.39)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,9.09)
  (2,21.97)
  (3,17.86)
  (4,21.15)
  (5,7.26)
  (6,19.53)
  (7,13.97)
  (8,4.17)
  (9,12.50)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,20.14)
  (2,22.22)
  (3,13.19)
  (4,15.97)
  (5,5.56)
  (6,11.11)
  (7,4.86)
  (8,8.33)
  (9,5.56)
  };
  \legend{PDF,CAI,simASR}
  \end{axis}
\end{tikzpicture}
\caption[Mean percentage of errors (Terms searched/found, by tool and participant]{Mean percentage of errors for terms searched/found (Mean value by tool per participant)}
\label{fig:ErrSFPar}
\end{figure}

\begin{figure}
%\includegraphics[width=0.9\linewidth]{images/Omissions_SF_Tool.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 12,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Complete omissions (\%)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend pos={north west},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,6.00)
  (2,2.50)
  (3,6.25)
  (4,2.78)
  (5,0.00)
  (6,3.70)
  (7,3.03)
  (8,8.33)
  (9,5.36)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,2.27)
  (2,4.55)
  (3,1.79)
  (4,1.92)
  (5,3.23)
  (6,0.00)
  (7,0.00)
  (8,1.67)
  (9,3.85)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,1.39)
  (2,1.39)
  (3,0.00)
  (4,0.00)
  (5,5.56)
  (6,0.00)
  (7,0.00)
  (8,0.00)
  (9,0.00)
  };
  \legend{PDF,CAI,simASR}
  \end{axis}
\end{tikzpicture}
\caption[Mean percentage of omissions (Terms searched/found), by tool and participant]{Mean percentage of omissions for terms searched/found (Mean value by tool per participant)}
\label{fig:OmSFPar}
\end{figure}

As for the previous analyses on the data for the terms searched/found, I conducted two Friedman tests (for the errors and the omissions) to test for significance.\largerpage[-2]\pagebreak

A significant difference was found for the effect of tools on the percentage of errors in the sentences for which a glossary query had been conducted ($\chi^2(2) = 6.88, p = 0.032$). The post-hoc Wilcoxon signed-ranks tests with a Bonferroni correction showed a significant difference between the simASR and the PDF condition ($p = 0.029$).

A statistically significant difference between tools was also found for the percentage of omitted sentences after a glossary query ($\chi^2(2) = 8.47, p = 0.014$). The post-hoc Wilcoxon signed-ranks tests with a Bonferroni correction showed a significant difference between the simASR and the PDF condition ($p = 0.014$).


\subsection{Behavioural measures: Analysis} \label{behaviouralresults}
In addition to the two performance measures discussed in the previous sections, six behavioural measures were analysed. In this section, I report and discuss the results of the tests conducted on the glossary queries, on the two time-lag measures of EVS and ICPD, and on three gaze-related measures: time to first fixation, average fixation duration, and total fixation time.
\subsubsection{Glossary queries} \label{queries}
In order to gain a picture of the test subjects' interactions with the tools during SI, I collected data pertaining to the queries performed in the PDF glossary and in InterpretBank. For the CAI-ASR mock-up, no analysis was conducted, as in this condition no action was required from the participants.

In addition, the analysis of the glossary queries was used to test how the two criteria followed in the speech design, i.e. stimulus position in the sentence and morphological complexity (see \sectref{material_MS}), affected the participants' search behaviour. In this case, no distinction was made between the tools, as my goal was to verify whether my assumptions about potential effects of position or morphological complexity (see \sectref{behaviouralm}) would be supported by the analysis.

Based on a methodology already adopted in previous studies \citep{prandi_uso_2015, prandi_use_2015, prandi_designing_2017, prandi_exploratory_2018}, I therefore considered the following categories of terms:

\begin{itemize}
    \sloppy
    \item Terms searched by tool;
    \item Terms searched/found by tool;
    \item Terms searched by position (sentence-medial and sentence-final);
    \item Terms searched/found by position (sentence-medial and sentence-final);
    \item Terms searched by morphological complexity (unigrams, bigrams, trigrams);
    \item Terms searched/found by morphological complexity (unigrams, bigrams, trigrams).
\end{itemize}

\subsubsubsection{Terms searched} \label{terms_searched}
This category represents the percentage of terms searched with each tool. Each speech contained 36 terms. For the CAI-ASR integration, the percentage was always 100\%, since the terms were all automatically shown on the screen. For this category, it was therefore interesting to verify whether there was a difference in the percentage of terms looked up with the PDF glossary and with InterpretBank, since for both conditions participants were required to choose whether they wanted to conduct a glossary query or not. The simASR condition was therefore not considered in this analysis.

When considering the whole sample, slightly more terms were searched when working with the PDF glossary (Mdn = 88.89\%) than with InterpretBank (Mdn = 83.33\%), precisely in four cases out of nine. One participant looked up the same number of terms with both tools (83.33\%). In another case (P7), all 36 terms were searched also when working with the CAI tool (see Figure \ref{fig:termssearched}).

A Wilcoxon signed-ranks test was conducted to test for significance. As expected, no statistically significant difference was found in the percentage of terms searched with the PDF and with the CAI tool ($T = 20.5, p = 0.67$).

%\todo{need data for figure}
\begin{figure}
%\includegraphics[width=0.9\linewidth]{images/Barplot_S_tool.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 100,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Queries (\%)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend pos={south east},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,88.89)
  (2,63.89)
  (3,83.33)
  (4,63.89)
  (5,69.44)
  (6,94.44)
  (7,91.67)
  (8,94.44)
  (9,88.89)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,69.44)
  (2,94.44)
  (3,83.33)
  (4,77.78)
  (5,88.89)
  (6,91.67)
  (7,100.00)
  (8,83.33)
  (9,72.22)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,100)
  (2,100)
  (3,100)
  (4,100)
  (5,100)
  (6,100)
  (7,100)
  (8,100)
  (9,100)
  };
  \legend{PDF,CAI,simASR}
  \end{axis}
\end{tikzpicture}
\caption[Percentage of terms searched]{Percentage of terms searched in the PDF, CAI and simASR condition for each participant. The value is always 100\% for the simASR condition.}
\label{fig:termssearched}
\end{figure}

\subsubsubsection{Terms searched/found} \label{terms_found}
One of the main indicators of how useful the tools prove as terminology support in the booth is the percentage of terms searched that were actually found by the tool. For this analysis, I did not consider whether the participant found the term in the glossary, but rather whether the term was visualised on the screen after having been looked up. It is possible that a term found by the tool may nonetheless be left untranslated or mistranslated if the participant was unable to localise it on the monitor.

Also in this case, the percentage is of course 100\% for the ASR-CAI mock-up for all participants, so I only considered the PDF and the CAI tool in the analysis.

\begin{figure}
%\includegraphics[width=0.9\linewidth]{images/Barplot_SF_tool.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 100,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Queries (\%)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend pos={south east},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,78.13)
  (2,91.30)
  (3,76.67)
  (4,78.26)
  (5,88.00)
  (6,76.47)
  (7,91.18)
  (8,79.41)
  (9,87.50)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,88.00)
  (2,97.06)
  (3,93.33)
  (4,92.86)
  (5,96.88)
  (6,96.97)
  (7,94.44)
  (8,100.00)
  (9,100.00)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,100)
  (2,100)
  (3,100)
  (4,100)
  (5,100)
  (6,100)
  (7,100)
  (8,100)
  (9,100)
  };
  \legend{PDF,CAI,simASR}
  \end{axis}
\end{tikzpicture}
\caption[Percentage of terms searched/found]{Percentage of terms searched/found  in the PDF, CAI and simASR condition for each participant. The value is always 100\% for the simASR condition.}
\label{fig:termsfound}
\end{figure}

After a visual inspection of the data (see Figure \ref{fig:termsfound}), when comparing the percentage of terms searched/found by each tool, the CAI tool seemed to perform better than the PDF glossary for each participant.

I therefore decided to conduct further tests to verify whether the observed difference was significant. I conducted a Wilcoxon signed-ranks test to verify whether the observed difference was statistically significant. A non-parametric test was selected due to the small sample size. The results show a significant difference ($T = 45.00, p = 0.008$) between the CAI (Mdn = 96.88\% successful queries) and the PDF condition (Mdn = 79.41\% successful queries).

\subsubsubsection{Terms searched by position} \label{terms_searched_position}
In the speeches prepared for the experiment, the stimuli were distributed equally between terms in the middle of the sentence and terms at the end of the sentence.

At a first visual inspection of the data per participant, I did not identify a clear trend supporting either of the hypotheses formulated in \sectref{hypotheses}, as in some cases the terms at the end of the sentence were looked up more often, while in others the opposite was true.

On average, however, the participants looked up more sentence-final terms (Mdn = 52.24\%) than sentence-medial terms (Mdn = 47.76\%), as shown by Figure~\ref{fig:BP_POS_S}.\largerpage

\begin{figure}
\includegraphics[width=\linewidth]{images/BP_POS_S.pdf}
\caption[Percentage of terms searched by position]{Percentage of terms searched according to their position in the sentence. More sentence-final terms were looked up than sentence-medial terms.}
\label{fig:BP_POS_S}
\end{figure}

A Mann-Whitney \textit{U} test showed a statistically significant difference between the two conditions ($U = 5.00, z = -3.14, p = 0.002$).

\subsubsubsection{Terms searched/found by position} \label{terms_found_position}
In order to further explore whether the position of the terms may have affected the success rate of terminology look-up, I calculated the percentage of terms searched/found in each of the two positions (sentence-medial and sentence-final) for each participant. No clear trend could be identified between the PDF and the CAI tool after a visual inspection of the data.

For the participants, it was overall easier to find terms when they were placed at the end of the sentence (Mdn = 53.42\%, sentence-medial terms: Mdn = 46.58\%), as shown by Figure \ref{fig:BP_POS_SF}.

\begin{figure}
\includegraphics[width=\linewidth]{images/BP_POS_SF.pdf}
\caption[Percentage of terms searched/found by position]{Percentage of terms searched/found according to their position in the sentence. More sentence-final terms looked up were found than sentence-medial terms.}
\label{fig:BP_POS_SF}
\end{figure}

A Mann-Whitney \textit{U} test showed a statistically significant difference between the sentence-medial and the sentence-final terms ($U = 7.50, z = -2.915, p = 0.004$).

\subsubsubsection{Terms searched by morphological complexity} \label{terms_searched_structure}
I then considered whether the morphological complexity of the terms used as stimuli (see \sectref{material_MS}) may have had an impact on the participants' search behaviour.

The first category of terms analysed were the terms searched. The participants looked up more unigrams (Mdn = 34.33\%) than bigrams and trigrams. More trigrams (Mdn = 33.33\%) were looked up than bigrams (Mdn = 32.84\%). However, the differences were small, as shown by the plotted data (Figure \ref{fig:BP_MC_S}).

\begin{figure}
\includegraphics[width=\linewidth]{images/BP_MC_S.pdf}
\caption[Percentage of terms searched by morphological complexity]{Percentage of terms searched according to their morphological complexity. Approximately the same percentage of unigrams, bigrams and trigrams were looked up.}
\label{fig:BP_MC_S}
\end{figure}

A Kruskall-Wallis test revealed that the difference was not statistically significant ($H(2) = 2.41, p = 0.30$). In other words, the morphological complexity of the terms did not affect the number of terms looked up.

\subsubsubsection{Terms searched/found by morphological complexity} \label{terms_found_structure}
In order to further explore my hypotheses on how term structure may affect the search behaviour and the success rate of the queries performed, I consider the percentage of uni-, bi- and trigrams searched/found with the PDF and the CAI tool, while I excluded the simASR condition for which the distribution was equal between the three categories.

In the sample, trigrams were the category of terms looked up that were found more often (Mdn = 36.21\%), followed by bigrams (Mdn = 31.91\%) and unigrams (Mdn = 31.03\%), as shown by the boxplot in Figure \ref{fig:BP_MC_SF}.

\begin{figure}
\includegraphics[width=\linewidth]{images/BP_MC_SF.pdf}
\caption[Percentage of terms searched/found by morphological complexity]{Percentage of terms searched/found according to their morphological complexity. Significantly more trigrams were found than unigrams.}
\label{fig:BP_MC_SF}
\end{figure}


I tested whether the observed difference was statistically significant using a Kruskal-Wallis non-parametric test as I had done for the terms searched. The difference was statistically significant ($H(2) = 11.85, p = 0.003$). Post-hoc Mann-Whitney tests with a Bonferroni correction revealed a statistically significant difference only between the number of unigrams and trigrams found in the glossary ($p = 0.002$). It appears that it was easier to find trigrams than it was to find unigrams.

\subsubsubsection{Additional observations} \label{additional_obs}
A series of additional phenomena could be identified while reviewing the Gaze Replays for data preparation. Given the small sample size, it is possible that these phenomena are isolated occurrences, but I report them nonetheless as they may be useful in formulating additional hypotheses on SI with digital terminology support to be tested in future studies.

In four cases out of nine, a handful of terms (nine in total) were shown on the screen when the PDF and/or the CAI tool was used, even though they had not been looked up by the participants. In most cases, they were left untranslated or mistranslated, probably because they had not been seen. In 3 cases, the glossary equivalent was used, which would indicate that they had been seen by the participants. Even though this was a very rare occurrence during the experiment, it is interesting to notice that this could represent an additional advantage of using digital tools for terminology look-up. A dedicated experiment would be necessary to explore this aspect further.

When searching for terms, several participants could not find the term they were looking for as they did not know its spelling. This was shown by the repetition of the queries with different spellings of the same term. In some cases, the mistakes were quite naive and with all probability due to the participants having misheard the term. An experienced conference interpreter would probably have recognised that the term pronounced by the speaker was ``fission'', and not ``fishing'', especially in the context of a speech about nuclear energy. For other terms, such as ``boule'', which may have a complex spelling for non-native speakers of English, the difficulty is more understandable. This phenomenon further stresses the importance of preparation for effective terminology search during interpreting, as well as the role of CAI tools as digital aids and not as a replacement for preparation strategies.

Additionally, I observed several instances of terms queried which I had not selected as stimuli, such as ``wheat'', ``power plant'' and ``coal''. Nonetheless, they apparently represented additional difficulties for some participants. This might explain errors or omissions and in general the breakdown of the interpreting process even in sentences where no stimulus query had been performed.

\subsubsection{Ear-voice span} \label{EVS}
For the PDF and CAI conditions, I only considered the terms which had been looked up and translated. The cases in which a search resulted in an omission of the term were not considered, as no relative time stamp was available. For the simASR condition, all terms were taken into consideration, as there were no cases of zero rendition for any participant in this condition.

As pointed out by \citet{alvstad_time_2011}, the EVS can vary considerably during an interpreting session and the average value is susceptible to minimum and maximum values. In my sample, I observed cases of very long EVS when compared to other values registered for the same participant. Therefore, for this analysis I report not only the average values, but also the median values, as suggested in \citet{alvstad_time_2011}.

The mean and median EVS varied considerably between subjects, which was to be expected as it can reflect different cognitive make-up, interpreting styles, strategies and reaction times. Nonetheless, all participants showed the same pattern, both for the mean and the median values: the EVS was consistently shorter when the simASR support was provided, followed by the CAI tool and the PDF (see Table \ref{tab:EVS}). The difference appeared even more evident at the analysis of median instead of mean values.

To confirm whether the differences observed at a first inspection of the data were statistically significant, I proceeded with a series of tests. Data were not normally distributed as one participant had longer EVS than all other participants. Therefore, I opted for a non-parametric Friedman test on the rank-transformed data for the terms searched and searched/found in the PDF, CAI and simASR condition. Since the ranking was the same both for mean and median values and for the terms searched and searched/found, as the same pattern was observed for all participants, I conducted only one test.

\begin{table}
\begin{tabular}{ccccccc}
\lsptoprule
\multicolumn{7}{c}{{Terms searched}} \\
\midrule
{Ptcpt} & \multicolumn{2}{c}{PDF} & \multicolumn{2}{c}{CAI} & \multicolumn{2}{c}{simASR}\\\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
& Mean & Median & Mean & Median & Mean & Median \\
\midrule
1 & 5277 & 4339 & 3789 & 3800 & 2153 & 2045 \\
2 & 7919 & 7210	& 6845 & 6560 & 3794 & 3089 \\
3 & 5563 & 5068 & 4264 & 4000 & 2378 & 2445 \\
4 & 5574 & 5290 & 3214 & 3003 & 1681 & 1567 \\
5 & 5432 & 5495 & 5075 & 4168 & 2516 & 2065 \\
6 & 5085 & 4690 & 3832 & 3300 & 1991 & 1875 \\
7 & 4835 & 4200 & 3403 & 2719 & 2810 & 2505 \\
8 & 5877 & 5070 & 2742 & 2605 & 1810 & 1490 \\
9 & 4447 & 4210 & 3224 & 2860 & 1833 & 1650 \\
\midrule
\multicolumn{7}{c}{{Terms searched/found}}\\
\midrule
{Ptcpt} & \multicolumn{2}{c}{PDF} & \multicolumn{2}{c}{CAI} & \multicolumn{2}{c}{simASR}\\\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
& Mean & Median & Mean & Median & Mean & Median \\
\midrule
1 & 5336 & 4351 & 3662 & 3782 & 2153 & 2045 \\
2 & 7390 & 6950 & 6650 & 6350 & 3794 & 3089 \\
3 & 5906 & 5114 & 4223 & 3886 & 2378 & 2445 \\
4 & 5532 & 5270 & 3108 & 2982 & 1681 & 1567 \\
5 & 5304 & 5208 & 4868 & 4166 & 2516 & 2065 \\
6 & 5237 & 4860 & 3705 & 3250 & 1991 & 1875 \\
7 & 4835 & 4200 & 3403 & 2719 & 2810 & 2505 \\
8 & 5876 & 4785 & 2742 & 2605 & 1810 & 1490 \\
9 & 4483 & 4210 & 3224 & 2860 & 1833 & 1650 \\
\lspbottomrule
    \end{tabular}
\caption{Mean and median EVS values (ms) for each condition, both for terms searched and for terms searched/found. The values are the same as for the simASR condition, as all terms were displayed on the screen.\label{tab:EVS}}
\end{table}

The differences were statistically significant ($\chi^2(2) = 18.00, p < 0.001$). Pairwise comparisons were conducted post-hoc as Wilcoxon signed-rank tests with a Bonferroni correction. They confirmed that the difference in EVS length between each tool and the other two was statistically significant ($p = 0.009$ for all comparisons).


Considering the median values (see Figures \ref{fig:EVS_S}, \ref{fig:EVS_SF}), which in this case seems more logical due to the higher values for P2 which bias the mean upwards, the average sample value seems to indicate that participants could gain 1.7s if they were working with the CAI tool as compared to a PDF glossary, and 1.2s when they used the ASR-CAI mock-up as compared to a standard CAI tool.

\begin{figure}
\includegraphics[width=\linewidth]{images/EVS_median_S.pdf}
\caption[Median EVS of terms searched]{Median EVS for terms searched}
\label{fig:EVS_S}
\end{figure}

\begin{figure}
\includegraphics[width=\linewidth]{images/EVS_median_SF.pdf}
\caption[Median EVS of terms searched/found]{Median EVS for terms searched/found}
\label{fig:EVS_SF}
\end{figure}

\subsubsection{Inter-cluster pause duration} \label{pause_duration}
In calculating the mean and median ICPD (see Figure \ref{fig:ICPDmedianS}), I only considered the cases in which a search had been performed in the tool, while I omitted the cases in which the continuation sentence had been completely left out, as this would have shown a very long ICPD actually due to an omission.

\begin{figure}
\includegraphics[width=\linewidth]{images/ICPD_median_S.pdf}
\caption[Boxplot of median ICPD after a query]{Boxplot of median ICPD (s) for each condition after a glossary query. A longer ICPD suggests faster processing of the preceding sentence cluster.}
\label{fig:ICPDmedianS}
\end{figure}

Since the same group was tested multiple times on the three conditions, I ran a Friedman test to verify whether the differences in the ICPD were statistically significant. The Friedman test confirmed that the difference was statistically significant ($\chi^2(2) = 14.00, p= 0.001$).

As I did for EVS, I also considered the median values of the ICPD. A Friedman test was conducted also on the median values and yielded similar results ($\chi^2(2) = 11.56, p = 0.003$).

By inspecting the data, I noticed that there were cases in which the average pause duration for the CAI tool was shorter than for the PDF, which I found quite surprising. With respect to the mean values, in one case the pause length was longer for the CAI tool than for the ASR mock-up (P4). I therefore also conducted post-hoc pairwise comparisons between the three conditions, both for the mean and the median values. The Wilcoxon signed-ranks tests with a Bonferroni correction for the mean values confirmed that the simASR ICPD was significantly longer (Mdn = 4.52s) when compared to the PDF (Mdn = 3.13s) condition ($p = 0.001$) or to the CAI (Mdn = 3.89s) condition ($p = 0.014$), but the same was not true for the other two conditions ($p = 1.00$).

If I consider the median values, a slightly different picture emerges. The differences in median ICPD were statistically significant only for the simASR tool (Mdn = 4.60s) compared to the PDF (Mdn = 3.15s) condition ($p = 0.003$). There was no statistically significant difference between the PDF and the CAI condition ($p = 1.00$) and between the CAI and simASR condition ($p = 0.055$). The median ICPD value for the CAI condition was 3.75s.

The same procedure was also followed for the pause durations between sentence clusters when terms had been found in the glossary\footnote{As in the previous cases, the values for the simASR condition are the same.}, considering both the mean and the median values. Non-parametric tests were used also in this case due to the small sample size. A Friedman test returned significant differences for the mean values ($\chi^2(2) = 14.89, p = 0.001$) and for the median values ($\chi^2(2) = 11.56, p = 0.003$).

\begin{sloppypar}
Post-hoc pairwise comparisons with Wilcoxon signed-ranks test with a Bonferroni correction yielded similar results to what had been observed for the searched terms. With concern to the mean values, the ICPD for the simASR condition (Mdn = 4.52s) were significantly longer ($p < 0.001$) than in the PDF condition (Mdn = 3.21s) and in the CAI condition ($p = 0.029$; Mdn CAI = 3.91s).
\end{sloppypar}

As for the median values (Figure \ref{fig:ICPDmedianSF}), the simASR ICPD (Mdn = 4.60) was also significantly longer only as compared to the PDF condition ($p = 0.003$; Mdn PDF = 3.33s). The other two contrasts did not show statistically significant differences. The median ICPD value for the CAI condition was 3.72s.

\begin{figure}
\includegraphics[width=\linewidth]{images/ICPD_median_SF.pdf}
\caption[Boxplot of median ICPD for terms searched/found]{Boxplot of median ICPD (s) for each condition after a successful glossary query. A longer ICPD suggests faster processing of the preceding sentence cluster.}
\label{fig:ICPDmedianSF}
\end{figure}

\subsubsection{Time to first fixation} \label{timetofirstfix}
To explore how fast the participants could identify the terms on the screen, I calculated the mean time to first fixation for the term AOIs for each participant under each condition. As for the previous metrics, I conducted a non-parametric Friedman test to establish whether the differences between the three conditions were statistically significant. The test showed a statistically significant difference ($\chi^2$(2) = 13.56, $p = 0.001$). Post-hoc Wilcoxon signed-rank tests with a Bonferroni correction showed statistically significant differences between the simASR and the PDF condition ($p = 0.018$) as well as between the ASR and CAI condition ($p = 0.018$). However, the difference between the CAI and the PDF condition ($p = 0.739$) was not significant.

\subsubsection{Average fixation duration} \label{fixation_dur}
In each condition and for each participant, I first calculated the average fixation duration for the term AOIs.

%\todo{need data for figure}
\begin{figure}
%\includegraphics[width=0.9\linewidth]{images/FixDurTermP.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 600,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {FixDur (ms)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend pos={south west},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,443.21)
  (2,409.88)
  (3,470.39)
  (4,441.51)
  (5,519.49)
  (6,223.61)
  (7,361.09)
  (8,547.06)
  (9,311.79)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,306.80)
  (2,365.15)
  (3,523.05)
  (4,526.61)
  (5,458.26)
  (6,231.01)
  (7,349.17)
  (8,421.84)
  (9,383.20)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,250.04)
  (2,276.91)
  (3,300.40)
  (4,222.62)
  (5,342.52)
  (6,159.18)
  (7,239.20)
  (8,379.85)
  (9,227.67)
  };
  \legend{PDF,CAI,ASR}
  \end{axis}
\end{tikzpicture}
\caption[Mean fixation duration on term AOIs for each condition]{Mean fixation duration (ms) on the term AOIs for each participant and each tool}
\label{fig:meanfixdur}
\end{figure}

When data was available for both eyes, I averaged the mean values. After plotting the data (Figures \ref{fig:meanfixdur} and \ref{fig:BPFixDurTermAOI}), I noticed that the average fixation duration in the simASR condition seemed shorter than in the other two conditions for all participants. In some cases, shorter fixations were observed for the CAI condition and in others for the PDF condition. I therefore decided to verify whether the observed differences were statistically significant.

I used a Friedman test to test for significance. The test showed a significant difference between the three conditions ($\chi^2(2) = 13.56, p < 0.001$). Post-hoc pairwise contrasts with a Bonferroni correction for multiple comparisons indicate a significant difference between the simASR condition (Mdn = 250.04) and the PDF (Mdn = 441.51) condition ($p < 0.003$). A statistically significant difference ($p = 0.007$) was also observed between the simASR and the CAI condition (Mdn = 383.20). No statistically significant difference was found between the PDF and the CAI condition ($p = 1.00$).

\begin{figure}
\includegraphics[width=\linewidth]{images/FixDur_TermAOI.pdf}
\caption[Boxplot of mean fixation duration on term AOIs]{Mean fixation duration (ms) on the term AOIs for each tool}
\label{fig:BPFixDurTermAOI}
\end{figure}

In addition to the average fixation durations on the term AOIs, I also considered the duration of the fixations on the tool area (Figures \ref{fig:FixDurToolP} and \ref{fig:BPFixDurToolAOI}).

%\todo{need data for figure}
\begin{figure}
%\includegraphics[width=0.9\linewidth]{images/FixDurToolP.jpg}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 600,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {FixDur (ms)},
  enlarge x limits = .07,
  bar width = 8,
%  nodes near coords,
  legend pos={south west},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,380.85)
  (2,330.25)
  (3,373.50)
  (4,522.15)
  (5,509.40)
  (6,288.35)
  (7,319.15)
  (8,445.25)
  (9,398.20)
  };
  \addplot+[lsMidBlue]
  coordinates {
  (1,265.90)
  (2,305.50)
  (3,330.80)
  (4,367.25)
  (5,417.90)
  (6,222.60)
  (7,266.05)
  (8,344.80)
  (9,314.85)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,247.35)
  (2,261.95)
  (3,277.70)
  (4,177.95)
  (5,399.95)
  (6,156.70)
  (7,233.90)
  (8,339.20)
  (9,225.70)
  };
  \legend{PDF,CAI,simASR}
  \end{axis}
\end{tikzpicture}
\caption[Mean fixation duration on tool AOI for each condition]{Mean fixation duration (ms) on the tool AOIs for each participant and each tool}
\label{fig:FixDurToolP}
\end{figure}

\begin{figure}
\includegraphics[width=\linewidth]{images/FixDur_ToolAOI.pdf}
\caption[Boxplot of mean fixation duration on tool AOIs]{Mean fixation duration (ms) on the tool AOIs for each tool}
\label{fig:BPFixDurToolAOI}
\end{figure}

I computed the average duration of fixations on the tool AOIs for each participant and conducted a Friedman test to verify the statistical significance of the differences observed on the plotted data. The test showed a statistically significant difference in the average fixation durations on the tool area between the three conditions ($\chi^2(2) = 18.00, p < 0.001$). Pairwise comparisons conducted post-hoc showed a statistically significant difference ($p < 0.001$) for the simASR condition (Mdn = 247.35) as compared to the PDF condition (Mdn = 380.85). No statistically significant difference was found for the other contrasts (Mdn CAI = 314.85).

\subsubsection{Fixation time} \label{fixation_time}
The last gaze-related metric considered was the fixation time on the tool AOI and on the speaker AOI under the PDF, the CAI, and the simASR condition.

%\todo{need data for figures}
\begin{figure}
% \subfloat{\includegraphics[width=0.66\linewidth]{images/19a_fix_time_PDF.png}}\\
% \subfloat{\includegraphics[width=0.66\linewidth]{images/19b_fix_time_CAI.png}}\\
% \subfloat{\includegraphics[width=0.66\linewidth]{images/19c_fix_time_ASR.png}}%
\subfloat[PDF]{
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height = 5cm,
  ymin = 0,
  ymax = 800,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Fixation time (s)},
  enlarge x limits = .07,
  bar width = 9,
%  nodes near coords,
  legend pos=north east,
  legend columns={-1},
  font=\footnotesize
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,242)
  (2,461)
  (3,363)
  (4,521)
  (5,229)
  (6,179)
  (7,403)
  (8,507)
  (9,559)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,261)
  (2,245)
  (3,255)
  (4,239)
  (5,302)
  (6,359)
  (7,214)
  (8,214)
  (9,180)
  };
  \legend{Speaker, Tool}
  \end{axis}
\end{tikzpicture}
}\\
\subfloat[CAI]{
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height = 5cm,
  ymin = 0,
  ymax = 800,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Fixation time (s)},
  enlarge x limits = .07,
  bar width = 9,
%  nodes near coords,
  legend pos=north east,
  legend columns={-1},
  font=\footnotesize
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,374)
  (2,431)
  (3,405)
  (4,620)
  (5,351)
  (6,223)
  (7,471)
  (8,697)
  (9,619)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,122)
  (2,209)
  (3,175)
  (4,142)
  (5,193)
  (6,261)
  (7,143)
  (8,95)
  (9,91)
  };
%   \legend{Speaker, Tool}
  \end{axis}
\end{tikzpicture}
}\\
\subfloat[simASR]{
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height = 4cm,
  ymin = 0,
  ymax = 800,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Fixation time (s)},
  enlarge x limits = .07,
  bar width = 9,
%  nodes near coords,
  legend pos=north east,
  legend columns={-1},
  font=\footnotesize
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,503)
  (2,594)
  (3,488)
  (4,386)
  (5,555)
  (6,265)
  (7,541)
  (8,715)
  (9,167)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,118)
  (2,102)
  (3,124)
  (4,59)
  (5,215)
  (6,146)
  (7,166)
  (8,101)
  (9,67)
  };
%   \legend{Speaker, Tool}
  \end{axis}
\end{tikzpicture}
}
\caption[Fixation time on the speaker and tool AOIs]{Fixation time (s) on the speaker and tool area for each condition}
\label{fig:fixationtime}
\end{figure}


The aim was to verify whether the participants spent a significantly different amount of time fixating the tool AOI as compared to the tool AOI under each condition.

The total fixation time was measured in seconds. I conducted a Mann-Whitney \textit{U} test for each condition to verify my hypothesis that the effect of the tool would be greater for the simASR condition, i.e. that when working with the ASR-CAI mock-up, the participants spent significantly more time looking at the speaker than at the tool.

For the simASR condition, the difference in the time spent fixating the speaker AOI (Mdn = 503.00) and the tool AOI (Mdn = 118.00) was significant ($U = 1.00, z = -3.49, p < 0.001$). A statistically significant difference was also found for the CAI tool ($U = 1.00, z = -3.49, p < 0.001$) between the speaker (Mdn = 431.00) and the tool AOI (Mdn = 143.00). For the PDF condition, the median values for the speaker and tool AOIs were respectively 403.00s and 245.00s. The difference was not significant ($U = 20.00, p = 0.070$).

\subsection{Subjective measure: The debriefing questionnaire} \label{debriefing}
After data collection, participants were asked to answer a brief questionnaire designed to collect additional qualitative data to help frame the quantitative analysis.

To start, they were asked to rank the speeches from the easiest to the most difficult. Overall, the coal industry speech (speech C) was considered slightly more difficult, while the transmutation speech (B) was considered slightly easier, as can be seen in Figure \ref{fig:difficultyspeech}.
%\todo{need data for figure}

\begin{figure}
%\includegraphics[width=0.75\linewidth]{images/7_difficulty_speeches.png}
\begin{tikzpicture}
  \begin{axis}[
  ybar=2pt,
  bar width=15pt,
  axis lines*=left,
  height=6cm,
  width = 8cm,
  ymin = 1,
  ymax = 3,
  xtick = {1,2,3},
  xticklabels = {Speech A:\\Biocrude,
  Speech B:\\Transmutation,
  Speech C:\\Coal industry
  },
  x tick label style = {align=center},
  ylabel = {Difficulty (easy > difficult)},
%  nodes near coords,
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,2)
  (2,1.78)
  (3,2.22)
  };
  \end{axis}
\end{tikzpicture}
\caption[Perceived difficulty of the speeches used in the experiment]{Bar plot of the participants' ratings of the perceived difficulty of the three speeches used in the main experiment (mean sample values)}
\label{fig:difficultyspeech}
\end{figure}

However, in order to exclude speech difficulty effects, the order of the speeches and of the tools had been randomised. Therefore, I can exclude that the content of the speeches may have had a significant effect on the interpreting performance and on the interaction with the tools.

Additionally, participants were asked to rank the tools from the most useful to the least useful and from the most distracting to the least distracting.

%\todo{need data for figure}
\begin{figure}
%\includegraphics[width=0.45\linewidth]{images/8a_distraction_tool.png}
%\includegraphics[width=0.45\linewidth]{images/8b_usefulness_tool.png}
\begin{subfigure}[b]{.5\textwidth}\centering
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  ylabel near ticks,
  enlarge x limits=0.25,
  axis lines*=left,
  width = \textwidth,
  ymin = 1,
  ymax = 3,
  xtick = {1,2,3},
  xticklabels = {PDF,CAI,simASR},
  bar width = 15pt,
%  x tick label style = {align=center},
  ylabel = {},
%  nodes near coords,
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,2.78)
  (2,1.89)
  (3,1.33)
  };
  \end{axis}
\end{tikzpicture}
\caption{Distraction potential}
\end{subfigure}\begin{subfigure}[b]{.5\textwidth}
\centering
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  ylabel near ticks,
  enlarge x limits=0.25,
  axis lines*=left,
  width = \textwidth,
  ymin = 1,
  ymax = 3,
  xtick = {1,2,3},
  bar width = 15pt,  
  xticklabels = {PDF,CAI,simASR},
%  x tick label style = {align=center},
  ylabel = {},
%  nodes near coords,
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,1.22)
  (2,2.11)
  (3,2.67)
  };
  \end{axis}
\end{tikzpicture}
\caption{Usefulness}
\end{subfigure}
\caption{Perceived distraction potential and usefulness of tools (mean sample values)\label{fig:distract_useful}}
\end{figure}

As can be seen in Figure \ref{fig:distract_useful}, the ASR-CAI mock-up was rated as the most useful and least distracting tool. The PDF glossary was considered the least useful and most distracting. As for the tool that participants would bring with them into the booth during a future assignment, most of them selected the ASR tool, two preferred the CAI tool and none indicated the PDF glossary as their preferred type of support.

In Table \ref{tab:proscons}, for each tool I report the characteristics identified as more useful and the key issues mentioned by the participants in descending order of frequency.

The most useful aspect of working with the PDF glossary was indicated as the possibility to see many entries at once.

As observed by one participant, this also provides the additional advantage of being able to identify a term in the glossary even though the spelling is unknown (provided that at least the first letter or sequence of letters has been typed correctly). Similar commentaries were also present in the responses (2 mentions) emphasising the usefulness of the alphabetical order, which facilitated a manual search whenever the spelling was unknown.

The program highlighted the first occurrence for the string searched. This was also considered as a useful visual aid in identifying the searched term. An additional helpful feature was identified in the ability of the program to search partial words and not only complete terms, which reduced the amount of time necessary for a search. In one case, no helpful features were mentioned, while in another case a participant laconically commented that ``vocab[ulary]'' was there.

As for the negative aspects of working with a PDF glossary, some comments reflect the other side of the coin of the features mentioned under ``most useful''. Very often (6 commentaries), reference is made to the need to navigate through the glossary if the first occurrence does not correspond to the term searched. This is a consequence of the fact that the results are not filtered out of the glossary, but rather highlighted in the body of the document. As stressed by one participant, this may prove distracting.

The way the search occurs within the document was also object of stark criticism. Here the main issues were identified in the need to clear the search field after each search, which may cause participants to lose precious time, and to click the enter key to start a search. Typing errors would also require a new search, as mentioned by two participants. These difficulties were summed up in the general commentary that ``looking up words is cumbersome'' in this modality.

\begin{table}
\small
\begin{tabularx}{\textwidth}{lQQ}
\lsptoprule
{Tool} & {Most useful} & {Most problematic}\\
\midrule
\multirow{6}{*}[-12pt]{{PDF}} & Overview of terminology (4) & Distraction due to navigation \newline in the file (6) \\
& Alphabetical order (2) & Search field not cleared \newline automatically - time loss (5) \\
& Highlighting of results (2) & Traditional type of search (4) \\
& Search for parts of words (1) & Cumbersome (1) \\
& Terminology is available (1) &\\
& Nothing (1) &\\
 \midrule
\multirow{6}{*}[-12pt]{{CAI}} & Progressive search (7) & Distraction due to typing \newline while speaking (4) \\
& Search field cleared \newline automatically (4) & No correction of wrong spelling \newline (2)\\
& Fuzzy search (2) & Too many/too few terms (1)\\
& Multiple results per query shown (2) & No assistance if term missed \newline or misheard (1)\\
& Speed (1) & Nothing (1) \\
& Easier to use than PDF (1) & \\
\midrule
\multirow{8}{*}[-16pt]{{ASR}} & No distraction due to typing (6) & Distraction / frustration if \newline expected term not shown (7) \\
& Low latency (3) & Inability to search for terms \newline not shown automatically (3)\\
& Easy to use (2) & High latency (1)\\
& No typing necessary (2) & Article not included (1)\\
& No time waste for queries (1) & \\
& Terminology support if term \newline misheard (1) & \\
& Font/colour (1) & \\
& In-picture solution (1) & \\
\lspbottomrule
\end{tabularx}
\caption[Most useful and most problematic aspects of tools]{Most useful and most problematic aspects mentioned by participants in relation to the three conditions (PDF glossary, CAI tool, ASR-CAI mock-up). The number of mentions is indicated in brackets. Participants could indicate more than one aspect.\label{tab:proscons}}
\end{table}




Specular to this feedback is the description of the helpful and hindering aspects of the booth function of the CAI tool InterpretBank. The dynamic search, which shows first results already while typing and does not require to actively start the query, was mentioned as a positive feature in seven cases. The main positive consequence is that it is ``not very time consuming''. The same was also mentioned in relation to the fact that the search field is automatically cleared ``after a few seconds'', which makes it easy to ``proceed to [the] next search''. The ability of the tool to show fuzzy matches was also mentioned as a helpful characteristic.

Two general comments summarise the overall impression of InterpretBank, described as fast and easier to use than the PDF glossary. Despite the many points of praise, the CAI tool also presented several shortcomings. In four cases, participants pointed out that, despite the speed of the search, typing was nonetheless required, which interfered with their listening and proved distracting, although not as much as the PDF glossary. In one aspect the PDF glossary may provide an advantage: as pointed out by one student, when he/she had not correctly understood the term, he/she was not able to find it in the glossary. The CAI tool is thus less useful when a term had been misheard, unlike in the case of the PDF glossary which may allow the interpreter to search for terms on the page or by scrolling down.

In one case, no negative characteristics were mentioned in relation to the CAI tool.

As for the ASR-CAI tool, most participants agreed that the main advantage for the interpreter consisted in its very low distraction potential. Several respondents remarked that typing took up some of their attention and concentration, while being automatically prompted with terminological pairs during the interpreting task allowed them to stay fully focused on listening and interpreting, also because they did not have to identify the term in a list of potential candidates.

Another positive factor was the very low latency of the system in showing terms on the screen. Interestingly, one participant pointed out that, while the tool was fast, the terms could have sometimes ``popped up earlier''.

Other helpful features were the absence of typing from the human-machine interaction, the lack of time loss due to glossary queries, and the fact that the term was presented directly next to the speaker video.

An issue mentioned by several participants in relation to InterpretBank, partly addressed in the PDF glossary by the ability to search in the whole glossary, lies in the difficulty to find terms when their spelling is unknown. In some cases, students had the necessity to look up unknown terms which had not been shown on the screen. This was the main issue pointed out for the ASR-CAI tool.

This expectation and feeling of dependency on the tool is aptly, albeit rather naively expressed in one commentary: ``in very few instances, I didn't understand a word from the original speech and the ASR didn't recognise it as [specialised] terminology and didn't translate it for me''.

Finally, one participant remarked that the terminological equivalent shown on the screen by the ASR tool did not include the indication of the grammatical gender.

\section{Discussion} \label{discussion}
In the present section, I discuss the hypotheses operationalised through Seeber's CLM (\sectref{CLM_with text}) and presented in \sectref{hypotheses} in light of the results presented in \sectref{results_MS}. The discussion concerns the effects of digital glossaries, CAI tools with manual look-up, and a mock-up, ASR-enhanced CAI tool on the process of SI.

These effects are explored by considering the performance, behavioural, and subjective measures adopted in the study (\sectref{measures}). This represents the basis for the validation of Seeber's CLM applied to CASI in \sectref{validation}. \sectref{limitations} concludes the chapter with a discussion of the limitations of the present study.

\subsection{Performance measures} \label{performancem_disc}
In the present study, data was collected using two performance measures (see \sectref{performanceresults}):

\begin{itemize}
    \item the degree of terminological accuracy achieved;
    \item the number of severe errors and complete omissions.
\end{itemize}

I will now discuss the results for each metric analysed.

\subsubsection{Terminological accuracy}	\label{termaccdisc}
The use of digital terminology support solutions was expected to affect the terminological accuracy achieved during SI (see \sectref{hypotheses}). Higher terminological accuracy would indicate lower cognitive effort required of the participants to operate the tool during SI (for the PDF and CAI condition) and to integrate the suggestions into their rendition. In particular, I expected significant differences between the three conditions. I proposed that the ASR tool would promote greater accuracy (see \sectref{hypotheses}). Similarly, based on the postulated optimisation of the query function for CAI tools (see \sectref{manual_query}), I expected the use of the CAI tool to result in better terminological accuracy than the PDF glossary.

\begin{sloppypar}
In the analysis, I first considered the accuracy achieved for all terms searched and I then focused on the terms searched/found. In considering all terms searched, i.e. both those which were found and those which were not found in the glossary, it appears that using an ASR-CAI tool as support may indeed promote more accurate renditions, both as compared to a traditional digital glossary and to a CAI tool with manual look-up. However, it should be noted that most close renditions resulted from successful queries. Therefore, discussing the category of close renditions when considering only the terms searched/found should be more revealing of the cognitive effort exerted by participants to integrate the suggestions into their renditions.
\end{sloppypar}

For the category of zero renditions, all tool contrasts showed statistically significant differences. In accordance with Seeber's CLM applied to technology-supported SI (see \sectref{manual_query}), I postulated that the CAI tool may interfere less than the PDF glossary with the production effort, even if a term was not found in the glossary. As a consequence, a CAI tool with manual look-up may allow the interpreter to use the available cognitive resources to at least produce an acceptable rendition of the term. On the other hand, I expected the PDF glossary to lead to a higher percentage of omitted terms due to a more complex human-machine interaction. The participants' ability to translate terms more accurately with a tool, independently of whether the query was successful or not, would suggest that lower cognitive effort is required to interact with said tool during SI. This would of course apply only to the two conditions requiring manual look-up, as under the simASR conditions all stimuli were shown on the screen. My hypothesis was confirmed, as the participants omitted or mistranslated fewer terms than with the PDF glossary not only when working under the simASR condition, but also under the CAI condition. Unsurprisingly, the fewest terms were omitted or mistranslated under the simASR condition. To sum up, the ASR-CAI mock-up seemed to perform better than the CAI tool and the PDF glossary, and the CAI tools seemed to interfere less with the SI process than the PDF glossary. This emerged especially from the relatively high percentage of terms left untranslated or mistranslated on average in the PDF condition (\sectref{accuracy}). These results suggest that, provided that no system failures occur, an ASR-enhanced CAI tool may promote more accurate renditions of specialised terminology and result in a lower percentage of omitted or misinterpreted terms as compared to a traditional digital glossary (in this case, in PDF format) and to a CAI tool with manual look-up.

The postulated differences in the terminological accuracy achieved with the three support systems were less evident for the successful queries (see \sectref{accuracy}). Data suggest that if a query is successful, the terminological quality achieved is similar for all solutions. What emerged from data analysis is, however, a superiority of bespoke tools for interpreters (be it standard or ASR-enhanced CAI tools) in preventing omissions or erroneous renditions of the specialised terms compared to a non-dedicated solution such as a digital glossary. The findings of the present study thus suggest that, even if a term is found in the PDF glossary, it may be more difficult to integrate it correctly into the rendition. One reason for this might be the fact that the target term is visually more difficult to identify on the screen (see \sectref{manual_query}).

Additionally, a first investigation of the transcriptions revealed that some students either misheard some stimuli (e.g. ``peak plants'' for ``peatlands'') or had difficulties looking up certain terms because they did not know how to spell them. This issue also emerged from the debriefing questionnaires (see \sectref{debriefing} and the discussion in \sectref{questionnairedisc}). This resulted in accuracy scores of 0 due to wrong renditions despite a glossary query, as the wrong term was looked up (``peak plants'' instead of ``peatlands'', in my example) or to zero renditions, because the term was not found in the glossary. I could have excluded those stimuli from my analysis, as they represent a sub-category that had not been explicitly accounted for in the speech design. However, I decided to include them nonetheless since the ASR-CAI mock-up solved the problem for the students who interpreted the text with the support of this tool. This may represent an additional advantage when using CAI with integrated ASR, albeit more for certain languages (such as English) than for others, and only if the term is correctly recognised by the ASR module. This would support the idea that an ASR-CAI tool might reduce negative split-attention effects due to better temporal contiguity (see \sectref{splitatt_audiovisual}) and support both the production and monitoring and the listening and comprehension tasks (see \sectref{automatic_query}). \citet{van_cauwenberghe_etude_2020} reached a similar conclusion, observing that an ASR tool might be useful especially in the interpretation of unknown unigrams (see also \sectref{accuracy}).

Overall, the results from the analysis of the terminological accuracy (both for terms searched and for terms searched/found) suggest that, when using an ASR tool for look-up, a higher degree of terminological accuracy can be achieved, especially as compared to a PDF glossary. Terms seem less likely to remain untranslated or to be mistranslated if they have been suggested by the ASR system. This is rather unsurprising and is in line with the positive results for the ASR tool observed by \citet[105--109]{van_cauwenberghe_etude_2020}.

The relatively high percentage of terms searched with an accuracy score of 0 in the PDF condition may point, on the other hand, to greater difficulties in finding the terms in the glossary, as also suggested by the data on the glossary queries (see \sectref{queries}). This would indicate greater cognitive effort in operating the tool during SI. If the term is found, however, the differences between the tools may be less pronounced.

Using a CAI tool rather than a traditional digital glossary does not seem to offer a significant advantage in promoting more accurate renditions. It may be possible that the act of actively starting a query in the tool is what poses the real difficulty, while the fact that the search function is supposedly more efficient in the CAI tools only provides a limited advantage. After all, in order to perform a query, several cognitive operations must be performed ahead of typing, i.e. acoustically understanding the term in question, deciding to perform the query and, when the results are offered, identifying the term to be used. Most of these operations are not necessary when using the ASR tool: the term is recognised by the system (although this depends on the system performance); from a strategic point of view, the decision is not whether to look up the term, but rather whether to accept the suggestion and integrate it into the rendition; in the mock-up used, no selection of the term had to take place as previous results were deleted and the participant could only see the last term pronounced by the speaker.

However, the statistically significant difference found between the CAI tool and the PDF glossary for zero/unacceptable renditions (see \sectref{accuracy_found}) suggests that it was easier to find terms when working with a CAI tool and that once found, they are less likely to lead to terminological issues or omissions in the rendition.

The postulated usefulness of bespoke tools in improving terminological accuracy was suggested, among others, by \citet{rutten_informations-_2007} and \citet{will_terminology_2007, will_zur_2015} (see \sectref{termwmodels}). Their hypothesis appears corroborated by the results of the present study, albeit rather for programs which do not require manually looking up terminology in the glossary during interpreting. This supports \citegen{fantinuoli_speech_2017} intuition that including ASR in CAI tools may result in a considerable advancement in the usefulness of CAI tools in-process.

On the other hand, while feasible in most cases, the manual look-up operation does not always yield positive results, which supports the hypothesis that cognitive effort might be higher when more manual-spatial and more visual-spatial resources are recruited and interfere with the other subprocesses involved in SI. The suggestion that operating a software program during SI to look up terminology is incompatible with the other efforts (see \sectref{CAI_attitudes}) appears, however, rather unjustified. In sum, interpreters may find in CAI tools, especially with integrated ASR, useful support solutions for dealing with specialised terminology, while being advised that, just as their users, the tools remain fallible. This emerged also when considering the errors and omissions occurring in concomitance with a glossary query, as discussed in the following section.

\subsubsection{Errors and omissions} \label{erromdisc}
The analysis of severe errors and complete omissions was conducted first by considering general trends in the sample and by also exploring differences resulting from the type of support used. As for the distinction between target and continuation sentences, my expectation was that while performing the query, the participants may have sufficient cognitive resources to retain in memory the content of the original target sentence but may not be able to dedicate enough resources to processing the continuation sentence while at the same time finishing to deliver the content of the previous sentence. Therefore, I expected more errors and omissions to occur in continuation sentences (see \sectref{hypotheses}).

Statistical analysis only partially supported my hypotheses. The study participants omitted more continuation sentences than target sentences. This is consistent with the hypothesis formulated above that attentional resources may be sufficient to process the content of the target sentence while a term was shown on the screen (simASR condition) or searched in the glossary (PDF and CAI condition), but may prove insufficient to also process the contents of the continuation sentence. Surprisingly, there was no statistically significant difference in the number of severe errors performed in target and continuation sentences. This is probably due to the fact that my participants had not prepared on the speech topics. However, it suggests that interacting with a terminology tool during SI is a non-trivial task, which may produce issues throughout the interpretation. This may point to a trade-off between the level of terminological accuracy and overall acceptability of the rendition when a terminology support tool is used during SI, and may also suggest that the interaction with the tool may in some cases excessively interfere with the other SI subprocesses, resulting in evident issues in the rendition.

One interpretation of the spillover effect for omissions observed in the sample may be the notion of exported load put forward by \citet{gile_local_2008}: even though the issue does not occur in the sentence containing the target term, problems may arise later in the rendition. Since in my study the speeches used presented a high terminological density and the study design prompted the participants to look up a high percentage of the stimulus terms, the decisive element might be the number of queries performed. It is possible that under more naturalistic conditions, with less dense speeches, the number of omitted or erroneously interpreted sentences may be lower. At the same time, these findings also underline the importance of preparation, which cannot be replaced by the mere availability of target language equivalents for the specialised terminology contained in the speech. Being able to look up a term in a glossary or visualising it on a monitor is no guarantee of a successful rendition of the sentence in which the term is embedded. Similar issues have been also found in MTPE, where it has been suggested that post-editors may focus ``more on the microlevel of the text than on the macro-level/the overall text.'' \citep[197]{culo2017}.

In addition to this global analysis, I also considered the effect that the different tools had on the number of severe errors and complete omissions during CASI. With reference to the number of errors and omissions coinciding with a glossary query, my hypothesis was that a lower percentage of severe errors and entirely omitted sentences would occur under the simASR condition as compared to the CAI and to the PDF condition. This is because I expected the ASR-CAI mock-up to interfere less with the SI process. The same was expected to occur for the CAI condition as compared to the PDF condition (see \sectref{hypotheses}).

Already at a first examination of the data considering all stimuli sentences (see \sectref{errors_omissions}), the use of simASR as support for terminology seemed to interfere the least with the overall interpreting process, as fewer errors and omissions were observed for this condition in the sample average. With reference only to the sentences for which the participants performed a query, additional trends emerged. In all sentences (both target and continuation), a term query or an automatic suggestion on the screen led to severe errors in 16.11\% of cases on average. This result may suggest that in most cases it was possible for the participants to look up a term in the glossary (or attend to the automatic suggestion on the screen) and still produce an acceptable sentence in the target language. The fact that the ASR-CAI mock-up, on average, led to a lower percentage of severe errors after a query also supports the hypothesis of lower interference, although there were notable exceptions and the same trend could not be observed for all participants.

As for the effect of the tool used on the number of erroneous or omitted sentences in target and continuation sentences, the only statistically significant difference observed was between the PDF and the simASR condition: fewer errors and omissions were observed in the simASR condition than in the PDF condition. It hence seems that the participants were better able to integrate into their rendition terms suggested automatically than terms which they had looked up in the PDF glossary. Using an ASR-CAI tool may therefore not only help translating the target term correctly (see \sectref{performancem_disc}), but also effectively rendering the utterance containing the specialised term.

The CAI tool also seemed to lead to an overall better performance than the PDF glossary, but the differences observed were not statistically significant. No statistically significant differences were observed also between the simASR and the CAI tool. These results are rather surprising, but it is possible that on larger samples, smaller effects may indeed be found. It should also be noted that the analysis of errors and omissions was conducted from a rather broad perspective. This may explain why the only statistically significant differences observed were between the tool expected to yield the best performance (i.e., the ASR-CAI mock-up) and the tool expected to yield the worst results (i.e. the PDF glossary).

To my knowledge, the only other study which analysed the number of omissions in addition to terminological accuracy for CASI is \citet{biagini_glossario_2015}. However, different from the present study, Biagini considered both partial and complete omissions, and reported values for the number of terms searched, found and correctly interpreted which did not lead to an omission. The results were not particularly encouraging, as only between 23.5\% and 43.3\% of terms (M = 33.43\%) correctly interpreted did not lead to an omission. In the present study, the fact that only 2.14\% (on average, range = 0.00\%â€“4.55\%) of correctly identified terms coincided with a complete omission when the CAI tool was used is a rather positive result. Of course, the two values are not directly comparable, as different categories of terms and different types of omissions were considered.

Additionally, \citegen{biagini_glossario_2015} experimental material did not present the same level of control as the speeches adopted in the present experiment, and it therefore cannot be excluded that the features of the speeches used, or the type and distribution of stimuli terms, may have impacted the result. It should also be noted that, in the present study, the values may have been higher if partial omissions had been included (see \sectref{performancem}).\footnote{This observation of course applies not only to the CAI tool with manual look-up also used in \citet{biagini_glossario_2015}, but also to the PDF glossary and to the ASR-CAI mock-up.}

The results of the analysis of both terminological accuracy (\sectref{termaccdisc}) and errors and omissions warrants a methodological reflection. The inclusion of a broader-level analysis beyond the term level highlighted phenomena which would have otherwise not emerged. It therefore appears highly valuable to include this type of evaluation in studies concerned with the impact of CAI tools on interpreters' cognitive effort. While terminological accuracy and errors and omissions were examined in the present study as indicators of cognitive effort, accuracy scores (for terms, but also and especially for numbers) have often been approached from the perspective of quality (e.g. in \citealt{defrancq_automatic_2020}). Without a more holistic analysis of the rendition, it seems unjustified to equate improved terminological accuracy (or improved numeral precision) with an overall improvement in the quality of the rendition. For studies investigating the quality of SI, it might therefore be useful to adopt evaluation frameworks which go beyond the assessment of individual elements of the rendition. Translation process research on CAT tools and MT or MTPE has used evaluation frameworks involving multiple elements of quality, such as MQM (see \sectref{accuracy_performance}). Using frameworks such as these, provided that they are adapted to the unique features of interpreting, may offer a standardised methodology which would in turn also improve the comparability of studies on CAI. In light of the technological turn \citep{fantinuoli2018interpreting} in interpreting, adopting a standardised methodology for quality evaluation may also help compare the quality of CAI not only with traditional SI, but also with MI, as has been done in TPR (e.g. \citealt{vardaro_translation_2019}) using MQM to evaluate the quality of both NMT and human post-editing of MT.

Additionally, it appears useful to also perform qualitative, rather than exclusively quantitative, analyses of the interpreters' performance with CAI tool support, as suggested by \citet{frittella_cai-supported_2021} for numbers, and similarly to the adoption of a ``communicative approach'' to analyse MI \citep{fantinuoli2021evaluation}. An approach of this type may also promote greater awareness on the part of interpreters of the role played by the tool and may help prevent effects such as those observed by \citet[123]{van_cauwenberghe_etude_2020} of unnecessary self-corrections after visualising automatic term equivalent suggestions on the monitor during SI. Self-corrections of this kind were observed anecdotally also in the present study, and suggest an excessive reliance on the tool, which was noted also in the first studies on the topic (e.g. \citealt{biagini_glossario_2015,prandi_uso_2015,prandi_use_2015}).

\subsection{Behavioural measures} \label{behaviouraldisc}
Six behavioural measures were adopted in the study (see \sectref{behaviouralm}):

\begin{itemize}
    \item number of terminological queries performed under the three conditions;
    \item the EVS for the terms searched and searched/found;
    \item the inter-cluster pause duration;
    \item time to first fixation on term AOIs;
    \item average fixation duration on term AOIs and tool AOIs;
    \item fixation time on tool and speaker AOIs.
\end{itemize}

The results of the analysis of the term queries provide insight into the in\-ter\-pret\-er-ma\-chine interaction and, to a more limited extent, into the technical effort involved in CASI. The two time-lag measures, i.e. EVS and ICPD, are indicators of the speed of processing. The three gaze-related metrics provide information as to the cognitive effort experienced under the three conditions and about attention allocation during SI with digital terminological support.

\subsubsection{Terminological queries} \label{queriesdisc}
Since the simASR condition did not involve active glossary look-up, the results of the analysis of the terminological queries may only provide insight into the differences between digital glossaries (PDF condition) and CAI tools with manual look-up. Based on the results of the pilot study and the findings in \citegen{biagini_glossario_2015} study comparing paper glossaries and CAI tools, my hypothesis was that the CAI tool would elicit more queries due to its hypothesised ease of use and higher querying speed thanks to the incremental search (see \sectref{hypotheses}).

When considering the percentage of terms searched (see \sectref{queries}), the results do not support my hypothesis. The percentage of terms searched in the PDF and the CAI tool did not differ significantly. This may suggest that the querying process in a PDF or in a CAI tool is more similar than expected in terms of effort (see \sectref{manual_query} and \sectref{hypotheses}), despite the postulated optimisation of the querying process in the CAI tool. It is, however, also possible that the extensive pre-test practice enjoyed by the study participants may have helped them improve their ability to perform manual queries while interpreting, even if the PDF was not optimised for this scope. Additionally, the informants may already have been used to looking up terminology in traditional digital glossaries during their regular interpreting classes.

Another aspect to consider is the nature of the materials used for data collection. The speeches developed for the experiment were highly terminology-dense, which may explain the similar percentage of looked-up terms under the two conditions. Furthermore, the lack of preparation on the topic in all likeliness contributed to the overall high number of queried terms. The percentage of term queries may, however, yield more interesting results in designs including a preparation phase, as was the case in \citet{biagini_glossario_2015} or \citet{prandi_uso_2015, prandi_use_2015}. For instance, Prandi had found a relatively high inter-subject variability in the percentage of queries performed with InterpretBank. Low percentages were due to the fact that only the essential terms were looked up and probably also due to the effective domain-specific and terminological preparation of the students involved in the experiment. Note, however, that in that first exploratory study students worked in pairs and sometimes did not look up terms themselves while interpreting.

A more useful indicator of effort in the present experiment may be the percentage of terms searched/found (see \sectref{queries}). In this respect, the statistically significant difference between the percentage of terms correctly displayed under the PDF and the CAI condition suggests that using a CAI tool may provide an advantage when manually searching for terminology as compared to a PDF glossary. In other words, an interpreter wishing to conduct manual queries in a digital glossary may be better advised to choose a dedicated CAI tool than a non-bespoke solution. The positive results for the CAI tool are in line with Prandi's (\citeyear{prandi_uso_2015, prandi_use_2015}) findings that a relatively high percentage of queries performed with InterpretBank were successful.

As for the impact of the position and morphological complexity of the terms on the participants' search behaviour, the results were mixed.
The position of the terms within the sentence seemed to influence the number of terms looked-up and found in the speech. The fact that significantly more sentence-final terms were looked up and found supports my hypothesis that participants may be better able to anticipate a query and prepare for it if the term is at the end of the sentence (see \sectref{hypotheses}). This result also supports the experimental design choice to control for the terms position in the speeches (see \sectref{material_PS}).

As for the morphological complexity and the postulated effects of length, the findings from the present study did not paint a unitary picture. Slightly more unigrams were searched on average in the sample, which would support my hypothesis that participants may be more inclined to look up shorter terms because the process may be more immediate. This result is also in line with the results of the pilot study. These results could, however, also be due to a number of additional reasons, even to the very terms chosen for the experiment. The small differences observed were not significant, which suggests that the morphological complexity of the terms did not seem to affect the participants' search behaviour. However, significantly more trigrams were found than unigrams. This would support my hypothesis that the more elements a term offers, the more likely it is for a term to be found in the glossary, because the interpreter has more material to exploit for the search. This is in line with evidence from studies on the length effect on auditory processing, as discussed in \sectref{workingmemory}. These results are also in line with \citegen{van_cauwenberghe_etude_2020} findings that unigrams tended to be interpreted with the glossary equivalent significantly more often than other categories of morphological complexity (bigrams â€“ pentagrams). Although the distribution of terms was not balanced in his speeches (only two pentagrams were present), he also reached the conclusion that especially short and unknown unigrams may pose a challenge to the interpreter (ibid., p. 119--120). This is consistent with the significantly lower number of unigrams than trigrams found by the participants of the present study. To sum up, these findings suggests that an ASR-CAI tool may provide valuable support especially for short and unknown terms.

If the findings discussed above are confirmed by examinations on larger samples, the analysis of how the morphological complexity affects the search behaviour may provide useful insights as to the strategies to adopt when using digital tools with manual look-up, as suggested in \citet{prandi_exploratory_2018}. However, the strategic aspect of CASI appears relevant not only when manual look-up is involved, but also when terms are automatically displayed in an ASR-CAI tool, as literature has shown that interpreters find processing multi-word terms challenging, as discussed in \sectref{compounds}. It would be interesting to explore whether the ability of interpreters to correctly integrate a suggestion provided by an ASR-CAI tool varies as a function of the morphological complexity of the terms adopted as stimuli. If it does, this would further motivate the choice to control for the variable ``morphological complexity'', as was done in the present study.

Overall, controlling for the position and morphological complexity of the stimulus terms included in the speeches appears a valuable design choice to consider also in future studies.

\subsubsection{EVS} \label{EVSdisc}
As a traditional metric used to explore cognitive effort, the EVS (see \sectref{EVS_EKS}) was also included as a behavioural measure. Two categories of terms were analysed: the terms searched and the terms searched/found, i.e. first all glossary queries and then only successful queries. Note that also in this case, the two categories of terms coincided for the simASR condition (see \sectref{EVS}). Additionally, in order to calculate the EVS, only those queries for which the term had been translated could be taken into consideration.

For both categories, I expected the average EVS to be shorter for the simASR condition, followed by the CAI tool and the PDF glossary (see \sectref{hypotheses}). The results presented in \sectref{EVS} would lend support to my hypothesis of lower cognitive effort when participants interpreted with the support of an ASR-CAI integration: once the terms have been visualised, they may be integrated more quickly into the rendition. This would speak in favour of the integration of ASR technology into CAI tools for live terminology support during interpreting.

It should also be noted that when the ASR-CAI mock-up was used, the average EVS for terms was in the lower range of EVS observed in interpreters (around 2s, see \sectref{timerelatedm}). This is rather unsurprising, as the latency for the ASR-CAI mock-up had been intentionally kept short, in line with the findings by \citet{montecchio_masterarbeit_maddalena_2021} that a high latency for an ASR tool may hinder a successful interpretation, and with \citegen{van_cauwenberghe_etude_2020} observation that the latency was sufficiently short when a real ASR tool was used, rather than a mock-up such as in the present experiment.

More interesting is the fact that the EVS was close to the higher end of average EVS values for the PDF condition. The five-second pause between sentence clusters was an artefact introduced for the purpose of the experiment, as detailed in \sectref{material_PS}. It therefore appears that in a real-life situation, an EVS of around five seconds, such as that observed for the PDF condition, may result in an even higher number of severe errors and complete omissions than were observed in the present experiment. For the CAI tool, the median EVS was also rather long (3.3s). Therefore, queries might be possible with this type of support, but nonetheless lead to errors or omissions in the rendition. Overall, this suggests that using an ASR tool may help prevent issues related to a long EVS and help keep dÃ©calage short.

Using the EVS as a measure of cognitive effort with ASR support, however, presents an evident limitation: in virtue of the fact that no manual query is required, the ASR tool is inherently faster. However, the EVS length may also be influenced by the syntactic structure chosen by the interpreter. It is possible that, under the simASR condition, the interpreter might choose a structure closer to that of the ST because the target language equivalent immediately becomes available. When a PDF glossary or a CAI tool with manual look-up is used, however, the interpreter may choose to restructure the target sentence to gain precious time during which to conduct the search. A longer EVS for the PDF or the CAI condition may therefore also suggest that the term was integrated into the interpreted sentence later down the line, and not necessarily that it took participants more time to find the term equivalent in the glossary. Differences in EVS may therefore be due not only to how fast the term can be found, but also to strategic choices made by the interpreter. Which of the two hypotheses applies remains unclear and should be further investigated. It is possible that a combination of both factors may contribute to the EVS length. Investigating whether interpreters choose specific syntactic structures to accommodate for a glossary query may also provide useful indications as to the best strategic approach to adopt when performing a query during SI.\largerpage

As for the postulated advantage provided by the CAI tool over the PDF glossary, the significantly shorter EVS for the stimulus terms (see \sectref{EVS}) shows that the interpreting process may indeed be quicker with the CAI tool. This result, in addition to the significantly lower percentage of term omissions or misinterpretations for the CAI tool as compared to the PDF glossary (see \sectref{errors_omissions}), represents an additional argument in favour of the use of a bespoke tool for interpreters. Especially in the case of a very fast and/or dense speech for which shortening the dÃ©calage may be a useful coping tactic, the CAI tool may therefore offer a non-negligible advantage compared to the PDF tool, while an ASR-CAI tool may be the best choice of all.

Although subject to the limitations discussed above, especially as concerns the ASR tool, the EVS may indeed be seen as a valuable indicator of how the use of a digital glossary, of a CAI tool with manual look-up and of an ASR-CAI tool affects the speed of processing in SI, which in turn reflects the effort exerted by the interpreter.

The two hypotheses formulated above as to the differences in EVS length also highlight the different dimensions of the cognitive effort experienced in CASI and reflect the make-up of the cognitive load imposed by the CASI task. As discussed in \sectref{CL}, there is an intrinsic load imposed by the interpreting task which is compounded by the extraneous load imposed by the individual tools. Similarly to the post-editing effort (see \citealt{krings2001repairing}), the effort exerted by the interpreter during CASI may therefore be described as CAI effort, comprising both the technical effort required to interact with the tool, the cognitive effort required to adjust one's own rendition to the interaction with the tool, and the temporal effort deriving from the time required to find the term equivalent in the glossary. Considering all three sub-efforts, the CAI effort may reasonably be expected to be lower with the support of an ASR tool than with a CAI tool or a non-bespoke digital glossary.

\subsubsection{ICPD} \label{ICPDdisc}
The inter-cluster pause duration (ICPD, see \sectref{pause_duration}), was used in the analysis as an additional indicator of cognitive effort. Since the silent pause introduced between two sentence clusters was always 5s long in the source text, I consider an ICPD of 5 or more seconds in the participants' delivery as indicative of a faster interpreting process. On the other hand, cluster-to-cluster spans shorter than 5s should be indicative of higher cognitive effort, or, at least, of a slower interpreting process. The closer to 0, the more complex the task and, presumably, the higher the cognitive effort. If the search requires a lot of time, it is possible that the subject will have to use most of the 5s silent pause to finish interpreting the passage. A longer pause would therefore indicate a more seamless integration of the term into the interpreted sentence, i.e. a more efficient interaction with the tool, and a shorter dÃ©calage.

On this basis, my hypothesis was that pauses between clusters in the renditions would be longer for the simASR condition than in the CAI and the PDF conditions (see \sectref{hypotheses}). For the other two conditions, on the other hand, I expected mean pause duration to be shorter, which would indicate a longer dÃ©calage, i.e. the need to use all or part of the silence between clusters to finish interpreting. I however expected the ICPD to be shorter for the PDF condition than for the CAI condition.

Overall, data show that, when using the simASR tool, the interpreting process was quicker, i.e. the dÃ©calage was shorter, and the participants could keep up better with the speaker than when working with a PDF glossary.

On the other hand, the hypothesis that the CAI tool may interfere less than the PDF glossary with the production effort in SI, but more than in the simASR condition, did not find support from the analysis of inter-cluster pause duration. As reported in \sectref{pause_duration}, significant differences were found only between the ASR-CAI mock-up and the PDF condition, but not between the CAI tool and the PDF glossary and between the CAI tool and the simASR tool. On the basis of these results, I may only affirm that the simASR seems to slow down the interpreting process less than a traditional digital glossary. It also appears that the CAI tool with manual look-up did not significantly slow down the interpreting process more than under the simASR condition.

I do not have an explanation for the discrepancy between the results for the EVS and the ICPD. As observed for the analysis of errors and omissions, these discrepancies may indeed be due to the small sample size. A qualitative analysis of the TT or a quantitative analysis of partial omissions in the TT may help qualify these findings. This goes beyond the scope of the present contribution. It would also be interesting to use other pause metrics, such as silent pauses as in \citet{gieshoff_impact_2021}, as an indicator of cognitive effort, either in place or in addition to the metrics used in the present experiment.

\subsubsection{Time to first fixation on term AOIs} \label{timefirstfixdisc}
As predicted by Seeber's CLM (see \sectref{automatic_query}), the simASR tool should facilitate the fast individuation of the term on the monitor, since the terms were displayed in isolation in the mock-up. The ASR-CAI integrations already available experimentally, such as the one offered by InterpretBank \citep{fantinuoli_computer-assisted_2017}, display the results in progressive order of appearance, with the most recently found term at the top of the list. If the interpreters are aware of this, they will probably be prepared to only look in that area of the tool window. Therefore, I suppose that results would be similar if a real tool of this kind was used rather than a mock-up like in my experiment. This remains, however, a hypothesis which should be tested experimentally.

The CAI tool presents the advantage of showing results while the query is in progress, which should promote a faster individuation of the target term than in the PDF condition. Additionally, in the version used for the experiment, the CAI tool InterpretBank by default shows a maximum of nine results on the screen, and the search stops after at least five terms have been found for the query.\footnote{This option was removed in later InterpretBank versions.} This was expected to further facilitate the individuation of the term on the screen and to reduce negative effects deriving from a visual search for the target term equivalent, as discussed in \sectref{manual_query}.

When using the PDF glossary, the participants could visualise the entire glossary page. In some cases, for instance for the terms on the last page of the glossary, fewer terms were shown, which would make it easier to identify the term searched for. This was a limitation which did not emerge in the pilot study. However, only a limited number of stimuli terms were present on the last page. In most cases, I expected participants to require more time to identify the term on the page due to having to sort through the large amount of visual material (see \sectref{hypotheses}).

The results presented in \sectref{timetofirstfix}, i.e. the significantly shorter times to first fixation for the simASR condition as compared to the PDF and CAI condition, are rather unsurprising. However, contrary to my expectation, the supposed advantage provided by the CAI tool was not confirmed by statistical testing. This is interesting, especially because many participants had identified as the most problematic aspects for the PDF glossary the lack of filtering among the results and the need to navigate in the document to find the correct equivalent. Only one participant had commented that the CAI tool had offered too many results after a query (see \sectref{performanceresults}).

Caution should however be exercised in interpreting these results as an indication that the CAI tool may not provide an advantage over the PDF glossary. When a CAI tool is used, it is possible that the term may be displayed on the monitor earlier than for the PDF glossary. This might explain why the time to first fixation was longer than expected under the CAI condition. This hypothesis seems justified considering that the EVS for the CAI condition was significantly shorter than for the PDF condition (see \sectref{EVSdisc}). It is therefore possible that the CAI tool was indeed able to find the target term faster than the PDF glossary, which would be an encouraging result.

The participants who had identified the term on the screen with greater delay when working with the CAI tool may simply have continued typing even though the target term was already visible on the monitor. This may indicate that interpreters may actually need to type less than they think is necessary when working with a CAI tool. At the same time, these results may also suggest that, in the phase between the start and the end of the query, the intermediate results may be more difficult to process. The search behaviour of the individual participants certainly is an important factor: for instance, touch typing may promote greater focus on the monitor and a quicker identification of the term equivalents.

\subsubsection{Average fixation duration on term and tool AOIs} \label{fixdurdisc}
Due to the lack of a manual query with the ASR-CAI mock-up, I hypothesised that the simASR condition would require lower (technical) effort than the PDF and the CAI condition (see \sectref{hypotheses}). In turn, I also expected that using the CAI tool would result in more limited additional effort than the PDF due to the progressive search. The progressive search was expected to facilitate the interpreter-tool interaction, resulting in an easier integration of the querying process into the interpreting process. The metrics of average fixation duration on the term and tool AOIs were expected to provide insight as to these hypotheses, specifically with reference to the cognitive effort experienced in processing the terms under the three conditions during SI and in interacting with the different tools.

The results for the average fixation duration on the term and tool AOIs partially support these hypotheses. The analysis of fixation durations on the term AOIs (see \sectref{fixation_dur}) showed significant differences between the simASR and the other two conditions and non-significant differences between the PDF glossary and the CAI tool. The significantly shorter fixation durations on the term AOIs found for the simASR condition as compared to the two other conditions lend support to my hypothesis that processing the term may be easier (and faster) when terminology is automatically presented on the screen.

However, the fact that no significant difference was found between the PDF glossary and the CAI tool for this metric may point to a higher degree of recruitment of cognitive resources by the CAI tool than expected, specifically of visual-spatial resources (see \sectref{manual_query}). My hypothesis is that this may be due to the need to visually identify the term in a list of results also when working with the CAI tool, despite the number of results being smaller than in the PDF condition.

The significant difference in the duration of fixations on the tool area (see \sectref{fixation_dur}) also supports the hypothesis that the ASR tool may recruit fewer attentional resources than the PDF glossary. The shorter fixations on the simASR area may be seen as an indication of a lower level of cognitive engagement with the tool.

However, the fact that no statistically significant differences were found for the fixations on the CAI tool area as compared to the other two conditions suggests that the CAI tool may not always be less distracting than the PDF tool, and not always more distracting than the ASR tool. It is possible that this may depend on the number of results displayed on the screen after a CAI tool query: for some terms, the query may yield only one result (similarly to the simASR condition); for others, there may be a rather long list of terms (similarly to the PDF condition).

Overall, a lower degree of attentional resources recruited by a terminology support solution may be expected to promote focused attention on the speaker (see \sectref{splitatt_audiovisual}). In other words, the interpreter might have felt the need to monitor the simASR tool significantly less than the PDF glossary. Using an ASR tool in particular may help monitor (e.g. \citealt{schaeffer2019monitoring}) the primary source of information (the speaker video). The metric fixation time, discussed below, may help gain further insight into this hypothesis.

\subsubsection{Fixation time on tool and speaker AOIs} \label{fixtimedisc}
The metric fixation time, i.e. the total duration of fixations, on the tool and speak\-er AOIs (see \sectref{fixation_time}), was expected to provide additional information about participants' monitoring of the tools as well as about the distribution of attention (see \sectref{splitatt_audiovisual}) between the speaker and the support tool. Due to the postulated lower degree of engagement with the tool in the simASR condition, I expected the participants to spend significantly more time processing the video stimulus in this condition than in the CAI and PDF condition, and, in turn, to spend more time looking at the speaker in the CAI condition than in the PDF condition (see \sectref{hypotheses}). In other words, in the simASR condition, they would be better able to monitor the speaker and focus their attention on the speaker video, while they would need to monitor the tool the least.

The results of the analysis for this metric (see \sectref{fixation_time}) support my hypothesis. Specifically, these results may be interpreted as evidence that, in the PDF condition, attention was more equally distributed between the speaker and the tool, which suggests that the tool required a considerable amount of attentional resources from the participants which could not be devoted to the speaker video. In turn, in the simASR and CAI conditions, it was probably easier for the participants to intentionally divide their attention between the speaker and the tool and to focus more on the speaker, particularly in the simASR condition.

Considering that the tool area only provided term equivalents, participants should ideally only attend to the tool when such information is provided or when they actively seek it during SI. In the PDF condition, however, the participants split their attention more between the speaker and the tool than in the CAI condition, and in some cases fixated more on the tool than on the speaker. The significant differences observed when comparing the duration of fixations on the tool AOIs and the total fixation time on the speaker AOI and the tool AOI support the choice of an ASR tool over a solution requiring manual look-up.

Despite the advantage which the ASR-CAI mock-up seemed to provide over the other solutions, it should be noted that all terminology support tools may cause a negative split attention effect during SI. Yet, thus far potential split attention effects do not seem to have been taken into consideration in the design of CAI tools, with and without integrated ASR. It still remains to ascertain how the placement of the information on the screen affects attention allocation during SI. Both the Booth Mode of InterpretBank \citep{defrancq_automatic_2020} and the SmarTerp prototype \citep{frittella_cai-supported_2021} present terms, numbers and named entities in separate areas or columns. If a negative split attention effect is identified which can be traced back to the current tool interfaces, it would be interesting to assess whether alternative interfaces can be designed in order to reduce said effect. For instance, solutions along the lines of what has been proposed through integrated subtitles \citep{fox_can_2018} may be explored: improving not only the temporal but also the spatial contiguity (see \sectref{splitatt_audiovisual}) for ASR tools might help reduce their extraneous load (see \sectref{chandler_sweller_CLT}). This, however, remains a hypothesis. Nonetheless, for the sake of CAI tool usability, it seems necessary to test different modes of display and determine their impact on the interpreting process and product.

\subsection{Subjective measure: The debriefing questionnaire} \label{questionnairedisc}
In addition to the quantitative measures discussed above, a questionnaire was administered to the participants at the end of the experiment to collect qualitative data. The answers to the debriefing questionnaire (see \sectref{debriefing}) allow to further qualify and disambiguate the results of the quantitative analysis discussed above.

The participants were asked to rank the tools according to how useful and how distracting they proved during the experiment. I expected the ASR-CAI tool to be ranked as most useful and least distracting, and the PDF glossary to be ranked as least useful and most distracting (see \sectref{hypotheses}). My hypothesis was supported. Overall, the participants' answers support the hypothesis that the PDF glossary may require more attentional resources than the CAI tool, and that the ASR tool may be preferable both to the PDF glossary and to the CAI tool. It is however interesting to note that two out of nine participants would bring the CAI tool and not the ASR tool into the booth. Let us therefore discuss the most useful and most problematic aspects pointed out by the participants, as they may represent useful indications for the optimisation of CAI tools and ASR tools.

With reference to the PDF glossary, participants appreciated being able to see several entries at once. This would explain why the fixation time was particularly high on the PDF area: they may have looked at the glossary also when they were not looking for a term, for example during the breaks between sentence clusters. It could be argued that this aspect depends on the way the glossary is organised, on the height of the rows and on the font size, but it seems logical that the presentation of vocabulary one page at a time favours an overview of the terminology. This might also explain why there was no significant difference in the number of queries between the PDF and the CAI tool, as discussed in \sectref{queriesdisc}: some participants may not have needed to query the glossary for certain stimuli because they could already see the target term on the monitor. This is however likely to have occurred in a limited number of cases, since the number of queries was overall relatively high.

Something similar applies to the alphabetical order, a feature which participants also appreciated as it helped them identify the terms on the screen. The alphabetical order is, however, not a prerogative of a PDF, Word or Excel glossary. Therefore, the alphabetical order may be considered as a useful feature per se. It may, however, be relevant mostly when little to no preparation is possible, or in general when the interpreters have not had time to study the glossary in detail, such as in the present experiment. Other glossary structures may be more effective under different conditions.

The participants also appreciated being able to scroll the page down to look for terms as an alternative to using the search bar. However, this is likely to require a conspicuous amount of time and might only prove useful in emergency situations. This is supported by the fact that most participants highlighted that the PDF might be distracting and cumbersome: the number of terms displayed on the screen at the same time may indeed prove overwhelming.
As for the CAI tool, most participants underlined that the dynamic search and the automatic clearing of the search field make the querying process less time-consuming than in the PDF glossary.

In this respect, it is necessary to remind the reader that, although the progressive (dynamic) search, the automatic clearance of the search field and the fuzzy search are specific to InterpretBank and not available in the PDF tool, a manual query without automatic clearance of the search field and without fuzzy results can also be performed with the CAI tool. If those settings had been selected for InterpretBank, the participants' commentaries for the two conditions would have probably been very similar. Since the aim was to compare three solutions which allow for different ways for the human interpreter to interact with the machine, the search settings which correspond to the most commonly selected features for the CAI tool InterpretBank and which constitute its main element of distinction were selected.

I found it interesting that, even though only several terms are shown in the CAI tool after a query, they may nonetheless be too numerous for the user to be able to identify the target term under the time pressure inherent to SI, as pointed out by some participants.

Finally, the simASR tool was often described as not particularly distracting, specifically because no typing was required, unlike in the CAI and in the PDF condition.

Replacing typing with ASR has after all been found beneficial for the translation process (e.g. \citealt{carl_comparing_2016}). The fact that the attitude of most participants towards the simASR tool was overall positive might seem to contrast with a certain adversity to MT observed among translators (see \sectref{CAI_attitudes}). The ASR-CAI tool mocked-up in the present study, however, is not based on the use of MT, but rather on a combination of an ASR module with an extraction module. The results presented on the monitor had therefore already been validated through the creation of an ad-hoc glossary. In theory, however, it would be possible to use MT without having prepared a glossary beforehand, similarly to the creation of automatic interlingual subtitles (e.g. \citealt{dessloch2018kit}). If a tool of this kind was used to provide support to interpreters during SI, it is possible that similar negative attitudes to the ones found for translators may also be found for interpreters.

Indeed, a certain level of distrust has already been suggested in studies using real tools instead of mock-ups, which, although a glossary was used as the basis for the terminology extraction, may not always display the expected result (see \citealt{van_cauwenberghe_etude_2020}).

Furthermore, while the fact that the human-machine interaction is minimal in the simASR condition and is therefore expected to improve concentration and reduce time loss, it results in an important limitation. When interpreters work with an ASR tool, they are unable to influence the queries performed by the tool and to actively query the glossary when an unknown term is not suggested automatically, as pointed out by some participants. This unfulfilled expectation may also lead to stress or distraction, as it may require reformulating the TT or re-evaluating the strategy chosen to deal with the lack of an available equivalent for the term in question. Interpreters may therefore experience the same ``cognitive friction'' (\citealt{OBrien_Ehrensberger-Dow_Connolly_Hasler_2017}, \citealt[19]{cooper_inmates_2004}) observed in translators working with CAT tools and MT outputs.

It should be noted that the differences in cognitive engagement and distraction potential observed between the tools may also be due to the user interface. The simASR tool was relatively bare, while more elements were present in the CAI tool and especially in the PDF glossary. This aspect could have been accounted for by developing three tools ad-hoc for the experiment only differing in the search algorithm (not progressive, progressive, and automatic) and in the display of results, with no difference in terms of user interface, brightness (through the use of grey tones in the UI, for instance), etc. Such a solution, while more time consuming in the experiment preparation phase, would have allowed to pin-point the source of difference between the tools with greater certainty, and to address the limitations discussed for the metric pupil size (see \sectref{pupillary_measures}).\largerpage

Nonetheless, even if the PDF tool attracted more attention and thus proved more distracting due to its UI, this would represent an additional argument in favour of the simplification of the UI of CAI tools, whether with or without ASR integration, to reduce the distraction potential of digital support tools for interpreters. TPR has addressed the issue of the distraction caused by the interface of CAT tools and MT (e.g. \citealt{OBrien_Ehrensberger-Dow_Connolly_Hasler_2017}). Studies have been dedicated to the optimisation of the UI of CAT tools and PE environments (e.g. \citealt{moorkens_assessing_2016}). It appears fundamental to also address the ergonomics of CAI tools, both with and without ASR integration, to maximise their support potential while reducing the extraneous load (see \sectref{chandler_sweller_CLT}) deriving from suboptimal interfaces. A first step in this direction has been taken through the \citet{EABM2021} project, which seeks to explore potential user preferences through questionnaires (see also \sectref{CL}). It appears necessary to explore this research question further to determine the impact of the tools' interface on the CASI process and at the same time improve the tools' usability.

By the same token, future experiments may also address the type of information presented by ASR tools. At present, most prototypes of an ASR integration into a CAI tool present the terms in isolation, devoid of any additional information. While this is easily modifiable and does not constitute an insurmountable obstacle in the use of the tool, it is also true that both traditional digital glossaries and most CAI tools easily allow the user to add columns or fields for accessory information, such as the column ``Booth Info'' in InterpretBank, which the user can choose to make visible in the conference modality. This feature may also be integrated into ASR-supported CAI tools, as suggested by one participant in the questionnaire.

Unsurprisingly, participants appreciated the low latency of the simASR tool, as the mock-up had been specifically prepared with this characteristic. As such, students experienced an ideal ASR-CAI integration, while real solutions may not perform as well as in the experimental setting. Although the results for the systems currently available are encouraging, especially for numbers (e.g. \citealt{defrancq_automatic_2020}), latency remains an essential factor, not to be underestimated in the design and use of ASR tools (see \citealt{montecchio_masterarbeit_maddalena_2021}). It was surprising to see that one participant mentioned ``high latency'' as a negative side of the simASR tool, as the latency was the same for all terms. My hypothesis is that in some cases, the participant in question may have preferred to know the equivalent faster in order to effectively integrate it into the syntactic structure chosen to formulate the TT.

One aspect in which the ASR tool is undoubtedly superior to the other two solutions is the assistance provided by the tool when a term has been misheard or not heard by the interpreter, due to the phonological interference experienced during SI \citep{diaz2019comprehension}, or simply because the term is unknown. This had emerged also during the analysis of how the morphological structure of the terms influenced the number of queries (see \sectref{queriesdisc}). An ASR engine should provide useful support in this cases, provided that the SR performs well, which may not always be the case due to the characteristics inherent to oral speech.

\subsection{Validation of the CLM applied to computer-assisted SI} \label{validation}
The present study was motivated by the intention to explore how various types of in-booth technological support for terminology affect the process of simultaneous interpreting. The hypotheses on the differences in human-machine interaction and the resulting cognitive effort between traditional digital glossaries, CAI tools with manual terminological lookup and ASR-enhanced CAI tools were formulated using Seeber's Cognitive Load Model of SI illustrated in \sectref{mymodel}. The present section validates the model in light of the results of the experiment discussed in the previous sections.

Based on Seeber's CLM of SI (\citeyear{seeber_cognitive_2011,seeber_multimodal_2017}), the three solutions were expected to differ mainly in the degree of recruitment of manual-spatial resources at the response stage (i.e. while interacting with the tool to find the stimulus term) and of visual-spatial resources at the perceptive-cognitive stage (for the identification of the equivalent on the monitor). In particular, it was hypothesised that the PDF glossary might recruit a higher degree of resources during the query (manual-spatial resources at the response stage during the production and monitoring task), as well as for the identification of the term equivalent on the monitor (visual-spatial resources, also during the production and monitoring task) (see \sectref{manual_query} and \sectref{hypotheses}).

CAI tools were also expected to recruit manual-spatial and visual-spatial resources, but also to facilitate the querying process due to the progressive search and to support the identification of the term equivalent by displaying only the relevant terms on the screen (see \sectref{manual_query}).

Finally, it was speculated that the simASR tool would recruit no additional attentional resources at the response stage during production and monitoring, as the terms were displayed automatically on the monitor. The simASR tool was also not expected to recruit visual-spatial resources for the identification of the term on the screen during the production and monitoring phase.

The findings of the experiment support the assignment of different demand vectors to model the differences in the level of resource recruitment for the different tools. In particular, the significantly lower number of terms found for the PDF tool as compared to the CAI tool, lends support to the hypothesis that using a traditional digital glossary may be more cognitively taxing than a bespoke solution. If the querying process is successful, it stands to reason that the additional effort interferes less with the production effort in the interpretation. Hence, assigning a lower demand vector (1) to the CAI tool than to the PDF tool (2) for the manual-spatial resources recruited during the response stage appears justified.

Due to the lack of significant differences in the time to first fixation between the PDF and the CAI condition (see \sectref{timefirstfixdisc}), I may assign a demand vector of 2 to the CAI tool for the recruitment of visual-spatial resources in the receptive-cognitive stage, thus equating it to the PDF glossary (the ASR tool would receive a value of 1). However, these results contrast with the significant difference between the PDF and the CAI condition found for the EVS, as discussed above (\sectref{EVSdisc}), which would justify a demand vector of 1 for the CAI tool. As observed in \sectref{timefirstfixdisc}, the findings for the CAI condition may be biased by the fact that I used the timestamp of the first moment in which the term AOI becomes visible on screen as a reference to calculate the time to first fixation. A significant difference may indeed be found if the end of the query was taken as a reference point instead. From an operational point of view, this might, however, prove problematic for the CAI condition, because participants may continue to type after having already found the term on the screen. The first fixation may therefore fall outside of the time span considered for the analysis. Due to the inconsistency between these findings, the assignment of a demand vector of 1 for visual-spatial resources in the CAI condition remains but a tentative conclusion, which may be further explored in future studies.\largerpage

The facilitation effect postulated for the multimodal presentation of the terms (visual-verbal and auditory-verbal modality) for the simASR condition is supported by the responses to the questionnaire. These findings also corroborate Seeber's proposal to model facilitation effects in the CLM. In particular, they support the choice to model the interaction with the ASR-CAI mock-up together with the listening and comprehension task \citep[472]{seeber_multimodal_2017}, which the multimodal presentation facilitated in the experiment. Here, a parallel may be drawn with the role of the visual trace of the TT in translation (see \citealt{schaeffer2019monitoring}). In translation, being able to see the TT may help monitoring the writing process and the translation choices. In interpreting, it may support the transcoding of the ST as well as the monitoring of the rendition of the TT, albeit with the limitation of individual terms.

The use of different demand vectors to model different levels of resource recruitment appears useful within the context of the present experiment and is in line with the role of these components in \citegen{wickens_multiple_2002} original model. Seeber made a different use of the demand vectors both in the early and in the later version of his models. As discussed in the literature review (see \sectref{seeber}), in his application of the model the demand vectors can only take a value of 0 or 1, or of 0.5 to model facilitation effects for multimodal integration.

This double role of the demand vectors appears rather difficult to reconcile in a single model. One might therefore follow two approaches. The first would consist in modelling multimodal and multicue integration and the level of resource recruitment in two separate models: one CLM of resource integration and interference, i.e. Seeber's application of Wickens' model as in \citet{seeber_multimodal_2017}, and one CLM of resource recruitment in SI, i.e. my application of the model as in \citet{prandi_designing_2017, prandi_exploratory_2018} and in the present work. The second approach might consist in expanding Wickens' model to include an additional layer of vectors: demand vectors as in Wickens' original model, and integration vectors as in Seeber's application of the model. To avoid doubling the demand vectors for multiple cues, which would take a full demand vector (1), it might be possible to sum the integration vectors as proposed by \citet{seeber_multimodal_2017} and to sum ``the average demand, across all resources, within a task (and [to sum] over both tasks)'', as originally intended by \citet[171]{wickens_multiple_2002}.

Figures \ref{fig:CM_PDF_final}, \ref{fig:CM_CAI_final} and \ref{fig:CM_ASR_final} represent the CLM applied to SI under the PDF, CAI and simASR condition following the second approach of combining demand and integration vectors.

\begin{figure}
\includegraphics[width=\linewidth]{images/Conflict matrix_PDF.png}
\caption[Conflict matrix: PDF condition]{Conflict matrix for the PDF condition}
\label{fig:CM_PDF_final}
\end{figure}

\begin{figure}
\includegraphics[width=\linewidth]{images/Conflict matrix_CAI.png}
\caption[Conflict matrix: CAI condition]{Conflict matrix for the CAI condition}
\label{fig:CM_CAI_final}
\end{figure}

\begin{figure}
\includegraphics[width=\linewidth]{images/Conflict matrix_ASR.png}
\caption[Conflict matrix: simASR condition]{Conflict matrix for the simASR condition}
\label{fig:CM_ASR_final}
\end{figure}

As discussed in the previous sections, there are several unanswered questions as to the level of resource recruitment and sub-task interference in CASI. Future studies conducted on larger samples might help provide answers which may lead to a further refinement of the model.

\section{Limitations} \label{limitations}\largerpage[2]
The main contribution of the present work lies in the methodology developed for the study of technology support for specialised terminology in simultaneous interpreting. Nonetheless, the study presents some limitations, which I discuss in this section, before providing my concluding remarks, reflecting on the methodological, practical and didactic implications of my findings, and addressing potential avenues for further research in this relatively new domain.
\subsection{Participants and choice of experimental design} \label{limit_design}
An important limitation for the generalisability of the experimental findings lies in the small population of participants that could be recruited for the study. As discussed in \sectref{sample_main}, despite clear efforts devoted to the recruitment of participants, I was only able to include nine students in the study, mostly due to a lack of time and, probably, interest on their part, and on a series of unforeseen factors. The small sample size clearly limits my ability to generalise the results of the present study to the population of interpreting students. To counteract this limitation, a within-subjects design was chosen for the experiment. As my aim was to compare simultaneous interpreting with different types of glossary support, a within-subject, repeated measures design allowed me to work with a smaller sample wherein each participant acted as his or her own control group. This decision proved particularly helpful, as recruitment of participants was more complex than anticipated. As the participants were randomly selected among advanced students of the master course in conference interpreting of the University of Mainz and showed, among other aspects, different levels of command of the English language and different typing speeds, I feel quite confident that the trends observed in the sample were not due to the individual characteristics of the participants, but would probably also be found in a larger sample.

Nonetheless, a within-subject design also presents some limitations, in particular due to a potential increase in Type 1 error due to repeated tests in statistical analysis. I took this into account in the choice of inferential statistics and by applying corrections when necessary.

Additionally, learning, boredom and fatigue effects may also represent issues not to be underestimated in within-subject designs. While boredom was not considered as a potential issue given the difficult task at hand\footnote{Although students may have been less motivated to perform well compared to an exam situation or to a real interpreting assignment. However, I can reasonably assume that a low level of motivation would have affected all trials equally.}, even more complex when performed by students, learning and fatigue may have posed important limitations.

In particular, it is possible that a certain practice effect may have ensued within-task and also from the first trial to the last. Specifically, it is possible that the students may have noticed the recurrence of the 5s pause between each continuation and introductory sentence. This may have caused them to intentionally slow down or interrupt their rendition of the source speech while looking up terms, as they were aware of the fact that they could catch up with the speaker in the short break between clusters. For this reason, the EVS measured cannot be generalised as the average EVS of students working with PDF, CAI or ASR glossaries during SI, as it is possible that different values may be observed in a more naturalistic setting with less controlled speeches.

I had taken the learning effect into account and attempted to exclude it from the equation by training the participants ahead of data collection. If students had developed strategies to work with the individual tools, this would have already happened before testing and not during the experiment. However, it is possible that the fixed structure of the speeches may have promoted a certain practice effect within-task, as the test speeches were made up of collated sentence clusters, unlike the practice speeches which were more naturalistic in nature. Nonetheless, if a learning effect occurred, it would have occurred for all three treatments. The fact that there was a significant difference in the EVS between the three conditions seems to suggest that the type of tool used to prompt the participant with specialised terminology did affect the speed of integration of the term into the rendition. It would be interesting to further explore this instance in more naturalistic settings and also while manipulating the speaker's delivery speed, as presentation rate has been proven to influence the interpreter's ear-voice lag (see \citealt{barik_simultaneous_1973,lee_ear_2002}).

As for fatigue, it is inevitable that students may have felt more tired at the end of the testing session. The trials lasted around 20 minutes each, of which 14 were occupied by the very demanding interpreting task. Given the high number of terms looked up on average in the PDF and CAI condition, it would have probably been sufficient to limit the length of each speech to around 10 minutes, in order to replicate the average duration of an interpreting exercise. I had controlled for a potential order effect experimentally by randomising the speech-tool combinations. It is interesting to notice that performance evaluations and accuracy scores were higher for the simASR condition across the board, even though in some cases the simASR treatment had been the last in the experiment. In my opinion, this might suggest that, even in situations of increased fatigue, the type of support provided by an ASR tool may help promote a high quality delivery, at least in terms of terminological accuracy and adherence to the source text in terms of informational content.

Finally, it should be stressed that, as in most other studies available on the topic (see \sectref{CAI_evaluation}), the experiment was conducted on students. Therefore, my findings cannot be generalised to the population of interpreters, for which the experiment outcomes may (or may not) have been very different.

\subsection{Use of simulated ASR}
The results for the simASR condition speak in favour of the adoption of CAI tools with integrated ASR as compared to traditional digital glossaries and CAI tool with manual look-up. Particularly, a tool with integrated ASR poses the advantage of reducing the additional cognitive load generated by manually querying the glossary for terminology. Nonetheless, it is important to remind the reader that, in the present experiment, the absolute ceiling performance of an ASR-enhanced CAI tools was simulated. This implied mocking-up a system with perfect precision and recall, excluding potential system failures from the experimental design. As discussed in \sectref{design_MS}, this was a conscious choice. It was taken to avoid further complicating an already highly complex experimental setup within the context of a study which presents an exploratory character and which mostly aims to offer a methodological contribution to the field of CAI research.

In light of these considerations, it is not possible to extrapolate the findings of the experiment to real-life CAI tool without hypothesising that system failures, which are bound to occur, may have a negative impact on interpreters' cognitive processing and their interaction with CAI tools. In some instances, it may even prove counterproductive to adopt an ASR-enhanced CAI tool during SI, precisely because even high-performing systems may offer sub-optimal support under certain conditions. On the one hand, this will require further research to explore the effects of system failures on SI, both from a process- and from a product-related perspective. On the other hand, increasing interpreters' AI literacy may assist them in selecting the right tool for the job and in adjusting their expectations towards AI-enhanced tools, thus contributing to making their use of said tools more effective.

\subsection{Priming} \label{priming}
When working with a glossary, be it in PDF format, in a CAI tool, or even printed on paper, it is possible that additional terms may be visualised while looking for another term simply because they are on the same page or because they appear in the list of results. In those cases, it is possible that for said terms a priming effect occurs: the interpreter may identify said terms faster when she looks them up after having already viewed them on screen. In the present experiment, a number of terms had already been displayed on the screen during previous queries. Priming may have determined shorter times to first fixation and shorter fixation durations for these terms.

Despite this limitation, in the present study I chose not to include this subset of terms in the analysis. This decision was made for reasons of practicality, as discussed in \sectref{data_prep}, but it would have certainly been interesting to explore this further. For instance, a sample of terms likely to have been primed may have been analysed for all participants. The first step would have consisted in the identification of the terms that had been looked up by all participants, for each of the three speeches and for both the PDF and the CAI condition. If the simASR tool was used, only one term was shown at a time, so no priming could have occurred. The next step would have consisted in reviewing the gaze replays for the participants working with the PDF glossary. When using a PDF glossary, it is possible to visualise terms occurring later in the speech only if they are alphabetically close to the term currently being looked up. With a CAI tool, it is more probable for terms to be visualised on screen even though they are mentioned later in the speech, for example because one of the elements making up the term contains the letter sequence typed by the test subject. Therefore, the PDF condition should be selected as the reference for the identification of the sample of terms (searched/found in the glossary) likely to have been primed. Finally, additional AOIs may be drawn on top of the regular AOIs to keep the two analyses separate.

While I did not consider this aspect in the analysis of gaze data, I may reformulate my conclusions as follows: even though the use of a traditional digital glossary or of a standard CAI tool may facilitate the identification of terms on the screen, the ASR integration nonetheless promotes a faster identification of the term on the monitor. However, it is possible that some of the differences observed between the PDF and the CAI tool may be explained by the priming effect rather than only by the different search mechanism.
\subsection{Eyetracking methodology} \label{limit_eyetracking}
The present study was the first to explore the cognitive implications of SI with CAI support with the eyetracking methodology in combination with more established methods focusing on the product of technology-supported SI, such as the analysis of terminological accuracy and errors and omissions in the rendition. In addition to the priming effect discussed above (\sectref{priming}), the collection of gaze data presented some methodological challenges, which have in part limited my interpretation of the experiment outcomes.

The first challenge is linked with the activation and deactivation of the areas of interest on the stimulus terms. In the PDF and CAI condition, in several occasions, the previous stimulus term was still visible on screen when the next term was pronounced by the speaker. To facilitate my analysis, I decided to deactivate the previous term as soon as the query for the new stimulus had been started by the participant. This was a necessary, albeit arbitrary decision, but which was applied to all stimuli in the same way. It would certainly be interesting to explore the gaze behaviour of the participants in situations in which both terms are visible on screen. However, this would have exceeded the scope of the present study, which is why I chose to exclude this aspect from my analysis. It should be noted, however, that in some cases the terms disappeared from the monitor soon after the delivery of the sentence cluster in question had been rendered in the target language. This was due either to the participant actively clearing the search field immediately after the query to prepare for the next search, or to the participant having looked up another term unknown to them, which I had not selected as stimulus (see \sectref{additional_obs}). This may explain shorter total fixation times on the term AOIs and even shorter fixation durations for some participants (see \sectref{fixation_dur}).

The second limitation is linked with the fact that, in the PDF and CAI conditions, the AOIs could only be placed on the terms which had been found in the glossary. This aspect of my experimental design made it impossible to use eyetracking to explore the cognitive impact of a failed query, which may have yielded interesting results.

Additionally, the metric ``time to first fixation'', which I used to contrast the recruitment of visual-spatial resources in the three conditions, is limited in its ability to compare tools with different query mechanisms, such as in this case. The first fixation on the term in the simASR condition is likely to coincide with processing, while the first fixation on terms in the other two conditions may only reflect the individuation of the term on the screen (\sectref{timetofirstfix}). For this reason, I did not adopt the metric ``first fixation duration'', as identifying the first duration reflecting cognitive processing of the term would have probably resulted in mere guesswork.

Despite some limitations, the use of the eyetracking methodology in this field of interpreting studies seems promising and may reveal aspects of hu\-man-ma\-chine interaction with CAI tools still unexplored with different methods.
