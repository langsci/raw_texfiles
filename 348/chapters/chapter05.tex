\chapter{Method} \label{chapter5}



The present chapter illustrates and discusses the methodology used in the present study, which was conducted to compare the effects of digital terminological support tools on the product and process of SI. The research work comprised a pilot study and a main study.

The pilot study was aimed at validating the methodology. Its results, already published in \citet{prandi_designing_2017,prandi_exploratory_2018}, and the adaptation of the design for the main experiment are discussed in \sectref{pilot_study}.

In \sectref{main_study}, the experimental materials and setup of the main study are presented. The results of the main study are presented and discussed in \chapref{chapter6}.

\section{Research gap} \label{researchgap}
As discussed in the literature review (\sectref{CAI_evaluation}), arguments in favour of and against the introduction of CAI tools and technologies such as ASR into the interpreting process are substantiated by empirical data only to a limited extent. The lack of data is accompanied by the lack of an empirically-validated research methodology for the combined collection and analysis of process and product data on SI with the support of digital tools. The existing empirical contributions have focused primarily on a product-oriented analysis through the collection of performance data. This currently represents a major barrier to the further development of empirical CAI research. Research has addressed the evaluation of terminological quality \citep{biagini_glossario_2015,prandi_uso_2015,prandi_use_2015,van_cauwenberghe_etude_2020} and, especially in the case of ASR support, the accuracy of number renditions (e.g. \citealt{defrancq_automatic_2020,pisani_measuring_2021}). Furthermore, most of these analyses have not expanded the focus beyond the unit of information for which support is offered, except for \citet{biagini_glossario_2015} (who also considered whether glossary searches produced omissions or serious errors), \citet{montecchio_masterarbeit_maddalena_2021} (who evaluated the impact on perceived fluency in addition to the accuracy of numerals rendition), and \citet{frittella_cai-supported_2021}, who conducted a qualitative evaluation of the interpretations of number-dense speeches. In some publications, subjective data were collected through retrospective questionnaires to gain further insight into the interaction with digital terminology support solutions (e.g. \citealt{defrancq_automatic_2020,pisani_measuring_2021}). To the best of my knowledge, no behavioural methods have been employed to date to substantiate these observations with more objective measures (as recommended e.g. by \citealt[391--392]{hansen_thedialogue_2008}), such as time-lag or fixation-based metrics.

\begin{sloppypar}
In addition to these methodological limitations, an evident research gap emerges. Research on CAI tools has focused mostly on the product, while the cognitive processes leading to specific phenomena in the target text have remained largely unexplored. The present contribution represents a first step in this direction by attempting a look into the cognitive inner-workings of CASI.
\end{sloppypar}

\section{Hypotheses on cognitive load in SI with terminology look-up} \label{hypotheses}
As previously discussed (\sectref{CL}), CL should not be seen as a static construct, but rather as a variable that changes constantly during the interpreting task as a function of the cognitive resources recruited by the co-occurring tasks. For the scope of the present inquiry, I chose to focus my analysis on the effect of terminology look-up and of automatic terminological prompts on the CL experienced during SI. As highlighted in the previous sections, different digital solutions for the terminological support of the interpreter may be expected to have a different impact on cognitive effort and, from a product-oriented perspective, also on the terminological accuracy of the interpreter's rendition. In the context of the present contribution, I focus on the differences resulting from the adoption of a digital glossary, of a CAI tool (InterpretBank), and of ASR as terminological support in the booth.

On the basis of what has been discussed so far and as predicted by Seeber's CLM applied to CASI (see \sectref{seeber}), it may be hypothesised that:

\begin{enumerate}
    \sloppy
    \item Tools which require manual terminology look-up (digital glossaries and standard CAI tools) may impose a higher CL due to the recruitment of additional manual-spatial resources during production and monitoring. Conversely, lower CL may be expected for ASR tools, which do not pose demands on manual-spatial resources. This should result in shorter EVS, shorter time on task, and shorter average fixation durations.
    \item As the CAI tool InterpretBank provides a series of postulated advantages (see \sectref{manual_query}) compared to standard digital glossaries, the test subjects may be able to identify the terms more quickly, at least when orthographic neighbours are shown on screen, resulting in shorter time to first fixation. Additionally, InterpretBank should impose a lower load on task coordination, as it does not require preparation for subsequent queries (see \sectref{manual_query}). This should result in a higher number of terms interpreted as per glossary and fewer serious errors and omissions compared to a digital glossary. It should also elicit more queries than a PDF glossary due to its posited higher user-friendliness and higher querying speed thanks to the incremental search.
    \item ASR support may facilitate not only production, but also comprehension, due to beneficial redundancy effects (if temporal contiguity is ensured, see \sectref{split_attention} and \citealt{seeber_when_2020}, \citealt{chmiel_multimodal_2020}). This should result in a higher number of terms interpreted as per glossary, and fewer serious errors and omissions compared to CAI tools and digital glossaries.
    \item ASR support should reduce negative split attention effects (see \sectref{split_attention}) due to increased temporal contiguity and to the absence of manual look-up compared to CAI tools and digital glossaries, resulting in shorter fixation time on the tool area and promoting attentional focus on the speaker.
\end{enumerate}


\section{Research approach} \label{approach}
The combination of product- and process-related data has been proposed as a means to improve TPR \citep{hansen_2007}. As advocated for instance by \citet{mellinger_computer-assisted_2019}, a product- and process-oriented perspective to the exploration of CAI appears necessary, if we are to advance our understanding of the impact of CAI tools on cognition in (simultaneous) interpreting and to develop tools truly tailored to the needs of interpreters and capable of addressing the inherent constraints of SI. In order to contribute to bridging the current research gap, both in terms of research object and of methodology, I therefore developed and tested a convergent mixed-method design for the investigation of the product and process of CASI with the support of different terminology look-up tools. The study adopts an experimental approach for the generation and collection of data under controlled laboratory conditions. The approach chosen is predominantly quantitative. Due to the complexity of the present object of study, ``so much less convenient to study than language fixed in writing'' \citep[48]{pochhacker_introducing_2004}, in the present contribution
multiple types of quantitative data related to the CL generated by CASI are collected and combined, and are corroborated by qualitative data related to the participants' experience in the interaction with the tools.

The study adopts methods derived from the TPR framework and includes performance, subjective, and behavioural measures combined in order to address the inherent limitations of each method. The choice of measures was motivated by the review of the methods used to measure cognitive load and their application in TPR (see \chapref{chapter4}). \tabref{tab:metrics_overview} presents an overview of the metrics selected for the study and the type of data collected. The measures chosen are discussed in detail in \sectref{measures}.


\begin{table}
 \begin{tabular}{lll}
 \lsptoprule
 {Measure} & {Type} & {Category} \\\midrule
 Terminological accuracy & quantitative & performance \\
 Errors/omissions & quantitative & performance \\
 Inter-cluster pause duration & quantitative & behavioural \\
 EVS & quantitative & behavioural \\
 Glossary queries & quantitative & behavioural \\
 Time to first fixation & quantitative & behavioural \\
 Average fixation duration & quantitative & behavioural \\
 Fixation time & quantitative & behavioural \\
 Debriefing questionnaire & qualitative & subjective \\
 \lspbottomrule
 \end{tabular}
\caption[Metrics included in the main study]{Metrics included in the main study, type (qualitative\slash quantitative) and category (subjective, performance, or behavioural)}
\label{tab:metrics_overview}
\end{table}

The hypotheses concerning digital terminological support in the booth (\sectref{hypotheses}) were explored using a within-subject design: in this way, as discussed in \citet[65]{lazar_research_2017}, ``the impact of individual differences is effectively isolated and the expected difference can be observed with a relatively smaller sample size'' (see also \sectref{recruitment_MS}). A small-N design \citep{smith_smallN_2018} need not necessarily be considered a limitation. In other disciplines, such as psychology, ``there is a long history of research [...] employing small-N designs'' (ibid., p. 2083). A small sample where multiple observations are performed on a limited number of participants may be even more informative than a large sample, as combining quantitative analysis with qualitative observations (e.g. from questionnaires) and exploring the same dataset through multiple metrics becomes more feasible. As a consequence, a researcher may be able to paint a more refined picture of the mechanisms underlying a specific process or observations made on the product. Considering that in\-ter\-pret\-er-ma\-chine interaction is still a largely unexplored topic in CAI, working with a small sample seems valid. As observed for instance by \citet[422]{hansen-schirra_translation_2020}, small-N studies ``are valuable to build hypotheses for larger studies''. The benefits of a small sample size have been recognised also in the area of human-computer interaction, where skilled participants are usually needed and cannot always be found easily \citep[65]{lazar_research_2017}. At the time of the experiment, CAI tools and ASR in the booth were still a mirage for many interpreters and only included in training to a limited extent \citep{prandi_cai_2020}. When the use of CAI tools becomes more mainstream (and this may already be the case due to rapidly changing working conditions during the COVID-19 pandemic), it may be more feasible to recruit larger samples. In order to test the hypotheses of the present study, involving well-practiced participants was deemed more valuable than recruiting a large sample.

Whenever possible during the study, I strove to preserve ecological validity, i.e. ``the naturalness of the investigated process'' \citep[386]{hansen_thedialogue_2008}. For this reason, real-life conditions were replicated insofar as they allowed for sufficient experimental control to be able to draw inferences from the collected data (see \citealt{spinner_ecological_2013} for an empirically-based discussion on ecological validity in reading research, largely applicable to experiments on translation and interpreting). However, as addressed in \sectref{material_PS}, \ref{procedure_PS}, \ref{material_MS}, \ref{design_MS}, it was necessary to limit the potential impact of confounding variables on results. Therefore, the stimuli were designed to reduce such impact (\sectref{material_PS} and \sectref{material_MS}).

The review of the research conducted using eyetracking on translation and interpreting (see \sectref{eyetracking_TPR}) shows that the technique may provide valuable insight into the mental processes at the core of translation and interpreting. In particular, the large body of TPR conducted using this technique, which has contributed to establishing a set of reference metrics taken to index cognitive load and effort, may prove especially valuable also for the present object of inquiry, which is why I opted to conduct an eyetracking study to address my research questions.

Since the present study is the first to explore SI with digital support for terminology from a preeminently cognitive perspective, it has an exploratory character and also aims at formulating further hypotheses in addition to providing initial findings on this complex research object.






\section{Pilot study: Method and results} \label{pilot_study}
In order to test the methodology for the main study, a pilot study was conducted at the University of Mainz/Germersheim between May and July 2017. The experimental design combined process and product-oriented data collection methods. Its primary focus was the validation of the stimuli designed for data collection. In this section, I describe the experimental setup illustrating the stimuli used and analysing the participants' deliveries and queries performed. I then highlight the limitations identified in the approach chosen and discuss the modifications applied to the experimental design ahead of the main experiment.


\subsection{Participants} \label{sample_PS}
For the pilot study, I was able to recruit six advanced students of the master's degree in conference interpreting of the University of Mainz/Germersheim. They had all received at least three semesters of instruction in simultaneous and consecutive interpreting and had English as their B (active) or C (passive) language. The sample was made up of three German natives (one male, two females) and three Italian natives (one female, two males). The two language combinations, comprising one Germanic and one Romance language, were chosen to ensure that the stimuli could be considered challenging independently of structural and linguistic similarities in the language pair, e.g. when cognates were included as stimuli. Participation in the study was voluntary and not remunerated, but compensated instead by allowing students to learn how to use a CAI tool which is not usually included in the regular interpreting curriculum. An additional benefit of participating in the study consisted in the added amount of practice hours with a laptop in the booth, which is rarely done systematically in class. This was, however, also a limitation, as students were required to schedule time for the practice sessions. This is probably the main factor contributing to the difficulties experienced during participant recruitment, a difficulty which was unfortunately also confirmed for the main study, as will be discussed in \sectref{sample_main}.



\subsection{Test subjects' training} \label{testsubjects_training}
Before taking part in data collection, it was necessary to prepare participants in order to ensure that they were all equally proficient in the use of the three tools compared in the study, so that I could exclude this variable during data analysis. The individual disposition towards technology continues to play a role despite training, of course, but no subject preparation can change this highly personal factor. Nonetheless, it is certainly easier to exclude the variable of lack of tool expertise if the participants have gone through a dedicated training beforehand.

The students were therefore invited to attend a preliminary preparatory meeting during which they were instructed on the basics of terminology management for conference interpreters. The training had an explicit practical focus, as previous studies \citep{prandi_uso_2015, prandi_use_2015} had highlighted the role of practice as more beneficial to familiarise the participants with the tool than a more theory-prone training. The preparatory meeting focused in particular on the search functions in Word, Excel and InterpretBank. The main difference between Word and Excel, both traditional tools for organising terminology, was that with Word participants could visualise all the results of a query in a column on the left hand-side of the screen, similarly to InterpretBank, but without the explicit optimisation for interpreters offered by InterpretBank. When working with Excel, they had to skip to the next occurrence manually. The five practice sessions that followed were organised to develop the ability to conduct queries while interpreting. All participants attended all practice sessions, which took place once a week for five weeks. During each session, their task consisted in interpreting three short speeches from English into their respective mother tongue (German or Italian) while using one of the tools to look up terminology. The order of the tools was changed at every session. The speeches used for the training were selected and adapted from the training material used in a previous study \citep{prandi_uso_2015, prandi_use_2015}, to which several authentic speeches were added that had been selected in order to ensure a certain progression from a more controlled to a more naturalistic practice environment. The topics chosen were medicine and biology. I prepared the glossary used for terminology look-up and made it available to all students for both language combinations and each tool, so that they all practised with the same material. The training was thus designed to guarantee equal practice time for each tool. It should be noted that I use the term ``training'' to indicate that practice was ensured before data collection. However, no specific input on the development of strategies for effective in\-ter\-pret\-er-ma\-chine interaction during SI was given.

After the last training session, the participants took a short proficiency test to verify whether they had all learned the basics of glossary querying with the three tools. The test consisted in a series of tasks to be performed with the tools and focused on the search function. Students were asked to record their screens while accomplishing the tasks. The screen recordings were later analysed: all students passed the test and could take part in data collection.


\subsection{Materials} \label{material_PS}
The speeches drafted for the pilot study were prepared based on a series of known effects and hypotheses related to features of oral speech perception and production and to linguistic and morphological characteristics of terms as stimuli. These considerations are described in the following subsections, while \sectref{featuresspeechesPS} illustrates the features of the speeches validated in the pilot study.


\subsubsection{Speech rate and presentation} \label{speechrate}
Speech rate is a prosodic factor which can be essential in determining the feasibility of the SI task. Especially excessively high input rates have been identified as a major stress factor in interpreting and may render the performance of SI impossible (e.g. \citealt{riccardi2015speech}). In SI, the ideal speech rate is comprised between around 95 and 120 wpm \citep{seleskovitch_language_1978,gerver_empirical_1976,seleskovitch_language_1978,lederer_traduction_1981,pio_relation_2003,seeber_temporale_2005}. Therefore, for the experiment it was necessary to select a speech rate which would make looking up terminology during SI challenging, but not impossible. This was necessary to test the trade-off between looking up terminology and delivering an acceptable interpreting performance.

An additional aspect to consider was the mode of presentation. Pre-prepared, read-out speeches tend to be associated with a faster speaking rate and a less spontaneous intonation, which can make the input more difficult to process. However, as required by the experimental nature of the present study, it was necessary to ensure consistency between subjects and to use comparable speeches. Therefore, while it would be possible to have the speaker deliver a speech live to maintain higher ecological validity and collect data in a more naturalistic setting, in an experimental setting, video recordings can be a way of maintaining the necessary degree of control while at the same time approximating the mode of presentation of the source speech to real-life conditions. Additionally, video-recorded speeches are usual training material for interpreting students. Therefore, using a video-recorded read-out speech at a comfortable speaking rate was expected to be perceived by the participants as similar to their asynchronous training sessions in terms of speed and prosodic aspects.



\subsubsection{Speech structure: Sentence processing} \label{sentenceprocessing}
When designing the experiment, I was also faced with the challenge of presenting participants with clearly defined stimuli that could then easily be correlated with responses. At the same time, it was necessary to preserve a certain degree of ecological validity in order not to alienate the test subjects with a too unfamiliar task – a common challenge in the investigation of the interpreting process in a laboratory setting. While I am interested in analysing CL at a local level (see \sectref{CL}), presenting participants with individual terms to interpret without any context would have excluded the element of simultaneity from my study. This would have counteracted the very goal of my investigation, as it is the simultaneous performance of cognitive tasks that makes challenges and limitations arise which would otherwise not be noticeable when considering the individual tasks on their own. In contrast, using unedited speeches and conducting the analysis at the text level would have likely introduced an excessive amount of potentially confounding variables and made stimulus-response correlations difficult to identify. For these reasons, I aimed for highly controlled stimuli while at the same time trying to make the task as realistic as possible.

A useful reference was identified in \citegen{seeber_cognitive_2012} methodology. In their study on cognitive load effects in symmetric and asymmetric sentence structures in SI, they presented stimulus sentences embedded in sentence clusters, i.e. the critical (target) sentence is enclosed in a pre-critical (introductory) and a post-critical (continuation) sentence. This approach presents several advantages: it makes data analysis more efficient by allowing for a focus on the target sentences (i.e. the sentences including the stimulus-term); it promotes the creation of comparable speeches, as they all present the same structure; and finally, it provides participants with the impression that they are interpreting a continuous speech rather than isolated sentences.

In experimental research, the inclusion of filler items is essential to minimise unintended repetition effects due to noticeable patterns in the stimuli (see \citealt[44--45]{conklin_eye-tracking_2018}, \citealt{keating_experimental_2015}). In other words, if every sentence contained a stimulus, the participants may notice it and adjust their look-up strategy when working in the PDF or CAI condition. The terminological density may also be too high to allow for queries during the interpreting task and the participants may stop looking up terms altogether, which would defeat the purpose of the study. To avoid eliciting unwanted effects, the pre-critical (introductory) sentences and the post-critical (continuation) sentences would have to be superficially as similar as possible. Crucially, they would need to be highly comparable in terms of length, number of clauses and syntax (see \citealt[16]{keating_experimental_2015}).


While one sentence between each stimulus and the next may have been sufficient to give participants some respite between two queries, the introduction of a continuation sentence was deemed useful to ascertain whether a glossary query may cause a trickle-down effect due to imported load (see \citealt{gile_local_2008}), leading to severe errors and omissions in the following sentence. In studies on sentence processing, ``the processing of a critical region in a sentence oftentimes continues or spills over onto the words immediately following the critical region'' \citep[6]{keating_experimental_2015}. This ``spillover effect'' (e.g. \citealt{rayner_lexical_1986}) is often noticeable not in the target region (i.e. the target sentence in this case), but rather in the spillover regions. Continuation sentences would therefore serve as a test-bed to verify whether coping with the stimulus term in the target sentence may lead to effects in the following textual material.



\subsubsection{Compound processing} \label{compounds}
In English and German, all compounds are right-headed \citep{arcara_word_2014,semenza_combining_2014}, while in Italian they can be either left- or right-headed (e.g. \citealt{ghiselli_sfide_2015}). The morphological structure of the terms was expected to affect search behaviour. Since a word-length effect has been observed for acoustic understanding, i.e. shorter words have been found to be more difficult to process (e.g. \citealt{barton_word-length_2014}), short unigrams (monosyllabic or bi-syllabic) may have posed a difficulty for a glossary search. Therefore, if they had not been understood, they would offer less linguistic material to perform a query (which can be conducted also for incomplete words). However, as working memory span has been shown to increase for short words, if the stimuli were correctly understood they may have interfered less with the other subprocesses, since they would pose a lighter load on WM. Conversely, I expected longer terms to be easier to process acoustically. In particular, multi-word terms (bigrams and trigrams) would facilitate the glossary search by offering more linguistic material: for instance, participants may choose not to look for the first element of the term, but rather for one of the other elements (e.g. the modifier instead of the head of the compound expression). Additionally, compound expressions have been identified as an element of difficulty in SI (see \citealt{ghiselli_sfide_2015}), and I therefore expected them to be looked up more often than unigrams.



\subsubsection{Position of terms in the speeches} \label{position}
The position of the terms in the sentence was also expected to play a role. In reading research, sentence-final stimuli have been found to elicit longer fixations, which suggests higher cognitive load. This is known as the sentence wrap-up effect \citep{warren_investigating_2009}, which has been explained with the integration of sentence meaning with preceding and following context: at the end of the sentence, the reader receives all the necessary clues to correctly gauge the meaning of the sentence. It should be noted that this effect has been found not only for sentence-final stimuli, but also for clause-final stimuli. While the phenomenon has been studied mainly with reference to written textual processing, a similar effect has been found in the eyetracking study by \citet{seeber_cognitive_2012}: verb-final sentences in German elicited significantly larger pupil dilations than verb-medial sentences in a study on SI from German into English. Similarly, a stimulus term placed at the end of the sentence may pose a higher strain on WM and result in CL possibly due to the need to store in memory previous units of information semantically or morphologically depending on the term. Furthermore, at the end of the target sentence, the interpreter would also have to start processing the following sentence: having to store the term in WM in addition to the previous information during the query and to finish interpreting the target sentence may increase the CL to the point of overloading or of resulting in serious issues or omissions in the continuation sentence. However, sentence-final stimuli offer the advantage of being preceded by more context than sentence-medial stimuli. This may facilitate semantic anticipation (e.g. \citealt{gile_basic_2009}). Hence, interpreters may be able to avoid a query altogether or apply the strategy of formulating their rendition with a less compromising syntactical structure, facilitating a seamless integration of the term after performing a query. For this reason, sentence-final stimuli may actually elicit lower additional load. If, on the other hand, the test subject does not anticipate that an unknown term requiring a glossary query is coming up, self-corrections may be necessary. This may be expected both for sentence-medial and sentence-final stimuli, resulting respectively in false starts and self-corrections or reformulations.



\subsubsection{Frequency of the stimuli} \label{frequency}
Finally, the frequency of the stimuli used was expected to affect the participants' search behaviour. It was reasonable to expect that more frequent terms may have been known to the students and not require a query, whereas less frequent terms may have required glossary look-up. CAI tools are generally considered to be more user-friendly and to take up fewer cognitive resources than traditional glossary tools (see \sectref{CAI_evaluation}), thus allowing for a higher number of queries. In his study contrasting paper glossaries and InterpretBank, \citet{biagini_glossario_2015} had found a higher number of terms searched in InterpretBank compared to the paper glossary condition. Introducing both frequent and infrequent terms as stimuli was expected to help explore the postulated advantages of CAI tools, which are designed to facilitate look-up. In particular, I hypothesised that participants would look up a higher percentage of no-query terms when working in the CAI condition due to a lower load on working memory (see \sectref{manual_query}, \ref{hypotheses}).\footnote{For the scope of the pilot study, the stimuli were not further controlled for linguistic aspects such as cognate status or level of concreteness, which can affect linguistic processing (see \sectref{featuresLSP} for a discussion of the features of language for special purposes). These aspects were considered in the main study (see \sectref{main_study}).}

\subsubsection{Features of the speeches used in the pilot study} \label{featuresspeechesPS}
The design of the speeches was motivated by the considerations illustrated above and the intention to test how specific features of the speeches would affect the participants' performance and interaction with the tools. Additionally, the naturalistic base material was modified in order to control for potential confounding factors.


The speeches used for data collection were based on a corpus of speeches collected ad-hoc from the speech repository of the EU Directorate General for Interpretation\footnote{\url{https://webgate.ec.europa.eu/sr/} (Accessed: 01.11.2021).} on the topics of energy sources. The speeches were on average 12 minutes long at an average speed of 122.26 words per minute (wpm). The speeches were read out by a male native speaker of British English and video recorded. Additionally, the speeches used as training materials ahead of the test were also presented in the same way. As the speaker was himself an interpreter trainer and conference interpreter, he was familiar with the use of pre-prepared material to deliver speeches for training purposes.

Each speech was made up of 36 sentence clusters, containing one stimulus each. Each sentence cluster was made up of:

\begin{enumerate}
   \item An introductory sentence, which provides context but should not elicit glossary queries;
    \item A target sentence containing the stimulus to be looked up;
    \item A continuation sentence, with the same features and role of the introductory sentence.
\end{enumerate}
Thus, each stimulus was separated from the following one by two filler sentences. Due to the presence of introduction and continuation sentences, each speech thus contained 66.7\% noncritical sentences and 33.3\% critical (target) sentences, as recommended in literature (e.g. \citealt[17]{keating_experimental_2015}).

The structure was repeated throughout each speech. I report an example from Speech 1 to illustrate how the speeches were structured:

\ea So we need to change this basic trend and this is why the urgency is there.\\
    In our policies, we should definitely address the need to improve \textit{vehicle efficiency}.\\
    But there is still much more I can do, in many other areas, as you are aware.
\ex At the EU level, there is another policy option that can help us.\\
    By focusing, for instance, on \textit{woody biomass fuels}, we can truly make a difference.\\
    They have the potential to help us respond to the challenges we're facing.
\z

The stimuli introduced in the speeches present a set of features that were chosen to control for additional confounding variables. Each text contained 36 terms, one per target sentence. The terms were symmetrically grouped by morphological structure, by position in the sentence, and by necessity of a query based on their frequency.

Specifically, of the 36 terms, 12 were unigrams (e.g. ``bioenergy''), 12 were bigrams, i.e. made up of two elements, usually noun-noun (e.g. ``energy poverty'') or adjective-noun compounds (e.g. ``tidal barrage'') and 12 were trigrams, in different combinations of adjectives and nouns (e.g. ``pressurised water reactor''). For the above-mentioned reasons, I decided to control for the presence of multi-word and single word expressions in the speeches by equally distributing the stimuli across the three categories. In order to control for potential sentence-final effects and to test my hypotheses on anticipation, half of the terms were placed at the end of the sentence and half in the middle.

Finally, I divided the terms equally into terms requiring a query and terms for which a glossary search was not deemed to be necessary. The terms classified as requiring a glossary search were highly technical terms that do not belong to the 10,000 most common English terms as per their frequency in the \citet{eng_web_2012,eng_news_2016,eng_wikipedia_2021} corpora.


\subsection{Procedure} \label{procedure_PS}
At the time of the pilot study, the adoption of ASR as in-booth support had just been theorised for the first time (see \citealt{fantinuoli_speech_2017,ortiz_computer-assisted_2018}). For this reason, and based on the results of the available inquiries into conference interpreters' terminological practices (see \sectref{habits_terminology_int}), the in-booth digital support solutions compared in the pilot study were Word glossaries, Excel glossaries and the CAI tool InterpretBank.


Data collection took place at the Translation and Cognition (TRA\&CO) Center of the University of Mainz/Germersheim, the university's neurolinguistic laboratory dedicated to Translation Studies. Before the start of the experiments, the students were briefed about the structure of the study and signed a waiver on the collection and treatment of their data. They were informed that they could decline their consent to the use of their data at any time. They were informed about the interpreting task they were going to complete during the experiment and were told the topic of the speeches (renewables and other energy sources). I am aware that not having participants prepare ahead of the interpreting task does not reflect usual professional practice. Nonetheless, I decided to sacrifice this aspect of ecological validity in favour of a more controlled experimental design. Preparation is highly personal, depending on several variables, not least also on motivation, and attempting to control and standardise the test subjects' preparation time and thoroughness would have probably proved to be impossible. An additional reason for excluding preparation from the experiment was the need to ensure that participants would be presented with a sufficient number of stimuli to look up in the glossary. As a result, the number of queries is probably much higher in the experimental setting than in a real-life interpreting assignment, but having sufficient data points per participant was necessary considering the small sample, and essential to draw initial conclusions on the comparison of the three tools. The students were asked to approach the interpreting task as if they had to interpret at a real conference on renewable energy and had had little time to prepare, but a colleague had made available a glossary at the last minute. They were not explicitly encouraged to look up terms while interpreting, but were asked to consider the glossary simply as an aid that they could resort to when necessary.

I prepared the glossary, which contained 421 terms, i.e. all the specialised terminology included in the texts plus additional terms related to the topic in question. The same terms were available for each tool and language combination. The glossary, prepared in InterpretBank and then exported and converted into .xlsx and .docx files, presented a simple tabular structure which only included the term and its equivalent in the target language. It could be argued that interpreters' glossaries may also be more complex, containing additional information such as synonyms, definitions and collocations. Moreover, their structure and content are very personal, as they reflect the individual preparation style, preferences and strategies. Accounting for each of these variables would have been extremely complex empirically. Therefore, I chose to focus on the minimum common denominator for the test, i.e. the terminological pair.


During the experiment, participants interpreted three speeches from English into their A language, as they had done during the practice sessions. They used a different tool each time. The order of speeches and tools was randomised to avoid effects due to fatigue ensuing during the test (\citealt[400]{spinner_ecological_2013}, \citealt[18]{keating_experimental_2015}) or to the individual speech/tool combination \citep[42--43]{conklin_eye-tracking_2018}. The participants' glossary queries were collected automatically in the log file generated by InterpretBank after each trial and manually for the Word and Excel condition by reviewing the screen recordings.\footnote{During the pilot study, the eyetracking set-up used during the main study was also tested. Therefore, the gaze replay videos exported from the data analysis tool of the eyetracker (SMI BeGaze) were available and could be used to reconstruct the queries in the Word and Excel glossaries. However, during the pilot study, gaze data was not analysed. The eyetracking setup is therefore described and discussed in greater detail in \sectref{material_MS} and \sectref{design_MS}.} The participants' interpretations were recorded with Audacity (\citeyear{audacity_2021}) and then transcribed following the HIAT transcription conventions \citep{rehbein_handbuch_2004} in Partitur Editor, the transcription tool of the Exmaralda Suite \citep{Schmidtworner2014}.




\subsection{Stimulus validation} \label{stimulus_validation}
The primary aim of the pilot study was verifying the validity of the a-priori classification of the stimuli in relation to glossary queries. In order to obtain sufficient data for a comparison between the Word, the Excel, and the CAI tool glossaries, it was necessary that the stimuli classified as requiring a query actually elicited a sufficient number of searches for all participants. While a certain degree of inter-subject variability should be expected, as each participant differs in terms of SI skills, world knowledge, and interpreting strategies, it was important for my classification to hold true at least in the majority of cases.

I therefore transcribed the students' deliveries and analysed them to verify:

\begin{itemize}
\item the total number of terms searched;
\item the number of terms searched classified as requiring a query (QN);
\item the number of terms searched classified as not requiring a query (NO QN).
\end{itemize}
As was to be expected, the percentage of terms searched classified as requiring a query varied considerably between participants, while it was quite similar for the terms not expected to elicit a query in the glossary. This percentage was similar for the German natives (PS1-01, PS1-02, PS1-03), although different terms were looked up. As can be seen in the graph below (Figure \ref{fig:searches_PS}), one participant (PS1-06) looked up a considerably lower number of terms.

\begin{figure}[ht]
%\centering
%\includegraphics[width=0.8\linewidth]{images/Terms_searched_PS.png}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 80,
  xtick = {1,2,...,6},
  xticklabels = {PS1-01,
  PS1-02,
  PS1-03,
  PS1-04,
  PS1-05,
  PS1-06},
  x tick label style = {font=\small},
  ylabel = {\%},
  nodes near coords,
  legend style={at={(0.5,-0.2)},anchor=north},
  legend columns={-1},
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,63)
  (2,63)
  (3,70)
  (4,50)
  (5,76)
  (6,31)
  };
  \addplot+[lsLightBlue]
  coordinates {
  (1,24)
  (2,24)
  (3,26)
  (4,22)
  (5,22)
  (6,0)
  };
  \legend{Terms searched QN (\%/54),Terms searched NO QN (\%/54)}
  \end{axis}
\end{tikzpicture}
\caption[Terms searched in pilot study]{Percentage of terms searched per stimulus category in the pilot study. QN = query necessary, NO QN = no query necessary \citep[47]{prandi_exploratory_2018}}
\label{fig:searches_PS}
\end{figure}

Additionally, it was important to verify which terms which had been classified as requiring a glossary search had not been looked up by any participant and vice versa. Out of 54 terms, five QN terms were not looked up in any case, while one NO QN term was looked up by all subjects. The terms in question would therefore require replacement or reclassification.


\subsection{Preliminary results} \label{results_PS}
A thorough description of the results of the pilot study is reported in \citet{prandi_designing_2017, prandi_exploratory_2018}, including a preliminary analysis of the strategies adopted during SI. While verifying hypotheses was not the main aim of the pilot study, I conducted a series of preliminary analyses on the transcribed interpretations to gain first insight into several expected phenomena in preparation for the main experiment. I first present the results of the analyses related to the stimuli classification and then continue with an analysis of product-related aspects.


\subsubsection{Preliminary effects on search behaviour} \label{searchbehaviourPS}
As previously stated, the position of the stimuli was expected to affect the participants' search behaviour (see \sectref{position}). Specifically, I expected the terms placed at the end of the sentence to result in more glossary queries than the terms in the middle of the sentence. In the small sample tested, sentence-final terms seemed to elicit more queries. This could be due to several reasons: first of all, participants could anticipate that a specialised term was going to come up based on the context and would prepare themselves to query the glossary. Additionally, it is possible that, anticipating the need for a glossary search, they would prefer a sentence structure in their delivery that would favour a search. This would certainly require the development of ad-hoc strategies to facilitate terminology look-up during SI and it could be argued that this would have been difficult for trainee interpreters. It may however be a behaviour likely to be observed in experienced interpreters who are highly proficient in the use of technology during SI. On the other hand, stimuli placed within the sentence could be more difficult to handle through a glossary query, as this would likely generate more noticeable disfluencies in the participants' rendition. It is possible that interpreters would therefore choose to use an alternative strategy, resulting in lower precision but in a more natural delivery and lower CL experienced.

In considering the morphological complexity of the terms (see \sectref{compounds}), I could identify a prevalence in searches for unigrams belonging to the ``NO QN'' category. This could be due to the fact that querying the glossary for a unigram is more straightforward than for bi- or trigrams: either the term is understood and looked up, or it is not. In the case of bigrams and trigrams, there is an additional layer of decision as interpreters may first have to decide which element of the term should be looked up (provided that all components have been understood). This would require additional cognitive resources unless a dedicated strategy has previously been developed and automatised. Therefore, participants may choose to adopt a different approach altogether. For terms requiring a query, no clear trend could be identified.


\subsubsection{Preliminary effects on accuracy} \label{accuracyPS}
I also conducted additional observations on this preliminary data concerning the accuracy achieved when working with the three different tools. In the context of the present experiment, accuracy is adopted as a performance measure (see \sectref{performance_measures} and \sectref{accuracy_performance}) to identify instances of cognitive effort in the interpreter-tool interaction, not as a component of quality to evaluate the target text. Accuracy is therefore operationalised as the fraction of terms translated as per glossary or with an equally acceptable term, which would indicate an effective use of the tool and/or limited cognitive effort exerted to produce the term.

The percentage of terms searched translated as per glossary was calculated and the results were analysed following a methodology already used in previous studies on CAI tool use in SI \citep{prandi_uso_2015,prandi_use_2015,biagini_glossario_2015}. I classified the term renditions adapting a classification put forward by \citet{wadensjo_interpreting_1998} for interpreting (see \sectref{termquality}). This framework was chosen for two main reasons. First, to promote comparability with previous studies. Second, because it adopts a broad categorisation and stresses the effectiveness of the rendition, which facilitates the operationalisation of accuracy as a correlate of effective interaction with the tool. For instance, as can be seen below, omissions and unacceptable renditions are grouped together as they both index unsuccessful or effortful queries. Other evaluation frameworks, such as MQM (see \sectref{termquality}), also include accuracy and terminology as issue types, but they consider them as distinct dimensions.\footnote{Additionally, in MQM omission is included as a sub-dimension of accuracy separate from mistranslation, which does not fit into the present operationalisation of incorrect renditions.} These frameworks may prove very useful in studies aiming to assess the target text quality in CAI, especially if it has been specified that the interpreter should use the equivalent contained in the glossary, but this kind of analysis goes beyond the scope of the present experiment.

According to the framework chosen, terms could be classified as:

\begin{description}
    \item[Close renditions:] no information is lost, the rendition is precise, the term is translated with the glossary equivalent or an adequate synonym;
    \item[Acceptable renditions:] some information is lost (e.g. through paraphrasing, the loss of an adjective in complex terms, a drop in register), but the general meaning is maintained;
    \item[Zero/unacceptable renditions:] this category groups renditions that completely or largely deviate from the original (the content is different) and terms that were left untranslated.
\end{description}
I assigned a value of 2 to close renditions, of 1 to acceptable renditions and of 0 to unacceptable and zero renditions. As for the degree of terminological precision achieved with Word and Excel glossaries and with InterpretBank, inter-subject variability is high. Nonetheless, Excel seems to lead to the worst terminological performance, i.e. to more frequent unacceptable renditions or terms left untranslated. This is probably due to the fact that when working with Excel it is necessary to manually skip to the next result, while with the other tools the results are all visualised together. This might make queries in Excel excessively cumbersome and time-consuming. InterpretBank seems to perform slightly better than Word in this respect.

I also considered the terminological accuracy in relation to the term structure and found that InterpretBank leads to higher accuracy for unigrams in five cases out of six (with the exception of PS1-06, whose search behaviour however differed greatly from that of the other participants). My hypothesis was that InterpretBank would prove more effective, leading to higher accuracy, especially for complex terms (bigrams and trigrams). I did not find significant differences for bigrams and trigrams: queries with InterpretBank proved more effective than Word and Excel in half of the cases.

The analysis of accuracy was conducted first at a microscopic level, that of terminology. However, a successful query that results in an accurately translated term may nonetheless require too much time and attention, thus leading to serious errors or even to the complete omission of information. I therefore expanded my analysis to the sentence level. The transcriptions were annotated following \citegen{barik_description_1971} classification of omissions, additions and errors. Barik's classification is very fine-grained. I decided to adopt only three of the categories identified by the author, as lower-level errors may have been due to other factors and may have been exposed to a higher degree of subjectivity in their identification, especially since I did not involve a second rater at this stage. I annotated the renditions according to the following elements:

\begin{itemize}
    \item substantial phrasing change (category E4 in \citealt{barik_description_1971});
    \item gross phrasing change (category E5 in \citealt{barik_description_1971});
    \item complete sentence omission (labelled M5).
\end{itemize}
Analysing partial omissions would certainly be interesting, but it would pose the methodological issue of defining units of information and of excluding the strategic use of omissions, which is difficult to evaluate post-hoc without involving the participants directly with a retrospective interview. Given the length of the experiment per participant (around 1.5 hours including the pre- and post-tests), I decided to reserve this analysis for future studies.

The renditions that did not present any issue or only minor issues (skipping omissions and mild phrasing changes) were grouped in a single category to streamline analysis. From this initial analysis, no clear trend could be identified. This is probably due to the variable number of terms searched, which made it difficult to compare performance between subjects.
\subsection{Discussion of pilot study results} \label{discussion_PS}
The pilot study overall validated the a-priori categorisation of the stimuli chosen for data collection. However, it highlighted a limitation of the initial design chosen for the experiment: given that only half of the stimuli introduced in the text were classified as requiring a glossary query due to their low frequency (i.e. 18 per condition, a total of 54), if participants only looked up a very limited number of terms, the data points collected for those participants would be too scarce to allow for a comparison with the rest of the sample. In order to overcome this limitation, I decided to replace the 54 stimuli classified as not requiring a query with less frequent, more specialised terms. This involved rewriting a considerable part of the speeches, but was nonetheless deemed necessary to ensure a more comparable search behaviour among participants.

As for the small effects observed for the stimuli position (see \sectref{position} for potential position effects on cognitive processing), it could be argued that the difference in search behaviour observed might be due to the terms chosen (e.g., cognate status, see also \sectref{material_MS}) rather than only to their position, even though the terms were equally distributed within the speeches (half were sentence-medial and half sentence-final). This is possible especially if one considers that only half of the terms in each position had been classified as requiring a query. This aspect could be further tested by switching the position of the stimuli used or by choosing a different set of terms. However, this would require a complete rewriting of the speeches, as in the present study the terms are not presented in isolation. Replacing the 54 stimuli classified as not requiring glossary look-up was expected to counterbalance this random effect.

Finally, while at the time of the pilot study ASR was still a hypothetical improvement of CAI tools, in the following months the first prototypes of ASR-CAI integrations started to appear (\citealt{fantinuoli_computer-assisted_2017}, see \sectref{CAI_overview}). Word and Excel glossaries may be considered both as examples of ``traditional'' digital ways to organise and consult terminology for interpreters. Furthermore, they may be accidentally modified by participants during data collection, an unforeseen issue which occurred several times during the pilot study. For this reason, in the main experiment, I decided to prepare a tabular glossary and present it as a PDF file to overcome these limitations. I also added a mock-up of an ASR-CAI hybrid to include the most recent development in CAI technology, which I hypothesised to be the most cost-effective in terms of additional CL and degree of accuracy achieved in the interpretation (see \sectref{hypotheses}).

To sum up, the pilot study represented a valuable step in the study design. The methodology developed and tested in the study is a novelty in the area of CAI research, especially in terms of the tools compared in the study, and of the rigour in the design of the materials used for data collection.

The pilot study was the first to specifically address solely digital support tools for terminology. Previous studies had either compared paper and digital glossaries (e.g. \citealt{biagini_glossario_2015}) or focused on a single CAI tool explored through be\-tween-group designs (e.g. \citealt{prandi_uso_2015,prandi_use_2015,gacek_softwarelosungen_2015}). The pilot study explored three different terminology support solutions, i.e. Word glossaries, Excel glossaries and CAI tools. The study also helped shed first light on the hypotheses guiding the present research work and refine the hypotheses, for instance by highlighting potential commonalities between Word and Excel glossaries, which informed the choice of the conditions to compare in the main study as discussed in \sectref{main_study}.

The empirical validation of the methodology laid first ground for the collection of process and product data under controlled experimental conditions. The main methodological gain for the field consists in the development and validation of a new method of speech design (based on \citealt{seeber_cognitive_2012}). The level of control and the number of variables considered in the development of the speeches is unprecedented, as previous studies had worked with naturalistic materials without controlling for potential variables affecting the outcome. The usefulness of the methodology proposed in \citet{prandi_designing_2017,prandi_exploratory_2018} has been recognised by a number of recent studies, which have drawn on the methods presented in this section. For instance, a recent study by \citet{van_cauwenberghe_etude_2020} on ASR support for terminology explored the role of morphological complexity for the accuracy achieved by his participants. He also derived several aspects of his evaluation framework from the categories used in the pilot study. Investigating number renditions in ASR-supported SI, \citet{frittella_cai-supported_2021} based the design of her speeches on the principles presented here.

Overall, the pilot study confirmed the validity of the design choices for the scope of the present research work, while at the same time highlighting weak spots in the design and supporting the optimisation of the methodology for the main experiment.


\section{Main study: Rationale and method} \label{main_study}
This section describes the methodology and results of the main experiment conducted to compare the process and performance of SI with the support of traditional digital glossaries, CAI tools and CAI tools with integrated ASR.
\subsection{Participants} \label{sample_main}
In this section, I describe the recruitment process for the experiment, highlighting the challenges encountered and how they motivated my choice of research design. I further provide information on the demographics of the sample on which the experiment was conducted.
\subsubsection{Participants recruitment} \label{recruitment_MS}
Like in the pilot study, participants were recruited among second-year students of the Master's Degree in Interpreting of the University of Mainz/Germersheim with German as their native language and English either as their B or their C language. This language combination was chosen because it is the most represented in the master's course and I thus hoped to reach a larger number of students to recruit for the experiment.

An e-mail was sent to the teaching staff of the English department, whom I asked to circulate the call among advanced interpreting students. I also asked for the collaboration of the student body representatives in circulating the e-mail and a leaflet containing the basic information about the experiment. Participants were also recruited through the University's Facebook groups, where the same information was posted in digital format. After a first round of e-mails sent during the Winter semester 2018/2019, which led to a few expressions of interest, a second round of e-mails was sent at the end of the Summer Semester 2019. Additionally, a recruitment e-mail was also sent to the University of Heidelberg, where I held a presentation of the study design and encouraged the students to take part in the study.

Participants were remunerated (\euro 50) and received a three-month free InterpretBank license. In addition, they had the opportunity to attend a free course on a very relevant and topical subject not usually offered during regular instruction. Despite the considerable recruiting efforts, only 15 students expressed their interest in taking part in the study. Of the initial 15 students recruited, it was possible to collect data from nine students. Data by two students had to be discarded due to technical issues which emerged during data collection, one student could not be calibrated due to an eyesight condition, two resigned after having confirmed participation and one could not be tested due to the university facilities being closed in the wake of the COVID-19 pandemic.

The small size of the sample recruited highlights a certain lack of interest for the subject, probably due to a general lack of awareness of the existence of CAI tools and the increasing relevance they are gaining in the interpreting profession (see also \citealt{prandi_cai_2020}). Another aspect that certainly had a negative impact on participant recruitment was time. Due to the nature of my investigation, which presupposes a sufficient level of expertise in tool usage, training was necessary prior to data collection. The training required the availability of around one hour of time per training session, and it is possible that second-year students may not have had enough time in their busy schedule due to exam preparation. A similar experiment conducted recently on the simultaneous interpreting of numbers with ASR support \citep{defrancq_automatic_2020} did not foresee practice sessions prior to data collection, but it could be argued that for ASR, the interaction with the tool mainly consists in getting used to seeing terminological or, in \citegen{defrancq_automatic_2020} study, numerical suggestions on the screen while interpreting. The interaction with a PDF glossary or a CAI tool certainly requires more extensive training, the benefits of which had been identified in previous work on the topic \citep{prandi_uso_2015, prandi_use_2015}. Structured training on all three conditions was therefore considered essential to ensure that equal practice time be devoted to all three tools, a variable which could affect results.

\begin{sloppypar}
Based on the considerations on small sample sizes outlined in \sectref{approach}, the speeches used for data collection contained a large number of stimuli, allowing numerous observations per participant. For each condition, participants were presented with 36 stimuli. The highest number of data points collected is for the ASR condition (36 for each participant, because the terms were shown automatically on the screen). For the two conditions which required an active search in the glossary (PDF and InterpretBank), the minimum number of data points per participant was 22.
\subsubsection{Characteristics of the participants recruited} \label{sample_characteristics}
The final sample was made up of nine advanced interpreting students. Working with students was a choice of practical nature, since they had to be trained on the tools before the experiment and the equipment and facilities for data collection were available at the university (see \sectref{design_MS}). Additionally, having all been trained at the same institution and having received comparable amounts of training in SI, they may be considered a relatively homogeneous sample. The challenges in recruiting representative samples of professionals are well-known in TIS (e.g. \citealt{hansen-schirra_translation_2020}). The main issue in working with students is that they ``exhibit different cognitive processes and behavioural patterns than professionals, which might in turn affect the generalizability of the results'' (ibid.). For this reason, care should be taken in generalising the results of the present study to the population of professional conference interpreters.
\end{sloppypar}

As will be described in more detail in \sectref{design_MS}, before collecting gaze data, I had my participants go through a series of tests to gain a more detailed picture of my sample. I measured their typing speed and the size of their English vocabulary (\sectref{pre-test}). I also asked them a series of questions to gauge their knowledge of the subjects of the speeches used during the experiment (\sectref{speechtop_MS}).

With a view to provide a more nuanced picture of the sample at hand and identify additional confounding variables which may play a role in the interpretation of the experimental results, at the end of data collection I asked the participants to fill out the Translation and Interpreting Competence Questionnaire (TICQ) developed by \citet{schaeffer_translation_2020}. The main benefits offered by the tool consist in its easy customisation, its availability in four languages (English, German, Spanish and Chinese) and the fact that it can be administered online. As a quantitatively valid tool, its use has been advocated to promote greater comparability in T\&I research \citep[102]{schaeffer_translation_2020}. Only module A (demographics) and module C (interpreting competence) were administered in the online format. While compiling the questionnaire, some students informed me that they had either misinterpreted some questions or that they were uncertain on how the questions could be applied to their status of interpreting students.\footnote{For instance, the questions in section C2 concerning their professional experience.} This resulted in missing data for some questions and in a mismatch between the answers to several questions for the same test subject. Competence was not explicitly operationalised as a variable in the present study, and I was later informed that the interpreting competence score was still under development.\footnote{Schaeffer, personal communication (2021).} For these reasons, I do not report the total score here, but rather extrapolate from the questionnaire the answers which are related to my research questions and which may provide further insight into the make-up of my sample. These answers were also expected to provide useful additional qualitative data to interpret findings on the participants' interaction with the tools in the booth. I report relevant results in the sections below and then discuss the results of the pre-tests.

\subsubsubsection{Demographics} \label{demog_MS}
The nine students recruited for the study were attending the Master's Degree in Conference Interpreting or the double Master's in Translation and Conference Interpreting at the time of the experiment. They had all received at least three semesters of instruction in simultaneous and consecutive interpreting. Seven were females, two were males (age M = 27.33 years, Mdn = 25). They were all native speakers of German; two of them were bilingual (with Arabic and Polish).

\subsubsubsection{Language proficiency} \label{lang_prof_MS}
In Module A3, the TICQ asks survey respondents to provide a self-rating of their knowledge of their L2 and L3 languages. In my sample, a small drawback consisted in the fact that two participants had indicated English as their L3, rather than as their L2. In the self-assessment question, no distinction is made between active and passive knowledge for L2, but only for L3. Therefore, if English had been selected as the participants' L3, the average rating was used. The mean self-reported value was 79.17 (on a scale from 0 to 100, $\text{SD}=7.71$).

\begin{figure}[H]
%\includegraphics[width=0.8\linewidth]{images/EN_knowledge.png}
%\centering
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 100,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Self-reported knowledge of English (\%)},
%  nodes near coords,
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,80)
  (2,80)
  (3,80)
  (4,67.5)
  (5,75)
  (6,90)
  (7,70)
  (8,90)
  (9,80)
  };
  \end{axis}
\end{tikzpicture}
\caption[Bar plot of the self-reported knowledge of English]{Bar plot of the self-reported knowledge of English (\%). $M=79.17$, $\text{SD}=7.71$}
\label{fig:EN_knowledge}
\end{figure}
%\todo{need data for figure}

\subsubsubsection{Interpreting experience} \label{experience_MS}
No students identified themselves as conference interpreters, although some of them reported having some professional experience.

Their estimated décalage (Section C3.2 of the TICQ) was comprised between 2 and 3s ($M=2.61, \text{SD}=0.48$). A question present in the TICQ particularly relevant to my research desiderata concerned the strategies adopted to deal with an unknown term while interpreting. The preferred strategies was ``Ask my partner'', i.e. seeking help from the boothmate, followed by ``Stalling'' (Figure \ref{fig:preferredstrategy}).

\begin{figure}
%\includegraphics[width=0.8\linewidth]{images/2_Preferred_strategy.png}
\begin{tikzpicture}
      \begin{axis}[
          ybar,
          ylabel=\%,
          xtick=data,
          axis lines*=left,
          width=.9\textwidth,
          height=5cm,
          ymin=0,
          ymax=100,
          xtick={1,2,3},
          xticklabels={Ask my partner,Look up in resource,Stalling},
          nodes near coords,
          bar width=1cm,
          enlarge x limits={abs={1.5cm}}
          ]
          \addplot+[lsDarkBlue] coordinates {(1,56) (2,11) (3,33)};
      \end{axis}
\end{tikzpicture}
\caption[Preferred strategy for unknown terms during SI]{Preferred strategy to deal with unknown terms during interpretation}
\label{fig:preferredstrategy}
\end{figure}

Only one participant indicated that they look up the term in their resources when they cannot recall it from memory, or they do not know its equivalent, which I found quite interesting considering the intensive training all students had attended prior to participating in the experiment and answering the TICQ. This might indicate that most of them prefer other, less demanding coping strategies in terms of attentional resources than performing a query in a digital glossary, which is probably considered an emergency tactic by most students in my sample. At the same time, the strategy of using an ASR-CAI hybrid to receive real-time terminology suggestions was not an available option for this question, which may have otherwise yielded different results.

\subsubsubsection{Typing speed and English vocabulary size} \label{pre-test}
The participants' typing speed was measured ahead of the experiment (in words per minute, wpm). In addition, I assessed the size of their English vocabulary using the online vocabulary test WordORnot \citep{CRR_WON_2014}. The test takes around four minutes and requires the participant to judge whether the words shown on the screen are actual English words or non-words. The final test score is an estimate of the participant's English vocabulary size. The results are illustrated in the graphs below (Figures \ref{fig:typingspeed} and \ref{fig:ENvocab}).

\begin{figure}
\caption[Participants' typing speed]{Typing speed of participants measured during the pre-test (wpm)}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 100,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {Typing speed (wpm)},
%  nodes near coords,
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,63)
  (2,50)
  (3,63)
  (4,87)
  (5,53)
  (6,65)
  (7,69)
  (8,52)
  (9,80)
  };
  \end{axis}
\end{tikzpicture}
\label{fig:typingspeed}
\end{figure}
%\todo{need data for figure}

\begin{figure}
%\includegraphics[width=0.8\linewidth]{images/5_EN_vocab.png}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 0,
  ymax = 100,
  xtick = {1,2,...,9},
  xticklabels = {P1,P2,P3,P4,P5,P6,P7,P8,P9},
%  x tick label style = {font=\small},
  xlabel = {Participant},
  ylabel = {English vocabulary (\%)},
%  nodes near coords,
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,37)
  (2,53)
  (3,61)
  (4,57)
  (5,60)
  (6,64)
  (7,53)
  (8,64)
  (9,63)
  };
  \end{axis}
\end{tikzpicture}
\caption[Participants' size of English vocabulary]{Estimated size of English vocabulary per participant (\%)}
\label{fig:ENvocab}
\end{figure}
%\todo{need data for figure}

The participants' typing speed and the size of their English vocabulary was tested as these factors may have an impact on their search behaviour, in particular on the amount of terms looked up and on the amount of terms that they were able to find in the glossary. It should be noted that a correlation between these two factors and the search behaviour may not be straightforward, as a number of scenarios may arise: a fast typer may search a low percentage of terms if he or she has a wide English vocabulary, but the opposite may also be true, because he or she may choose to only look up the terms considered strictly necessary and to adopt alternative strategies, such as paraphrasing, which should be easier to adopt if one has high language flexibility. Nonetheless, I preferred to be on the safe side in order to consider also these aspects, if necessary.

\subsubsubsection{Knowledge of speech topics} \label{speechtop_MS}
In addition to these data, participants had to answer a few questions which served a double purpose: to ascertain the participants' background knowledge of the topic of the speeches to interpret, which were different from the topics chosen for the training, and to provide the participants with at least a general overview of the speech topics. Not knowing the subject of the speeches may have caused additional stress ahead of the interpreting task. I first asked the participants to rate their knowledge of the key topics discussed in the speeches. As can be seen in Figure \ref{fig:knowledgetopics}, no topic was completely unknown to the students, and moderate differences could be found between the students' knowledge about the six topics.

\begin{figure}
%\includegraphics[width=0.8\linewidth]{images/6_topics.png}
\begin{tikzpicture}
  \begin{axis}[
  ybar,
  axis lines*=left,
  width = \textwidth,
  height=6cm,
  ymin = 1,
  ymax = 5,
  xtick = {1,2,...,6},
  xticklabels = {Renewable\\energy,
  Fossil\\fuels,
  Coal\\mining,
  Solar\\energy,
  Wind\\energy,
  Nuclear\\energy
  },
  bar width=20pt,
  x tick label style = {align=center},
%  xlabel = {Participant},
  ylabel = {Knowledge (self-rating 1--5)},
%  nodes near coords,
  ]
  \addplot+[lsDarkBlue]
  coordinates {
  (1,2.89)
  (2,2.44)
  (3,2.00)
  (4,2.67)
  (5,2.67)
  (6,2.44)
  };
  \end{axis}
\end{tikzpicture}
\caption[Participants' knowledge of key speech topics]{Self-ratings of knowledge of the main topics included in the speeches. Mean values for the sample on a scale from 1 (no knowledge) to 5 (expert knowledge)}
\label{fig:knowledgetopics}
\end{figure}
%\todo{need data for figure}

However, overall, none of the students considered themselves experts in the topic presented. Therefore, I could reasonably expect the students to look up a considerable number of terms during the experiment. Additionally, I verified the participants' terminological preparation on the topics in question. To this aim, I asked the students to write down three terms (with their German translation) which they associated with each of the six speech topics. I then checked which terms had been included and whether the students had indicated any term that was also present as a stimulus in the speeches. Of all the terms added by the participants, only four were similar or closely related to the stimuli selected. Of course, I cannot exclude that, just because a certain term was not mentioned during the briefing task, it was unknown to the student. Nonetheless, this task in the pre-test proved useful to verify which terms would first come to mind when thinking of a certain topic.
\subsection{Participants' training} \label{training_MS}
As in the pilot study, participants were required to attend a training course prior to data collection. The structure of the training was the same as during the pilot study, with the only difference that it was held online as a self-paced course. To this aim, I created an online course on the University's Moodle platform to which the study participants were enrolled after confirming their participation in the study. The introductory meeting was offered as a webinar that the students could attend live or watch later as a recording.

The online format of the training offered a double advantage: the participants could re-watch the recording as many times as necessary and they could more easily integrate the training in their schedules, with the additional option of repeating practice sessions if they needed additional training. I had hoped that a more flexible and customisable training schedule would encourage participation in the study. Unfortunately, as described above, this was not the case.

In terms of the training content, the only modification introduced was the substitution of the Word and Excel glossaries with a PDF glossary, and the introduction of the ASR-CAI mock-up to account for a third generation of CAI tools which participants would likely be able to work with in their foreseeable future careers. The condition that equal practice time be ensured for each tool was also maintained for the main experiment. I made sure that the students had actually completed the five practice sessions by asking them to upload the recordings of their interpretations online. The students were able to proceed to the following training session only after completing the previous one. After the training had been completed, the participants were asked to take a short quiz to demonstrate their proficiency in the use of the tools. The questions were formulated in order to verify that the students had acquainted themselves with the user interface and were able to correctly operate the PDF tool and InterpretBank. All students passed the test and could hence take part in the data collection.
\subsection{Materials} \label{material_MS}
The speeches that the participants were asked to interpret during data collection were very similar to the ones used during the pilot study, with a few alterations. The speeches are reported in the Appendix.

First, as discussed in \sectref{discussion_PS}, the 54 stimuli which had previously been classified as not requiring a query were replaced with less frequent, more specialised terms. This required an almost complete rewriting of the speeches. \tabref{tab:termsdistribution} shows the distribution of terms in the final speeches.

\begin{table}
\begin{tabular}{llccc}
    \lsptoprule
    {Total} & \multicolumn{2}{c}{Morph. complexity} & \multicolumn{2}{c}{{Position}}\\\midrule
   \multirow{6}{*}[-10pt]{36} & \multirow{2}{*}{Unigrams} & \multirow{2}{*}{12} & End & 6\\\cmidrule{4-5}
   & & & Middle & 6\\   \cmidrule{2-5}
   & \multirow{2}{*}{Bigrams} & \multirow{2}{*}{12} & End & 6\\   \cmidrule{4-5}
   & & & Middle & 6\\  \cmidrule{2-5}
   & \multirow{2}{*}{Trigrams} & \multirow{2}{*}{12} & End & 6\\   \cmidrule{4-5}
   & & & Middle & 6\\
   \lspbottomrule
    \end{tabular}
\caption[Distribution of stimulus terms in the main experiment]{Distribution of terms for each speech in the main experiment. The morphological complexity and position of terms were equally distributed within the speeches.\label{tab:termsdistribution}}
\end{table}

It should be noted that the terms selected as stimuli do not represent all do\-main-rel\-e\-vant terms which could be of interest to interpreters. A domain-specific but highly frequent word could also elicit a glossary query. Ultimately, the terms were selected with the aim to generate sufficient data points for each participant.

As shown in Table \ref{tab:termsdistribution}, the ratio for each category (position and morphological complexity) was maintained also in the main experiment. All proper names and figures were removed from the text, as they could constitute further sources of difficulty in addition to specialised terminology \citep{gile_regards_1995, gile_basic_2009}, which was in focus in the present study. Since the stimuli selected all had a very low frequency, they were expected to require a query in the glossary or a glance at the ASR suggestions in most cases. However, since the corpus of texts selected for the extraction of the stimuli was highly specialised, cognate facilitation effects may have occurred \citep{costa2000cognate}, for instance due to the prevalence of Latinisms, which are frequent in specialised discourse (see \sectref{LSP}). Since precedence was given to providing continuum by drafting entire speeches, it was not feasible to replace all cognate stimuli with non-cognates. Therefore, to control as much as possible for cognate facilitation effects, when a non-cognate translation was available, it was selected as target language equivalent for the glossary.

In addition, the cognate status of the stimuli across the three speeches used during the experiment was checked. To calculate the cognate status of the selected stimuli, I calculated the normalised Levenshtein ratio as:
\[\text{Levenshtein ratio}=1 - \frac{\text{Levenshtein distance}}{\max(\text{length source}, \text{length target})}\]

The Levenshtein distance \citep{levenshtein_binary_1966}, i.e. the smallest number of deletions, additions and substitutions required to transform the source into the target string, is divided by the maximum length of the strings compared to account for word length. The formula above yields a score comprised between 1 (perfect cognate) and 0 (no overlap). A higher score indicates that fewer deletions, additions or transpositions are required to obtain the target term \citep{schepens_distributions_2012}.

The ANOVA of the normalised Levenshtein scores for the stimuli of each text was found to be non significant ($F(2,105)=0.243$, $p=0.785$). On this basis, I did not expect cognate facilitation effects to significantly affect the participants' interaction with the support tools.

The speeches were recorded once again by the same speaker as in the pilot study. The structure made up of sentence clusters comprising an introductory, a target (T) sentence and a continuation (C) sentence was maintained. However, in order to facilitate the analysis and isolate the effects of each stimulus on décalage and CL, I introduced a 5s pause between each sentence cluster and the next. In literature, the average EVS for professional conference interpreters has been identified as comprised between 2s and 6s (e.g. \citealt{barik_simultaneous_1973,lederer_simultaneous_1978,oleron_research_2002,christoffels_components_2004,alvstad_time_2011,defrancq_corpus-based_2015}), so a 5s pause was deemed sufficient. The average length of the speeches used in the main experiment was 14'39'', with an average WPM of 104.3 (164.4 average syllables per minute). As shown in Table \ref{tab:features_speech}, although the content of the speeches varied, they were highly comparable in terms of length and speed.

To further verify whether the speeches were comparable, I analysed them using Coh-Metrix \citep{mcnamara_automated_2014}. Coh-Metrix automatically computes a series of indices of text cohesion and coherence which provide an estimation of text difficulty and can be used both for written and for oral texts. The Flesch Reading Ease Index \citep{flesch_new_1948}, a common index of text readability, and the SYNLE index, a reliable index of WM load \citep[70]{mcnamara_automated_2014}, were considered as indices of difficulty. The texts were highly comparable also according to these two indices. Therefore, the speech difficulty was not expected to affect performance.


\begin{table}
 \begin{tabular}{lc c c c c}
 \lsptoprule
 {Speech} & {Duration} & {wpm} & {spm} & {Flesch Reading Ease} & {SYNLE}\\
 \midrule
 A & 14'34'' & 105 & 165.8 & 59.92 & 3.57\\
 B & 14'46'' & 104 & 162.5 & 62.47 & 3.41\\
 C & 14'37'' & 104 & 164.9 & 61.35 & 3.71\\
 \lspbottomrule
 \end{tabular}
\caption[Features of the speeches used in the main experiment]{Features of the speeches used in the main experiment. For each speech, the table reports the duration, the number of words per minute (wpm) and of syllables for minute (spm), the Flesch Reading Ease score and the SYNLE score (left embeddedness, mean).\label{tab:features_speech}}
\end{table}


While interpreting, the students could see both the video of the speaker and the tool with the glossary on a split screen. The video was placed on the left hand-side, the glossary on the right-hand side of the monitor. I decided to also include the video of the speaker to reproduce a typical configuration of an SI task, where the speaker is usually visible (see \citealt{seeber_multimodal_2017}). Additionally, visualising the speaker video and the tool on screen may be compared to a typical RSI configuration (see \sectref{tech_mediation}), a setting in which the use of CAI tools, particularly with ASR integration, may be envisaged in the near future. The speaker video was also included for another purpose related to the hypotheses guiding the experiment: since the video was expected to keep the participants' attention on the screen, essentially serving as a fixation cross \citep{conklin_eye-tracking_2018}, it would support the collection of gaze data when the participants were not interacting with the tools. This data proved useful in investigating effects on attention allocation during CASI (see \sectref{fixtimedisc}).


\subsection{Apparatus used for data collection} \label{apparatus_MS}
Gaze data was collected with an SMI red250 remote eyetracker mounted on an external desktop monitor. No chin rest or head support was used, although it would have improved data quality \citep[83]{holmqvist_eye_2011}, because it would have constricted the participants' head movements during speech and limited their ability to look at the keyboard while typing. The use of a head-mounted tracker and of a chin rest would have promoted more precision in the collection of gaze data \citep[17]{conklin_eye-tracking_2018}, but it would have limited movement. The increased intrusiveness perceived may have caused greater stress. As pointed out by \citet[206]{hvelplund_eye_2014}, ``the [interpreter]'s eye movements may thus well be related to stress from not being able to look at what key is being pressed in addition to actual problem-solving activities arising from the [interpretation] itself''.


The students' interpretations were recorded in Audacity with a microphone placed on the table, next to the participant. At the beginning of each video, whether with or without the ASR-CAI mock-up, I added a set of instructions that the participants were required to follow during data collection. Participants were instructed to wait until they would hear a loud ``beep'' coming from the computer before putting on their headphones. This acoustic marker was added to the recording to facilitate the synchronisation of the original speech with the interpretations. It was loud enough to be picked up by the microphone used to record the participants' deliveries.
\subsection{Measures} \label{measures}
\chapref{chapter4} reviewed the main methods used to measure cognitive load and effort in translation and interpreting and highlighted how a combination of methods may represent a valuable experiment design choice to address the inherent limitations of each metric.

As introduced in \sectref{approach} (see Table \ref{tab:metrics_overview}), the present study combined performance, behavioural, and subjective measures. It appears trivial to say that the measures selected for the main experiment are not the only measures which may be used to explore CASI from a cognitive processing standpoint. Conducting a comprehensive evaluation of how all measures of CL apply to the research object in question goes beyond the scope of the present study. For instance, in the experiment only duration-based eyetracking measures were used, while future studies may adopt additional metrics such as the number of fixations. Similarly, the only subjective measure used was a short qualitative questionnaire, but future research may, for instance, also test the use of cued retrospection to gain further insight into the CAI process. The present section provides the rationale for the selected metrics and describes how they were applied to the analysis of the data collected during the main experiment.

\subsubsection{Performance measures} \label{performancem}
The study adopted the two performance measures included in the pilot study: terminological accuracy and number of errors and omissions.

\subsubsubsection{Terminological accuracy}
Assessing the terminological accuracy of terms consulted in the glossary or automatically prompted by ASR has become a standard measure in CAI research focused on terminology (e.g. \citealt{biagini_glossario_2015,prandi_uso_2015,prandi_use_2015}), as one of the main claims of such applications is that their use may lead to greater target text (TT) accuracy.

To conduct the evaluation, I used the same three-level grading scale adopted in the pilot study. The three categories were defined as in the pilot study, i.e. a value of 2 would be assigned to ``close renditions'' (terms translated as per glossary or through a synonym), a value of 1 to acceptable renditions, and a value of 0 to wrong renditions or to an omission of the term which had not been replaced by a paraphrase. For the evaluation, the terms were presented to the raters in an Excel table. The terms were listed in isolation, not embedded in the respective utterance, in their order of appearance in the source speech and with their respective glossary equivalent. In order to facilitate the evaluation, adequate synonyms were listed (if available) next to the glossary equivalent. Next to the columns with the terms, the glossary equivalent and the synonyms, the participants' renditions were listed. Next to each participant's rendition, the raters could enter the accuracy scores. For this evaluation, the prosodic component was excluded (no audio recordings of the renditions were provided).

\subsubsubsection{Errors and omissions}
In analysing the quality of the deliveries, it is paramount to consider not only whether the stimulus term was correctly rendered, but also whether the glossary query occupied cognitive resources which could not be allocated to the processes of listening and processing of the adjacent units of information. It is theoretically possible that a correctly identified term co-occurs with contresens or the complete omission of target and/or continuation sentences, as discussed in \sectref{sentenceprocessing}. Therefore, in addition to the analysis of the terminological accuracy, I expanded the investigation to the sentence level.
Since the individual interpreting skills also play a role, I chose to conduct the analysis at a more superficial level, only checking the presence of severe errors and complete omissions for the terms queried (in the InterpretBank and PDF glossary) and not venturing into a detailed analysis of other lower-level issues such as partial omissions in the interpreted sentences. This approach would require controlling for the individual interpreting competence and disambiguating between strategic and non-strategic omissions, additions and generalisations. These categories would be difficult to operationalise without subjective methods such as retrospective interviews, which I chose not to conduct because it would have further extended the duration of the experiment.

In analysing errors and omissions, I only considered the target and continuation sentences for which a glossary query had occurred. The goal was to establish whether a glossary query may result in an error or omission in the target or continuation sentence, which would indicate an interference between the two tasks. Therefore, I considered how the use of different tools affected this variable in order to answer the question: are there fewer complete omissions and severe errors when terms are presented on the screen automatically? Finally, in line with my hypotheses (see \sectref{hypotheses}), I explored where the majority of errors and omissions occurred, whether in the target sentence or in the continuation sentence.


\subsubsection{Behavioural measures} \label{behaviouralm}
\begin{sloppypar}
Six behavioural measures were included in the main experiment: glossary queries, EVS, inter-cluster pause duration (ICPD), time to first fixation, average fixation duration and fixation time.
\end{sloppypar}

\subsubsubsection{Glossary queries}
In order to gain a comprehensive picture of the test subjects’ interactions with the tools during SI, I collected data pertaining to the queries performed in the PDF glossary and in InterpretBank. As will be discussed in \sectref{queries}, all participants fixated all stimuli terms on the screen, therefore the analysis focused on the tools with manual look-up.

This metric was included as its usefulness had been highlighted both by previous research \citep{prandi_uso_2015,prandi_use_2015,biagini_glossario_2015} and by the pilot study (see \sectref{discussion_PS}). The categories of terms analysed are reported in \sectref{queries}.

\subsubsubsection{EVS}
A widely-used indicator of CL in interpreting is the EVS, as discussed in \sectref{EVS_EKS}. In the main experiment, I expected significant differences between tools in the EVS (see \sectref{hypotheses}). This may point to a greater ease of use of the ASR-CAI mock-up compared to the CAI tool and the PDF glossary, and of the CAI tool compared to the PDF glossary.

In the main experiment, I calculated the EVS for the terms searched by the participants and compared it for the three tools, both for each participant and for the whole sample. The EVS was calculated as the begin time of stimulus translation minus the end time of stimulus utterance by the speaker. The availability of time stamps for individual words offered by the automatic transcription obtained with the Speechmatics tool (see \sectref{recordings}) allowed overcoming a methodological challenge identified by \citet[155]{alvstad_time_2011}, i.e. the need to manually identify the beginning of the stimulus. Since this the timestamp was available automatically, the EVS could be measured with millisecond precision.

\subsubsubsection{Inter-cluster pause duration (ICPD)}
A short EVS may also suggest overall faster processing, but it does not measure it directly. Hence, I also analysed the duration of pauses between sentence clusters, i.e. between the last word of the continuation sentence pronounced by the speaker and the last word of the delivery pronounced by the participant for each sentence cluster. This may be considered as an additional measure of time lag and is geared towards a general analysis of how the speed of processing is influenced by the explanatory value of the tool used as support.

Since the speeches used in the main experiment presented a 5s pause between two sentence clusters (see \sectref{material_MS}), the ICPD may be used as an additional indicator of cognitive effort. More specifically, a long ICPD (of 5s or longer) may be interpreted as an indicator of fast processing.

\subsubsubsection{Time to first fixation}
The time to first fixation on the term AOI was expected to provide insight into the question of how the tool use affects the participants' speed in identifying the target term on the screen.
SMI BeGaze provides the metrics ``Time to first appearance'' and ``Entry time'' for each AOI. The metrics, in ms, represent respectively the moment in which the AOI first becomes visible on the monitor and the start time of the first fixation to enter the AOI. For each participant and under each condition, I therefore calculated the mean time to first fixation for the term AOIs by subtracting the timestamp for ``Time to first appearance'' from the timestamp for ``Entry time''.

\subsubsubsection{Average fixation duration on term and tool AOIs}
Fixation duration has been traditionally used as an indicator of cognitive effort in the processing of the ST (see \sectref{fixations}). In the present study, the focus was on the effort exerted by participants while interacting with the tools. The results therefore do not indicate how easy it is to interpret the target terms in different conditions but should rather be understood as an indication of the effort of human-machine interaction.

In each condition and for each participant, I calculated the average fixation duration on the term AOIs. Additionally, I also considered the duration of the fixations on the tool area.

\subsubsubsection{Fixation time}
In addition to the average fixation durations on the tool area, I also considered the total time spent fixating the speaker and the tool area. SMI BeGaze provides the metric ``Fixation time'', i.e. the ``sum of the fixation durations inside the AOI'' \citep[370]{begaze_2017}. In the present study, the metric represents the amount of time spent processing the speaker video and the side of the screen where the tool was displayed. This metric was expected to provide information on how attention was allocated to the different sources of visual and visual-verbal information available to the participants, specifically on how much the tool distracted the participants from the speaker (see \sectref{hypotheses}).

\subsubsection{Subjective measure: The debriefing questionnaire} \label{subjectivem}
Only one subjective measure was included in the main experiment: a short post-hoc qualitative questionnaire. The questionnaire was kept short to avoid extending the duration of the already relatively long experiment. The inclusion of the debriefing questionnaire was expected to provide additional data to help frame the quantitative analysis and to highlight phenomena not emerging from the quantitative analysis conducted with the above-mentioned performance and behavioural measures.

In the questionnaire, the participants were first asked to rank the speeches from the easiest to the most difficult. This was done to check for potential effects due to the nature of the speeches interpreted and not to the other variables controlled in the experiment. Additionally, the subjects were asked to rank the tools from the most useful to the least useful and from the most distracting to the least distracting. I also asked participants to include further details of which aspects of each tool they had found to be most useful and which most problematic. This was expected to help identify the reasons behind their preference. Finally, I explored their preferences as to the tool which they would bring with them into the booth during future assignments.

\subsubsection{A note on pupil size} \label{physiologicalm}
In previous publications reporting on the envisaged methodology for the present study \citet{prandi_designing_2017,prandi_exploratory_2018}, I had postulated the inclusion of the metric ``pupil size'' in addition to the measures discussed in the previous section and selected for the main experiment.

Pupil size, or pupil diameter, was used in previous studies investigating the interpreting process (see \sectref{pupillary_measures}), both as an indicator of global cognitive load generated by the task (e.g. \citealt{hyona_pupil_1995}) and to explore local variations in cognitive load as a response to specific features of the source speech \citep{seeber_cognitive_2012}.

As discussed in \sectref{pupillary_measures}, however, correlating variations in pupil size to variations in cognitive effort is not straightforward. There are a number of factors which may influence the pupillary responses to the stimuli presented. This represents an important limitation for the adoption of pupillometry as a method to explore cognitive load in interpreting.

The specific research object of the present investigation presents additional limitations to the ones discussed in \sectref{pupillary_measures}.

For instance, the way in which the terminological information was presented in the PDF glossary and in the CAI tool may have resulted in higher arousal in the participants than during the use of the ASR-CAI mock-up. In the PDF condition, the participants could see a large number of terms on the screen. In the CAI condition, the screen changed very quickly during the query. It is possible that participants may have been more visually stimulated by the interfaces in the PDF and the CAI condition due to the highly dynamic stimuli. Variations in pupil size would therefore reflect different levels of arousal rather than higher or lower cognitive load.

\begin{sloppypar}
Additionally, the ASR-CAI mock-up window was relatively empty: it was white most of the time apart from the moments in which the terms appeared. The higher screen luminance in this condition may have been expected to reduce the pupil size. This reduction in pupil size may therefore simply reflect a physiological reaction of the pupil to the brightness of the screen, not variations in cognitive effort. This is a common issue in human factors studies, for instance when participants interact with web pages \citep[530]{holmqvist_eye_2011}.
\end{sloppypar}

Finally, it should be noted that, in most studies on interpreting which adopted pupillometry to explore cognitive load (e.g. \citealt{hyona_pupil_1995,seeber_cognitive_2012}), the informants were not interacting with visual stimuli. The only exception is the study by \citet{gieshoff_impact_2018}. However, she was able to control for luminance across conditions and participants as the stimuli were not co-created by her participants, unlike in the present study for the two conditions which involve manual look-up.

For the limitations discussed above, I opted for a more cautious approach and chose not to include pupil size as a metric for the present study.
\subsection{Procedure} \label{design_MS}
The experimental design was very similar to the pilot study. I introduced several modifications and additional tests to round up and facilitate my subsequent data analysis. As described in \sectref{training_MS}, the main difference consisted in the replacement of the Word and Excel glossaries with a PDF glossary and the introduction of an ASR-CAI mock-up as the third condition. In the PDF glossary, the font size chosen was 16, as recommended in literature (\citealt[e.g.][261,]{obrien_eye_2009} \citealt[20]{hvelplund_eye_2014}, \citealt[37]{conklin_eye-tracking_2018}). A large font size was also chosen for the InterpretBank glossary and for the ASR-CAI mock-up (see Figure \ref{fig:ASR_tool}).


The ASR-CAI mock-up was prepared following a previous study on the automatic speech recognition of numbers by \citet{desmet_simultaneous_2018}, who had prepared a PowerPoint presentation containing all the numerals mentioned during the speech. I prepared a version of the recorded speech positioned on the left-hand side of the screen, as for the other conditions, while on the right-hand side of the screen, on a white background, I added the terms and their glossary equivalents shortly after they had been pronounced by the speaker.
\begin{figure}
% \subfloat{\includegraphics[width=0.66\linewidth]{images/5_7_a_PDF.png}}\\
% \subfloat{\includegraphics[width=0.66\linewidth]{images/5_7_b_CAI.png}}\\
% \subfloat{\includegraphics[width=0.66\linewidth]{images/5_7_c_ASR.png}}%
\subfloat[PDF]{
\includegraphics[width=0.8\linewidth]{images/5_7_a_PDF.png}
}\\
\subfloat[CAI]{
\includegraphics[width=0.8\linewidth]{images/5_7_b_CAI.png}
}\\
\subfloat[simASR]{
\includegraphics[width=0.8\linewidth]{images/5_7_c_ASR.png}
}\\
\caption[Experimental conditions (PDF, CAI and simASR)]{Experimental conditions (PDF, CAI and simASR). In all conditions, the speaker was shown on the left hand-side of the screen, while the terminology was presented on the right. AOIs were placed on the speaker video, the tool area, and on each stimulus term.}
\label{fig:ASR_tool}
\end{figure}

This made it possible to simulate a constant system latency (ca. 1s) and ensure synchronisation between the speaker video and the ASR-CAI mock-up.

As discussed in \sectref{material_MS}, only the terms present in the glossary were shown on the screen to simulate the behaviour of the ASR module, which uses the underlying glossary to identify relevant items in the generated transcription. The target language equivalents are those present in the glossary and which had been previously selected and validated during glossary creation. This architecture corresponds to the current state of the art for CAI tools (see \sectref{IB} and \sectref{CAI_evaluation}).

The font and colour chosen were the same as in the only available prototype of this kind (InterpretBank with ASR). In the mock-up, only one term was shown at a time on the screen and it disappeared after 3000ms. I decided not to use the actual ASR integration in the CAI tool InterpretBank, but rather to simulate it, as I would be comparing two mature and stable systems (PDF glossary and InterpretBank) with a system still under development and which at the time of the experiment did not yet work optimally for terminology recognition. Mistakes in the use of the PDF glossary and InterpretBank would be due to suboptimal usage, not to system performance, unlike for the ASR-CAI hybrid. Additionally, using the real system could have resulted in different levels of performance, making it difficult to compare participants' data for this condition. While it would have been possible to simulate system failures experimentally, for instance by testing the real tool on the speaker's video, following this approach would have introduced yet another variable into the experimental set-up and further complicated data analysis. As this exceeds my research questions, I decided not to explore this facet of interpreter-tool interaction in the present experiment. Thus, the results for the ASR condition represent the ceiling performance that participants may achieve when using this kind of support. To denote that the ASR-CAI mock-up represents the best performance obtainable with an ASR-enhanced CAI tool, this condition will henceforth be defined as ``simASR''.

On the day of the experiment, participants were briefed on the structure of the experiment and signed a consent form for the collection and use of their data. They were informed that they would be able to revoke their consent at any time and that their data would be made anonymous for analysis. Participants were remunerated with \euro 50 after the end of the experiment. Before interpreting, they went through a brief pre-test comprising a measurement of their typing speed and the size of their English vocabulary (see \sectref{pre-test}). Afterwards, they answered a series of questions to help them prepare for the interpreting task (see \sectref{speechtop_MS}).

After the preliminary tests were completed, participants were asked to sit in front of the monitor where the stimuli were going to be presented. In the Tra\&Co laboratory, a dedicated room is available for the collection of eyetracking data under stable light conditions. Participants were seated at a distance of around 60--65cm from the monitor. To keep the participants' position stable relative to the eyetracker and the monitor, a backpack was tied to the chair where they were sitting. They were asked to wear the backpack, which was adjusted in order not to restrict the participants' movements excessively, but to provide feedback if they shifted too close to the monitor. Nonetheless, a certain variation in their proximity to the screen was inevitable. Participants were seated on a chair without wheels to avoid them changing position during the experiment.

I explained what they were going to see on the monitor before the actual interpreting task would start and clarified any remaining doubts they may have on data collection. The microphone was placed on the table next to the participants to record their interpretation. Afterwards, the experiment was started.

The experiment was prepared in SMI Experiment Center \citep{SMIexpcenter}. Participants were calibrated at the beginning of the experiment. A nine-point calibration was performed at the beginning of the experiment and then repeated at the start of each new trial. During lengthy recording sessions, drift can occur, i.e. a gradual loss of synchronisation between the participant's gaze and the recorded position on screen (see \citealt[210]{hvelplund_eye_2014}), which warrants a re-calibration between trials. After the calibration, a dry screen recording was run during which I prepared the speech and the tool on the participant's monitor. Then, the second screen recording started, for which gaze data was recorded.

I asked the participants to start the video and to follow the instructions on the screen. They would hear a loud ``beep'' coming from the computer and would then put on their headphones. I then asked participants to place their headphones on the desk in front of them at the end of each trial. To avoid fatigue effects, which can affect results (see \citealt[400]{spinner_ecological_2013}, \citealt[18]{keating_experimental_2015}), I allowed participants to take a short break or drink a sip of water between trials. I then proceeded with the second and the third condition. In total, the eye-tracking part of the experiment lasted around one hour including breaks. \citet{conklin_eye-tracking_2018} recommend not to exceed a duration of one hour of time for eyetracking studies.

As in the pilot study, the order of the speeches and of the tools were counterbalanced using a randomised Latin square design to minimise order effects \citep[42--43]{conklin_eye-tracking_2018}. The impact of learning effects (e.g. \citealt[65]{lazar_research_2017}) was minimised by the training attended by participants for all three conditions (see \sectref{training_MS}).\largerpage

After the eyetracking section of the experiment was completed, I asked participants to answer some debriefing questions and sent them a link to fill out the TICQ questionnaire (see \sectref{sample_characteristics}).
\subsection{Data preparation} \label{data_prep}
In this section, I illustrate how the data was prepared and systematised ahead of the analysis, the results of which are reported in \sectref{results_MS}.

\subsubsection{Recordings} \label{recordings}
First, the audio recordings of the students' interpretations were cut in order to only contain the relevant material for the analysis. Due to the structure of the experiment prepared in SMI Experiment Center and in order to avoid accidental interference with the software during data collection, the recording had to be started in Audacity prior to the screen recording section of the experiment and therefore contained additional bits of audio. The trimming operation was not necessary for the original speeches, as they had been recorded prior to the experiment.

After trimming the recordings, the second step was aligning them with the original speech. In order to facilitate this step of the process, the original recording contained the auditory marker that was picked up also by the microphone used to record the students' renditions. After importing the two audio tracks into Audacity, the audio peak was identified in both audio tracks and used as a reference to align the source speeches and the renditions. In hindsight, a short and sharper sound would have been preferable, as it was sometimes difficult to identify the start of the peak, but the method proved nonetheless effective. Afterwards, the recordings of the students' interpretations were transcribed automatically using the commercial Speechmatics transcription service.\footnote{\url{https://www.speechmatics.com/} (Accessed 25.08.2020)} The transcripts thus generated were later corrected manually in order to remove any transcription error. As in the pilot study, the HIAT transcription conventions were followed \citep{HIAT}. Like other transcription systems, the Speechmatics ASR service provides the advantage of assigning time stamps to each word of the transcript, a feature which proved useful during data analysis. Even though the transcription was already available for the source speeches, they were nonetheless aligned using the dedicated feature also provided by the Speechmatics ASR service, in order to obtain time stamps for each word in the original speeches.

The third step consisted in importing the audio tracks and the relative transcriptions into the \citet{ELAN} transcription software. For each audio recording, a dedicated track was added containing the aligned transcription. An additional dependent track was added for the interpretations: it contained only the stimulus terms (source speeches) or their renditions by the test subjects. This facilitated the analysis, as it was possible to export each track separately, time stamps included. This was necessary to calculate the EVS between the terms pronounced by the speaker and the terms interpreted by the participants (see \sectref{EVS}).

To calculate the pause length between the sentence clusters (see \sectref{pause_duration}), the audio tracks and their transcriptions were imported into the transcription tool Partitur Editor \citep{Schmidtworner2014},\footnote{\url{www.exmaralda.org} (Accessed 2021-07-13)} which offers a feature to calculate pause length between two annotations automatically.


\subsubsection{Areas of interest} \label{AOI}
The other set of data to be prepared consisted in the eyetracking recordings. There were a total of 27 recordings, three for each of the nine participants, who worked in the three different conditions (PDF, CAI, simASR).

The choice of areas of interest (AOIs) was motivated by my hypotheses concerning attention allocation under the three conditions (see \sectref{hypotheses}). As I expected participants to be least visually (and cognitively) engaged in the simASR prompts and most focused on the glossary in the PDF condition, two large rectangle-shaped AOIs were placed on the main windows visualised by the participants: the speaker video and the tool window, to measure the time spent looking at the two areas of the screen. Additional AOIs were placed on the stimulus terms. For all AOIs, I used the hand-drawn method (see \citealt[1695]{hessels_area--interest_2016}).

At this stage of analysis, I chose a large AOI for the speaker video, although it would be interesting to conduct more fine-grained inquiries exploring how visual attention is shared, for instance, between the speaker's lip movements \citep{gieshoff_impact_2018,seubert_visuelle_2019} and gestures \citep{seeber_multimodal_2012} and the glossary, i.e. between different sources of visual input. The stimulus term AOIs extended over the terminological pair. This was done in first instance for a practical reason: drawing separate AOIs on the source term and on the target term would have doubled the number of AOIs per stimulus. With nine participants each potentially processing a total of 108 terms (36 per speech), this would have resulted in a total of 1944 AOIs, of which 1296 would have had to be hand-drawn anew due to each participant's idiosyncratic search behaviour. Moreover, for moving terms (i.e. in the PDF and CAI condition), the AOIs would have been dynamic, requiring manual adjustment for intervening frames. Having double as many AOIs would also have doubled the amount of data to be analysed. To keep data analysis feasible, I opted for a larger AOI comprising both the source and the target term.



SMI BeGaze requires AOIs to be drawn post-hoc in the AOI Editor. This was necessary also because each participant worked with a different combination of speech and tool and because the term AOIs were determined by the participants' search behaviour. Because I had to use the screen recording component in Experiment Center and each participant conducted different queries in the glossary, data could not be collected for all participants on the same stimuli. To overcome this limitation, the AOIs for the elements shared by all participants (speaker video and tools) were created as global AOIs. The AOIs placed on the stimulus terms had to be created as local AOIs (i.e., not shared across participants), but each term received a code to facilitate the comparison across participants. The recording started slightly before the moment in which the students started the video of the original speech and the first 30 seconds of the recording contained the instructions for the participants. Therefore, the areas of interest placed on the speaker video and on the tool were activated (made visible) as soon as the speaker appeared on screen by adding a key frame. The same operation was repeated at the end of the speech, when the video ended and a second key frame was added when the AOIs were deactivated. In this way, the data was only analysed for the time window in which the AOIs were visible on the screen.

The same principle was followed while placing the AOIs on the terms presented automatically on the screen in the simASR condition: the AOI was activated as soon as the term appeared on screen and deactivated when it disappeared. It was not possible to standardise this step completely across all participants as the speech/tool combination was randomised, but I was nonetheless able to apply the AOI definitions for each video and tool for subsequent participants and merely had to adjust the timing.

The most challenging step consisted in drawing the AOIs on the stimulus terms looked up by the participants during the PDF and CAI trials. The two conditions presented similar issues, although the CAI condition was particularly complex. In principle, the same procedure was followed as for the simASR condition: the AOIs were placed on the terms searched and found in the glossary, activated as soon as they appeared and deactivated when they were no longer visible on screen.

Due to the nature of the PDF glossary, which showed a large amount of terms on screen at the same time, when two or more terms were alphabetically close to each other, several terms were already visible on screen while the first query was being performed. I had initially planned to activate the AOIs for all terms as soon as they were present on screen and to deactivate them once they were no longer visible. However, this would have resulted in AOIs visible on the screen for a very long time. As they would not represent the current stimulus, they would confuse the analysis. This would have also been problematic from a practical standpoint. Since the AOIs on the terms had to be placed manually for each individual term and participant, and the screen view could change multiple times due to intermediate searches before the stimulus term was mentioned by the speaker, the procedure could have been highly prone to human error, as each video frame would have had to be checked for the presence of stimuli not yet mentioned by the speaker.

The issue was even more evident in the CAI tool condition. Due to the progressive search, in InterpretBank the screen view can change numerous times before the user has completed the query. Therefore, it would have been necessary to verify which terms were present on the screen at each intermediate view and to check whether they were yet to appear in the speech, requiring an incommensurate amount of time. These factors would have made the whole procedure unfeasible and difficult to reproduce in future studies.

For this reason, for the PDF and CAI conditions, I decide to activate one AOI at a time, according to the stimulus term currently eliciting a response by the participant. Despite this necessary simplification, it was nonetheless indispensable to adjust the position of the AOI for each key frame in which the term changed position on the screen, and to ensure that the correct term was being tracked during the whole typing burst until the query was completed and the term reached its final position on the screen. The use of dynamic AOIs in SMI BeGaze makes it possible to track moving objects on the screen, but nonetheless requires manual adjustments for objects not moving smoothly, which was the case in the present study. It should be noted that the AOIs thus placed on the recording only reflect the cases in which the term searched was found by the tool.

In the following chapter, I illustrate the results and discuss them in relation to my hypotheses.
