\chapter{Measuring cognitive load} \label{chapter4}





As pointed out in the literature, CL ``cannot be observed and measured directly'' \citep[648]{chen_construct_2017}. The reason for this is that CL is a multidimensional theoretical construct, the definition and nature of which are still debated both in the field of psychology, where it originated, and in Translation and Interpreting Studies (see \sectref{CL}). When aiming to estimate CL or to assess it empirically, researchers have therefore traditionally relied upon the observation and measurement of phenomena that arise as a consequence of high cognitive effort and reflect its variations \citep{paas_cognitive_2003, ehrensberger-dow_cognitive_2020}. Several methods and measures have been adopted to this aim, good overviews of which are offered by \citet{seeber_cognitive_2013}, based on \citet{paas_cognitive_2003}; \citet{chen_construct_2017}, who also presents measures of workload, a construct derived from human workload studies; and \citet{ehrensberger-dow_cognitive_2020}, who also report on innovative measures and thoroughly discuss their limitations. In this chapter, I review and discuss the methods and measures adopted in TPR for assessing CL as a key factor in the translation and interpreting process, in order to motivate and frame my methodological choices (\sectref{design_MS}).

Five main approaches may be identified in the measurement of CL or of the cognitive effort required by a particular task. The methods used to measure CL may be placed on a double continuum. On the one hand, methods may be divided according to the amount of intervention by the researcher. On the other hand, they may be classified by whether they require the active involvement of the experiment participant in providing cognitive-load related data, or whether this onus entirely falls on the researcher. This in turn reflects a more subjective versus a more objective type of measure. Due to inherent limitations of each type of measure, cognitive research often adopts a mixed-method paradigm, where several measures are combined ``in a complementary fashion to elucidate the phenomenon of cognitive effort from different angles'' \citep[222]{ehrensberger-dow_cognitive_2020}. For instance, performance measures may not provide reliable estimates of effort because a good or stable performance may mask the high mental effort invested to achieve this result. Combining this type of data with more direct measures of cognitive effort may unveil effects of a specific intervention on the load imposed by the task. As all methods present both advantages and potential drawbacks, as will be discussed in the following sections, a combination of methods represents a good way to maximise the potential of the individual methods while addressing their respective limitations (see \sectref{approach}).

\section{Theoretical approaches} \label{theoreticalm}
The first category of approaches for CL estimation comprises methods that do not require empirical experimentation and focus on the task, rather than on the task performer. They have been defined as analytical methods \citep[66]{paas_cognitive_2003} as they rely on a theoretical analysis of the task at hand conducted on the basis of models. Examples of this approach in interpreting research are Gile's Effort Models (\citeyear{gile_regards_1995, gile_basic_2009}) and Seeber's Cognitive Load Model of SI (\citeyear{seeber_thinking_2007, seeber_cognitive_2011}) and of SI with text \citep{seeber_multimodal_2017}. Analytical methods rely on the analysis of task characteristics and are thus detached from individual performance. They represent an a-priori estimation of the CL generated by a particular task \citep[647]{chen_construct_2017}.

They may be useful in producing hypotheses to be tested empirically, especially in order to contrast the relative load imposed by similar tasks  (see \citealt{seeber_cognitive_2011,seeber_cognitive_2012}). However, they may only provide an a-priori estimation of CL, not a direct measurement of cognitive or mental effort. Without experimental validation, they remain theoretical accounts of the postulated sources of CL for a task, and as a theory they are ``unable to take into account individual differences'' \citep[22]{seeber_cognitive_2013}.


\section{Subjective approaches}\label{subjective_approaches}
\largerpage
\begin{sloppypar}
Further on the continuum, subjective measures represent approaches which place the spotlight on the individual experience of the subject involved in the experiment. The adoption of subjective methods is based on the idea that ``in an attempt of in-depth understanding and in order to find causal relationships, it is necessary to ask the subject'' \citep[3]{carl_digital_2011}. While this offers the advantage of highlighting individual differences and identifying phenomena not accounted for in theoretical models, it is open to ``a possible contamination of data by memory and consciousness effects'' \citep[19]{seeber_cognitive_2013}. Subjective measures rely on the active involvement of the participant in providing a personal estimate of the effort experienced during task performance, mainly through introspection, post-hoc questionnaires, retrospective interviews or think-aloud protocols (TAP) (ibid.).
\end{sloppypar}





Subjective data are often reflected in unidimensional or multidimensional psychometric rating scales \citep[648]{chen_construct_2017}. The latter can prove particularly useful in identifying the individual factors that may contribute to variations in CL. An example is represented by the NASA Task Load Index (NASA-TLX, \citealt{hart_development_1988}), widely used in mental workload research. It requires rating on six subscales measuring mental, physical and temporal demands, performance, effort and frustration, which can be altered or integrated depending on the research design. \citet{chen_construct_2017} identifies the NASA-TLX as a potentially useful asset in research on simultaneous and consecutive interpreting. An adaptation of this rating scale was recently applied by \citet{gieshoff_impact_2018} in combination with pupillometry measures and by \citet{sun_measuring_2014} in a study on translation difficulty. An important caveat of this approach is that the rating should be conducted immediately after task completion and ``possibly be supplemented with cues (e.g., processed texts or recordings of the process) to facilitate recall'' \citep[224]{ehrensberger-dow_cognitive_2020}.

As suggested by \citet[275]{gilelei_translation_2020}, the inclusion of subjective methods in the research design, especially retrospection, may provide valuable clues for the interpretation of objective measures. A recent example is the adoption of retrospective questionnaires by \citet{chmiel_eye_2020} in a study on source language interference in sight translation and SI, which supported the research team in the interpretation of combined temporal measures and product analysis.

Subjective measures, however, present several disadvantages (see \citealt{ehrensberger-dow_cognitive_2020}). First, careful consideration is required in the choice of research devices to collect subjective data. In using questionnaires, one must consider that forced-choice questionnaires limit the subjects' reporting and might prime their answers, while open questions may be more difficult to process statistically. The issue of priming may also affect the outcomes of interviews due to the presence of the researcher, which may also reduce the participant's willingness to report experiences perceived as embarrassing, not noteworthy, or potentially undermining their perceived professionalism. This reluctance is an issue often encountered in studies involving professional interpreters as subjects. Retrospection must occur close to the experimental intervention to be effective, but this may significantly extend the experiment's duration and potentially result in fatigue effects. Finally, TAPs have the disadvantage of potentially slowing down the process due to the concurrent verbalisation and task performance \citep{jakobsen2003effects}. Additionally, only what is verbalised will become known to the researcher, while other important aspects may not emerge from studies only adopting subjective measures.

\section{Performance measures} \label{performance_measures}
Performance measures combine the advantage of analysing and comparing individual performances with the measurement of objective parameters. In cognitive psychology, they have traditionally been applied using primary task measures, i.e. by measuring task performance, or by requiring the concurrent performance of a second task and measuring how its performance is affected by variations in the CL generated by the first (secondary task measures). In order to provide reliable indications of CL, experiments conducted using this approach require a stringent empirical design in order to avoid effects due to variables not controlled by the researcher \citep[20]{seeber_cognitive_2013}. This may require sacrificing some ecological validity to avoid uncontrolled effects. When high ecological validity is to be maintained, secondary task measures may be unsuitable, as they interfere with the main task \citep[227]{ehrensberger-dow_cognitive_2020}. A downside of using performance measures is that they often involve human raters, which may affect the degree of objectivity of these measures and requires strictly defined evaluation criteria.

In studies on translation and interpreting, the two most common aspects of performance analysed are accuracy and disfluencies, especially pauses. This is based on the assumption that performance suffers from heightened or excessive cognitive effort and that issues of cognitive resource management may therefore emerge on the linguistic surface.


\subsection{Accuracy} \label{accuracy_performance}
As discussed in \sectref{termquality}, accuracy has been identified as an important indicator of performance, both for translation and for interpreting. Accuracy may be assessed either holistically, i.e. on the global level of the text, or componentially \citep[3]{tiselius_accuracy_2015}, on an analytical level, i.e. by evaluating errors or omissions in individual components of the product (paragraphs, sentences, or words). A potential drawback of assessing the accuracy of the target text, be it written or oral, is that it is necessary to clearly define and describe the scale used for the evaluation. In some cases, the involvement of the researcher in the evaluation may contribute to reduce discrepancies between evaluators \citep[395--397]{hansen_thoughts_2009}. Studies conducted on the product of machine translation engines have addressed the limitations of human evaluation by adopting established taxonomies for word or phrase-based assessment of MT, for instance in the area of post-editing studies (e.g. \citealt{vardaro_translation_2019,marzouk_german_2021}).\footnote{The reader may refer to \sectref{termquality} for an overview of said evaluation frameworks.} Such taxonomies may, however, also be adopted to evaluate the product of human translations, particularly through a contrastive approach with the MT output. An example is the study by \citet{carl_correlating_2010} on the differences between student and professional translators. The study used BLEU scores and human evaluations to assess the accuracy and fluency of the target texts, combining performance measures with behavioural measures (eye movements and keystrokes, see \sectref{behavioural_measures}).

Taxonomies of error types aimed at assessing the accuracy of the linguistic output in interpreting were proposed e.g. by \citet{barik_description_1971}, \citet{lambert_error_1994}, \citet[2002]{wadensjo_double_1993} and \citet{napier_interpreting_2004,napier_linguistic_2016}. The dimensions of accuracy range from the word to the text level and the error and omission categories may reach a high level of granularity, but may also be very simple depending on the scope of the study. For instance, in a previous contribution \citep{prandi_uso_2015,prandi_use_2015}, I evaluated the accuracy of the terms interpreted with a simple dychotomic classification of ``terms translated as per glossary'' (yes/no).







An issue in the assessment of accuracy in interpreting, mostly operationalised in terms of errors and omissions, lies in the fact that the taxonomy tends to be modified or devised anew by each researcher. Therefore, ``there are nearly as many error classification systems as there are empirical studies requiring an overall assessment of source–target correspondence'' \citep[143]{pochhacker_introducing_2004}. A limitation of the application of widely used frameworks for the assessment of translation as discussed above is that they have not been explicitly formulated for the oral modality. Consequently, they need to be adapted for the evaluation of the interpreting performance. An example can be found in \citet{xu_terminology_2015}. In her doctoral thesis on corpus-driven preparation for interpreters, she incorporated, merged and redefined several error categories from BlackJack, MeLLANGE and SAE J2450. Her error categories included incorrect terms, omissions, inappropriate collocations, grammatical errors, pronunciation errors and semantic errors. The scoring rubric was used to assess the accuracy of interpretations for which students had prepared either following a traditional approach or using corpus-driven terminology extraction. In TPR, a popular evaluation framework used to measure the accuracy of the translated product is MQM (see \sectref{termquality}). As will be discussed in \sectref{performancem_disc}, MQM and other standardised frameworks for quality evaluation in translation may also be adapted to interpreting (as in \citealt{xu_terminology_2015}) and used as methods for accuracy measurement in the area of CAI.

\subsection{Dysfluencies and pauses} \label{dysfluencies}
In psycholinguistics, pauses in speech production have been investigated as correlates of cognitive effort (e.g. \citealt{goldman-eisler_segmentation_1972}) and this association has been confirmed in a number of studies (e.g. \citealt{dragsted_segmentation_2004,lacruz-etal-2012-average,lacruz_post-editing_2014,kumpulainen_operationalisation_2015}). In spoken discourse, interruptions in the speech flow may, however, also be adopted intentionally as a rhetorical device for emphasis or simply to draw breath. In addition, pause patterns (duration and frequency) tend to vary from each individual to the next \citep{obrien_pauses_2006}. Hence, the interpretation of pauses as indicative of cognitive processing or speech planning is not unequivocal, and is therefore often complemented by other measures in research on translation and interpreting. In SI, since the source speech unfolds continuously during the task, pauses may also suggest that the interpreter is waiting for more linguistic material from the speaker. Furthermore, the position of pauses in the rendition may also reflect an alignment of the interpreter's output to the speaker's rhetorical style.

Methodologically, measuring pauses also poses a number of difficulties, for instance in establishing what should be considered as a filled or silent pause and in determining the minimum duration of pauses. Current audio processing tools represent useful research aids as they allow to automatically identify silent pauses according to a set of criteria selected by the researcher. Other types of dysfluencies usually require manual tagging and counting \citep[185]{gieshoff_impact_2021}. This can make the analysis of dysfluencies other than silent pauses rather laborious. For this reason, depending on the amount of data, researchers may choose to investigate macro or micro-locations of pauses \citep{dragsted_comprehension_2008}.

In interpreting, different levels of intentionality behind pause patterns emerge from studies comparing students and professional interpreters. Students' pauses tend to reflect hesitations and more effortful processing (e.g. \citealt{tissi_silent_2001,mead_exploring_2002,mead_methodological_2005}). Conversely, experienced interpreters show more deliberateness in their pause patterns (see \citealt{cecot_pauses_2001,ahrens_prosodic_2005,ahrens_pauses_2007}). Therefore, studies involving students as test subjects may find the analysis of pauses as indicators of cognitive effort useful. In a study comparing written translation and sight translation, \citet{dragsted_exploring_2009} calculated the number and duration of pauses to contrast text production patterns for interpreting and translation, suggesting that the inclusion of the oral modality in translator training may improve the output quality. A recent example of studies using pause patterns as correlates of CL is provided by \citet{gieshoff_impact_2018,gieshoff_impact_2021}, who involved 14 interpreting students as her test subjects and used silent pauses to contrast SI with and without visible lip movements. Filled pauses (\textit{uhm}) were analysed as indicators of cognitive effort in a corpus study by \citet{plevoets_effect_2016,plevoets2018cognitive} and were found to correlate with the informational load of the input speech.

\begin{sloppypar}
Pause-related metrics have also been successfully developed and implemented as indicators of cognitive effort in translation and post-editing (for a clear overview, see \citealt{lacruz_pause_2016}). For instance, a study on the coordination of comprehension and production processes in translation by \citet{dragsted_comprehension_2008} found that pauses indicate a coordination effort due to the transition between the two phases. In her study on post-editing, \citet{obrien_pauses_2006} found that pauses helped in the identification of a correlation between source text quality and post-editing effort (p. 17). In a study contrasting spoken translation (i.e. written translation obtained through target text dictation) and post-editing of MT, \citet{carl_comparing_2016} analysed the pause structure of target text production, finding a more coherent generation of translations when dictation was used. In research on the post-editing process, the two most widely used metrics are the average pause ratio (APR) and the pause-to-word ratio (PWR). Both were developed after \citegen{obrien_pauses_2006} influential work on post-editing, where she suggested a simple pause ratio measured as the total pause duration divided by the total duration of the post-editing task for a certain segment. The APR \citep{lacruz_post-editing_2014} is calculated as the average pause time divided by the average post-editing time per word for each segment. The APR was found to be reflective of cognitive effort, as the prediction that more numerous, shorter pauses may indicate more effortful post-editing was confirmed in a study on post-editing (Spanish to English) and later found not to be influenced by the pause threshold chosen \citep{lacruz-etal-2014-cognitive}. The PWR (ibid.) indicates the number of pauses divided by the number of source words per post-edited segment. This measure correlates strongly with APR as well as with other ratings of source quality (e.g. HTER, see \citealt{snover-etal-2006-study}). This measure was further refined in \citet{lacruz_literality_2018}, where ranges of pauses of different lengths were considered in the analysis. Clusters of short monitoring pauses were found to correlate with cognitive effort during post-editing, suggesting that they might be indicative of monitoring processes.
\end{sloppypar}

\section{Behavioural approaches} \label{behavioural_measures}
The collection of behavioural data, together with neurophysiological, hematic and cardiac correlates (see \sectref{neuromeasures_CL}), occupies the other extreme of the objectivity continuum. Behavioural measures do not rely on the subjective evaluation by participants nor on the product-oriented evaluation of the performance, but rather on the measurement of phenomena arising in the translator's or interpreter's brain and body that have been empirically linked with cognitive activity \citep[22]{jakobsen_TPR_2017}.

The higher degree of objectivity of these approaches represents a clear advantage compared to other measures: translation events may be measured and analysed to gain insight into the translation processes which guide the participant's behaviour. This fundamental assumption, underlying cognitive psychology and neuroscience, foregrounds the usefulness of empirical methods derived from these disciplines for the investigation of the translation and interpreting process. However, TPR has also developed its own additional measures and analysis tools to explore the translation process, hence the widespread adoption of mixed-method approaches in this research area. 

In the following subsections, I discuss the main behavioural measures which have been adopted to evaluate the cognitive load and effort involved in various types of translation activity. Behavioural measures may be divided in the two overarching categories of measures of time and measures of eye movement.

\subsection{Time-related measures} \label{timerelatedm}
Both for translation and for interpreting, it is generally assumed that there exists a ``rough correlation between time spent on translating a word or passage and the cognitive effort invested in solving a problem or in making a decision between competing solutions or strategies'' \citep[30]{jakobsen_TPR_2017}. Metrics based on the overall time required by the language processing task in question or by a sub-process thereof, or related to the transposition of individual elements of the source text into the target language may therefore provide useful and objective insights into the cognitive effort exerted by the participant. In research on written translation, post-editing and audio-visual translation, the use of time-based measures as indicators of the cognitive effort involved in producing a translation is facilitated by the use of keystroke logging, or key-logging, which enables a direct exploration of how the translation process unfolds over time. The advantage of the method is evident for the exploration of various types of translation activities, as testified by the development of dedicated tools, such as Scriptlog \citep{andersson_combining_2006}, Translog and Translog-II \citep{jakobsen_translog_2006}, and Inputlog \citep{leijten_keystroke_2013}. However, key-logging may also hold the potential to gain further insight into time-related aspects of human-machine interaction during computer-supported or computer-mediated interpreting. An example may be the automatic recording of the user activity in CAI tools to produce log files which can be used for empirical analysis, as has been done in studies using InterpretBank (see \citealt{biagini_glossario_2015,prandi_uso_2015,prandi_use_2015,prandi_exploratory_2018}).

With or without the support of key-logging, time measures have largely been employed in studies on interpreting as well as on translation. Two main types of time-based measures may be identified: on the one hand, speed-based measures, on the other, time-lag measures.
\subsubsection{Speed and task time} \label{speed}
The first subgroup of measures concerns the amount of time required to produce a unit of the target text or the entire target text. These measures are based on the assumption that difficult tasks tend to take longer than easier tasks \citep[87]{obrien_processing_2008}. An example may be word production time (WPT), i.e. the total time required to produce a word in the target text in translation, including revisions \citep[3]{carl_measuring_2016}. Another example may be found in \citet{obrien_processing_2008}, where processing speed was measured to establish the cognitive effort involved in the translation of segments with the support of different levels of fuzzy matches in the TM of a CAT tool environment.









On a broader level, some study designs may obtain useful information by measuring the time required to perform the entire task. For instance, \citet{jakobsen_eye_2008} compared ``task time'' in reading for comprehension, for translation, while interpreting and while translating. \citet{chmiel_eye_2020} measured ``translation time'' to compare SI and sight translation. This approach presents some limitations for interpreting, since especially SI is inherently constrained by the speaker's speech rate and because individual differences in the participants' delivery rate may be expected to play a role. Therefore, these factors should be taken into account when adopting overall measures of speed as indicators of effort.


\subsubsection{Time lag: EVS and EKS} \label{EVS_EKS}
The second type of time-related measures focus on the time-span elapsing between the time of production of a specific element in the target text (written or oral) and its appearance or perception in the source text. Often described with the general term of ``décalage'' \citep[418]{timarova_time_2015}, this time-span may represent different aspects of the process according to the object of investigation. Perhaps the two most widely adopted measures of this kind are the ear-voice span (EVS) in interpreting, especially simultaneous, and the eye-key span (EKS) in translation, which was derived from the first \citep{dragsted_comprehension_2008}.

The EVS has been used not only to explore temporal aspects of the interpreting process \citep[117]{pochhacker_introducing_2004}, such as processing speed, but it has also been shown to provide a reliable indication of cognitive effort in SI (e.g. in \citealt{treisman_effects_1965,barik_description_1971,shlesinger_corpus-based_1998,alvstad_time_2011}).

In interpreting, the duration of the EVS is taken as an indicator of cognitive effort: a shorter time required by the interpreter to produce the target-text equivalent of the source text, or of a specific source-text element, is generally understood as suggesting faster and, hence, less effortful processing. \citet{alvstad_time_2011} reviewed the various methods used to measure time lag in interpreting and compared EVS with EKS. The different methods used for the measurement of time-lag depend on the specific object of investigation, and can range from a broader and more surface-level analysis of EVS to establish reference values for SI, to a comparison of mean or median values across tasks or even to explore whether local variations in EVS reflect variations in cognitive processing. The most common approach is to measure time lag in number of seconds, but number of words has also been used.

In literature, the average EVS for professional conference interpreters has been identified as comprised between 2s and 4s (see for instance \citealt{barik_simultaneous_1973,lederer_simultaneous_1978,oleron_research_2002,christoffels_components_2004,defrancq_corpus-based_2015,alvstad_time_2011}), though shorter and longer time lags have also been observed. It may even be negative ``where true anticipation occurs'' \citep[419]{timarova_time_2015}. Overall, however, research has linked an EVS longer than 4s with a loss in accuracy \citep{lee_ear_2002,lee_tail--tail_2003,timarova_simultaneous_2014}. It is generally assumed that values longer than these point to processing issues on the part of the interpreter and may negatively affect the perception of quality, as they often result in long or frequent filled or silent pauses. However, the EVS has been found to vary both between and within tasks and to be influenced by external factors (e.g. the speaker's output rate, language combination) as well as by individual factors (e.g. the interpreter's own delivery rate, individual cognitive makeup, experience). For instance, professional interpreters have been found to have both longer \citep{englund_dimitrova_searching_2000} and shorter EVS \citep{timarova_simultaneous_2014} than trainees: this discrepancy might be interpreted as an indication that they are better able to adjust their décalage than interpreting students thanks to their expertise.

The interpretation of the EVS as indicative of cognitive effort is, therefore, not straightforward. This is particularly relevant when the EVS is not measured as a broad indicator of processing speed, but to contrast the speed of rendition of individual elements of the source text. Nonetheless, the EVS may be considered as a standard measure of cognitive effort in interpreting, and has been widely adopted in interpreting research, often in combination with further measures to address the already mentioned limitations.

The use of the EKS to investigate mental effort in translation has become possible thanks to the interfacing of key-logging and eyetracking (see \sectref{eyemeasures}), particularly with the development of dedicated data collection environments such as \textit{Translog-II} \citep{jakobsen_translog_2006}. Thanks to the integration of a gaze-to-word mapping tool, Translog-II allows to effectively triangulate gaze data and participants' typing behaviour, which provides detailed insight into the unfolding of the translation, post-editing or subtitling process and offers valuable clues on the load imposed by the subprocesses involved and on the strategies guiding the user's choices. Studies using EKS have produced telling results on the underlying processes of translation. For instance, \citet{shreve_coordination_2010} identified two different translating styles typical of professional and budding translators. The first favour an integrated style, where comprehension, production and revision processes are strictly interlinked, as shown by the shorter EKS, comparable to that of simultaneous interpreters (an average 2.8s in her group of participants). The second tend to work sequentially, as shown by the longer EKS (7.2s on average), which indicates longer time elapsing between the comprehension and the translation of a specific text unit.
\subsection{Eye movement measures and eyetracking} \label{eyemeasures}
The measurement of movements of the eye and of other phenomena linked to the eye physiology is possible thanks to a technique called eyetracking. Put simply, eyetracking is ``a technology for recording eye movements'' \citep[398]{jakobsen_translation_2020}.

The adoption of the eyetracking methodology for cognitive inquiries into different translation activities rests on two fundamental theoretical tenets, subsumed in \citegen[331]{just_theory_1980} eye-mind hypothesis and immediacy assumption. The eye-mind hypothesis postulates that observable eye movements, i.e. physical manifestations of overt attention, are strictly linked with covert attention and cognitive processes. Therefore, the object of visual attention is assumed to be the object of cognitive attention \citep[250]{schwieter_eye_2017} and the measurement of eye movements can be indicative of concurring mental processes. According to the immediacy assumption, there is no ``lag between what is being fixated [i.e. looked at for a period of time long enough to process it] and the hidden cognitive processes that take place inside the mind'' (ibid.). The eye-mind assumption has however been challenged by the notion of mind drifting (see \citealt{posner_orienting_1980,smallwood_restless_2006}): it is possible for the mind to start wandering while looking at an object, a phenomenon commonly experienced in everyday life. This poses an important limitation on the eye-mind hypothesis, namely that this mind drifting cannot be observed nor measured, as it is not reflected in eye movements.

Additionally, it has been empirically demonstrated that there is a certain lag between the focus of visual attention and what is being cognitively processed at a given moment \citep[409]{jakobsen_translation_2020}. Essentially, the eyes ``seem to behave somewhat like a dog on a leash held by the mind rather than there being a perfectly straightforward relationship'' \citep[34]{jakobsen_TPR_2017}. Nonetheless, as observed by \citet{hvelplund_eye_2014, schwieter_eye_2017}, while the researcher should be aware of these potential drawbacks, mind drifting during a cognitively taxing task such as translation is probably a rare occurrence. In interpreting, the probability of a cognitive shift of this kind can reasonably assumed to be even more remote, particularly during SI due to the immediacy of the task. Owing to this, eye movements can reliably be interpreted as ``correlates of cognitive processing in translation'' and interpreting, as validated in neighbouring disciplines \citep[211]{hvelplund_eye_2014}.

Eyetracking has been used in several research areas, chiefly psychology, psycholinguistics and cognitive sciences as a way to empirically investigate human behaviour, cognition and attention \citep[248]{schwieter_eye_2017}, and at a more basic level ``to study the physiological mechanics of human eye movements'' \citep[398]{jakobsen_translation_2020}. The potential of this technique, however, makes it suitable to the investigation of behaviour, cognition and attention allocation also in cognitively taxing activities such as translation and interpreting, as will be discussed in \sectref{eyetracking_TPR}. In the following sections, I describe the equipment used in eyetracking and introduce the main measurements, with a focus on fixation-based and related metrics, as they have found wide application in TPR and in studies on human-computer interaction and may therefore also be applied to cognitive inquiries into interpreter-computer interactions.
\subsubsection{Equipment and applications} \label{eyetracker}
The device used to produce a recording of where the eyes are looking is called eyetracker. What is recorded is not the image of the eye itself, but rather the reflection of infrared light on the cornea of the eye \citep[54]{duchowski_eye_2017}. The reflected light is recorded at a rate comprised between 30 and 2,000 Hz, which corresponds to 30 to 2,000 samples recorded per second \citep[399]{jakobsen_translation_2020}.

Different types of eyetracker are available, each presenting benefits and potential drawbacks. Head-mounted eyetrackers, for instance, often used in combination with chin rests, bite bars or forehead rests, allow for high quality data, as the precision of the eyetracker is enhanced by the steady position of the subject's head. On the other hand, it poses severe restrictions on the participants' freedom of movement, which may make this type of eyetracker unsuitable for certain types of research. On the other end of the spectrum are tracking eyeglasses, which offer the advantage of being very portable and of allowing researchers to take their investigation out of the laboratory, as they make it possible to record eye movements on several planes, i.e. on surfaces different from a computer monitor. These eyetrackers, however, have the limitation of allowing recordings only at slow speed (30 Hz). A compromise between the two is offered by stand-alone or attachable remote eyetrackers (see \citealt{ehrensberger-dow_challenges_2014}). They are limited to the monitor they are integrated in or mounted on, but they are relatively unobtrusive as they do not require to be worn by the participant. At the same time, they allow for high-frequency recordings, essential due to the speed of eye movements, especially during translation \citep[36]{jakobsen_TPR_2017}.

Despite its vast potential for the objective exploration of cognition and attention, the eyetracking methodology presents some limitations. First, it requires strict experimental conditions, as the lack of control of aspects such as light intensity or the participants' distance from the screen may affect data quality and results. Second, while a certain level of control can be achieved through a proper laboratory set-up \citep{rosener_eye_2016}, other aspects, particularly those linked to participants' characteristics and behaviour, may pose non-negligible issues to the researcher. A common example is the type of glasses or lenses worn by participants, which may create artefacts or impede the correct recording of eye movements. Finally, stress due to the experimental conditions cannot be ignored, although it may be contained by the use of remote eyetrackers which are less invasive and do not require a modification of the task being performed. Despite being contactless, however, they still require calibration, which makes the experimental condition more apparent. Nonetheless, as pointed out in the literature (\citealt[e.g.][206]{hvelplund_eye_2014},  \citealt[390]{hansen_thedialogue_2008}), stress may be perceived by participants due to the simple fact of being observed during task performance (the so-called white coat effect). While these potential drawbacks and external influencing factors cannot be eliminated entirely and should be considered during experimental design and data analysis, the advantages offered by eyetracking far outweigh its limitations, which explains its popularity in translation process research.
\subsubsection{Fixation-based measures} \label{fixations}
Eyetracking allows to conduct several types of measurements. The most popular eyetracking measures are fixation-based measures and saccades, quick eye movements occurring between fixations \citep{poole_eye_2005}. From fixations and saccades, other metrics may be derived, for instance gaze measurements. Eyetracking also allows to collect additional types of data related to eye movements and eye physiology, such as blink rate and pupil size/dilation (see \sectref{pupillary_measures}).

Due to the popularity of the eyetracking technique, a series of metrics have become established as the norm of reference for the investigation of translation in a cognitive framework, often borrowing from neighbouring disciplines. At the same time, TPR has offered further confirmation of the ability of certain indicators to account for specific variations in cognitive effort or in the allocation of attentional resources, and to measure the ``number and patterns of translation process activities, the duration of and switches between activities, or the fluency in production'' \citep[225]{ehrensberger-dow_cognitive_2020}.

In particular, fixation measures and measures derived therefrom have been widely used as dependent variables in TPR to determine the cognitive effort involved in the translation process, as they are more sensitive to linguistic factors \citep{gaskell_eye_2007}. For this reason, as they will be relevant for the present inquiry, here I focus on these measures.\footnote{A thorough description of eyetracking measures applicable to translation process research goes beyond the scope of the present contribution. For a comprehensive discussion, a useful reference may be found in \citet{conklin_eye-tracking_2018}.}

Fixation-based measures provide useful indications concerning the ``quality and intensity of cognitive attention in a task'' \citep[402]{jakobsen_translation_2020}. A fixation is a period of time during which the eye remains stable (fixated) on a target, which is necessary for the ocular system to bring the object into focus \citep[46]{duchowski_eye_2017}. Its length is usually understood to be comprised between 200 and 300 ms in reading (\citealt[373]{rayner_eye_1998}, \citealt[381]{holmqvist_eye_2011}), but it ``may be as long as several seconds'' \citep{karsh_looking_1983,young_survey_1975}, and as short as 30--40ms \citep[413]{holmqvist_eye_2011}. In particular, the duration and number of fixations index the amount of cognitive attention on the fixated object or area of interest (AOI). At the same time, they indicate the intensity of cognitive effort required for the performance of a task, for instance for the processing of individual words or phrases in translation. In literature, longer or more numerous fixations have often been associated with ``a deeper and more effortful cognitive processing'' (\citealt[413]{holmqvist_eye_2011}, \citealt[212]{hvelplund_eye_2014}, \citealt[1214]{lacruz_pause_2016}).

Among early measures, first fixation durations are particularly indicative of the amount of attention generated by an item, especially in reading when the area of interest is a single word (see \citealt[124]{conklin_eye-tracking_2018}). Among late measures, fixation count and total gaze time, also defined as total fixation duration or total fixation time, are also used as indicators of attention and processing effort. If the interest is in how processing unfolds over time, time-related fixation measures may provide useful insights. Two widely used metrics of this kind are time to first fixation, which indicates how much time elapses before the AOI is fixated for the first time, and average fixation duration \citep[129]{conklin_eye-tracking_2018}. Studies concerned with the focus of attention of a translator or of an interpreter may investigate gaze or fixation patterns to identify which area of the visual stimulus attracted the participant's attention \citep{ehrensberger-dow_cognitive_2020}.

In the following section, I present some examples of how the method and measures discussed above have been applied in TPR, with a focus on inquiries into interpreting. While the following overview has no ambition of completeness, it will contribute to highlight the role of eyetracking as a useful methodology for TPR.

\subsubsection{Eyetracking in Translation Process Research} \label{eyetracking_TPR}
The wide application of eyetracking to the study of the translation process is due to its ability to offer objective, real-time measures of the mental processes involved in translation, often without requiring excessive manipulation of the task itself. In TPR, eyetracking measures have been widely adopted, often in combination with other behavioural measures as well as subjective and performance measures or with physiological methods such as PET or fMRI (see \sectref{neuromeasures_CL}). Over the past twenty years, the eyetracking method has been used to study the cognitive processes underpinning translation, subtitling, post-editing and interpreting. The eyetracking technique has been applied to the exploration of a variety of aspects linked to the translation process and profession, such as ``translation expertise, competence and experience, cognitive effort, reading in translation, human-computer and human-information interaction, metaphor processing, directionality, reception of translated material'' \citep[251]{schwieter_eye_2017}. As such, eyetracking research in translation and interpreting represents a relatively recent, albeit very productive innovation in Translation and Interpreting Studies.

Particularly relevant to the present contribution is the adoption of eyetracking as a research methodology to study the processing effort required by different translation tasks, specifically to explore how cognitive resources are allocated, distributed, and coordinated during translation or during specific translation subtasks \citep[254]{schwieter_eye_2017}. A large body of research has adopted eyetracking to this aim. For instance, a number of studies have successfully investigated reading for and during translation with eyetracking, which has become a standard methodology in this field of research. \citet{jakobsen_eye_2008} compared reading for comprehension, as preparation for translating, for sight translation and during translation. In addition to time on task, they analysed fixation counts, average fixation duration and gaze times, finding a constant progression in all of these metrics from the first to the last type of translation task. A similar experiment contrasting reading for comprehension, for oral summarisation and for sight translation conducted by \citet{alves_towards_2011} found longer fixation durations for sight translation. \citet{schaeffer_measuring_2014} measured translation effort in the production of the target-text equivalent for literal translations by measuring gaze and translation time, while in a comparable study, \citet{dragsted_indicators_2012} analysed total reading time and number of fixations as indicators of translation effort.

To study interpreting, the eyetracking methodology has been adopted more extensively in research on modes of interpreting which involve a written and/or visual component (e.g. sight translation and consecutive interpreting) or for settings which require greater interaction (dialogue interpreting). It has recently seen a revival due to the low intrusiveness, high temporal resolution and relatively limited cost of eyetrackers, especially of the remote type, which interfere less with the interpreting process.

The first study applying eyetracking to interpreting comes from outside Interpreting Studies. \citet{mcdonald_simultaneous_1981} used eyetracking to investigate how ambiguous phrases are interpreted and parsed in sight translation, comparing the comprehension and the target text production phase and including the mechanisms underlying the identification of errors during the task.

A series of studies adopting the eyetracking method followed, which focused on further testing the applicability of the method (in conjunction with fMRI) to the study of directionality in interpreting \citep{chang_testing_2009}, of sight translation performed by trainee interpreters \citep{way_eye_2013} and professional interpreters \citep{dragsted_speaking_2007}, also in terms of ``syntactic disruption and visual interference'' in sight translation \citep{shreve_cognitive_2010}. In these studies, fixation-based measures were used, often as a combination of several metrics, chiefly fixation duration and counts. An example is \citet{dragsted_exploring_2009} who combined these metrics with heatmaps and key-logging to contrast written and sight translation, elucidating differences in how interpreters and translators process the source text.

More recent studies using fixation durations are the one by \citet{seubert_visuelle_2019} on visual input in SI and by \citet{seeber_when_2020} on multimodal processing during SI with text. Seubert used fixation durations and gaze patterns to explore the effects of various sources of visual and written information on professional interpreters' attention allocation. \citet{seeber_when_2020} investigated visual attention measuring the proportion of fixations and mean dwell time on five areas of the written input (see also \sectref{CLM_with text}). Another application of eyetracking to research on interpreting lies in the use of pupillary measures, discussed in \sectref{pupillary_measures}.

Eyetracking has not only been used to explore simultaneous or sight interpreting, but may also yield valuable insight into visual attention and cognitive load and effort in dialogue interpreting, characterised by a higher degree of interaction. A first step in this direction was recently taken by \citet{tiselius_gaze_2020}, who used eyetracking glasses to collect data on the gaze patterns of experienced and inexperienced interpreters during dialogue interpreting. They suggest that gaze aversion during interpreting may index cognitive effort. In addition, the use of eyetracking may yield valuable information on how the note-taking process unfolds, as explored for instance by \citet{kuang_computerized_2019}.

Particularly interesting is the recent turn of eyetracking TPR towards an investigation of the interaction between translators and the tools they use as support for the translation task, from the consultation of digital resources such as websites and online terminological databases, to the use of CAT tools and translation workbenches during post-editing. As observed by \citet[403]{jakobsen_translation_2020}, ``a fascinating feature of gaze data is that they can be interpreted both as documentation of how well an interface design works, how well a translator interacts with a computer program, and as documentation of a translator's mental effort in carrying out a task''. This characteristic of eyetracking measures may therefore prove useful to study the interaction between interpreters and CAI tools. Previous research has exploited eyetracking to this aim in studies on translation tasks of different kinds. An early example of this application is offered by \citet{obrien_pauses_2006}, who used eyetracking to explore how the use of translation memories affects cognitive effort, finding that exact matches elicit the lowest effort, as indicated by pupil size. More recently, \citet{alves_investigating_2016} explored cognitive effort in post-editing tasks conducted in two different environments, i.e. with interactive and standard machine translation, using the metrics fixation count and fixation duration (average and median) as correlates of effort. \citet{carl-etal-2016-english} contrasted post-editing and translation dictation through speech recognition combining key-logging and time-based measures (e.g. translation duration, translation dictation duration) with gaze durations (see also \citealt{carl_comparing_2016}). In a study on the processes of error identification and correction in machine translated texts and post-edited machine translations, \citet{vardaro_translation_2019} combined early measures (first fixation durations and first pass durations) and late measures (total reading time and regression path durations). Finally, in the area of audio-visual translation, \citet{tardel_attention_2021} investigated cognitive effort by combining total fixation count, average visit duration, total reading time, relative attention to video and video replay time (p. 122).
\section{Physiological measures} \label{neuromeasures_CL}
Additional methods have also been used to explore cognitive processing in written and oral translation, such as measures of ``cardiac, hematic, electro-dermal, ocular, muscular and cerebral responses'' \citep[25]{seeber_cognitive_2013}. Since they are based on the activation of the autonomic nervous system \citep[224]{ehrensberger-dow_cognitive_2020}, these measures are less subjective and are language independent, which increases comparability between studies. In addition, like gaze-related metrics, these measures are continuous, thus allowing for a detailed account of local variations of CL.

Some methods relying on physiological measures can, however, be rather intrusive and require expensive equipment as well as a high level of expertise by the researcher for their successful integration into the empirical design.

Because of their complexity, cost and intrusiveness, most psy\-cho-phys\-i\-o\-log\-i\-cal approaches have found rare application in studies on translation and interpreting. Nonetheless, physiological measures may substantiate claims about CL variations in translation and interpreting tasks because of their objectivity. In studies on CL in interpreting, physiological methods have mainly comprised measures of the brain, of the eye, and of the heart \citep[649]{chen_construct_2017}, which is why I focus on these metrics in the following sections.

\subsection{Brain measures} \label{brain}
\begin{sloppypar}
Brain imaging methods comprise different techniques, either exploiting the electrical or magnetic activity of nerve cells (i.e. electroencephalography (EEG) and magnetoencephalography (MEG)) or hemodynamic changes in the brain (i.e. positron emission tomography (PET), functional magnetic resonance imaging (fMRI), and near-infrared spectroscopy (NIS)). \citet{tommola_images_2000} provide an accessible overview and discussion of the different methods. Research on the neurocognition of translation and interpreting has employed mostly the EEG, the fMRI and the PET methods.  
\end{sloppypar}

For instance, \citet{petsche_brain_1993} used electroencephalography (EEG) to identify which areas of the brain activate in SI and in shadowing. EEG is a non-invasive and affordable method, which provides excellent temporal resolution. When used to investigate the interpreting process, however, EEG presents an important limitation: because mandibular movements can generate artefacts, the tasks must be performed covertly, which may give rise to a series of effects unaccounted for and also prevent its combination with performance measures. Additionally, its spatial resolution is quite poor, a limitation which MEG does not have, although the issue of artefacts remains also with this method. Due to these limitations, the use of EEG has been more popular in studies on translation. Some examples are \citegen{oster_lexical_2019} work on the translation of cognates or the extensive body of research by \citeauthor{garcia2013brain} (e.g. \citeyear{garcia2013brain,garcia_translating_2015,garcia_neurocognition_2019}).\footnote{An excellent introduction to neurocognitive inquiries into translation and interpreting is provided by \citet{garcia_neurocognition_2019}.}

Functional magnetic resonance imaging (fMRI) is also a non-invasive technique, which offers the opposite advantages and limitations compared to EEG: it can be useful for the localisation of brain functions due to its high spatial resolution, while its low temporal resolution and high sensitivity to head movements (including overt speech) limit its application to study interpreting, not least because the scanner environment is very noisy \citep{tommola_images_2000}. Nonetheless, research on the neural basis of interpreting has successfully utilised the method. Some examples are \citet{hervais-adelman_executive_2011,hervais-adelman_fmri_2015} or research on directionality by \citet{chang_testing_2009} and \citet{kalderon_neurophysiologie_2017}. Kalderon used the method also to explore the effect of interpreting expertise in brain activation.

PET is a highly invasive method, as it requires the intravenous administration of radioactive ligands \citep[25]{seeber_cognitive_2013}. This may severely limit sample size. While its temporal resolution is limited, PET provides good spatial resolution and ``seems to be the only one that allows the investigation of the entire multi-effort process of SI'' \citep[18]{tommola_images_2000}. In \citet{price_functional_1999}, PET was used to compare translation and interpreting, while \citet{rinne_translating_2000} and \citet{tommola_images_2000} used PET to compare speech shadowing and SI, including the effect of directionality on activation patterns.


\subsection{Heart rate measures} \label{heart}
Cardiac system measures such as heart rate, heart rate variability (i.e. variation in time intervals between consecutive heart beats, see \citealt{thayer_meta-analysis_2012}), and blood pressure have been used to measure the impact of a variety of language processing tasks on stress (e.g. \citealt{lambert_putting_1994,korpal_linguistic_2017,kurz_physiological_2003}). However, as higher stress experienced may signal increased cognitive effort, such measures may prove useful in explorations on mental effort in the translation and interpreting process.

Heart rate measures have been adopted to study the interpreting process, especially after what has been defined as the ``psycho-affective turn in Interpreting Studies'' \citep[298]{korpal_interpreting_2016}, i.e. a renewed interest for the role of psycho-affective factors in interpreting, for instance for aptitude testing (see also \citealt{chabasse_gibt_2009}). As reported by \citet[304]{korpal_interpreting_2016}, additional markers of stress used in interpreting research are skin conductance level (e.g. \citealt{garzone_physiological_2002,kurz_physiological_2003}), cortisol concentration (used in \citealt{blumenthal_stress_2006} and in \citealt{AIICworkload2002}), and IgM levels \citep{moser-mercer_remote_2005}, often used in combination with and integrated by subjective measures, such as self-reported measures of stress. In TPR, an example of the combination of multiple types of measures, including an extensive set of heart rate measures, can be found in \citet{herbig2021multi}, who explored how objective measures relate to the subjectively reported CL in MTPE.

\subsection{Pupillary measures} \label{pupillary_measures}
Among physiological approaches, the collection of pupillometry data represents the least invasive and the more suited method to be applied to investigations of the interpreting process, in particular of SI, which, for its continuous and real-time nature, does not easily allow for the adoption of more invasive techniques, as discussed above.

Pupillary measures are also often used in the investigation of cognitive processes and attention. The pupil size and dilation, measured in millimetres, are taken to index the amount of processing effort exerted by a test subject to perform a certain task. The rationale behind the use of measures of pupil size and dilation in cognitive studies lies in the assumption that the diameter of the pupil reflects the difficulty of the task at hand or the working load posed by it (see for instance \citealt{hess_pupil_1964,just_intensity_1993,kahneman_pupil_1966}). The wider the pupil, the higher the difficulty and/or the working load. The interpretation of pupil measures is, however, far from being this straightforward, as the pupil has been found to contract and dilate in relation to a wide range of stimuli and psychological and affective states. The diameter of the pupil can be influenced not only by workload, but also by emotion and anticipation, fatigue, diabetes, age, pain and drugs (see \citealt[426]{holmqvist_eye_2011} for a detailed discussion). However, some of these factors can be controlled experimentally, for instance by excluding participants with relevant pre-existing conditions or of a certain age group from data collection. To ensure good data quality, it is preferable to work with systems that have ``a fixed distance between camera and eye'' \citep[530]{holmqvist_eye_2011}. Remote eyetrackers may expose the researcher to the risk of introducing artifacts in the data. Most importantly, pupils react particularly to changes in luminance, which requires a stable source of light and a controlled environment during data collection. Ideally, the study design should include stimuli with equal brightness and contrast. A major complication in using pupillometry to investigate cognitive processing lies in the fact that pupillary responses do not occur immediately after stimulus presentation, but with a certain latency. Pupil dilation latency varies according to the stimulus presented, with latencies comprised between 150 and 400 ms for light stimuli or between 300 and 500 ms for interpreting, as found by \citet[605]{hyona_pupil_1995} (see \citealt[215]{hvelplund_eye_2014}). This means that dilation data might refer to what the participant was processing before stimulus onset. For the aforementioned reasons, measures of pupil size and dilation are best analysed in combination with other metrics, and caution should be exercised when interpreting pupillary measures.

In the 90s, three groundbreaking studies sought to establish the feasibility of the application of pupillometry to the exploration of processing effort in SI \citep{tommola1986mental,tommola_mental_1990, hyona_pupil_1995}. As pupil diameter had been found to positively correlate with CL (the larger the pupil, the higher the load) during cognitive processes, including language comprehension, the researchers sought to establish whether pupillometry could be adopted as a diagnostic method to index variations in processing effort during interpreting. The choice of this physiological indicator was motivated by the fact that it was hypothesised to comply with the key requirements identified by \citet{kahneman_attention_1973}: the ability to reflect load differences between and within tasks and between subjects. In other words, capturing differences in processing load during interpretation required an online, real-time and highly sensitive metric capable to co-vary with global and local variations in CL. In those first attempts at integrating this measure into the exploration of the interpreting process, pupillometry was used to compare listening (without comprehension testing), shadowing and SI into the participants' mother tongue \citep{tommola_mental_1990} and, later, to investigate the effect of word difficulty and directionality on CL during SI \citep{hyona_pupil_1995}. In the first study, the hypothesis was that SI would yield the largest average pupil diameter, followed by shadowing and listening. In the second experiment, pupil size was expected to be larger for the words classified as difficult to translate and for the repetition of words in the foreign language.

The intention behind these initial studies was rather of methodological nature. The results confirmed the adequacy of pupillometry as a method to investigate mental load during interpreting and related language tasks. Specifically, mean pupil size was found to be larger in the interpreting task. An order effect was also found, meaning that pupil dilation is larger at the beginning of the task. In the second experiment \citep[609]{hyona_pupil_1995}, pupils were found to dilate on average by 0.40 mm during the translation of difficult words, while maximum dilation was found to be of 0.57 mm. Another important finding was that pupillary latency, i.e. the delay in pupillary response to a task, was between 300 and 500 ms \citep[605]{hyona_pupil_1995}, which aligns with previously reported values \citep{beatty_task-evoked_1982,hoeks_pupillary_1993}. These pioneering studies paved the methodological way for the adoption of pupillometry in interpreting process research, showing its feasibility for the investigation of this cognitively-taxing task.

Pupillometry was recently adopted by \citet{seeber_cognitive_2012} and by \citet{gieshoff_impact_2018} to study, respectively, the effect of verb-final and verb-initial syntactical structures in German to English SI on local CL and of multimodal input presentation on mental workload in SI.

In their discussion of results, Seeber and Kerzel highlight an important aspect of the analysis of SI conducted under different conditions or on different source materials (as in the case analysed by the authors). While the differences in mean pupil diameter and dilation identified by \citet{hyona_pupil_1995} were relatively large, which was to be expected due to the comparison of different tasks, when the comparison is made between the same task with more subtle modifications, the differences in pupil-based measures are likely to be less obvious. This was the case in Seeber and Kerzel's study, which found that pupils dilated by 3.996 and 4.048 mm while interpreting symmetrical and asymmetrical syntax respectively, as measured at the end of the task, i.e. in the temporal break between the stimuli sentences. This is an important methodological understanding, which should also be taken into account in the present study seeking to compare SI conducted with varying digital support for terminology look-up.

\citet{gieshoff_impact_2018} found an effect of audio-visual input (i.e. the presence of the speaker's video) on pupil size: its decrease was less marked than in the audio-only condition. On the other hand, background noise did not have an effect. She also found an effect of task, as she compared listening to interpreting. She found that pupil sizes decreased more largely in listening than in the SI task.

Results from the studies presented ``illustrate the great potential of pupillometry as a method and [task-evoked pupillary responses] as a measure of cognitive load in simultaneous interpreting'' \citep[27]{seeber_cognitive_2013}. However, the methodological challenges highlighted in the aforementioned studies underline how, despite its strengths, pupillometry requires a stringent experimental setup to allow the researcher to isolate stimulus responses. Additionally, pupillometry provides the best results when applied to isolated and short stimuli. When used over longer periods of time, local variations are averaged out and the technique does not offer useful insights into the underlying cognitive process (see for instance \citealt{schultheis_assessing_2004}).

The present chapter has reviewed the different methods and measures used to estimate CL in translation and interpreting. As discussed, all measures present advantages and disadvantages. A combination of several types of metrics may therefore prove beneficial to address the inherent limitations of each and maximise the benefits provided by the individual metrics. This is the approach commonly followed in TPR and it will also be adopted in the present study. The following chapter illustrates the methodology used in the experiment and discusses the measures chosen for data collection in \sectref{measures}.
