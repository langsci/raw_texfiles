\chapter{\label{ch:4}Methodologie}


\section{\label{sec:4.0}{Einleitung}}

In diesem Kapitel wird die Forschungsmethodik der Arbeit vorgestellt. Wie das vorherige Kapitel zeigt, existiert für Evaluationsstudien im Bereich der KS in Zusammenhang mit der MÜ keine Standardmethodik. Vor diesem Hintergrund wird in der vorliegenden Studie ein Mixed-Methods-Triangulationsansatz angewandt, auf dessen Basis die Vorteile der Replizierbarkeit eines quantitativen Ansatzes und der Realitätsnähe eines qualitativen Ansatzes ausgeschöpft sowie die Nachteile des jeweiligen Ansatzes minimiert werden konnten. Das Kapitel ist wie folgt strukturiert: Zunächst werden die Forschungsmethodikgefolgt von der Operationalisierung und der Validität der Arbeit genauer erläutert. Das Studiendesign kann als eine Reihe von Entscheidungen betrachtet werden, die der Forscher im Laufe seiner Forschung unter Berücksichtigung der Reliabilitäts- und Validitätsaspekte trifft, nachdem er aus den Fehlern vorheriger Studien gelernt bzw. von ihren Ergebnissen profitiert hat. Daher widmet sich der Abschnitt Studiendesign der ausführlichen Darstellung der Auswahl der analysierten Regeln und der untersuchten MÜ-Systeme wie auch der detaillierten Präsentation des Aufbaus des Datensatzes. Außerdem werden im Rahmen dieses Abschnitts die methodischen Überlegungen und Testläufe, die zum Design der drei implementierten Evaluationsmethoden geführt haben, sowie die Vorgehensweisen der durchgeführten Analysen präsentiert.

\section{\label{sec:4.1}Forschungsmethodik}

Wie die Literaturübersicht im Kapitel 3 darlegt, stellt die Bewertung der Qualität des Outputs der maschinellen Übersetzung (MÜ) eine komplexe Aufgabe dar, die noch komplexer wird, wenn sie in Zusammenhang mit der Kontrollierten Sprache untersucht wird. Daher war es erforderlich einen Forschungsansatz anzuwenden, mit dem diese Komplexität behandelt werden kann. Mithilfe eines dreiphasigen Mixed-Methods-Triangulationsansatzes strebt die Studie an, den Einfluss einzelner Regeln der Kontrollierten Sprache auf die Qualität\footnote{{{{F}}}\textrm{ür die Definition der Qualität im Rahmen dieser Studie siehe \sectref{sec:4.4.5.1}.}} des MÜ-Outputs zu untersuchen. Nachfolgend wird der Mixed-Methods-Ansatz, seine Umsetzung über drei Phasen sowie die Triangulation der angewandten Methoden näher erläutert.

Eine der ersten Definitionen des Mixed-Methods-Ansatzes entstand 1989 von Greene, Caracelli und Graham im Bereich der Evaluation. Nach dieser Definition umfasst ein Mixed-Methods-Design mindestens eine quantitative Methode („designed to collect numbers“) und eine qualitative Methode („designed to collect words“) (\citealt{CreswellClark2007}: 2). \citet{JohnsonOnwuegbuzie2004} definierten den Mixed-Methods-Ansatz als „the class of research where the researcher mixes or combines quantitative and qualitative research techniques, methods, approaches, concepts or language into a single study”. Im Grunde wurde das Konzept des Mixed-Methods-Ansatzes auf verschiedene Weise definiert. Vor diesem Hintergrund untersuchten \citet{JohnsonEtAl2007} in einem häufig zitierten Artikel des Journal of Mixed Methods Research (JMMR) 19~Definitionen sehr bekannter Studien. Der Artikel reflektiert die zahlreichen Unterschiede, die diese Studien, unter anderem in Bezug auf die Faktoren „was wird gemixt“ (z.~B. Methoden, Methodologien, Forschungstypen), „wann findet das Mixing statt“ (z.~B. bei der Datenerhebung, \nobreakdash-analyse, \nobreakdash-interpretation) sowie „Ziel des Mixing“ aufweisen. Sie zeigen, dass das Wort „Methods“ (in „Mixed Methods“) eine breite Interpretation und Verwendung hat und somit die Einbeziehung von Fragestellungen und Strategien hinsichtlich der Methoden der Datenerhebung sowie der Forschungsmethoden zulässt \citep{JohnsonEtAl2007}. Im Endeffekt besteht die zentrale Prämisse dieses Ansatzes darin, dass “the use of quantitative and qualitative approaches in combination provides a better understanding of research problems than either approach alone” (\citealt{CreswellClark2007}: 5). Bei der vorliegenden Studie ging es nicht nur darum, durch den Mixed-Methods-Ansatz ein besseres Verständnis der Forschungsprobleme zu erlangen, vielmehr wäre das Ziel der Studie ohne diesen Ansatz nicht erreichbar. Denn jede der implementierten Methoden lieferte Daten, die für die Analyse in der darauffolgenden Methode erforderlich war, wie unten näher beschrieben ist.

Des Weiteren wurden die Ergebnisse der drei angewandten Methoden (Fehlerannotation, Humanevaluation und automatische Evaluation des MÜ-Outputs) trianguliert. Der Triangulationsansatz wurde in den 1970er Jahren von Denzin entworfen und als die „combination of methodologies in the study of the same phenomenon“ definiert (\citealt{Kuckartz2014}: 44f.). Mit der Kombination mehrerer Methoden zielt man darauf ab, das Vertrauen in die Validität der Ergebnisse zu steigern (\citealt{FreyEtAl1991}: 24; \citealt{Kuckartz2014}: 47). \citet[23]{SaldanhaOBrien2014} betrachten die methodologische Triangulation als „the backbone of solid, high quality research”. Bei einer Methodentriangulation sind drei Resultate möglich (\citealt{ErzbergerKelle2003}): eine vollständige oder Teilübereinstimmung der Ergebnisse, sich gegenseitig ergänzende oder unterschiedliche bzw. widersprüchliche Ergebnisse. Alle genannten Arten der Ergebnisse können aufschlussreich sein: eine Übereinstimmung untermauert die Ergebnisse; sich ergänzende Resultate können das Ergebnisbild vervollständigen; und unterschiedliche bzw. widersprüchliche Ergebnisse können die Aufmerksamkeit des Forschers auf eine offene Frage richten.


\begin{figure}

\scalebox{.7}{
\begin{tikzpicture}[
node distance = 10pt and 20pt,
squarednode/.style={rectangle, draw=lsGuidelinesGray, fill=lsGuidelinesGray!5, thin, text width=4.5cm},
headnode/.style={rectangle, draw=tmnlpone, fill=tmnlpone!10, thin, text width=4.5cm},
]
\node[squarednode](mid4){Post-Editing der MÜ};
\node[squarednode](mid3)[above=10mm of mid4]{Analyse der Unterkategorien der Q-Parameter};
\node[squarednode](mid2)[above =10mm of mid3]{Vergleich der Stil- und Inhaltsqualität};
\node(triangulation)[above=2mm of mid3]{Methodeninterne Triangulation~~~~~~};
\node[headnode](mid1)[above=of mid2]{Humanevaluation};
\node[headnode](left1)[left = of mid4]{Automatische Evaluation};
\node[squarednode](left2)[below=of left1]{Vergleich der AEMs-Scores (TERbase \& hLEPOR)};

\node[headnode](right1)[right=of mid4]{Fehlerannotation};
\node[squarednode](right2)[below=of right1]{Vergleich der Fehleranzahl};
\node[squarednode](right3)[below= of right2]{Vergleich der Fehlertypen};
\node[squarednode](right4)[below= of right3]{Analyse der Anno Gruppen};

%lines
\draw[<->](left2.5)--++(10pt, 0)|-(mid2.175)
% node[pos=.5, left, align=right]{Methodeninterne Triangulation}
node [pos=.3, left, align=right]{Q-Veränderung nach den beiden\\Methoden (Korrelationen)};
\draw[<-](left2.355)--++(15pt, 0)|-(mid4.west);
\draw[<->](right3.west)--++(-10pt, 0)|-(mid2.5)
node[pos=.3, right, align=left]{Q-Veränderung bei den\\Fehlertypen (Korrelationen)};
\draw[<->](right4.west)--++(-15pt, 0)|-(mid2.355)
node[pos=.1, left, align=right]{Q-Veränderung bei den\\Annotationsgruppen};
\draw[<->](mid2.south)--(mid3.north);


\end{tikzpicture}
}

\caption{\label{fig:4:6}Methodik der Arbeit -- Mixed-Methods-Triangulationsansatz}
\end{figure}

Die quantitativen und qualitativen Daten können gleichzeitig oder in aufeinanderfolgenden Phasen erhoben und analysiert werden (\citealt{JohnsonOnwuegbuzie2004}; \citealt{SaldanhaOBrien2014}: 23). In der vorliegenden Studie wurden die drei genannten Methoden aufeinander aufgebaut und entsprechend in der unten dargestellten Reihenfolge durchgeführt (\figref{fig:4:6}).

Grundsätzlich bildet der Datensatz zwei Szenarien („vor“ vs. „nach“ dem Einsatz der KS-Regel) ab, die mithilfe der drei Methoden bottom-up analysiert und verglichen werden. In der ersten Phase wurden quantitative Daten aus der Fehlerannotation (siehe \sectref{sec:4.4.4.3}) gewonnen, die durch den Vergleich der beiden Szenarien eine Abnahme bzw. Zunahme der Fehleranzahl aufzeigen und Aufschluss über die aufgetretenen Fehlertypen geben. Ferner wurden die aufgetretenen Fehlertypen qualitativ analysiert. Darüber hinaus wurden Annotationsgruppen\footnote{{Die Daten wurden binär bzw. dichotom aufgeteilt (keine Fehler aufgetreten „0“; Fehler aufgetreten „1“), daraus wurden vier Annotationsgruppen in Bezug auf die KS-Stelle gebildet: (1) RR: MÜ ist vor und nach dem Einsatz der KS-Regel fehlerfrei; (2) FF: MÜ beinhaltet vor und nach dem Einsatz der KS-Regel Fehler; (3) RF: MÜ ist nur vor dem Einsatz der KS-Regel fehlerfrei; (4) FR: MÜ ist nur nach dem Einsatz der KS-Regel fehlerfrei. Für die Definition der KS-Stelle siehe \sectref{sec:4.4.2.1}.}} gebildet, nach denen zwischen fehlerfreien und fehlerhaften MÜ vor und nach dem Einsatz der KS-Regel unterschieden wird. Diese quantitativen und qualitativen Analysen der Fehlerannotation zeigen zwar eine Tendenz eines potenziellen Anstiegs bzw. Abstiegs der Qualität an, jedoch kann auf deren Basis keine klare Aussage zu der Qualität getroffen werden. Dies konnte erst in der zweiten Phase durch die Humanevaluation (siehe \sectref{sec:4.4.5.4}) anhand der quantitativen Daten (Qualitätsbewertung auf Likert-Skalen in den beiden Szenarien) sowie weiterer qualitativer Daten (Angabe und Erläuterung der Qualitätskriterien) aufgedeckt werden. Denn die Humanevaluation lieferte zum einen eine direkte Bewertung der Qualität des MÜ-Outputs und zum anderen konnte der Zusammenhang zwischen den Fehlertypen (aus der ersten Phase) und der Qualität quantitativ ermittelt und qualitativ anhand der Erläuterungen der Bewerter näher beleuchtet werden. Ein weiterer Output der zweiten Phase stellte eine Alternativübersetzung dar, die die Übersetzer für jede stilistisch bzw. inhaltlich kritisierte oder fehlerhafte MÜ vorschlugen. Die dritte Phase baut weiter auf der zweiten Phase auf, indem die vorgeschlagenen Alternativübersetzungen zur Qualitätsbewertung mittels zwei automatischen Evaluationsmetriken „TERbase“ und „hLEPOR“ (vor vs. nach dem Einsatz der KS-Regel) herangezogen wurden (siehe \sectref{sec:4.4.6.5}). Darüber hinaus wurde die Korrelation zwischen den Differenzen der AEM-Scores von TERbase und hLEPOR und der Differenz der Qualität in den beiden Szenarien untersucht, um herauszufinden, inwiefern die Ergebnisse der beiden Methoden übereinstimmen (d.~h. ob ein Anstieg der Qualität gleichzeitig mit besseren AEM-Scores vorkam bzw. ein Abstieg der Qualität mit schlechteren AEM-Scores einherging). Auf diese Weise wurde der Mixed-Methods-Ansatz in dieser Studie eingesetzt, um die Forschungsfrage behandeln zu können.

Denzin unterscheidet zwischen einer methodeninternen Triangulation („within method triangulation“) und einer methodenexternen Triangulation („between method triangulation“) \citep[47]{Kuckartz2014}. Neben der methodenexternen Triangulation zwischen den drei genannten Methoden wurde bei der Humanevaluation eine methodeninterne Triangulation angewendet (\figref{fig:4:8}). Hierbei bestand die Aufgabe\footnote{{{{Für eine genaue Beschreibung der „Testaufgaben“ siehe „Darstellung der Evaluation“ unter \sectref{sec:4.4.5.2}.}}}} darin, für die Übersetzungsqualität eine Punktzahl auf der Likert-Skala zu vergeben, nachdem der Teilnehmer die relevanten Qualitätskriterien angekreuzt und kurz kommentiert bzw. die Alternativübersetzung vorgeschlagen hatte. Durch den Einsatz von mehreren Techniken innerhalb der Humanevaluation konnte die interne Konsistenz erhalten, die Zuverlässigkeit optimiert und die Daten genauer interpretiert werden.

Zusammenfassend zeigt die obige kurze Darstellung der angewandten Methoden, wie die durchgeführten qualitativen und quantitativen Analysen sich bei der Erarbeitung der Forschungsfragen gegenseitig ergänzten. Anhand der qualitativen Daten und Analysen konnten die quantitativen Daten erörtert bzw. interpretiert werden. Gleichzeitig wurden die quantitativen Daten auf Basis der statistischen Tests validiert. Auf diese Weise verleihen die quantitativen Analysen der Studie die objektive Validität, die nur mit qualitativen Analysen nicht gegeben wäre. Zudem ermöglichte eine Triangulation der verschiedenen Methoden die Ergebnisse zu untermauern und gleichzeitig Widersprüche zu einer weiteren Untersuchung aufzudecken.

\section{\label{sec:4.2}Operationalisierung}

Durch die Operationalisierung beschreibt der Forscher die beobachtbaren Merkmale des untersuchten Konzepts (\citealt{FreyEtAl1991}: 95). Genau genommen bezieht sich die Operationalisierung auf „the operations involved in measuring the dependent variable“ (\citealt{SaldanhaOBrien2014}: 24). Bei der Operationalisierung müssen drei Kriterien erfüllt sein (\citealt{FreyEtAl1991}: 95): \textit{Adäquatheit} durch eine umfassende Beschreibung des untersuchten Konzepts, \textit{Exaktheit} i. S. v. allgemein anerkannt und \textit{Klarheit} für den Leser. Die Operationalisierung betrifft nicht nur quantitative Forschungsansätze, sondern ist gleichermaßen bedeutsam für qualitative Ansätze (\citealt{SaldanhaOBrien2014}: 24).

Als abhängige Variable steht die MÜ-Qualität in der vorliegenden evaluativen Studie im Mittelpunkt. Sie wird umfassend auf Basis dreier bewährten Evaluationsmethoden anhand einer Reihe von klar definierten quantitativen und qualitativen Messwerten, die im Bereich der MÜ-Evaluation allgemein anerkannt sind, gemessen. Der Einsatz einer Fehlertypologie ist ein herkömmlicher Ansatz zur Qualitätsbewertung, nicht nur bei der MÜ, sondern ursprünglich auch in der Translationswissenschaft (vgl. \citealt{VilarEtAl2006}; \citealt{SaldanhaOBrien2014}: 101). Die Humanevaluation sowie die automatische Evaluation sind die klassischen Methoden der Qualitätsbewertung in der MÜ (siehe \sectref{sec:3.3.3.1} und \sectref{sec:3.3.3.2}). Die Stil- und Inhaltsqualität wurden in Anlehnung an Hutchins und Somers definiert (siehe \sectref{sec:4.4.5.1}). Der Vergleich der Qualität des MÜ-Outputs vor vs. nach dem Einsatz der einzelnen KS-Regeln erfolgte auf Grundlage der nachstehenden qualitativen und quantitativen Daten (\tabref{tab:4:2}):


\begin{table}
\begin{tabularx}{\textwidth}{Qp{.3\textwidth}}
\lsptoprule
{\textbf{Messwerte}} & {\textbf{Evaluationsmethoden}}\\
\midrule
{{\textbullet} Vergleich der Fehleranzahl im Allgemeinen}  & {auf Basis der Fehleran-}\\
{\textbullet} Vergleich der Fehlertypen & notation\\
%\hhline%%replace by cmidrule{-~}
{\textbullet} Vergleich der Fehleranzahl jedes Fehlertyps & \\
%\hhline%%replace by cmidrule{-~}
{\textbullet} Analyse der Aufteilung der Annotationsgruppen & \\
\midrule
{{\textbullet} Vergleich der Stil- und Inhaltsqualität (5er-Likert-Skala)} & {auf Basis der Humanevaluation}\\
{\textbullet} Analyse der beeinflussten Qualitätskriterien (Checkboxen) & \\
%\hhline%%replace by cmidrule{-~}
{\textbullet} Analyse der Korrelation zwischen den Fehlertypen und der Stil- und Inhaltsqualität & \\
%\hhline%%replace by cmidrule{-~}
{\textbullet} Analyse der Qualität jeder Annotationsgruppe & \\
\midrule
{{\textbullet} Vergleich der AEM-Scores} & {auf Basis der automati-}\\
%\hhline%%replace by cmidrule{-~}
{\textbullet} Analyse der Korrelation zwischen den AEM-Scores und der Stil- und Inhaltsqualität & schen Evaluation\\
\lspbottomrule
\end{tabularx}
\caption{\label{tab:4:2}Überblick der analysierten Daten vor und nach dem KS-Einsatz bei den angewandten Methoden}
\end{table}

Nach dieser Darstellung der Forschungsmethodik und der Operationalisierung richten wir nachfolgend den Blick auf die Validität, das Studiendesign sowie seine Details bei den einzelnen Evaluationsmethoden.

\section{\label{sec:4.3}Validität}

Durch die Validität strebt der Forscher an, die Genauigkeit seiner Schlussfolgerungen (sog. Interne Validität) und die Generalisierbarkeit seiner Ergebnisse (sog. Externe Validität) sicherzustellen \citep{FreyEtAl1991}. Die \textit{interne Validität} kann durch drei Quellen gefährdet werden: durch den Forscher (d. h. durch seinen „personal attribute effect“ bzw. „unintentional expectancy effect“), die Studienvorgehensweise (i. S. v. die angewandten Verfahren im Allgemeinen und während der Datenanalyse) und die Studienteilnehmer (bzgl. ihrer Auswahl, ihres Verhaltens und der Entwicklung dieses Verhaltens im Laufe der Studie sowie bzgl. eines potenziellen gegenseitigen Einflusses). Die \textit{externe Validität} kann von drei Faktoren beeinflusst werden: dem Sampling, der ökologischen Validität und der Forderung nach einer Replikation. (ebd.: 125ff.) Im Folgenden wird erläutert, wie die genannten Aspekte der internen und externen Validität in der Studie bei der Erstellung des Datensatzes sowie der Durchführung der Analysen berücksichtigt wurden.

\textit{Für die Erstellung des Datensatzes} wurde ein Korpus aus zehn technischen Dokumenten verschiedener Unternehmen erstellt. Die Ausgangssätze, die einen Verstoß gegen eine/mehrere der analysierten KS-Regeln aufwiesen, wurden automatisch mithilfe des CL-Checkers CLAT\footnote{\url{http://www.iai-sb.de/de/produkte/clat}{{{ [abgerufen am 23.12.2014]}}}} \citep{Rösener2010} identifiziert. Aus den identifizierten Sätzen wählte die Forscherin die Testsätze nach klar definierten Kriterien unter Berücksichtigung einer möglichst ausgewogenen Aufteilung aus allen Quellen aus. Die Aufbereitung der Ausgangssätze und der Einsatz der KS-Regeln wurde von der Forscherin nach einem vordefinierten Muster durchgeführt. Es folgte eine Qualitätssicherung, in der über zwei Schritte die Qualität der Ausgangssätze vor und nach dem Einsatz der KS-Regeln orthografisch, grammatisch und stilistisch von einer erfahrenen Übersetzerin und zwei professionellen Linguisten geprüft wurde. Wurde ein Satz von den Prüfern kritisiert, so wurde er durch einen neuen Satz ersetzt, der ebenfalls geprüft wurde. Für die genaueren Details zur Erstellung und Aufbereitung des Datensatzes siehe \sectref{sec:4.4.3.1}.

Bezüglich der Validität der Selektion der untersuchten Regeln wurden diese nach vordefinierten Auswahlkriterien (siehe \sectref{sec:4.4.2.1}) festgelegt. Nicht alle analysierten Regeln haben nach \citet{tekom2013} die Übersetzbarkeit im Fokus; einige Regeln zielen auf die Lesbarkeit bzw. die Verständlichkeit ab. Jedoch zeigen die weiteren zitierten Studien (siehe \sectref{sec:4.4.2.2}), dass die analysierten Regeln durch die Reduzierung der Satzkomplexität, Vereinfachung der Satzstruktur bzw. Verminderung der Ambiguität die maschinelle Übersetzbarkeit -- bei den früheren MÜ-Ansätzen -- verbessern (vgl. \citealt{BernthGdaniec2001}; \citealt{Reuther2003}; \citealt{Siegel2011}; \citealt{Congree2018}). Auf diesem Wege tragen auch Regeln, die die Verständlichkeit in den Mittelpunkt stellen, indirekt zur Verbesserung der Übersetzbarkeit bei. Diese allgemeine Wirkung der KS-Regeln auf die MÜ wurde von \citet[53]{FiedererO’Brien2009} bestätigt und wird im Rahmen dieser Studie angesichts der Entwicklung des jüngsten MÜ-Ansatzes der NMÜ und seines vielversprechenden flüssigen Outputs getestet.

\textit{Die Fehlerannotation} wurde von einem in DE-EN vereidigten Übersetzer mit sechs Jahren Berufserfahrung durchgeführt. Anschließend wurden die annotierten Fehler von zwei professionellen Linguisten geprüft. Aufgrund der großen Anzahl der MÜ-Sätze (2.160 Sätze) prüfte jeder Linguist die Hälfte der MÜ-Sätze. Im Fall, dass er der Annotation nicht zustimmte, musste er die Übersetzung ebenfalls annotieren. Der zweite Linguist prüfte im Anschluss beide Annotationen und entschied sich für eine davon. Für die Details der Fehlerannotation siehe \sectref{sec:4.4.4}.

\textit{In der Humanevaluation} wurden die Teilnehmer nach einem „purposive sampling“ ausgewählt (vgl. \citealt{O’Brien2006}: 127), d. h. sie mussten bestimmte Kriterien in Bezug auf die Sprachkompetenzen und die translatorischen Qualifikationen (siehe \sectref{sec:4.4.5.3}) erfüllen. Die Evaluation wurde von einer diversifizierten Teilnehmergruppe von acht (vier weiblichen und vier männlichen)  qualifizierten Übersetzern mit Englisch als Muttersprache aus zwei Ländern (zwei aus England und sechs aus den USA) durchgeführt. Gleichzeitig war die Teilnehmergruppe weitgehend homogen in Bezug auf die Teilnehmerqualifikation und -erfahrung. Alle Teilnehmer sind semiprofessionelle Übersetzer (\citealt{Jääskeläinen1993}: 99f.). Sie besaßen bereits einen Bachelorabschluss im Fach Translation und befanden sich im letzten Semester des Masterstudiengangs Translation. Die Übersetzungserfahrung von sieben Teilnehmern variierte zwischen einem und zwei Jahren. Die letzte Teilnehmerin hatte vier Jahre Erfahrung als Übersetzerin in Teilzeit. Ursprünglich fiel die Entscheidung zwischen professionellen und semiprofessionellen Übersetzern aufgrund der eingeschränkten Verfügbarkeit und der hohen Kosten von professionellen Übersetzern auf semiprofessionelle. Im Nachhinein zeigte der Post-Test, dass die Einstellung der Teilnehmer zur MÜ grundsätzlich als neutral eingestuft werden kann: Sechs Teilnehmer verwenden MÜ-Systeme. Die übrigen zwei beschäftigen sich überwiegend mit literarischen Übersetzungen und verwenden die MÜ selten. Professionelle Übersetzer haben nicht selten eine kritische Einstellung zur MÜ (vgl. \citealt{O’Brien2006}: 126). Da die Studie sich nicht mit der MÜ-Qualität an sich beschäftigt, sondern gezielt mit dem Effekt der KS auf die MÜ, könnte eine allgemein negative Einstellung zur MÜ für die Analyse nicht zielführend sein bzw. sich negativ auf das Ergebnis auswirken. Somit erwies sich die Entscheidung, semiprofessionelle Übersetzer einzustellen, als sinnvoll. Ferner wurde zur Förderung der Teilnehmermotivation die Teilnahme vergütet.

Alle Interessenten hatten die gleiche Chance an der Studie teilzunehmen. Sobald ein Interessent, der die Auswahlkriterien erfüllte, die Forscherin kontaktierte, wurde er in die Studie aufgenommen. Die Qualitätsdefinitionen wurden klar dargestellt und die Testanweisungen wurden mithilfe von Beispielen erläutert (siehe Anhang~\ref{app:1}). Alle Teilnehmer erhielten zuerst einen Probetest, mit dessen Hilfe sichergestellt wurde, dass sie mit dem Testablauf, -aufbau bzw. den Testanweisungen vertraut sind. Alle Rückfragen wurden vor Beginn der Testphase ohne jeglichen Hinweis zum Ziel und Hintergrund der Studie geklärt. Jeder Teilnehmer durfte den Test zu einer für ihn passenden Uhrzeit und ortsflexibel durchführen, sofern er konzentriert und ohne Unterbrechung arbeiten konnte. Diese Flexibilität bzw. die Durchführung ohne Anbindung der Teilnehmer an einen fremden Ort mit vorgegebenen kontrollierten Settings trägt zur Erhöhung der ökologischen Validität bei.

Eine Teilnahmevoraussetzung bestand darin, dass der Teilnehmer mindestens einen Test pro Tag durchführt, um eine Unterbrechung zu vermeiden, die ggf. sein Verhalten und somit das Intrarater-Agreement hätte beeinflussen können. Dadurch, dass die Evaluation während der Semesterferien stattfand, waren die Teilnehmer nicht an der Universität. Die Mehrheit von ihnen war in ihrer Heimat. Dies schließt entsprechend das Risiko eines „intersubject bias“ weitgehend aus (vgl. \citealt{O’Brien2006}: 127), d. h. die Möglichkeit, dass die Teilnehmer sich während der Testphase austauschen und entsprechend gegenseitig beeinflussen.

Die MÜ-Sätze aller Systeme vor und nach dem KS-Einsatz wurden randomisiert auf 44 Tests aufgeteilt. Zudem erhielten die Teilnehmer die Tests in unterschiedlicher Reihenfolge. Diese doppelte Randomisierung trug dazu bei, zwei Risiken zu minimieren: erstens, das Risiko einer „Maturation“, d.~h. „sentences they graded later in the cycle will get a different look than the ones they graded earlier“ \citep[209]{White2003}; zweitens, dass Qualitätsbewertungen zwischen benachbarten Sätzen so unabhängig wie möglich gehalten werden (vgl. \citealt{Hamon2007}).\footnote{{{{Beide Risiken sind ausführlich unter „Ablauf der Evaluation“ (\sectref{sec:4.4.5.2}) erläutert.} }}} Die Tests wurden in Form von Excel-Tabellen erstellt (\figref{fig:4:9}), einem Programm, mit dem alle Teilnehmer vertraut sind. Dies unterstützt die ökologische Validität. Zur Sicherstellung der Vollständigkeit der Daten war der Testablauf so gestaltet, dass der Teilnehmer am Ende des Tages die bewerteten Tests abgibt. Sie wurden sofort auf Vollständigkeit geprüft. Anschließend erhielt er die Tests des folgenden Tages.

Die Qualitätsdefinitionen (\citealt{HutchinsSomers1992}, siehe \sectref{sec:4.4.5.1}) wurden in den Qualitätskriterien integriert (Abschnitt [3a] und [3b] in \figref{fig:4:8}), d. h. nicht nur in Form von Definitionen am Anfang des Tests zur Verfügung gestellt, wie es in Evaluationsstudien typisch ist. Die Teilnehmer wurden aufgefordert, die zutreffenden Qualitätskriterien anzukreuzen, zu kommentieren bzw. posteditieren und anschließend einen Score zu vergeben. Somit hatten alle Teilnehmer eine direkte und einheitliche Basis für die Vergabe der Qualität-Scores, die sicherlich zu den hohen Intrarater- und Interrater-Agreements beiträgt.

Ferner wurden die Ausgangssätze den Teilnehmern bei der Evaluation zur Verfügung gestellt. Da der Test zum Teil eine PE-Aufgabe beinhaltet und die Norm im PE lautet, dass dem Post-Editor der Ausgangstext zur Verfügung steht (\citealt{O’Brien2006}: 130), unterstützt die Darstellung der Ausgangssätze die ökologische Validität (ebd.).

Da der Fokus der Studie darin liegt, einen potenziellen Einfluss der einzelnen KS-Regeln zu untersuchen, war es als erstes erforderlich, die Stelle in der MÜ zu ermitteln, die durch den Einsatz der KS-Regel beeinflusst werden konnte (bezeichnet als die „KS-Stelle“).\footnote{{{{In der Studie bezeichnet die „KS-Stelle“ den Teil des Ausgangssatzes, der bei dem Einsatz der KS-Regel modifiziert werden muss, und sein Äquivalent im Zielsatz.}}} } Fehler in der MÜ, die außerhalb der KS-Stelle lagen, mussten vor der Humanevaluation korrigiert werden, um sicherzustellen, dass der Unterschied in den beiden Versionen „vor KS“ und „nach KS“ nur an der KS-Stelle auftritt. Dieser Schritt wurde von der Forscherin durchgeführt. Anschließend wurde die Qualität der MÜ-Sätze nach der Korrektur der Fehler außerhalb der KS-Stelle über zwei Phasen von drei qualifizierten erfahrenen Übersetzern auf potenzielle Fehler sowie stilistische Akzeptanz außerhalb der KS-Stelle geprüft. Wurde eine MÜ kritisiert, so wurde sie von der Humanevaluation ausgeschlossen (Genaueres dazu unter \sectref{sec:4.4.3.1} Schritt [10]).

Ferner wurde darauf geachtet, dass die MÜ-Sätze in Bezug auf die Existenz bzw. Nicht-Existenz von Fehlern in der Humanevaluation in ähnlichem Verhältnis wie in der Annotation repräsentiert sind. Dieser Schritt ist für die Datenanalyse essentiell, da eine Unausgewogenheit zur Verzerrung der Ergebnisse der Humanevaluation führen würde. Die Ausgewogenheit der MÜ-Sätze wurde nach definierten Kriterien realisiert (siehe \sectref{sec:4.4.3.2}).

\largerpage
In Bezug auf den unterschiedlichen Prozentsatz der analysierten MÜ-Sätze aus jedem System (\figref{fig:4:7}): Da der NMÜ-Output eine sehr geringe Anzahl von Fehlern enthielt und die Auswahlkriterien für MÜ-Sätze diejenigen Sätze für die Humanevaluation ausschlossen, die eine hohe Anzahl von Fehlern enthielten (siehe Kriterien in [8] unter \sectref{sec:4.4.3.1}), wurde eine größere Anzahl von NMÜ-Sätzen im Vergleich zu den Sätzen anderer Systeme ausgewertet. Obwohl die Bewertung einer gleichen Anzahl von Sätzen jedes MÜ-Systems aus Gründen der Unverzerrtheit als ideal betrachtet werden kann, könnte dies nur durch eine Reduktion der Anzahl der analysierten NMÜ-Sätze realisiert werden. Für dieses Vorgehen wäre es wichtig gewesen, konkrete Ausschlusskriterien für die Reduzierung der Anzahl von NMÜ-Sätzen zu definieren. Bei solchen Ausschlusskriterien wären drei Szenarien denkbar: (1) Ausschluss fehlerhafter NMÜ-Sätze, was zu deutlich besseren Ergebnissen bei Google Translate als die aktuellen Ergebnisse führen würde; (2) Ausschluss fehlerfreier NMÜ-Sätze. Letzteres wäre ein subjektives Vorgehen, da es mit den anderen Systemen nicht realisierbar wäre. Das letzte mögliche Szenario wäre ein (3) Ausschluss einer Mischung aus fehlerfreien und fehlerhaften NMÜ-Sätzen. Das würde jedoch erneut die Frage nach den anzuwendenden Ausschlusskriterien aufwerfen. Aus diesem Grund war der Prozentsatz der analysierten NMÜ-Sätze ein wenig höher als der der anderen Systeme.

\textit{Die automatische Evaluation} wurde zur Erhöhung der Objektivität mithilfe zweier Metriken gemessen. Basis der Messung war die Verwendung von Referenzübersetzungen, die von den Teilnehmern der Humanevaluation angegeben wurden. Zur Berücksichtigung von unterschiedlichen PE-Möglichkeiten wurden zwei Referenzübersetzungen pro MÜ-Satz verwendet. Bei der Auswahl der zwei Referenzübersetzungen wurde darauf geachtet, dass die Referenzübersetzungen möglichst von allen Teilnehmern gleichermaßen verwendet werden (\sectref{sec:4.4.6.2}).

Wie von \citet{FreyEtAl1991} empfohlen, wurden mehrere Testläufe (Pilottests) zur Reduzierung von Messfehlern durchgeführt. Konkret wurden Testläufe bei der Entscheidung zur Darstellung/Nichtdarstellung der Ausgangssätze und Markierung/Nichtmarkierung der KS-Stelle während der Evaluation, Prüfung der Klarheit der Testanweisungen bzw. -aufgaben, Festlegung der Anzahl der Sätze pro Test sowie Qualitätssicherung der Ausgangs- und Zielsätze durchgeführt. In Bezug auf die Objektivität umfassten die drei implementierten Methoden eine große Anzahl an quantitativen Messungen. Bei den Analysen wurden die Daten systematisch und regelmäßig in definierten Excel-Tabellen aufgezeichnet und innerhalb kurzer Zeit dokumentiert. Gleichzeitig wurden die quantitativen Messungen durch die Triangulation -- wie in den vorherigen Abschnitten dargestellt -- mit qualitativen Analysen kombiniert. Die Triangulation erhöht wiederum die Messvalidität (measurement validity) (ebd.: 124). Um eine Replikation zu ermöglichen, strebte die Forscherin danach, die Schritte der Datenerhebung, -aufbereitung, -analyse sowie die methodische Vorgehensweise zusammen mit der Begründung sämtlicher getroffenen Entscheidungen im folgenden Abschnitt ausführlich zu dokumentieren.

\section{\label{sec:4.4}Studiendesign}
\largerpage
\subsection{Auswahl des analysierten Sprachenpaars und der MÜ-Systeme}
\label{sec:4.4.1}
Das Ziel der Studie besteht darin, den Einfluss des Einsatzes einzelner KS-Regeln auf die Qualität des MÜ-Outputs bei verschiedenen MÜ-Ansätzen zu untersuchen. Die vier bekannten MÜ-Ansätze (RBMÜ, SMÜ, HMÜ und NMÜ) wurden durch die Analyse von fünf MÜ-Systemen analysiert. Somit hat die Studie drei Dimensionen: die KS-Regeln, die MÜ-Systeme und das Sprachenpaar. Für den einzelnen Vergleich der neuen Regeln und der fünf Systeme wurde die Sprachenpaarvariable auf Deutsch > Englisch fixiert. Das analysierte Sprachenpaar ist für international agierende Unternehmen aus deutschsprachigen Ländern von großer Relevanz. Innerhalb der Europäischen Union fordert die Maschinenrichtlinie 2006/42/EG, dass alle schriftlichen und verbalen Informationen und Warnhinweise in der Amtssprache des EU-Mitgliedstaates beiliegen müssen, in der das Produkt in Verkehr gebracht bzw. in Betrieb genommen wird (\citealt{tekomRGAlbDonau2010}). Dementsprechend ist eine englische Übersetzung der Betriebsanleitung für Irland, England und Malta erforderlich. Des Weiteren stellen weitere englischsprachige Länder außerhalb der EU große Märkte für Produkte aus dem deutschsprachigen Raum dar; z. B. waren die USA laut einer aktuellen Statistik nach Exportwert der wichtigste Handelspartner Deutschlands im Jahr 2018 \citep{Statista2019}. Mit diesen Dimensionen und anhand eines angemessen großen Datensatzes war der Umfang der Studie gut zu bewältigen. Eine Erweiterung der Studie auf ein zweites Sprachenpaar wäre aus Zeit- und Kostengründen schwer realisierbar. Die Untersuchung von zwei Sprachenpaaren mit kleineren Datensätzen könnte für die Durchführung einer zuverlässigen statistischen Analyse nicht ausreichend sein. Die Untersuchung weiterer Sprachenpaare ist somit erstrebenswert (siehe \sectref{sec:7.3}).

Die fünf untersuchten MÜ-Systeme bestanden aus einem RBMÜ-System, einem SMÜ-System, zwei Hybridsystemen sowie einem NMÜ-System. Die Auswahlkriterien der fünf MÜ-Systeme waren, dass sie (1) vom Deutschen ins Englischen übersetzen, (2) kostenlos zugänglich sind und (3) alle MÜ-Ansätze abdecken. Die empirische Studie wurde Ende 2016 durchgeführt. Zu dieser Zeit war Google Translate das erste bzw. einzige online zugängliche NMÜ-System. Da Google Translate ein generisches Black-Box-System ist, mussten alle anderen MÜ-Systeme, um eine einheitliche Untersuchungsbasis zu gewährleisten, ebenfalls generische Black-Box-Systeme sein.\footnote{{Für den Umgang mit den spezifischen Termini im Rahmen der Studie siehe Schritt [4] unter \sectref{sec:4.4.3.1}.} } Ein Black-Box-System wird wie folgt definiert: „a system which has been trained and tuned a priori and for which we cannot access the model parameters or training data for fine-tuning or improvements” (\citealt{MehtaEtAl2020}: 1).\footnote{{Mehr zum Thema Black-Box- vs. Glas-Box-Evaluation unter \sectref{sec:3.3.2}.} } Da die Hybridsysteme unterschiedlich aufgebaut werden, wurden zwei Hybridsysteme in der Studie verwendet. Folgende MÜ-Systeme wurden verwendet:\footnote{Die obige Beschreibung der Systeme schildert die Systemarchitekturen gemäß ihrem Stand zum Zeitpunkt der Durchführung der Studie (Ende 2016 -- Anfang 2017). In der Zwischenzeit wurden die Systeme weiterentwickelt. 2017 bzw. 2018 kamen die neuronalen Systeme von Systran, SDL und Bing auf den Markt:
\url{http://www.systransoft.com/systran/translation-technology/pure-neural-machine-translation};
\url{https://www.sdl.com/ de/about/news-media/press/2018/sdls-neural-machine-translation-sets-new-industry-standards-with-state-of-the-art-dictionary-and-image-translation-features.html};
\url{https://www.microsoft.com/dede/translator/blog/2018/11/14/nextgennmt}
[abgerufen am 16.04.2019]}

\begin{itemize}
\item Das regelbasierte System \textit{Lucy LT KWIK Translator} von Lucy Software and Services GmbH.\footnote{online zugänglich unter:
\url{http://www.lucysoftware.com/english/machine-translation/lucy-lt-kwik-translator-}} Lucy LT ist das Nachfolgesystem des alten METAL MT Systems (\citealt{MartinSerra2014}). Nach Alonso und Thurmair ist Lucy LT „a commercial rule-based machine translation system with sophisticated hand-written transfer and generation rules“ \citep{AvramidisEtAl2014}.
\item Das statistische System \textit{SDL Free Translation}.\footnote{online zugänglich unter:
\url{https://www.freetranslation.com/de}} Laut SDL arbeitet das System nach einem rein statistischen Ansatz.\footnote{„What Is Machine Translation and How Does It Work?“ \url{https://sdl.uservoice.com/knowledgebase/articles/256030-what-is-machine-translation-and-how-does-it-work.} [abgerufen am 06.12.2016].}
\item Die Hybridsysteme \textit{Bing} von Microsoft\footnote{online zugänglich unter:
\url{https://www.bing.com/translator}} und \textit{Systran}.\footnote{online zugänglich unter:
\url{http://www.systranet.com/translate}} Bing ist ein „statistisches MÜ-System mit sprachspezifischen Regelkomponenten für das Zerlegen und Zusammensetzen von Sätzen“ (\citealt{WerthmannWitt2014}: 84), wobei Systran ursprünglich ein regelbasiertes System war und in den letzten Jahren zu einem hybriden System weiterentwickelt wurde (ebd.).
\item Das neuronale System \textit{Google Translate}.\footnote{online zugänglich unter: \url{https://translate.google.de}.} Die Übersetzung auf Basis von neuronalen Netzen ist der aktuellste Ansatz der MÜ, den Google Ende 2016 mit der Umstellung seines Systems in Betrieb setzte. Das Modell von Google Translate besteht aus drei Komponenten \citep{WuEtAl2016}: einem Encoder-Netzwerk, einem Decoder-Netzwerk sowie einem Attention-Netzwerk.\footnote{{{{Mehr Details unter \sectref{sec:3.2.4}.}}}}
\end{itemize}


Der Datensatz wurde mit den fünf genannten Systemen übersetzt. Die genaue Vorgehensweise ist unter \sectref{sec:4.4.3} erläutert.

\subsection{Die analysierten KS-Regeln}
\label{sec:4.4.2}
In diesem Abschnitt wird die Auswahl der analysierten Regeln begründet und die KS-Stelle ausführlich erklärt. Daraufhin folgt eine Darstellung der analysierten Regeln zusammen mit deren Anwendungsbegründungen, Umsetzungsmustern und KS-Stellen. Zum Schluss werden die analysierten Regeln und ihre gezielten Wirkungen im Hinblick auf das untersuchte Sprachenpaar (Deutsch-Englisch) diskutiert.

\subsubsection{\label{sec:4.4.2.1}Auswahl der analysierten KS-Regeln}

Die in der vorliegenden Arbeit analysierten KS-Regeln stammen aus der Leitlinie „Regelbasiertes Schreiben. Deutsch für die Technische Kommunikation“ (\citeyear{tekom2013}) der Gesellschaft für Technische Kommunikation -- tekom~e.~V.\footnote{\url{https://www.tekom.de}} Dank einer engen Zusammenarbeit zwischen Experten aus dem Hochschulwesen, der Industrie, Dienstleistungsunternehmen sowie Softwarefirmen bieten die tekom-Re\-geln ein umfassendes Regelwerk auf sämtlichen Sprach- und Dokumentationsebenen (siehe \tabref{tab:02:1}). Die tekom-Leitlinie stellt einen branchenübergreifenden Standard für die technische Dokumentation dar. Im Mittelpunkt steht die Steigerung der Qualität und die Reduzierung der Kosten im Dokumentations- und Übersetzungsprozess (\citealt{DrewerZiegler2014}: 217f.). Die tekom-Regeln werden aktuell sowohl in der Forschung als auch in der Industrie weitestgehend umgesetzt.\footnote{{Die Tekom-Regeln sind der Kernregelsatz in zwei marktführenden CL-Checkern, nämlich Acrolinx (\url{https://www.acrolinx.de/produktuberblick}) und CLAT (vgl. \citealt{Geldbach2009}). Mehr zum CLAT und seiner Entwicklung unter \sectref{sec:2.5}.}} Aus diesen Gründen wurden in der Studie Regeln aus dieser Leitlinie untersucht. Die analysierten Regeln wurden gemäß den folgenden Kriterien ausgewählt:

\begin{enumerate}[label = {(\arabic*)}, align = left]
\item \textit{Regeln, die auf genau einen Satz angewendet werden können:}

Da die Studie den Einfluss einzelner KS-Regeln untersucht, erfolgten die Analysen auf Satzebene. Daher wurden die Regeln, die mehrere Sätze oder die Dokumentstruktur betreffen, ausgeschlossen.

\item \textit{Regeln, die in allen relevanten Sätzen nach einem festen Muster angewendet werden können:}

Da die Anwendung von verschiedenen Mustern bei dem Einsatz einer KS-Regel das Ergebnis unterschiedlich beeinflussen kann (vgl. \citealt{Roturier2006}: 74), wurde in allen relevanten Sätzen ein festes Muster angewendet. Dies ermöglichte wiederum, die Anzahl der unabhängigen Variablen in Grenzen zu halten. Die angewendeten Umsetzungsmuster bei den einzelnen Regeln sind in \sectref{sec:4.4.2.2} dargestellt.

\item \textit{Regeln, für die eine KS-Stelle definiert werden kann:}

Zur Untersuchung des Einflusses der einzelnen KS-Regeln wurde für jede Regel eine KS-Stelle definiert. Die KS-Stelle ist der Teil des Ausgangssatzes, der bei dem Einsatz der KS-Regel modifiziert werden muss, und sein Äquivalent im Zielsatz. Beispielsweise wurde die Regel „Kurze Sätze formulieren“ ausgeschlossen, da sie sich auf den ganzen Satz bezieht. Entsprechend kann dafür keine konkrete KS-Stelle definiert werden, die bei der Fehlerannotation und der Humanevaluation analysiert und verglichen werden könnte (die KS-Stelle ist unten in diesem Abschnitt ausführlich erklärt).

\end{enumerate}

Die Auswahl der Regeln aus der tekom-Leitlinie erfolgte in zwei Schritten: Im ersten Schritt [A] wurden die Regeln aus zwei definierten Gruppierungen der tekom, „Regeln für übersetzungsgerechtes Schreiben“ und „Basisregeln“, nach den obengenannten Kriterien ausgewählt. Da aus den beiden Gruppierungen der tekom nur vier KS-Regeln die Auswahlkriterien erfüllten, wurden alle KS-Regeln der tekom-Leitlinie im zweiten Schritt [B] geprüft, um weitere Regeln zu identifizieren, die die Auswahlkriterien erfüllen. Es folgt eine detaillierte Darstellung der beiden Schritte:


\paragraph*{[A]} In der aktuellen Auflage der Leitlinien von tekom findet man zwei für die Studie relevante Gruppierungen von Regeln:

\begin{description}[font=\normalfont\bfseries]
\item[Erste Gruppierung:] Regeln für übersetzungsgerechtes Schreiben

Diese Gruppierung beinhaltet die Regeln, die u.~a. für eine korrekte Verarbeitung durch Übersetzungswerkzeuge besonders relevant sind (\citealt{tekom2013}: 136f.). In der folgenden Tabelle sind alle Regeln dieser Gruppierung enthalten (linke Spalte). In den anderen Spalten wird -- zusammen mit der Begründung -- erwähnt, welche Regeln dieser Gruppe analysiert bzw. ausgeschlossen wurden:
\end{description}

\noindent\framebox[\textwidth]{\parbox[t]{.45\textwidth}{\raggedright\bfseries Übersetzungsgerechtes Schreiben}\parbox[t]{.5\textwidth}{\raggedright\bfseries Wurde die Regel analysiert? Wenn nicht, warum?}}
\acceptbox{S 102 eindeutige pronominale Bezüge verwenden}{S 102 eindeutige pronominale Bezüge verwenden}
\acceptbox{S 204 keine Wortteile weglassen}{S 204 keine Wortteile weglassen}
\rejectbox{S 101 pronominale Bezüge über Satzgrenzen vermeiden}{Regel bezieht sich auf mehrere Sätze (erfüllt das 1. Auswahlkriterium nicht).}
\rejectbox{S 202 keine Sätze ohne Verb formulieren}{Dieser Verstoß kam im Korpus der Bedienungsanleitungen selten vor.

Bsp.: Kühler defekt?}
\rejectbox{S 306 Aufzählungen als Listen darstellen

S 307 Satz nicht durch eine Liste unterbrechen}{Die Regel bezieht sich in manchen Fällen zwar auf einen Satz (d.~h. erfüllt das 1. Auswahlkriterium), jedoch geht der Satz über mehrere Zeilen. Dies würde die Humanevaluation erschweren, da die Probanden leicht von Stellen außerhalb der KS-Stelle abgelenkt werden können.}
\rejectbox{S 309 Klammereinschübe vermeiden}{Mehrere Umformulierungen sind möglich; meist durch die Bildung von zwei Sätzen (erfüllt das 2. Auswahlkriterium nicht).}
\rejectbox{Z 114 Verwendung von umbruchgeschützten Leerzeichen festlegen}{Dieser Verstoß kam im Korpus der Bedienungsanleitungen sehr selten vor.}

\begin{description}[font=\normalfont\bfseries]
\item[Zweite Gruppierung:] Basisregeln

Diese Gruppe bildet das Ergebnis zweier Umfragen ab, mit dem Ziel der Ermittlung der Regeln, die unter den tekom-Experten die größte Akzeptanz finden \citep[144]{tekom2013}. In der folgenden Tabelle sind alle Regeln dieser Gruppierung enthalten (linke Spalte). In den anderen Spalten wird -- zusammen mit der Begründung -- erwähnt, welche Regeln dieser Gruppe analysiert bzw. ausgeschlossen wurden:
\end{description}

\noindent\framebox[\textwidth]{\parbox[t]{.45\textwidth}{\raggedright\bfseries Basisregeln}\parbox[t]{.5\textwidth}{\raggedright\bfseries Wurde die Regel analysiert?}}
\acceptbox{S 102 eindeutige pronominale Bezüge verwenden}{S 102 eindeutige pronominale Bezüge}
\acceptbox{S 201 Bedingungen mit ``Wenn'' formulieren}{S 201 Bedingungen mit ``Wenn'' formulieren}
\acceptbox{S 504: Passiv in bestimmten Informationseinheiten vermeiden}{S 504: Passiv in bestimmten Informationseinheiten vermeiden (Anweisungen, Sicherheits- und Warnhinweise)}
\rejectbox{T 101: einheitliche Überschriften definieren}{Regel bezieht sich auf ein komplettes Dokument (erfüllt das 1. Auswahlkriterium nicht).}
\rejectbox{S 103: missverständliche Genetivkonstruktionen vermeiden}{Einsatz der Regel kann auf verschiedene Weise erfolgen (erfüllt das 2. Auswahlkriterium nicht).}
\rejectbox{S 301: Häufung von Nominalphrasen vermeiden}{Einsatz der Regel kann auf verschiedene Weise erfolgen (erfüllt das 2. Auswahlkriterium nicht).}
\rejectbox{S 302: zu lange Sätze vermeiden}{Das Kürzen kann auf verschiedene Weise erfolgen (erfüllt das 2. Auswahlkriterium nicht).}
\rejectbox{S 304: Häufung von Präpositionalphrasen vermeiden}{Einsatz der Regel kann auf verschiedene Weise erfolgen (erfüllt das 2. Auswahlkriterium nicht).}
\rejectbox{S306: Aufzählungen als Listen darstellen

S307: Satz nicht durch eine Liste unterbrechen}{Diese Regeln können sich zwar auf einen Satz beziehen (1. Auswahlkriterium), jedoch geht der Satz über mehrere Zeilen. Dies würde die Humanevaluation erschweren, da die Probanden leicht von Stellen außerhalb der KS-Stelle abgelenkt werden können.}
\rejectbox{S 311: Häufung von Nebensätzen vermeiden}{Einsatz der Regel kann auf verschiedene Weise erfolgen (erfüllt das 2. Auswahlkriterium nicht).}
\rejectbox{S 401: Sachlogische Reihenfolge einhalten}{Einsatz der Regel kann auf verschiedene Weise erfolgen (erfüllt das 2. Auswahlkriterium nicht).}
\rejectbox{S 505: Nominalisierungen vermeiden}{Satzstruktur muss -- je nach Satz -- angepasst werden; KS-Stelle ist schwer abzugrenzen (erfüllt das 3. Auswahlkriterium nicht).}
\rejectbox{S 510: einheitliche Satzmuster}{Regel bezieht sich auf mehrere Sätze (erfüllt das 1. Auswahlkriterium nicht).}
\rejectbox{B 101: Komposita aus zwei Basismorphem immer ohne Bindestrich}{Relevante Fälle waren im Korpus der Bedienungsanleitungen selten vertreten.}

\paragraph*{[B]} Aus den obengenannten Gruppierungen haben vier KS-Regeln die Auswahlkriterien erfüllt. Daher wurden alle KS-Regeln der tekom-Leitlinie genauer geprüft, um weitere Regeln für die Analyse zu identifizieren. In der tekom-Leitlinie sind die Regeln wie folgt gegliedert:

\medskip
\noindent\framebox[\textwidth]{\parbox[t]{.45\textwidth}{\raggedright\bfseries Regeln}\parbox[t]{.5\textwidth}{\raggedright\bfseries Wurde die Regel analysiert?}}
\rejectbox{\textbf{Regeln zur Dokumentstruktur}: Standardgliederung, einheitliche und kurze Überschriften, Indexeinträge, usw. (ebd.: 31 ff.)}{Regeln beziehen sich auf das gesamte Dokument (erfüllen das 1. Auswahlkriterium nicht).}
\rejectbox{\textbf{Regeln zur Informationsstruktur} i.~S.~v. Darstellung der Informationen (ebd.: 51 ff.)}{Allgemeine Regeln zur Darstellung von den Informationen in Tabellen, Glossaren, Listen, mit Aufzählungen usw. (erfüllen das 1. Auswahlkriterium nicht).}
\acceptbox{\textbf{Satzregeln}

Diese Regeln stellen die Hauptzielgruppe der Analyse dar.

(ebd.: 59 ff.)}{\textbf{(1) Regeln zur Vermeidung von mehrdeutigen Konstruktionen}

S 102 eindeutige pronominale Bezüge verwenden


\textbf{(2) Regeln zur Vermeidung von unvollständigen Konstruktionen}


S 201 Bedingungen als „Wenn“ –Sätze formulieren

S 204 keine Wortteile weglassen


\textbf{(3) Regeln zur Vermeidung von komplexen Konstruktionen }

S 303 Partizipialkonstruktionen vermeiden

\textbf{(4) Stilistische Regeln}


S 501: Vorgangspassiv vermeiden

S 502: Passiv mit Täterangabe vermeiden

S 503: Passiv mit Modalverben vermeiden

S 504 Passiv in bestimmten Informationseinheiten vermeiden

S 511 Konstruktionen mit „sein + zu + Infinitiv“ vermeiden}
\rejectbox{\textbf{Satzregeln}

Diese Regeln stellen die Hauptzielgruppe der Analyse dar.

(ebd.: 59 ff.)}{Die weiteren 32 Satzregeln erfüllen ein oder mehrere Auswahlkriterien nicht.}
\rejectbox{\textbf{Wortregeln}

Diese Kategorie umfasst Regeln zur Wortbildung, Abkürzungen sowie Verwendung von Benennungen und Zahlen. (ebd.: 89 ff.)}{Die Analyse der Studie erfolgt auf Satzebene; die Wortebene ist kein Bestandteil der Studie.}
\acceptbox{\textbf{Regeln zu den lexikalischen Vorgaben}

(ebd.: 106 ff.)}{L 103 Funktionsverbgefüge vermeiden

L 114 Überflüssige Präfixe vermeiden}
\rejectbox{\textbf{Regeln zu den lexikalischen Vorgaben}

(ebd.: 106 ff.)}{L 101 Keine ungenauen Verben verwenden

Ungenaue Verben kamen im Korpus der Bedienungsanleitungen selten vor.

Bsp.: machen, holen, geschehen}
\rejectbox{\textbf{Regeln zur Rechtschreibung}

(ebd.: 114)}{R 101 Einheitlichen Rechtschreibstil verwenden (erfüllt das 2. Auswahlkriterium nicht).}
\acceptbox{\textbf{Regeln zur Zeichensetzung}

(ebd.: 115 ff.)}{Z 103b: Für zitierte Oberflächentexte gerade Anführungszeichen $"$\ldots$"$ verwenden}
\rejectbox{\textbf{Regeln zur Zeichensetzung}

(ebd.: 115 ff.)}{Die weiteren Regeln erfüllen ein oder mehrere Auswahlkriterien nicht bzw. relevante Fälle waren im Korpus der Bedienungsanleitungen selten vertreten.}
\rejectbox{\textbf{Regeln zum Platzsparen beim Schreiben}

(ebd.: 129 ff.)}{Regeln wie P 101 Kurz formulieren, P 102 Kurze Wörter verwenden, P~104 Konsistenz halten usw. erfüllen ein oder mehrere Auswahlkriterien nicht bzw. beziehen sich auf Ebenen, die kein Bestandteil der vorliegenden Studie sind (z. B. Wortebene). Die Analyse der Studie erfolgt auf Satzebene.}



\medskip
\textit{Die KS-Stelle} -- Nachdem die zu analysierenden Regeln ausgewählt wurden, war es erforderlich, zu beobachten, auf welchen Teil der MÜ sie sich auswirken. Durch den Einsatz der einzelnen Regeln verändert sich der MÜ-Output sowohl semantisch als auch syntaktisch. Zu dieser Veränderung tragen mehrere Faktoren gleichzeitig bei, darunter der Ansatz des jeweiligen MÜ-Systems, die Trainingsdaten zusammen mit der Einführung der Kontrollierten Sprache. Da der Fokus der Studie auf dem Einfluss der KS-Regeln auf den MÜ-Output liegt, war es notwendig, die Stelle in der MÜ zu definieren, die direkt durch den Einsatz der KS-Regeln beeinflusst wird. Diese Stelle wird in der Studie als die \textit{KS-Stelle} bezeichnet und wurde folgendermaßen definiert:
\begin{quote}
Die KS-Stelle ist der Teil des Ausgangssatzes, der bei dem Einsatz der KS-Regel modifiziert werden muss, und sein Äquivalent im Zielsatz.
\end{quote}
Im Zielsatz ist die KS-Stelle die exakte Übersetzung der Stelle, die durch den Einsatz der KS-Regel im Ausgangssatz umformuliert wurde. In anderen Worten ist sie die \textit{kürzeste} Stelle im MÜ-Output, die durch den Einsatz der KS-Regel beeinflusst werden muss. Sollten weiteren Stellen im MÜ-Output vom KS-Einsatz beeinflusst werden, kämen vier Möglichkeiten in Betracht:

\begin{itemize}
\item Fall 1: Die KS-Stelle wurde \textit{positiv} beeinflusst und außerhalb der KS-Stelle ebenfalls \textit{positiv} beeinflusst $\to$ Ergebnis: Die KS-Regel hat einen starken \textit{positiven} Einfluss;
\item Fall 2: Die KS-Stelle wurde \textit{negativ} beeinflusst und außerhalb der KS-Stelle \textit{positiv} beeinflusst $\to$ Ergebnis: die KS-Regel hat einen \textit{negativen} Einfluss;
\item Fall 3: Die KS-Stelle wurde \textit{negativ} beeinflusst und außerhalb der KS-Stelle ebenfalls \textit{negativ} beeinflusst $\to$ Ergebnis: Die KS-Regel hat einen eindeutigen \textit{negativen} Einfluss;
\item oder Fall 4: Die KS-Stelle wurde \textit{positiv} beeinflusst und außerhalb der KS-Stelle \textit{negativ} beeinflusst $\to$ Ergebnis \textit{muss geklärt werden}.
\end{itemize}

Auf Basis dieser Möglichkeiten liefert eine Analyse der KS-Stelle in den ersten drei Fällen eine ausreichende Aussage zum Einfluss der einzelnen Regeln. Der vierte Fall wurde bei den einzelnen Regeln (d.~h. auf Regelebene) und den einzelnen Systemen (d.~h. auf MÜ-Systemebene) näher untersucht (siehe \sectref{sec:5.2.2}).

Nachdem die Sätze maschinell übersetzt wurden, wurde das Äquivalent der KS-Stelle bei jeder Regel im Zielsatz identifiziert. Die Analyse der KS-Stelle -- anhand der drei angewandten Methoden -- ermöglichte es, den Einfluss der Regeln \textit{einzeln} zu untersuchen. Im Vergleich zu Studien, in denen der allgemeine Einfluss der Kontrollierten Sprache auf den MÜ-Output untersucht wird, stellt die Untersuchung auf Regelebene eine besondere Schwierigkeit dar, denn bei einer Untersuchung des allgemeinen Einflusses der KS bewertet der Forscher den kompletten Output.

Die Idee der KS-Stelle ist ähnlich wie die der „Rich Points“, die die PACTE\footnote{{{{PACTE steht für Process in the Acquisition of Translation Competence and Evaluation \citep{PACTE2011}.}}}} Group in Anlehnung an das Konzept der „Wissenschaftlichen Ökonomie“ (Scientific Economy) von Giegler anwendet \citep{PACTE2011}. Bei den Rich Points handelt es sich um „specific source-text segments that contained ‚prototypical‘ translation problems“ (ebd.: 10). Zur Erleichterung der Datenerfassung und -analyse im Bereich der Translationskompetenz und -evaluation legt die PACTE Group bei ihrer Untersuchung zur Identifizierung und Lösung von Translationsproblemen (ebd.) den Fokus auf die „Rich Points“.

\subsubsection{\label{sec:4.4.2.2}Darstellung der analysierten KS-Regeln und ihrer gezielten Wirkung}

Dieser Abschnitt bietet eine detaillierte Darstellung der analysierten Regeln: Zuerst werden die analysierten tekom-Regeln beschrieben. Außerdem wird die Begründung der Regelanwendung laut der tekom sowie die gezielte Wirkung jeder Regel laut vorherigen Studien angegeben. Danach wird demonstriert, wie die Regel umgesetzt wurde (Umsetzungsmuster). Anschließend wird die KS-Stelle vor und nach dem KS-Einsatz spezifiziert und anhand eines Beispiels vorgestellt.
% \clearpage
% \framebox[1.05\textwidth]{\parbox{\textwidth}{
\paragraph*{Z 103b: Für zitierte Oberflächentexte gerade Anführungszeichen $"$\ldots$"$ verwenden}

Nach dieser Regel sollen Oberflächentexte, z.~B. Texte in Softwareoberflächen oder Displaytexte in Geräten, in geraden Anführungszeichen stehen (\citealt{tekom2013}: 117).

\begin{description}[font=\normalfont\bfseries]
\item[Begründung der Anwendung laut tekom:] Die Anführungszeichen erhöhen die Lesbarkeit. Im Vergleich zu der Verwendung von verschiedenen Schriftarten oder Schriftgraden sind gerade Anführungszeichen optisch nicht störend. Zudem unterstützen die Anführungszeichen eine korrekte Übersetzung. (ebd.: 118)
\item[{\parbox[t]{\textwidth}{Begründung der Anwendung bzw. die gezielte Wirkung der Regel laut vorhe-\\rigen Studien:}}] Eine Wirkung auf die MÜ ist nachvollziehbar, denn laut \citet[2]{Reuther2003}: „Punctuation marks are very sensitive with respect to all applications where linguistic processing is done automatically.”

\item[Umsetzungsmuster:]
~ \\
\textbf{Vor-KS}: Oberflächentext ohne Anführungszeichen\\
\textbf{Nach-KS}: Oberflächentext angegeben in geraden Anführungszeichen

\item[KS-Stelle:]
~ \\
\textbf{Vor-KS}: Oberflächentext ohne Anführungszeichen\\
\textbf{Nach-KS}: Oberflächentext mit geraden Anführungszeichen

\item[Beispiele:]~ \\
  \textit{Wählen Sie danach die Option \txgray{Software automatisch installieren}.}\\
  \textit{Wählen Sie danach die Option \txgray{$"$Software automatisch installieren$"$}.}\\
\end{description}
% }}
% \clearpage
\hrule
\paragraph*{L 103: Funktionsverbgefüge vermeiden}

Nach dieser Regel soll das bedeutungstragende Verb anstatt des Funktionsverbgefüges verwendet werden (\citealt{tekom2013}: 107).

\begin{description}[font=\normalfont\bfseries]
\item[Begründung der Anwendung laut tekom:] Die Verwendung des bedeutungstragenden Verbs steigert die Präzision und Direktheit der Aussage (ebd.).

\item[{\parbox[t]{\textwidth}{Begründung der Anwendung bzw. die gezielte Wirkung der Regel laut vorhe-\\rigen Studien:}}] Das Vermeiden ausdrucksschwacher Verben reduziert die Ambiguität \citep{Siegel2011}. Formulierungen wie das Funktionsverbgefüge „machen den Sachverhalt unnötig kompliziert und erschweren das Textverständnis und die Übersetzung.“ \citep[13]{Congree2018}

\item[Umsetzungsmuster:]
~ \\
\textbf{Vor-KS}: Funktionsverbgefüge\\
\textbf{Nach-KS}: Das Funktionsverbgefüge wird durch das bedeutungstragende Verb ersetzt.

\item[KS-Stelle:]
~ \\
\textbf{Vor-KS}: Funktionsverbgefüge\\
\textbf{Nach-KS}: bedeutungstragendes Verb

\item[Beispiele:]~ \\
  \textit{Im oberen Abschnitt können Sie \txgray{Einstellungen} für die angezeigten Module \txgray{vornehmen}.}\\
  \textit{Im oberen Abschnitt können Sie die angezeigten Module \txgray{einstellen}.}\\
\end{description}

\hrule
\paragraph*{S 201: Konditionalsätze mit ‚Wenn‘ einleiten}

Nach dieser Regel (\citealt{tekom2013}: 66; auch bekannt als „Bedingungen als ‚Wenn‘-Sätze formulieren“) sollen Konditionalsätze mit der Konjunktion ‚wenn‘ oder ‚falls‘ eingeleitet werden.

\begin{description}[font=\normalfont\bfseries]
\item[Begründung der Anwendung laut tekom:] Durch die Satzstruktur „Wenn-Neben\-satz-Hauptsatz“ wird das „Bedingung-Folge-Verhältnis“ ersichtlich und somit die Textverständlichkeit erhöht (ebd.: 67).

\item[{\parbox[t]{\textwidth}{Begründung der Anwendung bzw. die gezielte Wirkung der Regel laut vorhe-\\rigen Studien:}}] Die Regel sollte das Parsing aufgrund der Komplexität der Übersetzung elliptischer Konstruktionen verbessern. \citet[3]{Reuther2003} thematisiert die Komplexität der Übersetzung elliptischer Konstruktionen wie folgt: „Human and machine parsing mechanisms have to reconstruct the missing elements, which results in readability problems or, in the case of MT systems, in failed parses.“

\item[Umsetzungsmuster:]
~ \\
\textbf{Vor-KS}: Der Satz beginnt mit dem Verb.\\
\textbf{Nach-KS}: Der Satz ist mit ‚Wenn‘ eingeleitet. Wenn der Satz vor-KS mit ‚so‘ formuliert ist, wurde ‚so‘ aus stilistischen Gründen nach-KS entfernt.

\item[KS-Stelle:]
~ \\
\textbf{Vor-KS}: Verb am Satzanfang\\
\textbf{Nach-KS}: ‚wenn‘ +~Verb

\item[Beispiele:]~ \\
  \textit{\txgray{Ist} die Seriennummer des Gerätes bekannt, kann im Feld \ldots}\\
  \textit{\txgray{Wenn} die Seriennummer des Gerätes bekannt \txgray{ist}, kann im Feld \ldots}\\
  \\
  \textit{\txgray{Werden} die vordefinierten Werte \txgray{verändert}, so erfolgt die Umrechnung automatisch.}\\
  \textit{\txgray{Wenn} die vordefinierten Werte \txgray{verändert werden}, erfolgt die Umrechnung automatisch.}\\
\end{description}

\hrule
\paragraph*{S102: Eindeutige pronominale Bezüge verwenden}

Nach dieser Regel (\citealt{tekom2013}: 60) sollen Pronomen vermieden werden, wenn sie mehrdeutig sein könnten. Anstelle des Pronomens soll das Bezugswort wiederholt werden, damit der Bezug eindeutig erkennbar wird.

\begin{description}[font=\normalfont\bfseries]
\item[Begründung der Anwendung laut tekom:] Der Leser eines technischen Dokuments ist in der Regel weniger mit der Thematik vertraut als der Autor (ebd.: 61). Ferner wird diese Regel von der tekom (ebd.: 137) für ein übersetzungsgerechtes Schreiben empfohlen.

\item[{\parbox[t]{\textwidth}{Begründung der Anwendung bzw. die gezielte Wirkung der Regel laut vorhe-\\rigen Studien:}}] Diese Regel wird von \citet{Congree2018} zur Vermeidung von Mehrdeutigkeit empfohlen. Gleichzeitig spielen Pronomen eine wesentliche Rolle in der natürlichen Sprache, sodass der Autor bei der Anwendung der Regel fallabhängig zwischen der maschinellen Übersetzbarkeit und einem natürlich klingenden Text abwägen muss (\citealt{BernthGdaniec2001}).

\item[Umsetzungsmuster:]
~ \\
\textbf{Vor-KS}: Der Satz beinhaltet ein Pronomen.\\
\textbf{Nach-KS}: Das Pronomen wird durch das Nomen ersetzt.

\item[KS-Stelle:]
~ \\
\textbf{Vor-KS}: Pronomen (Personalpronomen und Demonstrativpronomen)\\
\textbf{Nach-KS}: Nomen bzw. Demonstrativpronomen und Nomen (inkl. damit verbundener Fehler in der Wortstellung)\footnotemark{}

\item[Beispiele:]~ \\
  \textit{Je früher ein Fleck behandelt wird, umso größer ist die Wahrscheinlichkeit, \txgray{ihn} rückstandslos zu entfernen.}\\
  \textit{Je früher ein Fleck behandelt wird, umso größer ist die Wahrscheinlichkeit, \txgray{den Fleck} rückstandslos zu entfernen.}\\
  \\
  \textit{Sofern auf der Oberfläche alte Kleberreste anhaften, sind \txgray{diese} vollständig zu entfernen.}\\
  \textit{Sofern auf der Oberfläche alte Kleberreste anhaften, sind \txgray{diese Kleberreste} vollständig zu entfernen.}\\
\end{description}

\hrule
\paragraph*{S 303: Partizipialkonstruktion vermeiden}

Nach dieser Regel (\citealt{tekom2013}: 70) soll statt der Partizipialkonstruktion eine einfache Satzstruktur mit mehreren kurzen Sätzen oder Nebensätzen verwendet werden.

\begin{description}[font=\normalfont\bfseries]
\item[Begründung der Anwendung laut tekom:] Die Partizipialkonstruktion erschwert die Textverständlichkeit (ebd.).

\item[{\parbox[t]{\textwidth}{Begründung der Anwendung bzw. die gezielte Wirkung der Regel laut vorhe-\\rigen Studien:}}] Komplexe Satzstrukturen, wie die Partizipialkonstruktionen, sind sowohl für den Menschen als auch das Maschinen-Parsing problematisch, sodass sie die Lesbarkeit und die Übersetzbarkeit im Kontext der MÜ beeinträchtigen \citep{Reuther2003}. Daher wurde diese Regel zur Förderung der maschinellen Übersetzbarkeit von \citet{BernthGdaniec2001} aufgeführt.

\item[Umsetzungsmuster:]
~ \\
\textbf{Vor-KS}: Der Satz beinhaltet eine Partizipialkonstruktion.\\
\textbf{Nach-KS}: Die Partizipialkonstruktion wird in einem Nebensatz aufgelöst.

\item[KS-Stelle:]
~ \\
\textbf{Vor-KS}: alle Wörter, die das Nomen beschreiben, angefangen mit dem Artikel, falls vorhanden\\
\textbf{Nach-KS}: die konvertierten Wörter von der Version vor-KS (inkl. des Kommas innerhalb der KS-Stelle)

\item[Beispiele:]~ \\
  \textit{\txgray{Speziell auf diese Lautsprecher abgestimmtes Zubehör} erhalten Sie in unserem Webshop.}\\
  \textit{\txgray{Zubehör, das speziell auf diese Lautsprecher abgestimmt ist}, erhalten Sie in unserem Webshop.}\\
\end{description}

\hrule
\paragraph*{Passiv vermeiden}~\\

\textbf{S 501:} Vorgangspassiv vermeiden

\textbf{S 502:} Passiv mit Täterangabe vermeiden

\textbf{S 503:} Passiv mit Modalverben vermeiden

\textbf{S 504:} Passiv in bestimmten Informationseinheiten vermeiden

Nach diesen vier Regeln (\citealt{tekom2013}: 79 ff.) soll die Verwendung des Passivs vermieden und stattdessen sollen die Sätze in Aktiv formuliert werden.

\begin{description}[font=\normalfont\bfseries]
\item[Begründung der Anwendung laut tekom:] Die Passivkonstruktion ist oft nicht eindeutig und lässt den Handelnden unklar. Wenn der Täter genannt werden soll, eignet sich die Aktivformulierung. (ebd.:~79) Insbesondere bei Anweisungen, Sicherheits- und Warnhinweisen wird die Aktivkonstruktion empfohlen, damit dem Leser klar wird, wer die Handlung ausführt oder ausführen soll. Warnungen wirken motivierend, wenn sie im Aktiv formuliert sind, da der Leser direkt angesprochen wird. (ebd.:~81)

\item[{\parbox[t]{\textwidth}{Begründung der Anwendung bzw. die gezielte Wirkung der Regel laut vorhe-\\rigen Studien:}}] Das Vermeiden des Passivs führt zur Verbesserung der MÜ-Qualität \citep{Siegel2013}.\footnotemark{} \citet{Reuther2003} empfahl diese Regel, um Parsing-Probleme umgehen zu können.  \citet[190]{BernthGdaniec2001} erkennen: „Passive voice plays a role in creating the right focus in a sentence, among other things.“ Sie empfehlen gleichzeitig, das Passiv zum Zweck einer besseren maschinellen Übersetzbarkeit zu vermeiden, wenn es aus stilistischer Sicht nicht notwendig sei (ebd.).

\item[Umsetzungsmuster:]
~ \\
\textbf{Vor-KS}: Satz formuliert in Passiv\\
\textbf{Nach-KS}: Satz formuliert in Aktiv

\item[KS-Stelle:]
~ \\
\textbf{Vor-KS}: Form von ‚werden‘ +~Partizip II\\
\textbf{Nach-KS}: Verb bzw. Subjekt +~Verb, wenn das Subjekt im Passivsatz nicht enthalten war und erst im Aktivsatz hinzugefügt wurde

\item[Beispiele:]~ \\
  \textit{Bei der Arbeit mit elektrischen Geräten \txgray{sollte} stets ein Sicherheitsstecker \txgray{verwendet werden}.}\\
  \textit{Bei der Arbeit mit elektrischen Geräten \txgray{verwenden Sie} stets einen Sicherheitsstecker.}\\
  \\
  \textit{Das Programm \txgray{wird} vom Hersteller wie folgt \txgray{eingestellt}.}\\
  \textit{Der Hersteller \txgray{stellt} das Programm wie folgt \txgray{ein}.}\\
\end{description}

\hrule
\paragraph*{S 511: Konstruktionen mit „sein +~zu +~Infinitiv“ vermeiden}

Nach dieser Regel (\citealt{tekom2013}: 86) soll die Passiv-Ersatzkonstruktion „sein +~zu +~Infinitiv“ bei Anweisungen vermieden werden. Stattdessen sollen sie durch einen Infinitiv oder direkte Anrede formuliert werden.

\begin{description}[font=\normalfont\bfseries]
\item[Begründung der Anwendung laut tekom:] Diese Passiversatzkonstruktion ist umständlich und der Leser wird nicht direkt angesprochen. Der Infinitiv bzw. eine direkte Anrede fördert die schnelle und richtige Umsetzung der Handlung (ebd.).

\item[{\parbox[t]{\textwidth}{Begründung der Anwendung bzw. die gezielte Wirkung der Regel laut vorhe-\\rigen Studien:}}]  Diese Regel bzw. eine Formulierung mithilfe des Imperativs wird von \citet{Congree2018} für eine präzise und deutliche Anweisung empfohlen.

\item[Umsetzungsmuster:]
~ \\
\textbf{Vor-KS}: Der Satz ist mit einem Passiversatz (sein +~zu und das Verb am Satzende) formuliert.\\
\textbf{Nach-KS}: zwei mögliche Varianten:

{}- Imperativ am Satzende

{}- Imperativ am Satzanfang

Beide Varianten wurden bei der Übersetzung mit MÜ-Systemen getestet. Da die erste Variante mit mehr Fehlern in dem MÜ-Output verbunden war, wurde entschieden, die zweite Variante als Einsatzmuster zu verwenden. Wenn der Satz vor-KS mit ‚so‘ formuliert war, wurde ‚so‘ aus stilistischen Gründen nach-KS entfernt.

\item[KS-Stelle:]
~ \\
\textbf{Vor-KS}: sein +~zu + das Verb am Satzende\\
\textbf{Nach-KS}: Imperativ +~Subjekt

\item[Beispiele:]~ \\
  \textit{Die Herstellerangaben \txgray{sind} stets \txgray{zu beachten}.}\\
  \textit{\txgray{Beachten Sie} stets die Herstellerangaben.}\\
  \\
  \textit{Ist ein mehrstufiges Modul parametriert, \ul{so} \txgray{sind} die externen Kontakte \txgray{zu} \txgray{verriegeln}.}\\
  \textit{Ist ein mehrstufiges Modul parametriert, \txgray{verriegeln Sie} die externen Kontakte.}\\
\end{description}

\hrule
\paragraph*{L 114: Überflüssige Präfixe vermeiden}

Nach dieser Regel sollen Verben mit Präfixen vermieden werden, wenn das Verb ohne Präfix die gleiche Bedeutung hat (\citealt{tekom2013}: 111).

\begin{description}[font=\normalfont\bfseries]
\item[Begründung der Anwendung laut tekom:] Die Kürzung vereinfacht den Satz und reduziert Segmentvarianten bei der MÜ (ebd.).

\item[{\parbox[t]{\textwidth}{Begründung der Anwendung bzw. die gezielte Wirkung der Regel laut vorherigen Studien:}}]  Die Verwendung trennbarer Verben in ihrer getrennten Form erhöht die Komplexität der Satzstruktur \citep{Siegel2011}. Diese Regel führt zur Reduzierung der Ambiguität (\citealt{BernthGdaniec2001}) und damit zur Verbesserung der maschinellen Übersetzbarkeit (ebd.) bzw. Erhöhung der MÜ-Qualität \citep{Siegel2013}.

\item[Umsetzungsmuster:]
~ \\
\textbf{Vor-KS}: Verb mit Präfix\\
\textbf{Nach-KS}: Eliminierung des Präfixes

\item[KS-Stelle:]
~ \\
\textbf{Vor-KS}: Verb mit Präfix (trennbare und untrennbare Verben)\\
\textbf{Nach-KS}: Verb ohne Präfix

\item[Beispiele:]~ \\
  \textit{\txgray{Überprüfen} Sie, ob ausreichend Wasser im Wassertank vorhanden ist.}\\
  \textit{\txgray{Prüfen} Sie, ob ausreichend Wasser im Wassertank vorhanden ist.}\\
  \\
  \textit{\txgray{Wählen} Sie die Option $"$Software von einer bestimmten Liste installieren$"$ \txgray{aus}.}\\
  \textit{\txgray{Wählen} Sie die Option $"$Software von einer bestimmten Liste installieren$"$.}\\
\end{description}

\hrule
\paragraph*{S 204: Keine Wortteile weglassen}

Nach dieser Regel sollen Wörter vollständig ausgeschrieben werden (\citealt{tekom2013}: 68).

\begin{description}[font=\normalfont\bfseries]
\item[Begründung der Anwendung laut tekom:] Ziel der Regel ist die Unterstützung des Verständnisses. Insbesondere bei der Übersetzung ist das Weglassen von Wortteilen ungeeignet. (ebd.)

\item[{\parbox[t]{\textwidth}{Begründung der Anwendung bzw. die gezielte Wirkung der Regel laut vorherigen Studien:}}]  Durch die Beschreibung der Ellipsen von Halliday und Hasan (\citealt{BernthGdaniec2001}: 189) als „specific structural slots to be filled by elsewhere“ wird die Problematik ihrer MÜ ersichtlich. Die MÜ-Systeme verfügen nicht immer über Quellen zum Füllen der fehlenden Slots, daher führen die tekom (2013: 68) sowie mehrere weitere Regelsätze (\citealt{BernthGdaniec2001}; \citealt{Reuther2003}; \citealt{Siegel2011}; \citealt{Congree2018}) diese Regel zur Unterstützung der maschinellen Übersetzbarkeit, Reduzierung der Ambiguität sowie ggf. Erhöhung der Textverständlichkeit auf.

\item[Umsetzungsmuster:]
~ \\
\textbf{Vor-KS}: Der Satz beinhaltet Wortteile.\\
\textbf{Nach-KS}: Die fehlenden Wortteile werden vervollständigt.

\item[KS-Stelle:]
~ \\
\textbf{Vor-KS}: Wörter mit weggelassenen Teilen\\
\textbf{Nach-KS}: vollständige Wörter

\item[Beispiele:]~ \\
  \textit{Die \txgray{Ist- und Sollwerte} des zweiten Regelkreises werden nach der Konfiguration angezeigt.}\\
  \textit{Der \txgray{Istwert und der Sollwert} des zweiten Regelkreises werden nach der Konfiguration angezeigt.}\\
\end{description}
\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{{{{Siehe Berechnungsvorgehensweise von Wortstellungsfehlern in \sectref{sec:4.4.4.1} unter „Die Fehlertypen“.}}}}
\stepcounter{footnote}\footnotetext{{{{Die MÜ-Qualität wurde auf einer Skala von nn bis 3, in der nn für „Verbesserung nicht erforderlich“; 1 für „keine Verbesserung“; 2 für „leichte Verbesserung“; 3 für „starke Verbesserung“ stehen, gemessen.}}}}

%\clearpage

Wie die obige Darstellung zeigt, haben nicht alle analysierten Regeln nach der \citet{tekom2013} die Übersetzbarkeit im Fokus; einige Regeln zielen auf die Lesbarkeit bzw. die Verständlichkeit ab. Jedoch zeigen die weiteren zitierten Studien (\citealt{BernthGdaniec2001}; \citealt{Reuther2003}; \citealt{Siegel2011}; \citealt{Congree2018}), dass die analysierten Regeln durch die Reduzierung der Satzkomplexität, Vereinfachung der Satzstruktur bzw. Verminderung der Ambiguität die maschinelle Übersetzbarkeit -- bei den früheren MÜ-Ansätzen - verbessern. Auf diesem Wege tragen auch Regeln, die die Verständlichkeit in den Mittelpunkt stellen, indirekt zur Verbesserung der Übersetzbarkeit bei. Diese allgemeine Wirkung der KS-Regeln auf die MÜ wurde von \citet[53]{FiedererO’Brien2009} wie folgt bestätigt:

\begin{quote}
Another method for improving MT output is to apply Controlled Language (CL) rules to the source text in order to reduce ambiguities and complexity. CL rules generally make the source text input more suitable for MT by reducing sentence length and eliminating problematic features. (\citealt{FiedererO’Brien2009}: 53)
\end{quote}

In ihrer Studie mit dem Titel „Two in one -- Can it work?“ bildete \citet{Reuther2003} zwei KS-Regelsätze, den ersten Regelsatz zur Verbesserung der Lesbarkeit und Verständlichkeit und den zweiten zur Verbesserung der Übersetzbarkeit. Sie fand heraus, dass sich die beiden Regelsätze nicht allzu sehr unterschieden. Die Regeln der Lesbarkeit und Verständlichkeit waren eine Teilmenge der Regeln der Übersetzbarkeit. Auf Basis dieser Ergebnisse untersuchte Reuther, wie eine KS-Konformitätsprüfung (CL-Check) in der Praxis am effizientesten durchgeführt werden kann; sie empfahl eine gemeinsame automatisierte Verarbeitung (common automated processing) der Verständlichkeitsprüfung und der Übersetzbarkeitsprüfung. (ebd.) Vor diesem Hintergrund wird der Einfluss der aufgeführten Regeln auf die MÜ-Qualität in der vorliegenden Studie empirisch untersucht.

In dieser empirischen Untersuchung wird der Output von vier MÜ-Ansätzen, darunter des NMÜ-Ansatzes, verglichen. Wie bisherige Studien zeigen, verzeichnet der NMÜ-Ansatz einen großen Fortschritt bei der Erstellung von Output, der deutlich flüssiger ist und weniger Morphologie- und Grammatikfehler im Vergleich zum davor dominanten Ansatz, PBMÜ, aufweist (vgl. \citealt{BentivogliEtAl2016}; \citealt{KlubičkaEtAl2017}; \citealt{ToralSanchez-Cartagena2017}). Gleichzeitig wird an dem NMÜ-Output kritisiert, dass seine hohe Flüssigkeit in manchen Fällen mit einer bedenklichen Adäquatheit einhergehe (vgl. \citealt{CastilhoEtAl2017a}; \citealt{Köhn2017}). Dementsprechend erfordere das Post-Editing des NMÜ-Outputs eine hohe Präzision bzw. einen hohen kognitiven Aufwand, um Adäquatheits- und stilistische Fehler zu identifizieren und zu korrigieren (vgl. \citealt{Volk2018}; \citealt{VardaroEtAl2019}). Angesichts dieser Entwicklung dürfen die Adäquatheit und der Stil bei einer MÜ-Evaluation nicht außer Acht gelassen werden. Daher nimmt die vorliegende Studie die einzelnen Regeln unter die Lupe, um ihre Wirkung auf die MÜ-Qualität sowohl stilistisch als auch inhaltlich im Sinne der Verständlichkeit und Genauigkeit empirisch zu testen.


\subsubsection{\label{sec:4.4.2.3}Diskussion der analysierten Regeln und ihrer gezielten Wirkung im Hinblick auf das untersuchte Sprachenpaar}

Nachdem die analysierten Regeln dargestellt wurden (\sectref{sec:4.4.2.2}), werden sie und ihre gezielten Wirkungen in diesem Abschnitt im Hinblick auf das untersuchte Sprachenpaar (Deutsch-Englisch) diskutiert. Die Studienergebnisse hängen weitgehend vom untersuchten Sprachenpaar ab. Ein Vergleich der MÜ-Outputs von Sprachenpaaren wie Deutsch <> Arabisch, Englisch <> Arabisch und Deutsch <> Englisch zeigt, dass die MÜ-Systeme bei dem Sprachenpaar Deutsch <> Englisch einen deutlich besseren Output liefern. Dies ist auf mehrere Gründe zurückzuführen, darunter die Entwicklungs- und Forschungsintensität sowie die Verfügbarkeit von zweisprachigen Korpora bzw. Trainingsdaten. Diese Aspekte tragen zwar zu einem besseren Output im Falle des Sprachenpaars Deutsch-Englisch bei, dennoch bleiben die Sprachunterschiede ein Hindernis, dessen Bewältigungsgrad von vielen Faktoren wie der Domäne, dem Texttyp und der Textkomplexität abhängt.

Das analysierte Sprachenpaar Deutsch > Englisch weist mehrere bekannte Unterschiede auf. Die deutsche Sprache verfügt über eine reichere Flexionsmorphologie im Vergleich zur englischen Sprache \citep[11]{Hawkins1986}. Sämtliche grammatischen Unterscheidungen innerhalb der englischen Flexionsmorphologie sind im Deutschen vertreten, umgekehrt ist dies aber nicht der Fall (ebd.). Diese Tatsache kann aber aus translatorischer Sicht vorteilhaft sein. Alleine die Unterscheidung zwischen vier Kasus im Deutschen (Nominativ, Akkusativ, Dativ und Genitiv) gegenüber nur einem einzigen Kasus im Englischen stellt eine große Vereinfachung bei der Übersetzung aus dem Deutschen ins Englische dar. Ein weiterer Unterschied besteht darin, dass die deutsche Sprache im Gegensatz zur englischen Sprache über einen höheren Grad an Wortstellungsfreiheit -- auch Grad der „configurationality“ genannt -- verfügt (vgl. \citealt{Hawkins1986}: 37ff.). Englisch wird als „configurational“ Sprache betrachtet, da es eine starre Wortstellung hat, während Deutsch mehr Freiheit bei der Wortstellung -- im Rahmen seiner Satzregeln -- anbietet und entsprechend als „less-configurational“ Sprache klassifiziert wird. Da die Studie sich mit der Übersetzung aus dem Deutschen ins Englische befasst, kann die eingeschränkte „configurationality“ der deutschen Sprache eine Übersetzungsschwierigkeit darstellen. Denn je starrer die Wortstellung eines Satzes ist, desto einfacher ist es für die MÜ-Systeme, ihn zu parsen.

Im Folgenden wird auf die sprachlichen Unterschiede bei den untersuchten Regeln eingegangen und entsprechend der Studienfokus in Bezug auf den Einfluss der einzelnen Regeln auf den MÜ-Output abgeleitet:

\subsubsubsection{Regel „Für zitierte Oberflächentexte gerade Anführungszeichen $"$\ldots$"$ verwenden“}

Laut Duden können Anführungszeichen im Deutschen zur Hervorhebung verwendet werden.\footnote{{{{Online unter: \url{https://www.duden.de/sprachwissen/rechtschreibregeln/anfuehrungszeichen\#D8}.}}}} {Orthografisch haben Anführungszeichen nach} \citet[254]{Nerius2007} {die Rolle der Kennzeichnung fremder Äußerungen, wie direkter Rede und Zitaten, aber auch Titeln und Überschriften innerhalb des Satzverbandes.} {{Oberflächentexte sind „alle Texte einer Softwareoberfläche oder Texte, die sich auf einem Gerät befinden“ (\citealt{tekom2013}: 117)} }{und gelten somit als fremde Äußerungen innerhalb des Satzverbandes. Auf dieser Basis wird diese Regel von der tekom zur Erhöhung der Lesbarkeit wegen der verbesserten Orientierung empfohlen. Dabei haben die Anführungszeichen gegenüber weiteren Hervorhebungstechniken (wie der Verwendung von anderen Schriftarten, Schriftgraden oder Schriftschnitten) laut tekom den Vorteil, dass sie das Schriftbild weniger beeinflussen.} (\citealt{tekom2013}: 118) Im {E}nglischen {hingegen} ist die Verwendung von Anführungszeichen keine gängige Hervorhebungstechnik. {\citet{McMurrey2006} empfiehlt den technischen Redakteuren: „limit quotation marks to the traditional usage, which includes quoted speech; numbers, letters, or words referred to as such“; seine Begründung dafür lautet: „Quotation marks, like capital letters, tend to create a busy, distracting text and therefore should be avoided“ (ebd.).}

{Eine korrekte Anwendung dieser Regel fördert der \citeauthor{tekom2013} zufolge (\citeyear{tekom2013}: 118) „nicht nur die Konsistenz, sondern auch die Übersetzung.“ Dies ist nachvollziehbar, denn laut \citet[2]{Reuther2003}: „Punctuation marks are very sensitive with respect to all applications where linguistic processing is} done automatically.” Dementsprechend kann diese Regel die MÜ auf zweierlei Weise beeinflussen: Einerseits -- anders als im Deutschen -- kann das Auftreten der Anführungszeichen in der englischen MÜ aufgrund der untypischen Verwendung als optisch störend bzw. ablenkend wahrgenommen werden. Andererseits hat die Verwendung von Anführungszeichen in Zusammenhang mit der MÜ im Gegensatz zu einer Hervorhebung mittels der Verwendung anderer Schriftarten oder Formatierungen (z.~B. fett, kursiv) den Vorteil, dass die Grenze des Oberflächentexts innerhalb des Satzverbandes gekennzeichnet wird (vgl. \citealt{Nerius2007}: 253). Diese Kennzeichnung kann die MÜ-Systeme bei der Tokenisierung und dem Parsen unterstützen, was wiederum eine korrekte Übersetzung fördert, wie von der \citet[118]{tekom2013} beabsichtigt. In der Studie wird entsprechend untersucht, inwiefern die Anwendung der Anführungszeichen bei der Tokenisierung und Erkennung der Oberflächentexte als feste bzw. spezifische Begriffe, die oft als Mehrwortentitäten auftreten, förderlich ist und wie sie sich auf die MÜ-Qualität auswirkt.

\subsubsubsection{Regel „Funktionsverbgefüge vermeiden“}
Generell wird das Funktionsverbgefüge kritisiert und u. a. als „Folterinstrument für Verben und Leser“ bezeichnet (\citealt{BaumertVerhein-Jarren2012}: 106). {\citet{Heine2017} stellt} Funktionsverbgefüge als „ein typisches Beispiel für Phänomene, die weder (ausschließlich) mit grammatischen Regeln noch als lexikalische Einheiten erklärbar sind“ dar und verdeutlicht wie die Satzsyntax zusammen mit der lexikalischen Komponente des MÜ-Systems entscheidend für einen fehlerfreien MÜ-Output sind. Das Vermeiden des Funktionsverbgefüges bzw. ausdrucksschwacher Verben ist in mehreren KS-Regelsätzen enthalten. Die \citet[107]{tekom2013} empfiehlt diese Regel, um die Präzision und Direktheit der Aussage zu steigern. Weitere Studien zeigen, dass das Vermeiden ausdrucksschwacher Verben die Ambiguität reduziert \citep{Siegel2011}. Außerdem ist diese Regel ein Bestandteil der Stilregeln von \citet{Congree2018} und wird mit der folgenden Begründung empfohlen: „Solche Formulierungen machen den Sachverhalt unnötig kompliziert und erschweren das Textverständnis und die Übersetzung.“ (ebd.: 13)\footnote{{{{Congree Language Technologies GmbH ist eines der marktführenden Unternehmen für KS-Softwareprodukte.} }}}


Das Funktionsverbgefüge im Deutschen ist mit der „light verb construction“ im Englischen vergleichbar. Trotz des Vorhandenseins in beiden Sprachen wird seine Übersetzung aufgrund der Transferprobleme als Herausforderung betrachtet (vgl. \citealt{BaumertVerhein-Jarren2012}: 107). Einige Funktionsverbgefüge haben Pendants im Englischen (z.~B. ‚zur Entscheidung kommen‘ $\to$ ‚to make a decision‘); bei anderen Funktionsverbgefügen ist dies nicht der Fall (z.~B. ‚in Verbindung setzen‘). Hat das Funktionsverbgefüge ein Pendant im Englischen und ist die Satzstruktur \textit{nicht} komplex, so ist eine korrekte MÜ des Funktionsverbgefüges zu erwarten. Hat das Funktionsverbgefüge ein Pendant im Englischen und ist die Satzstruktur komplex, kann dies in einer fehlerhaften MÜ resultieren. In dem Fall würde die Regelanwendung durch die Verwendung des bedeutungstragenden Verbs die Satzstruktur vereinfachen, was zur Erleichterung des Parsens und wiederum zur Verbesserung des MÜ-Outputs beitragen würde. Hat das Funktionsverbgefüge kein Pendant im Englischen, dann sollte ebenfalls die Anwendung der Regel durch die Reduzierung der Ambiguität und die Vereinfachung der Satzstruktur (vgl. \citealt{Siegel2011}; \citealt{Congree2018}) hilfreich sein. Außerdem kann der Effekt, der von der \citet[107]{tekom2013} bei der Ausgangssprache, beabsichtigt ist, nämlich den Satz konkreter und direkter zu gestalten, sich in der Zielsprache widerspiegeln. Dies wiederum würde dazu beitragen, dass die MÜ aufmerksamkeitserregend wirkt.

Trotz der damit einhergehenden Schwierigkeiten ist ein vollständiger Verzicht auf das Funktionsverbgefüge nicht möglich, da „manche Funktionsverbgefüge Bedeutungsnuancen tragen, die mit dem einfachen Verb schwer oder gar nicht gesetzt werden können“ (z. B. kann ‚eine Maschine in Betrieb zu setzen‘ komplexe Verfahren beinhalten und ist somit in manchen Fällen nicht mit ‚einschalten‘ gleichzustellen) (\citealt{BaumertVerhein-Jarren2012}: 107). „Für andere Funktionsverbgefüge fehlt die Entsprechung völlig“ (z. B. ‚in Ordnung halten‘) (ebd.). Somit begründet das potenzielle Fehlen eines äquivalenten einfachen Verbes die Notwendigkeit der Verwendung des Funktionsverbgefüges. Vor dem Hintergrund untersucht die Studie, inwiefern der bisher realisierte MÜ-Fortschritt eine korrekte Übersetzung des Funktionsverbgefüges ermöglicht.

\subsubsubsection{Regel „Konditionalsätze mit ‚Wenn‘ einleiten“}
Anders als im Deutschen, in dem der Konditionalsatz mit dem Verb eingeleitet werden kann, beginnen die Konditionalsätze im Englischen in der Regel mit ‚If‘. Ausnahmen sind nur in der Umgangssprache oder den literarischen Inversionsstrukturen zu finden \citep[307]{Swan1980}: Im Umgangsenglisch kann das ‚If‘ weggelassen werden, während in den literarischen Inversionsstrukturen der Konditionalsatz mit ‚Were‘, ‚Should‘ und ‚Had‘ anstelle von ‚If‘ beginnen kann. Neben den sprachlichen Unterschieden thematisiert \citet[3]{Reuther2003} die Komplexität der Übersetzung elliptischer Konstruktionen wie folgt: „Human and machine parsing mechanisms have to reconstruct the missing elements, which results in readability problems or, in the case of MT systems, in failed parses.“ Aufgrund der sprachlichen Unterschiede und der Parsing-Problematik von elliptischen Konstruktionen sollte die Regelanwendung die Satzstruktur eindeutiger gestalten und somit die Systeme dabei unterstützen, die Konditionalsätze korrekt ins Englische zu übersetzen. In der Studie wird untersucht, inwiefern die MÜ einen Fortschritt bei dieser Art der elliptischen Konstruktion gemacht hat bzw. ob die Regelanwendung für eine fehlerfreie Übersetzung erforderlich ist.

\subsubsubsection{Regel „Eindeutige pronominale Bezüge verwenden“}
Pronominale Bezüge lassen sich in beiden Sprachen wiederholen. Einige Pronomen im Deutschen, wie das Personalpronomen ‚sie‘ oder das Demonstrativpronomen ‚diese‘, können -- je nach Satzstruktur -- mehrdeutig sein, denn die Erkennung des Pronomenbezugs (sog. Koreferenzauflösung)\footnote{{{{Durch die Koreferenzauflösung wird die Entität, worauf sich die Koreferenz bzw. das Pronomen bezieht, identifiziert (vgl. \citealt{Ng2017}).}}}} stellt in der MÜ eine Schwierigkeit dar (vgl. \citealt{Ng2017}). Daher wird diese Regel von der \citet[137]{tekom2013} für ein übersetzungsgerechtes Schreiben bzw. von \citet{Congree2018} zur Vermeidung von Mehrdeutigkeit empfohlen. Gleichzeitig spielen Pronomen eine wesentliche Rolle in der natürlichen Sprache, sodass der Autor bei der Anwendung der Regel fallabhängig zwischen der maschinellen Übersetzbarkeit und einem natürlich klingenden Text abwägen muss (\citealt{BernthGdaniec2001}). Die Studie untersucht, ob diese Abwägung nach jetzigem Entwicklungsstand der MÜ weiterhin notwendig ist. In anderen Worten, ob sich in der Zwischenzeit die Koreferenzauflösung verbessert hat, wodurch die Begünstigung eines natürlichen Stils für gewünschte Textsorten möglich ist.

\subsubsubsection{Regel „Partizipialkonstruktion vermeiden“}
Sowohl im Deutschen als auch im Englischen wird die Partizipialkonstruktion in der Funktion eines Relativsatzes verwendet (vgl. \citealt{Königs2004}: 182ff.). In beiden Sprachen wird diese Art der Partizipialkonstruktion zur Sprachökonomie verwendet; zudem wird sie im Deutschen als gehobener Stil empfunden (ebd.). Vor diesem Hintergrund ist es erforderlich, dass die MÜ-Systeme (zumindest einfache) Partizipialkonstruktionen korrekt übersetzen können. Dennoch zählen Partizipialkonstruktionen zu den komplexen Satzstrukturen. Komplexe Satzstrukturen sind sowohl für den Menschen als auch das Maschinen-Parsing problematisch, sodass sie die Lesbarkeit und die Übersetzbarkeit im Kontext der MÜ beeinträchtigen \citep{Reuther2003}. Daher wurde diese Regel\footnote{{{{Regel „Avoid post-modifying adjective phrases” bei \citet{BernthGdaniec2001}, die auch von Bernth und Gdaniec für Deutsch empfohlen wurde.}}}} zur Förderung der maschinellen Übersetzbarkeit von \citet{BernthGdaniec2001} aufgeführt. Die Regel kann dementsprechend insbesondere bei komplexen Partizipialkonstruktionen nützlich sein, um die Satzstruktur sowohl für den Menschen als auch das Maschinen-Parsing zu vereinfachen. In der Studie wird die Nützlichkeit der Regel nach dem derzeitigen Entwicklungsstand der MÜ untersucht und ihre Auswirkung auf die MÜ inhaltlich und stilistisch gemessen.

\subsubsubsection{Regel „Passiv vermeiden“}
„Kaum eine Debatte um sprachliche Fragen in der Technischen Dokumentation wird so hartnäckig und engagiert geführt wie die der Passiv-Verwendung.“ \citep{Muthig2003} Die Verwendung des Passivs ist ein häufig diskutiertes Thema in der technischen Redaktion sowohl im Englischen als auch im Deutschen. Die Regel „Passiv vermeiden“ ist ebenfalls ein Bestandteil der tekom-Leitlinie für die englische Sprache „Regelbasiertes Schreiben -- Englisch für deutschsprachige Autoren“ \citep{Siegel2014}. Im Aktiv wird der Leser direkt angesprochen und zum Handeln angeregt, daher empfiehlt die \citet[81]{tekom2013} seine Anwendung bei der Formulierung von handlungsorientierten Informationseinheiten, wie z.~B. Sicherheitshinweisen. Im Passiv hingegen steht die Handlung im Mittelpunkt. (vgl. \citealt{BaumertVerhein-Jarren2012}: 67f.) Die Verwendung vom Passiv kann sinnvoll sein, „wenn der Handelnde nicht bekannt ist oder bewusst nicht genannt werden soll“ (\citealt{tekom2013}: 80). Auf dieser Basis sollten die beiden Konstruktionen in der technischen Dokumentation unterschiedlich je nach Kontext und Satzintention eingesetzt werden. Dementsprechend ist es notwendig, dass die MÜ-Systeme beide Formen -- bei klarer Satzstruktur und adäquatem Terminologiemanagement -- fehlerfrei übersetzen können.

Bisherige Studien fanden jedoch heraus, dass das Vermeiden des Passivs zu einer „starken Verbesserung“ der MÜ-Qualität führt \citep{Siegel2013}.\footnote{{{{Die MÜ-Qualität wurde auf einer Skala von nn bis 3, in der nn für „Verbesserung nicht erforderlich“; 1 für „keine Verbesserung“; 2 für „leichte Verbesserung“; 3 für „starke Verbesserung“ stehen, gemessen.}}}} \citet{Reuther2003} empfahl diese Regel, um Parsing-Probleme umgehen zu können. \citet[190]{BernthGdaniec2001} erkennen: „Passive voice plays a role in creating the right focus in a sentence, among other things.“ Sie empfehlen gleichzeitig, das Passiv zum Zweck einer besseren maschinellen Übersetzbarkeit zu vermeiden, wenn es aus stilistischer Sicht nicht notwendig sei (ebd.). Nach Bernth und Gdaniec (ebd.: 191) könne der MÜ das Passiv aus zwei Gründen schwierig fallen: „It can be very difficult to disambiguate between stative and dynamic passives […]; (t)he argument assignments in passive constructions may differ between source and target language.“ Aufgrund der Notwendigkeit beider Formen untersucht die Studie, inwiefern der bisher realisierte MÜ-Fortschritt eine korrekte Übersetzung des Passivs ermöglicht.

\subsubsubsection{Regel „Konstruktionen mit ‚sein + zu + Infinitiv‘ vermeiden“}
Die Konstruktion „sein +~zu +~Infinitiv“ ist im Deutschen eine Form des Passiversatzes für ein Passiv mit Modalverb (Beispiele: ‚Die Regeln müssen beachtet werden‘ $\to$ ‚Die Regeln sind zu beachten‘; ‚Die zweite Aufgabe soll zunächst bearbeitet werden‘ $\to$ ‚Die zweite Aufgabe ist zunächst zu bearbeiten‘), vgl. \citet[92]{Teich2003}. Ins Englische wird diese Form des Passivs als reguläres Passiv übersetzt (\citealt{KönigGast2012}: 161) (‚The rules must be followed‘; ‚The rules are to be followed‘). Hierbei kann der kontrastive Konstruktionsunterschied zwischen dem Deutschen und Englischen (vgl. \citealt{Teich2003}: 93) zu MÜ-Problemen führen. \citet{Reuther2003} argumentiert, dass die systemspezifischen Eigenschaften die MÜ solcher stilistischen Phänomene beeinflussen. Darüber hinaus wird der Leser mit dem Passiversatz nicht direkt angesprochen, daher wird diese Regel (eine Formulierung mithilfe des Imperativs) von der \citet[28]{tekom2013} für „eine schnelle und richtige Handlungsumsetzung“ und von \citet{Congree2018} für eine präzise und deutliche Anweisung empfohlen. Die Studie untersucht, inwiefern die MÜ-Systeme nach dem jetzigen Entwicklungsstand die Passiversatzkonstruktion fehlerfrei übersetzen können und wie diese Regel sich auf die MÜ-Qualität, vor allem die Stilqualität, auswirkt.

\subsubsubsection{Regel „Überflüssige Präfixe vermeiden“}
Bei dieser Regel wurde im Rahmen der Studie sowohl trennbare als auch untrennbare Verben\footnote{{{{Die analysierten Sätze enthalten 15 Sätze, in denen das Präfix getrennt am Satzende bzw. Nebensatzende erscheint sowie 9 Sätze mit untrennbaren Verben oder trennbaren Verben, in denen das Präfix ungetrennt vom Verb erscheint.}}}} mit überflüssigen Präfixen analysiert. Untrennbare Verben sollten, solange sie ein Äquivalent im Englischen haben, unproblematisch übersetzt werden. Bei trennbaren Verben wird davon ausgegangen, dass sie -- je nach Komplexitätsgrad des Satzes -- den MÜ-Systemen Schwierigkeiten beim Parsen bereiten, wenn sie in der getrennten Form im Satz vorkommen. Dies ist z.~B. der Fall, wenn das trennbare Verb ein Bestandteil eines Hauptsatzes (ohne Modalverb) ist; das Präfix erscheint am Satzende getrennt vom Verb (Bsp. ‚Speichern Sie die angezeigten Werte lokal auf der Festplatte ab‘). Im Englischen sind die ‚phrasal verbs‘ ebenfalls Verben, die aus einem Verb und einem Partikel bestehen. Allerdings ist die Satzstruktur im Englischen anders. Im Englischen kann das Partikel in manchen Fällen am Satzende platziert werden, in anderen Fällen hingegen ist dies nicht korrekt.\footnote{{{{Während sowohl} }},{{{She turned off the light}}}\textrm{‘}{{{ als auch} }},{{{She turned the light off}}}\textrm{‘}{{{ richtig sind, ist nur} }},{{{She looked after her brother}}}\textrm{‘}{{{ korrekt (} }},{{{She looked her brother after}}}\textrm{‘}{{{ wäre falsch).}}}}

Nicht nur die \citet[111]{tekom2013} empfiehlt diese Regel zur Vereinfachung des Satzes und Reduzierung der Segmentvarianten bei der MÜ; diese Regel ist auch in mehreren KS-Regelsätzen enthalten. \citet{Siegel2011} findet, dass die Verwendung trennbarer Verben in ihrer getrennten Form die Komplexität der Satzstruktur erhöhe. \citet{BernthGdaniec2001} führen diese Regel zur Reduzierung der Ambiguität und damit Verbesserung der maschinellen Übersetzbarkeit. \citet{Siegel2013} zeigt, dass die Regelanwendung mit einer Verbesserung der MÜ-Qualität verbunden sei. Aufgrund der dargestellten sprachlichen Unterschiede und je nach Satzlänge und~-komplexität könnten die Systeme Schwierigkeiten haben, das Präfix als Teil des Verbs zu erkennen. In der Studie wird die Anwendung dieser Regel unter die Lupe genommen, um zu untersuchen, inwiefern sie nach der heutigen Entwicklung der MÜ zur Verbesserung der maschinellen Übersetzbarkeit erforderlich ist.

\subsubsubsection{Regel „Keine Wortteile weglassen“}
Das Weglassen von Wortteilen ist eine Form der elliptischen Konstruktionen. Ellipsen werden von \citeauthor{HallidayHasan1976} (\citeyear{HallidayHasan1976} zit. nach \citealt{BernthGdaniec2001}: 189) wie folgt definiert: „omission of something in the text, with the condition that what is omitted (or ellipted) is presupposed“. Ziel des Weglassens von Wortteilen in beiden Sprachen ist die Reduzierung bzw. die Sprachökonomie (vgl. „Quantitätsmaxime“ von \citealt{Grice1975}: 26)\footnote{{{{Die Quantitätsmaxime von \citet[26]{Grice1975} bezieht sich auf die Quantität der Information: „Make your contribution as informative as is required (for the current purposes of the exchange) [\ldots] not more informative than is required”.}}}} sowie die Erhöhung der Lesbarkeit (\citealt{BernthGdaniec2001}). Durch die Beschreibung der Ellipsen von Halliday und Hasan (ebd.:~189) als “specific structural slots to be filled by elsewhere” wird die Problematik ihrer MÜ ersichtlich. Die MÜ-Systeme verfügen nicht immer über Quellen zum Füllen der fehlenden Slots, daher führen die \citet[68]{tekom2013} sowie mehrere weitere Regelsätze (\citealt{BernthGdaniec2001}; \citealt{Reuther2003}; \citealt{Siegel2011}; \citealt{Congree2018}) diese Regel zur Unterstützung der maschinellen Übersetzbarkeit, Reduzierung der Ambiguität sowie ggf. Erhöhung der Textverständlichkeit auf.

Bei dem Sprachenpaar Deutsch – Englisch können die unterschiedlichen Rechtschreibregeln in Zusammenhang mit dieser Regel eine Rolle spielen: Im Deutschen wird ein Bindestrich bei der Abkürzung sowohl von Substantiven (z. B. ‚Vor- und Nachteile‘) als auch von Adjektiven (z. B. ‚prozess- und produktabhängig‘) verwendet. Im Englischen hingegen kann der Bindestrich bei der Abkürzung von Adjektiven (z.~B. ‚process- and product-oriented‘), aber nicht bei der Abkürzung von Substantiven verwendet werden. Aufgrund dieser orthografischen Unterschiede und dadurch, dass bei der Anwendung dieser Regel die Verwendung des Bindestriches entfällt, wird erwartet, dass die Regel die MÜ-Systeme bei der Tokenisierung unterstützt. Des Weiteren kann der Geläufigkeitsgrad der abgekürzten Begriffe eine besondere Rolle bei der Korrektheit ihrer MÜ spielen.\footnote{{{{Da} }}\textrm{die Studie generische Black-Box-Systeme untersuchte, bei denen keine Terminologieintegration erfolgen konnte, wurden firmen- und produktspezifische Termini durch geläufige Begriffe ersetzt. Für die Auswahlkriterien der untersuchten Systeme siehe \sectref{sec:4.4.1}. Für den genauen Umgang mit den spezifischen Termini im Rahmen der Studie siehe Schritt [4] unter \sectref{sec:4.4.3.1}.}} Es wird davon ausgegangen, dass gebräuchliche abgekürzte Begriffe in ihrer abgekürzten Form in den MÜ-Systemen lexikalisch hinterlegt sind bzw. in den Trainingsdaten vorkommen. Dementsprechend wäre in dem Fall die Wahrscheinlichkeit einer korrekten Übersetzung hoch und die Regelanwendung -- zum Zwecke der maschinellen Übersetzbarkeit -- nicht erforderlich. Die Regelanwendung kann sogar bei Begriffen, die in ihrer abgekürzten Form gängig sind (z.~B. ‚Gebrauchs- und Bedienungs\textit{anleitung}‘ $\to$ ‚Gebrauchs\textit{anleitung} und Bedienungs\textit{anleitung}‘), stilistisch unakzeptabel sein, was berücksichtigt werden muss, wenn der Stil bei der entsprechenden Textsorte Vorrang haben sollte. Bei ungebräuchlichen abgekürzten Begriffen hingegen könnte die Verwendung der vollständigen Wörter (d.~h. eine Anwendung der Regel) eine korrekte MÜ fördern, wie es in den obengenannten Regelsätzen beabsichtigt ist. Die Angabe von vollständigen Wörtern kann einerseits insbesondere bei kritischen Kontexten (z.~B. Sicherheitshinweise) inhaltlich für höhere Klarheit sorgen. Auf der anderen Seite kann ebenfalls hier die Wiederholung von Wörtern (wie ‚Konfiguration‘ in ‚Eingangskonfiguration und Ausgangskonfiguration‘ (input configuration and output configuration)) im Deutschen sowie im Englischen stilistisch kritisch betrachtet werden. Daher wird bei dieser Regel typischerweise dem Autor ein gewisser Freiraum für die Entscheidung über die Regelanwendung je nach den gegebenen Umständen eingeräumt (\citealt{BernthGdaniec2001}). Vor diesem Hintergrund ist es von Interesse, in der Studie zu untersuchen, ob in der Zwischenzeit die MÜ dieser Art der Ellipsen sich nach jetzigem Entwicklungsstand der Systeme verbessert hat, wodurch von einem prägnanten Stil profitiert werden kann.

\subsection{Datensatz: Beschreibung und Aufbereitung}
\label{sec:4.4.3}
In diesem Abschnitt wird der Studiendatensatz präsentiert. Nach einer zusammenfassenden Beschreibung des Datensatzes werden der Aufbau des Datensatzes sowie die Aufbereitungsschritte für die Analysemethoden genauer erläutert.

{Kurze Beschreibung des Datensatzes:} Bei den analysierten Daten handelt es sich um eine korpusbasierte Testsuite. 216 Ausgangssätze wurden aus einem Korpus von zehn deutschen Benutzerhandbüchern für Geräte, Software und Maschinen extrahiert. Diese Ausgangssätze verstoßen gegen mindestens eine der neun untersuchten KS-Regeln. Die KS-Regeln wurden einzeln in den jeweiligen Sätzen angewendet. Somit entstanden zwei Versionen von jedem Satz: eine Version vor dem Einsatz der KS-Regeln (nachstehend „vor-KS“ genannt) und eine Version nach dem Einsatz der KS-Regeln (nachstehend „nach-KS“ genannt). Beide Versionen wurden von den fünf MÜ-Systemen ins Englische übersetzt. Entsprechend wurde ein Datensatz aus 2.160 MÜ-Sätzen (216 Ausgangssätze * 2 Versionen * 5 Systeme) gebildet. Dieser Datensatz wurde komplett im Rahmen der Fehlerannotation analysiert. 1.100 MÜ-Sätze von den 2.160 wurden im Rahmen der Humanevaluation bewertet.

\subsubsection{\label{sec:4.4.3.1}Details der Erstellung und Aufbereitung des Datensatzes}

Zur Untersuchung des Einflusses der \textit{einzelnen} KS-Regeln musste die Analyse auf Satzebene erfolgen. Wie unter \sectref{sec:3.3.2} „Evaluationsdesign“ erklärt, wurde entschieden, die Studie auf Basis einer korpusbasierten Testsuite (vgl. \citealt{BalkanFouvry1995}) durchzuführen, um von der Textauthentizität und sprachlichen Varianz eines Textkorpus (vgl. \citealt{Engelberg2009}) zu profitieren und gleichzeitig eine gewisse Steuerung über die Datenkonstruktion beizubehalten. Eine gewisse Steuerung des Datensatzes war erforderlich, um die Komplexität eines natürlichen Texts -- nach klar definierten Kriterien -- in einem Rahmen zu reduzieren, der der Analyse der KS in Zusammenhang mit der MÜ dienlich ist. Die Vorgehensweise zur Erstellung und Aufbereitung des Datensatzes erstreckt sich über die folgenden zehn Schritte:

\begin{enumerate}[label = {[\arabic*]}, align = left]
\item Erstellung eines einsprachigen Korpus
\item Prüfung der Konformität mit den analysierten KS-Regeln
\item Auswahl der Testsätze (Ausgangssätze)
\item Aufbereitung der Testsätze (Ausgangssätze)
\item Einsatz der KS-Regeln
\item Qualitätsprüfung der Ausgangssätze (DE)
\item Übersetzung und Fehlerannotation
\item Auswahl der MÜ-Sätze (Zielsätze) für die Humanevaluation
\item Aufbereitung der MÜ-Sätze (Zielsätze) für die Humanevaluation
\item Qualitätsprüfung der Zielsätze (EN)
\end{enumerate}

Im Folgenden werden die Vorgehensweise zusammen mit einer genauen Erläuterung der Auswahlkriterien sowie die Qualitätsprüfungsverfahren des Datensatzes detaillierter dargestellt:

\paragraph*{[1] Erstellung eines einsprachigen Korpus:} Es wurde ein einsprachiges Korpus aus zehn deutschen Betriebsanweisungen, Pflegeanleitungen, Regelungen sowie Benutzerhandbüchern von Haushaltsgeräten, Software und Maschinen verschiedener Hersteller erstellt. Die verschiedenen Dokumente wurden online als PDF-Dateien heruntergeladen und in Textdateien konvertiert. Die Dateien wurden durch das Entfernen sämtlicher Grafiken, Tabellen, Auflistungen und Überschriften bereinigt. Die Absätze wurden in einzelne Sätze zerlegt und aufgelistet.

\paragraph*{[2] Prüfung der Konformität mit den analysierten KS-Regeln:} Die Textdateien wurden in CLAT auf Verstöße gegen die neun analysierten KS-Regeln\footnote{{{{Verstöße gegen einige Regeln wie „Funktionsverbgefüge vermeiden“ konnten von CLAT nicht immer erkannt werden (meistens werden nur bekannte FVG wie ‚erfolgen‘ oder ‚vornehmen‘ gut erkannt), daher wurden die Sätze in manchen Fällen manuell extrahiert.}}}} untersucht. Sätze, die Verstöße aufweisen, wurden in neun Excel-Tabellen extrahiert. Jede Excel-Tabelle beinhaltet die Sätze mit den Verstößen gegen eine bestimmte Regel unter Angabe der Quelle des Satzes sowie der mithilfe von Excel ermittelten Satzlänge.

\paragraph*{[3] Auswahl der Testsätze (Ausgangssätze):} Zum Filtern der Testsätze wurden sie in einem \textit{einleitenden Schritt mit den fünf} verwendeten MÜ-Systemen übersetzt. Unter Berücksichtigung einer möglichst ausgewogenen Auswahl aus allen Quellen (d.~h. Benutzerhandbüchern) wurden die ersten enthaltenen Sätze ausgewählt, die die folgenden Kriterien erfüllen:


\begin{itemize}
\item Die maschinelle Übersetzung von mehr als zwei MÜ-Systemen ist akzeptabel: Die Sätze wurden in den Datensatz aufgenommen, wenn die MÜ grundsätzlich akzeptabel war. In anderen Worten wurden die Sätze, deren MÜ mit den meisten Systemen unverständlich waren, ausgeschlossen. Da der Fokus der Studie darin besteht, den Einfluss einzelner KS-Regeln auf die MÜ genauer zu analysieren (und nicht den MÜ-Output im Allgemeinen zu beurteilen), ist die Untersuchung einer komplett unverständlichen MÜ nicht zielführend.
\item Die MÜ von mindestens zwei MÜ-Systemen beinhaltet mindestens einen Fehler: Es wurden die Sätze ausgeschlossen, die sowohl vor-KS als auch nach-KS von allen Systemen fehlerfrei übersetzt wurden. Diese sind in den meisten Fällen technische Standardsätze oder AGB, die aufgrund ihres häufigen Auftretens insbesondere von SMÜ-Systemen fehlerfrei übersetzt werden.
\item Die Sätze ergeben alleinstehend, sprich ohne weitere Kontextinformation, Sinn, damit die Teilnehmer der Humanevaluation die Satzsemantik der Ausgangssätze ohne Kontext erkennen und entsprechend die MÜ bewerten können.
\end{itemize}

Zum Schluss wurden 24 Sätze bei jeder KS-Regel ausgewählt (insgesamt 216 Sätze). Somit liegt die Anzahl der analysierten Sätze pro Regel über der einschlägiger Studien. \citet{Roturier2006} analysierte zwei bis 14 Sätze pro Regel (insgesamt 54 Regeln; 304 Sätze). \citet{O’Brien2006} untersuchte insgesamt 130 Segmente bei 29 NTIs (Negative Translatability Indicators), d. h. durchschnittlich 4,5 Segmente pro NTI, wobei die Anzahl der Segmente pro NTI variierte. In Dohertys Korpus wurden insgesamt 33 Verstöße gegen zehn KS-Regeln identifiziert, wobei die Anzahl der Verstöße bei den einzelnen Regeln unterschiedlich ausfiel und zwischen einem und zwölf Verstößen lag \citep[103]{Doherty2012}.

Im Rahmen dieser Studie schafft die Analyse einer gleichen Anzahl an Sätzen pro Regel sowohl bei der quantitativen als auch bei der qualitativen Analyse eine einheitliche Untersuchungsbasis. Eine weitere Erhöhung der Anzahl der Sätze könnte die Integrität der Ergebnisse gefährden. MÜ-Evaluationsstudien thematisieren die Abwägung zwischen der Größe des Datensatzes und der Integrität der Ergebnisse (\citealt{FiedererO’Brien2009}). \citet[2]{Coughlin2003} analysierte 124 MÜ-Sätze und machte die Beobachtung: „Rating translation quality is both tedious and repetitive“. Es kam vor (ebd.), dass die Bewerter identischen Sätzen unterschiedliche Scores gaben oder Sätze, die mit der Referenzübersetzung identisch waren, nicht mit dem besten Score bewerteten. In einer weiteren MÜ-Evaluationsstudie wurde der Datensatz auf 180 Sätze begrenzt, um das „risk of boredom and its negative consequences such as inconsistent evaluation or diminishing powers of judgement“ zu minimieren (\citealt{FiedererO’Brien2009}: 59). In \tabref{tab:4:4} sind die analysierten Dokumente sowie die Aufteilung der Sätze genauer angegeben:


\begin{sidewaystable}
\small
\begin{tabularx}{\textwidth}{p{7cm}YYYYY}
\lsptoprule
\textbf{Dokument}\footnotemark{} & \textbf{Anzahl der Wörter des Dokuments} & \textbf{Anzahl der Sätze des Dokuments}  & \textbf{\% im Korpus} & \textbf{Anzahl der analysierten Sätze} & \textbf{\% im Datensatz}\\
\midrule
Verlegeanweisung und Reinigungsverfahren eines Teppichbodens & 2.263 & 143 & 6,5~\% & 18 & 8,3~\%\\
Gebrauchs- und Pflegeanleitung für keramikversiegeltes Kochgeschirr sowie AGB & 1.198 & 74 & 3,4~\% & 10 & 4,6~\%\\
Bedienungsanleitung eines Feinstzerkleinerers (ein Schneidesystem zum Zerkleinern von Lebensmittel-, Chemie- bzw. Medizinprodukten) & 5.851 & 494 & 22,4~\% & 35 & 16,2~\%\\
Handbuch einer Software zur Parametrierung eines Volumenstrom-Kompaktreglers (der Volumenstrom-Kompaktregler ist für Labor- und Pharmaanwendungen bestimmt) & 9.776 & 678 & 30,7~\% & 92 & 42,6~\%\\
Gepäckregelung einer Fluggesellschaft & 913 & 53 & 2,4~\% & 4 & 1,9~\%\\
Bedienungsanleitung eines Milchaufschäumers & 1.310 & 94 & 4,3~\% & 9 & 4,2~\%\\
Pflege- und Bedienungsanleitung eines Küchenmöbels & 3.459 & 223 & 10,1~\% & 15 & 6,9~\%\\
Technische Beschreibung und Bedienungsanleitung eines Heimkino-Lautsprecher-Sets & 2.065 & 156 & 7,1~\% & 14 & 6,5~\%\\
Bedienungsanleitung einer elektrischen Zitruspresse (Elektro-Hausgerät) & 1.031 & 82 & 3,7~\% & 12 & 5,6~\%\\
Betriebsanweisung einer Serie von Haus- und Gartenpumpen & 2.154 & 210 & 9,5~\% & 7 & 3,3~\%\\
\midrule
& \textbf{30020} & \textbf{2207} & \textbf{100~\%} & \textbf{216} & \textbf{100~\%}\\
\lspbottomrule
\end{tabularx}
\caption{\label{tab:4:4}Anteil der analysierten Sätze im Verhältnis zu der Größe der jeweiligen Quelle}
\end{sidewaystable}

\footnotetext{{{{Die Forscherin richtet ihren Dank an alle Firmen, die zum Forschungszweck ihre Dokumente zur Verfügung gestellt haben. Ohne diese Dokumentation wäre die Studie nicht zustande gekommen. Auf Wunsch der meisten Firmen bleiben die Herstellernamen anonym.}}} }

Wie \tabref{tab:4:4} zeigt, steht der Anteil der analysierten Sätze im Verhältnis zu der Größe der jeweiligen Quelle.

\paragraph*{[4] Aufbereitung der Testsätze (Ausgangssätze):}
Nach \citet{KingFalkedal1990} „[t]he major problem with setting up test suites of this kind is that of interaction between different linguistic phenomena. The standard solution is to reduce the linguistic complexity of all items other than the item of direct interest to an absolute minimum”. Dementsprechend wurden die ausgewählten Sätze wie folgt aufbereitet:

\subparagraph*{Orthografie}

\begin{itemize}
\item \textit{Eigennamen bzw. Bezeichnungen} wurden großgeschrieben (z. B. ‚Manueller‘ in ‚manueller Betrieb‘ im Satz ‚Die Funktion manueller Betrieb soll ausgewählt werden‘).
\item \textit{Rechtschreibfehler} wurden korrigiert, z. B. wurden überflüssige Kommas gelöscht.
\end{itemize}

\subparagraph*{Lexik}
\begin{itemize}
\item \textit{Anonymisierung der Sätze}: Firmen- und Produktnamen wurden z.~B. durch ‚die Firma‘, ‚das Programm‘, ‚das Gerät‘ usw. ersetzt.
\item \textit{Überflüssige Details}, Beispiele, Aufzählungen wurden eliminiert, um die Satzlänge zu kürzen.
\item \textit{Füllwörter und Adverbien} wurden eliminiert, wenn bei mehr als zwei MÜ-Systemen ersichtlich war, dass sie für die MÜ problematisch sind. Das galt für Adverbien, solange sie für die Satzsemantik nicht zwingend notwendig waren.
\item \textit{Firmen- sowie produktspezifische Termini und ungebräuchliche Fachtermini} wurden durch geläufige Begriffe ersetzt, z. B. wurde ‚Gerät‘ anstatt ‚Feinstzerkleinerer‘, ‚Steckdose‘ anstatt ‚Schutzkontaktsteckdose‘ verwendet. Ein Terminus wurde ersetzt, wenn er in den zwei häufig verwendeten Online-Wörterbüchern „dict.cc“ oder „leo.org“ nicht vorhanden war. Diese Anpassung wurde aus zwei Gründen vorgenommen; (1) damit sich die Bewerter während der Humanevaluation nicht mit der Suche nach genauen Übersetzungen für solche Termini beschäftigen; (2) aufgrund der Spezifität solcher Termini ist nicht zu erwarten, dass sie von den untersuchten generischen MÜ-Systemen korrekt übersetzt werden (siehe Auswahlkriterien der Systeme unter \sectref{sec:4.4.1}). In der Regel führen die Unternehmen Teminologiedatenbanken und integrieren sie in ihre implementierten Übersetzungsprogramme oder MÜ-Systeme. Die Terminologieintegration wäre zur Zeit der Durchführung der empirischen Studie (Ende 2016 -- Anfang 2017) zwar in der RMBÜ, SMÜ und HMÜ, jedoch nicht in der NMÜ, möglich gewesen (vgl. \citealt{Eisold2017}). Damit die Studie auf einer einheitlichen Basis durchgeführt wird, wurden alle Systeme in ihrem Ist-Zustand, d. h. ohne Terminologieintegration oder Training mit domänenspezifischen Daten, verwendet.\footnote{{{{Mehr zur Entwicklung der NMÜ im Bereich der Domänenadaptation bzw. Terminologieintegration unter \sectref{sec:3.2.4}.}}}}

Das Ersetzen solcher seltenen Termini wurde bei allen analysierten KS-Regeln mit Ausnahme der Regel „Für zitierte Oberflächentexte gerade Anführungszeichen verwenden“ vorgenommen, da Oberflächentexte und Bedienelemente naturgemäß firmen-, produktspezifische bzw. ungebräuchliche Termini beinhalten. Durch diese Ausnahme konnte der Effekt der Verwendung der geraden Anführungszeichen auf die Übersetzung von seltenen Termini bei den unterschiedlichen Systemen getestet und untersucht werden.

\item Sätze mit mehr als einem spezifischen bzw. ungebräuchlichen Wort wurden von der Analyse ausgeschlossen, um zu vermeiden, dass mehrere Änderungen im Satz vorgenommen werden.
\end{itemize}

\subparagraph*{Grammatik}

\begin{itemize}
\item \textit{Satzstruktur} wurde nicht verändert.
\end{itemize}

\subparagraph*{Satzlänge bei den einzelnen Regeln}

Die durchschnittliche Satzlänge betrug regelübergreifend 12 Wörter. Die Satzlänge variierte zwischen 5 und 17 Wörtern, wobei 97,2~\% der 216 Sätze eine Länge zwischen 8 und 16 Wörtern\footnote{{{{Nur bei 6 Sätzen lag die Länge bei 5 Wörtern (1 Satz), 6 Wörtern (2 Sätze), 7 Wörtern (2 Sätze) und 17 Wörtern (1 Satz).}}}} aufwiesen. Eine Spanne von 8 Wörtern ist relativ groß. Dies war jedoch nicht zu vermeiden, da bei manchen Regeln, wie z.~B. der Regel „Eindeutige pronominale Bezüge verwenden“, zwangsläufig ein Satz mit einem Haupt- und Nebensatz erforderlich ist. Eine Untergrenze der Satzlänge wurde im Vorfeld nicht gesetzt. Maßgeblich war, dass der Satz vollständig ist (d.~h. Phrasen wurden ausgeschlossen). Längere Sätze (über 17 Wörter) wurden aus zwei Gründen ausgeschlossen: (1) Obwohl die meisten KS-Regeln eine Satzlänge bis 25 Wörter erlauben (vgl. \citealt{WellsAkisEtAl2003}; \citealt{O’Brien2003}; \citealt{AikawaEtAl2007}; \citealt{FiedererO’Brien2009}; \citealt{Mügge2013}), so steigt mit der Länge der analysierten Sätze, auch die Wahrscheinlichkeit, dass die Bewerter während der Humanevaluation von Teilen außerhalb der KS-Stelle abgelenkt werden. (2) Eine Aufbereitung von langen oder komplexen Sätzen nach der Vorgehensweise unter Schritt [4] wäre mit mehreren Anpassungen und ggf. mehreren Bearbeitungsmöglichkeiten verbunden. Hierfür können keine klar definierten Bearbeitungsregeln festgelegt werden. Die Länge der Sätze bei den einzelnen Regeln sah wie folgt aus (\tabref{tab:4:5}):


\begin{table}
\fittable{
\begin{tabular}{lrrrrrrrrr}
\lsptoprule
& \textbf{anz} & { \textbf{fvg}} & \textbf{kos} & { \textbf{nsp}} & \textbf{pak} & { \textbf{pas}} & \textbf{per} & { \textbf{prä}} & \textbf{wte}\\
\midrule
\textbf{Mittelwert} &  11 & { 11} &  13 & { 14} &  11 & { 11} &  10 & { 12} &  12\\
\textbf{Max} &  15 & { 15} &  15 & { 16} &  15 & { 15} &  15 & { 16} &  17\\
\textbf{Min} &  8 & { 8} &  8 & { 8} &  7 & { 8} &  5 & { 8} &  7\\
\textbf{Median} &  11 & { 11} &  13 & { 15} &  11 & { 11} &  11 & { 11} &  12\\
\textbf{Standardabw.} &  2,12 & { 2,05} &  1,89 & { 2,33} &  1,86 & { 1,90} &  2,71 & { 2,33} &  2,54\\
\lspbottomrule
%\tablevspace
%\multicolumn{10}{p{\textwidth}}{\textbf{anz:} Für zitierte Oberflächentexte gerade Anführungszeichen verwenden} \\
%\multicolumn{10}{p{\textwidth}}{\textbf{fvg:} Funktionsverbgefüge vermeiden} \\
%\multicolumn{10}{p{\textwidth}}{\textbf{kos:} Konditionalsätze mit ‚Wenn‘ einleiten} \\
%\multicolumn{10}{p{\textwidth}}{\textbf{nsp:} Eindeutige pronominale Bezüge verwenden} \\
%\multicolumn{10}{p{\textwidth}}{\textbf{pak:} Partizipial-konstruktionen vermeiden}\\
%\multicolumn{10}{p{\textwidth}}{\textbf{pas:} Passiv vermeiden} \\
%\multicolumn{10}{p{\textwidth}}{\textbf{per:} Konstruktionen mit „sein +~zu +~Infinitiv“ vermeiden} \\
%\multicolumn{10}{p{\textwidth}}{\textbf{prä:} Überflüssige Präfixe vermeiden} \\
%\multicolumn{10}{p{\textwidth}}{\textbf{wte:} Keine Wortteile weglassen}
%\hhline%%replace by cmidrule{------------~~}
%\\
%\lspbottomrule
\end{tabular}
}
\caption{\label{tab:4:5}Länge der Sätze bei den einzelnen KS-Regeln}
\bspnote{\textbf{anz:} Für zitierte Oberflächentexte gerade Anführungszeichen verwenden\\
\textbf{fvg:} Funktionsverbgefüge vermeiden\\
\textbf{kos:} Konditionalsätze mit ‚Wenn‘ einleiten\\
\textbf{nsp:} Eindeutige pronominale Bezüge verwenden\\
\textbf{pak:} Partizipial-konstruktionen vermeiden\\
\textbf{pas:} Passiv vermeiden\\
\textbf{per:} Konstruktionen mit „sein +~zu +~Infinitiv“ vermeiden\\
\textbf{prä:} Überflüssige Präfixe vermeiden\\
\textbf{wte:} Keine Wortteile weglassen
}
\end{table}

\paragraph*{[5] Einsatz der KS-Regeln}

Nachdem die Ausgangssätze, die gegen die KS-Re\-geln verstoßen, im vorherigen Aufbereitungsschritt festgelegt wurden (Version vor-KS), erfolgte eine Umsetzung der neun KS-Regeln. Sollte ein Satz Verstöße gegen mehr als eine KS-Regel enthalten, wurde nur der für die untersuchte Regel relevante Verstoß eliminiert, denn die Anwendung mehrerer Regeln würde eine Abgrenzung der Auswirkungen jeder Regel erschweren. Zudem ist eine Korrektur verschiedener Verstöße in einem Satz nicht immer möglich. Vorherige Studien thematisierten diese Herausforderungen, die mit der Untersuchung des Einflusses \textit{einzelner} KS-Regeln einhergehen (vgl. \citealt{NybergEtAl2003}; \citealt{Doherty2012}: 30; \citealt{RamirezPolo2012}: 274).\footnote{{{{Eine Darstellung der Forschungsherausforderungen ist unter \sectref{sec:3.4.3} enthalten.}}}} Ramirez Polo (ebd.: 274) betonte, dass „it is not always straightforward to determine which rule had an effect on the quality of the segment, since many segments were affected by more that one rule”. \citet[74]{Roturier2006} wendete ebenfalls nur die untersuchte Regel im Satz an, um zu vermeiden, dass „some of the test suite’s segments could be contaminated and the internal validity of the study would be undermined” (ebd.). Das Ergebnis dieses Schrittes ist der Erhalt einer zweiten Version von jedem Ausgangssatz, die KS-regelkonform ist (Version nach-KS).

\paragraph*{[6] Qualitätsprüfung der Ausgangssätze (DE)}

Vor der Durchführung der maschinellen Übersetzung war es zunächst erforderlich, die Qualität der Ausgangssätze zu prüfen, da die Qualität des Ausgangstexts einen unmittelbaren Einfluss auf die des MÜ-Outputs hat. Mehrere Studien betonten die Bedeutung der Ausgangstextqualität bei der MÜ-Evaluation (vgl. \citealt{Brunette2000}; \citealt{CastilhoO’Brien2016}). In der Regel wird der Fokus bei der Evaluation auf den Zieltext als das Endprodukt gelegt, wobei der Ausgangstext außer Acht gelassen wird (vgl. \citealt{Molnár2012}).

In der vorliegenden Studie wurde die Qualität der Ausgangssätze wie folgt geprüft: Zuerst wurden die vor-KS- und nach-KS-Ausgangssätze von einer erfahrenen (7~Jahre Berufserfahrung) Übersetzerin mit Deutsch als Muttersprache orthografisch und grammatisch geprüft. In einem zweiten Schritt wurden die Ausgangsätze auf stilistische Akzeptanz geprüft. Die Prüfung wurde von zwei deutschen Linguisten durchgeführt: Der erste ist ein Akademiker im Bereich der Translationswissenschaft, Lexikograf und Autor von sechs Wörterbüchern. Der zweite ist Universitätsprofessor für Übersetzungswissenschaft mit dem Schwerpunkt MÜ und technische Übersetzung. Diese Prüfung zielte darauf ab, stilistisch kritische Sätze zu identifizieren. Stilistisch kritisch ist ein Satz dann, wenn er von einem deutschen Muttersprachler auf die vorhandene Weise nicht formuliert werden würde (d.~h. nicht authentisch klingt). Bei der Prüfung sollte zunächst der erste Beurteiler bei jedem Satz ein Häkchen setzen, um zu signalisieren, ob er den Satz stilistisch akzeptabel findet oder nicht. Sätze, die als stilistisch kritisch beurteilt wurden, durchliefen eine erneute Prüfung durch den zweiten Beurteiler.

Von den insgesamt 216 Ausgangssätzen wurden 27 Sätze (12,5~\%) nach-KS von dem ersten Beurteiler als stilistisch kritisch eingestuft. Beispielsweise war bei der Regel „Partizipialkonstruktionen vermeiden“ die Formulierung nach-KS des folgenden Satzes umstritten:

\begin{description}[font=\normalfont]
\item[Vor-KS:] \textit{Das Gerät verbindet sich mit \ul{der neu gewählten Netzwerkadresse}.}

\item[Nach-KS:] \textit{Das Gerät verbindet sich mit \ul{der Netzwerkadresse, die neu gewählt wird}.}
\end{description}

12 der 27 Sätze wurden ebenfalls vom zweiten Beurteiler als stilistisch kritisch klassifiziert. Diese Sätze wurden durch neue Sätze ersetzt und nach demselben Ablauf geprüft. Es blieben 15 Sätze (6,9~\% der 216 Ausgangssätze) der stilistisch kritischen Sätze umstritten. Die 15 Sätze kamen bei drei der neun KS-Regeln vor: vier Sätze in „Partizipialkonstruktionen vermeiden“, fünf Sätze in „Eindeutige pronominale Bezüge verwenden“ und sechs Sätze in „Keine Wortteile weglassen“.\footnote{{Bei der statistischen Analyse zeigten diese stilistisch kritischen Sätze keinen signifikanten Unterschied im Vergleich zu dem Rest der Sätze im Hinblick auf die Fehleranzahl- und die Qualitätsveränderung}{{{.}}}}

\paragraph*{[7] Übersetzung und Fehlerannotation}

Die Ausgangssätze vor und nach dem Einsatz der KS-Regeln wurden mit den fünf MÜ-Systemen ins Englische übersetzt. Bei der Übersetzung der zwei Versionen der 216 Sätze mit fünf MÜ-Sys\-te\-men ergibt sich ein Datensatz von 2.160 MÜ-Sätzen. Die Übersetzungsfehler in den MÜ-Sätzen wurden nach der festgelegten Fehlertaxonomie von einem in DE-EN vereidigten Übersetzer mit sechs Jahren Berufserfahrung annotiert (siehe \sectref{sec:4.4.4.1} und \sectref{sec:4.4.4.2}). Daraufhin wurden die annotierten Fehler von zwei professionellen Linguisten geprüft. Aufgrund der großen Anzahl der MÜ-Sätze (2.160 Sätze) prüfte jeder Linguist nur die Hälfte der MÜ-Sätze. Die Prüfung fand in Form einer Bewertung jedes Satzes statt, in der der Prüfer ankreuzen sollte, ob er der Annotation zustimmt oder nicht. Im Fall, dass er der vorherigen Annotation nicht zustimmte, musste er die Übersetzung annotieren. Der zweite Linguist prüfte im Anschluss beide Annotationen und entschied sich für eine davon.

Anschließend wurde die KS-Stelle nach den Regeln in \sectref{sec:4.4.2.2} identifiziert. Die Identifizierung der KS-Stelle in der MÜ erfolgte manuell, wie die Definition der KS-Stelle besagt, als Äquivalent der durch den Einsatz der KS-Regel umformulierten Stelle im Ausgangssatz. Eine automatische Identifizierung der MÜ einer bestimmten Stelle im Ausgangssatz auf Basis von Techniken des Wort-Alignments war laut vorherigen Studien in manchen Fällen ungenau bzw. fehlerhaft (vgl. \citealt{KöhnEtAl2003}; \citealt{OchNey2004}; \citealt{Callison-BurchEtAl2007}), daher hat die Forscherin diesen Schritt manuell durchgeführt.

\paragraph*{[8] Auswahl der MÜ-Sätze (Zielsätze) für die Humanevaluation}

Nicht alle 2.160 annotierten MÜ-Sätze wurden in der Humanevaluation bewertet. Dies ist auf zwei Gründe zurückzuführen:

Erstens -- da jeder Ausgangssatz in zwei Versionen (vor-KS und nach-KS) von fünf Systemen übersetzt wurde, existierten für jeden Ausgangssatz 10 MÜ. Die 10 Übersetzungen sind in vielen Fällen relativ ähnlich. Dementsprechend war es schwer vorstellbar, dass die Teilnehmer sie trotz Eintönigkeit konzentriert bzw. ohne negativen Einfluss auf ihre Leistung bewerten könnten. Um diesem Risiko entgegenzuwirken, war es erforderlich, die Anzahl der MÜ auf 5--6 pro Ausgangssatz einzugrenzen.

Zweitens -- Um das Studienziel (Untersuchung des Einflusses der \textit{einzelnen} KS-Regeln) zu realisieren, war es erforderlich, dass die Bewerter in der Humanevaluation möglichst nur die KS-Stelle\footnote{{Mehr zur „KS-Stelle“ unter \sectref{sec:4.4.2.1}.}} im Fokus behielten. Dementsprechend mussten alle Fehler außerhalb der KS-Stelle korrigiert werden. Dieser grundlegende Schritt war unerlässlich, andernfalls hätten die Bewerter die außerhalb der KS-Stelle aufgetretenen Fehler bei der Bewertung mitberücksichtigt. Nicht alle MÜ-Sätze lassen sich außerhalb der KS-Stelle korrigieren, \textit{ohne dass diese Korrektur die KS-Stelle beeinflusst}.

In der Regel sind Übersetzungen, die mehr als 2-3 falsche Wörter\footnote{{{{Ein falsches Wort kann einen oder mehrere Fehler aufweisen, z. B. Großschreibfehler und falsche Wortstellung.}}}} beinhalten, schwer zu korrigieren,\footnote{{{{Eine genaue Beschreibung der Aufbereitung der MÜ-Sätze ist unter Schritt [9] aufgeführt.}}}} ohne dass die KS-Stelle von der Korrektur beeinflusst werden würde. Dieser Grenzbereich stellte das Ausschlusskriterium bei der Auswahl der Zielsätze für die Humanevaluation dar. Das Ausschlusskriterium wurde folgendermaßen umgesetzt:

\begin{itemize}
\item \textit{Im Falle der Übersetzungen, die innerhalb der KS-Stelle Fehler beinhalten,} wurden die Sätze analysiert, in denen max. 2 Wörter \textit{außerhalb} der KS-Stelle Fehler beinhalten. Das Ausschlusskriterium wurde unabhängig von der Fehleranzahl \textit{innerhalb} der KS-Stelle umgesetzt, vorausgesetzt die Fehler außerhalb der KS-Stelle konnten ohne Einfluss auf die KS-Stelle korrigiert werden.
\item \textit{Im Falle der Übersetzungen, die nur außerhalb der KS-Stelle Fehler beinhalten,} wurden die Sätze analysiert, in denen max. 3 Wörter Fehler beinhalten, solange die Korrektur der Fehler außerhalb der KS-Stelle keinen Einfluss auf die KS-Stelle hat.
\end{itemize}

In einem zweiten Schritt wurde überprüft, ob die bei jeder Regel häufig vorgekommenen Fehlertypen durch das Ausschlusskriterium nicht reduziert wurden. Sollte ein häufig vorgekommener Fehlertyp nicht in einem ähnlichen Verhältnis repräsentiert sein, wurden die ausgeschlossenen Übersetzungen nochmal überprüft und die Anzahl der zulässigen falschen Wörter erhöht, damit eine Übersetzung mit diesem Fehlertyp miteinbezogen werden konnte. Die einzige Voraussetzung hierfür ist, dass diese MÜ außerhalb der KS-Stelle gut korrigierbar ist (d.~h. ohne Einfluss auf die KS-Stelle zu haben), z. B. befinden sich die Fehler im Hauptsatz und die KS-Stelle im Nebensatz.

\paragraph*{[9] Aufbereitung der MÜ-Sätze (Zielsätze) für die Humanevaluation}

Nachdem die Zielsätze nach den genannten Kriterien herausgefiltert wurden, wurden sie für die Humanevaluation aufbereitet. Die Aufbereitung bestand aus zwei Schritten: einer Korrektur der Fehler außerhalb der KS-Stelle sowie einer Vereinheitlichung der MÜ-Sätze außerhalb der KS-Stelle.

Fehler außerhalb der KS-Stelle wurden mit der minimalen Anzahl von Edits korrigiert. \tabref{tabex:2} zeigt, wie die Fehler außerhalb der KS-Stelle korrigiert wurden:


\begin{table}
    \begin{tabularx}{\textwidth}{lQ}
\lsptoprule
\textbf{Vor KS} & Der Hersteller \textbf{übernimmt} keine \textbf{Haftung} für Schäden, die durch nicht bestimmungsgemäßen Gebrauch entstanden sind. \\
\tablevspace
SMÜ SDL & The manufacturer \txblue{accepts} no \txblue{liability} for damage caused by improper use.\\
HMÜ Systran & The manufacturer does not \txred{take over} \txblue{liability} for damage, which resulted from not intended use.\\
RBMÜ Lucy & The manufacturer does not \txred{take over} \txblue{liability} for damages which arose through use \itul{not purpose-appropriate}.\\
\midrule
\textbf{Nach KS} & Der Hersteller \textbf{haftet} nicht für Schäden, die durch nicht bestimmungsgemäßen Gebrauch entstanden sind.\\
\tablevspace
SMÜ SDL & The manufacturer \txblue{is} not \txblue{liable} for damage caused by improper use.\\
HMÜ Systran & The manufacturer \txblue{is} not \txblue{responsible} for damage, which resulted from not intended use.\\
RBMÜ Lucy & The manufacturer \txblue{is} not \txblue{liable} for damages which arose through use \itul{not purpose-appropriate}.\\
\lspbottomrule
    \end{tabularx}
    \caption{Beispiel 2}
    \label{tabex:2}
    \bspnote{
Die KS-Stelle ist farblich dargestellt: \txblue{Blau} wird für die korrekten Teile der Übersetzung verwendet; \txred{rot} für die falschen Teile.
Fehler \textit{außerhalb} der KS-Stelle, die korrigiert wurden, sind \itul{kursiv und unterstrichen} dargestellt.
}

\end{table}

In \tabref{tabex:2} geht es um die KS-Regel „Funktionsverbgefüge vermeiden“. Nur drei von den fünf untersuchten Systemen werden dargestellt, da die MÜ der zwei weiteren Systeme (Bing und Google Translate) identisch mit der MÜ von SDL war. In diesem Beispiel beinhaltete nur die MÜ von Lucy einen Fehler außerhalb der KS-Stelle (‚not purpose-appropriate‘). Bei der Aufbereitung der MÜ-Sätze für die Humanevaluation wurde dieser Fehler korrigiert.

Der zweite aufbereitende Schritt war die Vereinheitlichung der MÜ aller MÜ-Systeme außerhalb der KS-Stelle. Dieser Schritt wurde aus zwei Gründen vorgenommen: Durch die Vereinheitlichung (1) lässt sich der Einfluss verschiedener Fehlertypen auf die MÜ desselben Ausgangssatzes vergleichen; und (2) können die MÜ der KS-Stelle in den einzelnen Systemen einander gegenübergestellt werden. Das Ergebnis dieses Schritts war im Falle des vorherigen Beispielsatzes wie folgt (\tabref{tabex:3}).


\begin{table}
    \begin{tabularx}{\textwidth}{lQ}
\lsptoprule
\textbf{Vor KS} & Der Hersteller \textbf{übernimmt} keine \textbf{Haftung} für Schäden, die durch nicht bestimmungsgemäßen Gebrauch entstanden sind. \\
\tablevspace
SMÜ SDL & The manufacturer \txblue{accepts} no \txblue{liability} for \txgray{any damage} \txgray{caused by improper use.}\\
HMÜ Systran & The manufacturer does not \txred{take over} \txblue{liability} for \txgray{any} \txgray{damage caused by improper use.}\\
RBMÜ Lucy & The manufacturer does not \txred{take over} \txblue{liability} for \txgray{any} \txgray{damage caused by improper use.}\\
\midrule
\textbf{Nach KS} & Der Hersteller \textbf{haftet} nicht für Schäden, die durch nicht bestimmungsgemäßen Gebrauch entstanden sind.\\
\tablevspace
SMÜ SDL & The manufacturer \txblue{is} not \txblue{liable} for \txgray{any damage caused by} \txgray{improper use.}\\
HMÜ Systran & The manufacturer \txblue{is} not \txblue{responsible} for \txgray{any damage} \txgray{caused by improper use.}\\
RBMÜ Lucy & The manufacturer \txblue{is} not \txblue{liable} for \txgray{any damage caused by} \txgray{improper use.}\\
\lspbottomrule
    \end{tabularx}
    \caption{Beispiel 3}
    \label{tabex:3}
    \bspnote{
\txgray{Hervorgehobene Stellen} zeigen die vereinheitlichten Stellen in der MÜ auf.
}

\end{table}

Übersetzungen, die außerhalb der KS-Stelle in allen Systemen nicht vereinheitlicht werden können, wurden von der Humanevaluation ausgeschlossen, damit die Bewertungen aller MÜ jedes Ausgangssatzes vergleichbar bleiben.

Zwei Ausnahmen waren bei der Aufbereitung der MÜ-Sätze zu berücksichtigen: (1) Der Wortstellungsfehler wurde von der Korrektur außerhalb der KS-Stelle ausgenommen. Eine Korrektur der Wortstellung außerhalb der KS-Stelle würde dazu führen, dass die Wortstellung innerhalb der KS-Stelle geändert bzw. korrigiert wird. In diesem Fall wurden beide Wortstellungsfehler (innerhalb und außerhalb der KS-Stelle) beibehalten. \tabref{tabex:4} zeigt diesen Fall:


\begin{table}
    \begin{tabularx}{\textwidth}{p{.25\textwidth}Q}
\lsptoprule
\textbf{Vor KS} & Bei der Arbeit mit elektrischen Geräten \textbf{sollte} stets ein Sicherheitsstecker \textbf{verwendet werden.}\\
\tablevspace
SMÜ SDL

Fehlerannotation & When working with electrical \itul{equipment} \txred{should} always be \bfitul{a safety plug} \txred{is used}.\\
SMÜ SDL

Humanevaluation & When working with electrical \itul{devices}, \txred{should} always be \bfitul{a safety plug} \txred{is used}.\\
\lspbottomrule
    \end{tabularx}
    \caption{Beispiel 4}
    \label{tabex:4}
    \bspnote{
Die KS-Stelle ist farblich dargestellt: \txblue{Blau} wird für die korrekten Teile der Übersetzung verwendet; \txred{rot} für die falschen Teile.
Fehler \textit{außerhalb} der KS-Stelle, die korrigiert wurden, sind \itul{kursiv und unterstrichen} dargestellt; bzw. \bfitul{fett, kursiv und unterstrichen} bei Wortstellungsfehlern \textit{außerhalb} der KS-Stelle, die nicht korrigiert wurden.
}

\end{table}


In \tabref{tabex:4} geht es um die KS-Regel „Passiv vermeiden“. In diesem Beispiel wurde der Wortstellungsfehler (in ‚a safety plug‘) nicht behoben, damit die Wortstellung der KS-Stelle nicht korrigiert wird. Der andere, semantische Fehler (‚equipment‘) außerhalb der KS-Stelle wurde korrigiert.

(2) Der zweite Sonderfall trat bei der Regel „eindeutige pronominale Bezüge verwenden“ auf. Bei dieser Regel spielt der Bezug bei der Bewertung eine Hauptrolle. Hierbei wäre eine semantische Vereinheitlichung aller MÜ außerhalb der KS-Stelle irreführend. Dieser Fall wird durch \tabref{tabex:5} veranschaulicht. Die vor-KS Version lautete ‚Je früher ein Fleck behandelt wird, umso größer ist die Wahrscheinlichkeit, \textit{ihn} rückstandslos zu entfernen‘:


\begin{table}
    \begin{tabularx}{\textwidth}{p{.25\textwidth}Q}
\lsptoprule
\textbf{nach KS} & Je früher ein Fleck behandelt wird, umso größer ist die Wahrscheinlichkeit, \textbf{den Fleck} rückstandslos zu entfernen.\\
\tablevspace
HMÜ Systran

Fehlerannotation &  \itul{Ever in former times} \txgray{a mark} is treated, \itul{all the more largely} is the probability to remove \txblue{the mark} residueless.\\
HMÜ Systran

Humanevaluation & \itul{The earlier} \txgray{a mark} is treated, \itul{the higher the possibility} of removing \txblue{the mark} without leaving any residue.\\
\tablevspace
RBMÜ Lucy

Fehlerannotation & \itul{Each formerly} \txgray{a spot} is treated, \itul{the larger the probability} is to remove \txblue{the spot} \itul{free of residues}.\\
RBMÜ Lucy

Humanevaluation & \itul{The earlier} \txgray{a spot} is treated, \itul{the higher the possibility} of removing \txblue{the spot} \itul{without leaving any residue}.\\
\lspbottomrule
    \end{tabularx}
    \caption{Beispiel 5}
    \label{tabex:5}
    \bspnote{
Die KS-Stelle ist farblich dargestellt: \txblue{Blau} wird für die korrekten Teile der Übersetzung verwendet; \txred{rot} für die falschen Teile.
Fehler \textit{außerhalb} der KS-Stelle, die korrigiert wurden, sind \itul{kursiv und unterstrichen} dargestellt; bzw.\bfitul{fett, kursiv und unterstrichen} bei Wortstellungsfehlern \textit{außerhalb} der KS-Stelle, die nicht korrigiert wurden.
}

\end{table}

Nach der Regel „eindeutige pronominale Bezüge verwenden“ wurde das Pronomen ‚ihn‘ durch seinen Bezug ‚den Fleck‘ in der nach-KS Version ersetzt. Das Wort ‚Fleck‘ wurde von den beiden Systemen unterschiedlich übersetzt (‚mark‘ bei Systran; ‚spot‘ bei Lucy). Entsprechend übersetzten die Systeme den Bezug \textit{innerhalb} der KS-Stelle (blauer Text im Beispiel) unterschiedlich. Bei der Aufbereitung wurde die Übersetzung des Worts ‚Fleck‘ \textit{außerhalb} der KS-Stelle (hervorgehoben im Beispiel) bei den unterschiedlichen Systemen \textit{nicht} vereinheitlicht. Alle anderen unterschiedlichen Stellen in den Übersetzungen wurden vereinheitlicht.

\paragraph*{[10] Qualitätsprüfung der Zielsätze (EN)}

Nachdem die Übersetzungsfehler außerhalb der KS-Stelle korrigiert wurden, war es erforderlich sicherzustellen, dass die Inhalts- und Stilqualität außerhalb der KS-Stelle (trotz etwaiger Fehler innerhalb der KS-Stelle) akzeptabel sind. Diese Qualitätsprüfung fand über zwei Phasen statt: In der ersten Phase wurde ein Testlauf für alle MÜ-Sätze, die im vorherigen Schritt aufbereitet wurden, durchgeführt. In diesem Testlauf bewertete eine qualifizierte irische Übersetzerin mit sieben Jahren Berufserfahrung die Inhalts- und Stilqualität der MÜ-Sätze.\footnote{{Im Testlauf wurden die Qualitätsdefinitionen vereinfachter bzw. kürzer (als unter \sectref{sec:4.4.5.1}) angegeben, da es im Test primär um die Darstellung der Ausgangssätze ging.}}


\begin{figure}
\includegraphics[width=\textwidth]{figures/Beispiel6.png}
\caption{\label{figex:4:6}Erster Schritt der Qualitätsprüfung}
\end{figure}

\newpage
Wie \figref{figex:4:6} zeigt, bestand die Aufgabe im Testlauf darin, die Inhalts- und Stilqualität auf zwei 5-Likert-Skalen zu bewerten und etwaige Fehler bzw. stilistisch kritische Stellen zu kommentieren. MÜ-Sätze, die aufgrund einer Stelle außerhalb der KS-Stelle schlecht bewertet wurden, wurden herausgefiltert. Zwei wesentliche Ergebnisse lieferte dieser Testlauf: Erstens wurde dadurch sichergestellt, dass die MÜ-Sätze \textit{außerhalb} der KS-Stelle keine orthografischen, lexikalischen, grammatischen bzw. semantischen Fehler beinhalten. Zweitens zeigten die Kommentare, dass die bemängelten Stellen primär aus stilistischen Gründen kritisiert wurden.

Auf Basis dieser Ergebnisse wurden die aufbereiteten Zielsätze in der zweiten Phase erneut in einer anderen Testform von zwei Beurteilern geprüft. Der erste Beurteiler ist ein Englischmuttersprachler aus den USA, ein qualifizierter Übersetzer mit mehr als 20 Jahren Berufserfahrung. Die zweite Beurteilerin ist ebenfalls eine Englischmuttersprachlerin, aus Großbritannien, eine qualifizierte Übersetzerin und CELTA-geprüfte Trainerin für Englisch als Fremdsprache. Das Augenmerk lag hierbei auf dem Stil der korrigierten Zielsätze unabhängig von möglichen Fehlern \textit{innerhalb} der KS-Stelle. Mit anderen Worten, die Sätze wurden überprüft, um sicherzustellen, dass die MÜ (außerhalb der KS-Stelle) nach der Fehlerkorrektur mit der minimalen Anzahl von Edits stilistisch akzeptabel blieb. Hierfür wurde die KS-Stelle in jedem Satz markiert und die Bewerter mussten bei jedem Satz ankreuzen, ob die MÜ außerhalb der KS-Stelle stilistisch akzeptabel ist oder nicht (Ja/Nein-Antworten). MÜ-Sätze, die beide Beurteiler als stilistisch inakzeptabel einstuften, wurden von der Humanevaluation ausgeschlossen. MÜ-Sätze, die ein Beurteiler in dieser Phase sowie der Prüfer im Testlauf (d. h. in der ersten Phase) stilistisch kritisierten, wurden ebenfalls von der Humanevaluation ausgeschlossen.


\subsubsection{\label{sec:4.4.3.2}Anzahl und Ausgewogenheit der Zielsätze}

Nachdem die Zielsätze in den letzten Schritten mehrmals gefiltert wurden, war es erforderlich sicherzustellen, dass die Annotationsgruppen sowohl in der Fehlerannotation als auch in der Humanevaluation in vergleichbaren Verhältnissen repräsentiert sind; beispielsweise wenn bei einer Regel in der Fehlerannotation 30~\% der Übersetzung RR, 20~\% FF, 25~\% FR und 25~\% RF waren, wurde bei der Humanevaluation darauf geachtet, dass die vier Gruppen in sehr ähnlichen Verhältnissen repräsentiert sind. Eine vergleichbare Repräsentation der Annotationsgruppen in den beiden Analysen ist essentiell, um einer Verzerrung des Gesamtergebnisses vorzubeugen. Dadurch, dass die Auswahl- bzw. Ausschlusskriterien meistens fehlerorientiert waren, wurden mehr Übersetzungen der Gruppen, die Fehler beinhalten, ausgeschlossen, nämlich FF, RF und FR. Aus diesem Grund war die Gruppe RR in der Humanevaluation überpräsentiert und musste reduziert werden. Diese Reduzierung erfolgte gemäß den folgenden Kriterien:

\begin{itemize}
\item \textit{Im Falle der Ausgangssätze, die von 3 Systemen oder weniger vor und nach dem Einsatz der KS-Regel richtig übersetzt wurden (Gruppe RR),} wurde der Output eines Systems ausgeschlossen.
\item \textit{Im Falle der Ausgangssätze, die von 4 oder 5 Systemen vor und nach dem Einsatz der KS-Regel richtig übersetzt wurden (Gruppe RR),} wurden die Outputs zweier Systeme ausgeschlossen.
\end{itemize}

Insbesondere bei der Annotationsgruppe RR war der MÜ-Output (meist innerhalb der KS-Stelle) von mehreren Systemen oft identisch. Bei einer Überrepräsentation waren daher diese identischen MÜ-Outputs geeignete Kandidaten für den Ausschluss nach den genannten Kriterien.

Auf Systemebene erfolgte der Ausschluss auf randomisierter Basis, so dass alle Systeme in ungefähr vergleichbaren Prozentsätzen vertreten blieben. Sollte innerhalb einer Regel nach den genannten Kriterien, z.~B. bei vier Ausgangssätzen, jeweils der Output eines Systems ausgeschlossen werden, so wurde bei jedem Ausgangssatz randomisiert ein anderes System ausgeschlossen. \figref{fig:4:7} zeigt die Aufteilung der Zielsätze in der Humanevaluation bei den einzelnen MÜ-Systemen.


\begin{figure}
%\includegraphics[width=\textwidth]{figures/d3-img003.png}
%7
\begin{tikzpicture}
    \pie[radius=2,
        text=inside,
        style={ultra thin},
        color={lsNightBlue,lsDarkBlue,lsMidDarkBlue,lsMidBlue,lsLightBlue},
%         sum=auto,
%         text=legend,
        rotate=90] {25/Google,
                   20/Lucy,
                   18/Bing,
                   17/Systran,
                   20/SDL};
    \end{tikzpicture}

\caption{\label{fig:4:7}Aufteilung der Zielsätze in der Humanevaluation auf Systemebene}
\end{figure}

\tabref{tab:4:6} zeigt die Verhältnisse der Annotationsgruppen in der Fehlerannotation sowie der Humanevaluation auf Regelebene. Daraus wird ersichtilich, dass die Annotationsgruppen in der Humanevaluation in vergleichbarem Verhältnis wie in der Fehlerannotation stehen.


\begin{table}
\small
\begin{tabularx}{\textwidth}{Xrrrr}
\lsptoprule
& \textbf{FR} & \textbf{FF} & \textbf{RF} & \textbf{RR}\\
\midrule
\multicolumn{5}{l}{\textbf{Regel „Konstruktionen mit ‚sein + zu + Infinitiv‘ vermeiden“}}\\
 Fehlerannotation & 27,8~\% & 20,6~\% & 13,4~\% & 38,1~\%\\
 Humanevaluation & 27,5~\% & 20,8~\% & 12,5~\% & 39,2~\%\\
\tablevspace
 \multicolumn{5}{l}{\textbf{Regel „Partizipialkonstruktion vermeiden“}}\\
 Fehlerannotation & 9,2~\% & 37,8~\% & 23,5~\% & 29,6~\%\\
 Humanevaluation & 9,2~\% & 41,7~\% & 21,7~\% & 27,5~\%\\
\tablevspace
\multicolumn{5}{l}{\textbf{Regel „Eindeutige pronominale Bezüge verwenden“}}\\
 Fehlerannotation & 16,9~\% & 13,0~\% & 13,0~\% & 57,1~\%\\
 Humanevaluation & 17,5~\% & 13,3~\% & 11,7~\% & 57,5~\%\\
\tablevspace
\multicolumn{5}{l}{\textbf{Regel „Konditionalsätze mit ‚Wenn‘ einleiten“}}\\
 Fehlerannotation & 27,4~\% & 16,7~\% & 3,6~\% & 52,4~\%\\
 Humanevaluation & 24,2~\% & 20,8~\% & 5,0~\% & 50,0~\%\\
\tablevspace
\multicolumn{5}{l}{\textbf{Regel „Keine Wortteile weglassen“}}\\
 Fehlerannotation & 18,2~\% & 28,4~\% & 13,6~\% & 39,8~\%\\
 Humanevaluation & 18,3~\% & 28,3~\% & 10,8~\% & 42,5~\%\\
\tablevspace
\multicolumn{5}{l}{\textbf{Regel „Passiv vermeiden“}}\\
 Fehlerannotation & 7,2~\% & 30,1~\% & 14,5~\% & 48,2~\%\\
 Humanevaluation & 8,3~\% & 29,2~\% & 13,3~\% & 49,2~\%\\
\tablevspace
\multicolumn{5}{l}{\textbf{Regel „Funktionsverbgefüge vermeiden“}}\\
 Fehlerannotation & 29,4~\% & 20,0~\% & 8,2~\% & 42,4~\%\\
 Humanevaluation & 29,2~\% & 20,8~\% & 8,3~\% & 41,7~\%\\
\tablevspace
\multicolumn{5}{l}{\textbf{Regel „Überflüssige Präfixe vermeiden“}}\\
 Fehlerannotation & 17,4~\% & 15,2~\% & 2,2~\% & 65,2~\%\\
 Humanevaluation & 13,3~\% & 15,8~\% & 3,3~\% & 67,5~\%\\
\tablevspace
\multicolumn{5}{l}{\textbf{Regel „Für zitierte Oberflächentexte gerade Anführungszeichen $"$…$"$ verwenden“}}\\
 Fehlerannotation & 32,4~\% & 40,5~\% & 1,4~\% & 25,7~\%\\
 Humanevaluation & 30,8~\% & 41,7~\% & 1,7~\% & 25,8~\%\\
\lspbottomrule
\end{tabularx}
\caption{\label{tab:4:6}Verhältnisse der Annotationsgruppen in der Fehlerannotation sowie der Humanevaluation auf Regelebene}
\end{table}


Die Gesamtzahl der von den Teilnehmern evaluierten MÜ-Sätze betrug 1.100 Sätze. Darüber hinaus gab es insgesamt 545 MÜ-Sätze, die bei mehreren MÜ-Systemen identisch waren (d.~h. die Ausgangssätze wurden von verschiedenen Systemen identisch übersetzt). Für jeden Ausgangssatz wurde nur eine Instanz der identischen MÜ-Sätze in der Humanevaluation bewertet, somit waren 95 der 545 Instanzen in den 1.100 humanevaluierten MÜ-Sätzen enthalten. Alle weiteren identischen wiederholten Instanzen (450 von 545) erhielten denselben Score der bewerteten Instanzen. Daher basieren die gelieferten Ergebnisse der Humanevaluation auf einer Gesamtzahl von 1.550 MÜ-Sätzen (1.100 + 450). \tabref{tab:4:7} präsentiert den Anteil der humanevaluierten MÜ-Sätze von den 240 fehlerannotierten MÜ-Sätzen pro Regel.


\begin{table}
\begin{tabularx}{\textwidth}{Xr}
\lsptoprule
Für zitierte Oberflächent. gerade Anführungsz. verwenden & 62~\%\\
Funktionsverbg. vermeiden & 70~\%\\
Konditionals. mit ‚Wenn‘ einleiten & 70~\%\\
Eindeutige pronomin. Bezüge verwenden & 64~\%\\
Partizipialkonst. vermeiden & 82~\%\\
Passiv vermeiden &69~\%\\
Konstr. mit „sein +~zu +~Infinitiv“ vermeiden & 81~\%\\
Überflüssige Präfixe vermeiden &77~\%\\
Keine Wortteile weglassen& 73~\%\\
\lspbottomrule
\end{tabularx}
\caption{\label{tab:4:7}Anteil der humanevaluierten MÜ-Sätze von den 240 fehlerannotierten MÜ-Sätzen pro Regel}
\bspnote{N der Humanevaluation = 1.550 MÜ-Sätze bestehend aus 1.100 humanevaluierten MÜ-Sätzen plus 450 wiederholten MÜ-Sätzen}
\end{table}

Wie \tabref{tab:4:7} zeigt, wurden bei den Regeln über 60 \% der annotierten MÜ-Sätze humanevaluiert. Der Anteil der evaluierten MÜ-Sätze bewegte sich zwischen 62 \% und 82 \%.

\subsection{Design der Fehlerannotation}
\label{sec:4.4.4}
\textit{Das Ziel der Fehlerannotation} liegt darin, die Übersetzungsfehler vor und nach dem Einsatz der einzelnen KS-Regeln zu identifizieren und zu vergleichen. Die Fehlerannotation lieferte Ergebnisse zu Fehleranzahl und -typen sowie zur Erscheinung bzw. Eliminierung von bestimmten Fehlertypen in Zusammenhang mit einer bestimmten Regel. Darüber hinaus wurden die Ergebnisse der Fehlerannotation mit denen der Humanevaluation trianguliert (siehe \sectref{sec:4.4.5.4}).

Wie unter \sectref{sec:4.4.3.1} detailliert erklärt, umfasst der Datensatz zwei Versionen von jedem Ausgangssatz: Die \textit{vor-KS-Version} stellt den Ausgangssatz mit dem Verstoß gegen eine der neun analysierten KS-Regeln dar. In der \textit{nach-KS-Version} wurde der Verstoß durch den Einsatz der entsprechenden KS-Regel behoben.\footnote{Für
    mehr zu der Vorgehensweise beim Einsatz der Regeln siehe \sectref{sec:4.4.3.1} (Schritt [5]) sowie \sectref{sec:4.4.2.2}, die die Darstellung der analysierten Regel genauer erläutert.
} Bei der Fehlerannotation wurden die Fehler in der maschinellen Übersetzung beider Versionen identifiziert und einem Fehlertyp zugeordnet. Die Auswahl und Festlegung der Fehlertaxonomie sowie der Ablauf der Fehlerannotation werden im Folgenden genauer erläutert.

\subsubsection{\label{sec:4.4.4.1} Auswahl und Festlegung der Fehlertaxonomie}

Vorherige Studien, wie in der Diskussion der Fehlerannotationsmethode (unter \sectref{sec:3.3.3.1}) detailliert dargestellt, bieten zahlreiche Fehlertaxonomien an, die sich jedoch im Kern ähneln. Dadurch, dass die Taxonomien für unterschiedliche Zwecke und Zielgruppen entwickelt wurden, variieren sie in Hinsicht auf ihren Granularitätsgrad. Einige Fehlertaxonomien sind detailliert (vgl. \citealt{Flanagan1994}; \citealt{Correa2003}), andere wenden wenige aber breite und mehrstufige Klassifikationen (vgl. \citealt{VilarEtAl2006}; \citealt{FarrúsEtAl2010}) an. Infolge dieser Diversität zielten \citet{VilarEtAl2006} mit ihrer Fehlertaxonomie darauf ab, eine explizite Fehlerklassifikation zu entwickeln. Vilar et al. (ebd.) lieferten eine dreistufige Fehlertaxonomie, die aus fünf Hauptkategorien (fehlende Wörter, Wortstellungsfehler, falsche Wörter, unbekannte Wörter und Zeichensetzungsfehler) besteht und häufig in der Evaluationsforschung verwendet wird (u.a. in \citealt{AvramidisKöhn2008}, \citealt{Bojar2011} und \citealt{PopovićBurchardt2011}). Aufgrund der Explizität, Übersichtlichkeit sowie des angemessenen Granularitätsgrads und Umfangs der Fehlertaxonomie von \citet{VilarEtAl2006} wurde sie in der vorliegenden Studie als Basis der Fehlerannotation angewendet. Der Umfang der in dieser Taxonomie enthaltenen Fehlertypen war für das Studienziel angebracht, da es in der Studie nicht allgemein um die Bewertung von MÜ-Outputs verschiedener Systeme geht, sondern um eine explizite Bewertung der Fehler, die in Zusammenhang mit den analysierten KS-Regeln auftreten. Die Anwendung der Fehlertaxonomie von Vilar et al. (ebd.) schließt jedoch nicht aus, dass eine andere umfangreichere Taxonomie, wie z. B. das MQM-Framework, zur Analyse herangezogen werden kann. Dies wäre insbesondere bei der Analyse von weiteren KS-Regeln, die feinkörnige bzw. weitere spezifische Fehlertypen erfordern, sinnvoll.

Die Festlegung einer Fehlertaxonomie fand auf Basis eines Bottom-up-Ansat\-zes in zwei Schritten statt. Im ersten Schritt wurde die Fehlertaxonomie von \citet{VilarEtAl2006} zur Orientierung herangezogen. In diesem Schritt wurden bei jeder Regel sieben Sätze annotiert. Auf Basis dieser Annotation stellte es sich heraus, dass die Fehlertaxonomie von Vilar et al. einen Fehlertyp „Redewendungen“ (idiomatic expressions)\footnote{{Das Beispiel von \citet{VilarEtAl2006} dafür war: „It’s raining cats and dogs.”}} beinhaltet, der in den analysierten MÜ-Sätzen nicht auftrat. Des Weiteren führen Vilar et al. einen Fehlertyp für den „Stil“ auf. In dieser Studie wird der Stil nicht als Fehlertyp betrachtet, sondern als ein Übersetzungsproblem, dessen Einfluss im Rahmen der Humanevaluation gesondert bewertet wird. Beide Fehlertypen (Redewendungen und Stil) werden von Vilar et al. (ebd.: 698) als „less important“ bezeichnet, dennoch wurden sie nicht wegen ihrer Bedeutung, sondern aus den genannten Gründen ausgeschlossen. Auf der anderen Seite kamen drei Fehlertypen vor, die nicht direkt in der Fehlertaxonomie von Vilar et at. jedoch im Datensatz vertreten sind. Diese Fehler sind „Großschreibung“, „Konsistenzfehler“ und „Kollokationsfehler“. Die drei Fehler traten überwiegend bei drei Regeln\footnote{{Großschreibungsfehler bei der Regel „Für zitierte Oberflächentexte gerade Anführungszeichen verwenden“; Konsistenzfehler bei „Eindeutige pronominale Bezüge verwenden“ und Kollokationsfehler bei „Funktionsverbgefüge vermeiden“.}} auf, daher war es erforderlich, sie explizit hinzuzufügen, damit sie direkt (d.~h. nicht als Unterkategorie) analysiert werden. \tabref{tab:4:8} listet die Fehlertypen nach Vilar et al. gegenüber den aus dem ersten Analyseschritt resultierenden Fehlertypen auf.


\begin{table}[t]
\begin{tabularx}{\textwidth}{Qll}
\lsptoprule
\textbf{Fehlertypen} & \makecell[tl]{\textbf{\citet{VilarEtAl2006}}\\(Bezeichnung nach Vilar)} & \makecell[tl]{\textbf{Vorliegende}\\\textbf{Studie}}\\
\midrule
{\textbf{Orthografie}} & {} & \\
 Zeichensetzungsfehler & Ja (Punctuation) & Ja\\
 Großschreibungsfehler & Nein & Ja\\
\tablevspace
 {\textbf{Lexik}} & {} & \\
 Wort ausgelassen & Ja (Missing words) & Ja\\
 Wort extra eingefügt & Ja (Extra words) & Ja\\
 Wort unübersetzt geblieben & Ja (Unknown words) & Ja\\
 Konsistenzfehler & Nein & Ja\\
\tablevspace
{\textbf{Grammatik}} & {} & \\
 falsche Wortart / Wortklasse & Ja (Incorrect form) & Ja\\
 falsches Verb & Ja (Incorrect form) & Ja\\
 Kongruenzfehler & Ja (Incorrect form) & Ja\\
 falsche Wortstellung & Ja (Word order) & Ja\\
\tablevspace
{\textbf{Semantik}} & {} & \\
 Verwechselung des Sinns & Ja (Sense) & Ja\\
 falsche Wahl & Ja (Wrong choice) & Ja\\
 Kollokationsfehler & Nein & Ja\\
 Redewendungen & Ja (Idioms) & Nein\\
\tablevspace
\textbf{Stil} & Ja (Style) & Nein\\
\lspbottomrule
\end{tabularx}
\caption{\label{tab:4:8}Fehlertypen nach \citet{VilarEtAl2006} gegenüber den der vorliegenden Studie}
\end{table}

In der dreistufigen Fehlertaxonomie von \citet{VilarEtAl2006} wurden zum Teil die Fehlertypen und ihre Ursachen gemischt. Die erste Stufe beinhaltet z.~B. die Kategorien „Missing words“ und „Unknown words“. In der zweiten Stufe folgen bei „Missing words“ die Unterkategorien „Content words“ und „Filler words“ (d.~h. eine genauere Spezifizierung des Fehlertyps), während die „Unknown words“ in der zweiten Stufe in „Unknown stem“ und „Unseen forms“ (Ursache des Fehlers) unterteilt werden. Der Einsatz einer mehrstufigen Fehlertaxonomie hätte die Analyse unübersichtlich und unnötig kompliziert gestaltet. Daher wurden die identifizierten Fehlertypen im zweiten Schritt auf einer Stufe gruppiert und den relevanten linguistischen Hauptkategorien (Orthografie, Lexik, Grammatik, Semantik) zugeordnet. Diese linguistisch motivierte Klassifizierung war dem Studienziel dienlich, denn auf diese Weise konnte bei jeder KS-Regel genauer analysiert werden, in welchem linguistischen Zweig die Fehler ihre Wurzel haben.

Nach der oben geschilderten Vorgehensweise wurde die angewandte Fehlertaxonomie festgelegt, mit der die Übersetzungsfehler vor und nach dem Einsatz der KS-Regel identifiziert und verglichen werden können. Dadurch, dass die Taxonomie auf Basis des analysierten Datensatzes und Sprachenpaars festgelegt wurde, kann eine Anpassung bei der Analyse anderer Texttypen bzw. Sprachenpaare (vgl. ebd.: 701) erforderlich sein. Insgesamt besteht die angewandte Fehlertaxonomie aus den folgenden 13 Fehlertypen (\tabref{tab:4:9}).

%\textbf{Die Fehlertypen}


\begin{table}
\begin{tabularx}{.5\textwidth}{lX}
\lsptoprule
& \textbf{Orthografie}\\
\textbf{1} &  Zeichensetzungsfehler\\
\textbf{2} &  Großschreibungsfehler\\
\tablevspace
& \textbf{Lexik}\\
\textbf{3} &  Wort ausgelassen\\
\textbf{4} &  Wort extra eingefügt\\
\textbf{5} &  Wort unübersetzt geblieben\\
\textbf{6} &  Konsistenz\\
\tablevspace
& \textbf{Grammatik}\\
\textbf{7} &  falsche Wortart/Wortklasse\\
\textbf{8} &  falsches Verb\\
\textbf{9} &  Kongruenzfehler\\
\textbf{10} &  falsche Wortstellung\\
\tablevspace
& \textbf{Semantik}\\
\textbf{11} &  Verwechslung des Sinns\\
\textbf{12} &  falsche Wahl\\
\textbf{13} &  Kollokationsfehler\\
\lspbottomrule
\end{tabularx}
\caption{\label{tab:4:9}Die analysierten Fehlertypen}
\end{table}

Auf \textit{orthografischer} Ebene wurden \textit{Zeichensetzungs}{}- sowie \textit{Großschreibungsfehler} identifiziert. Bei Sätzen, die mehrere Zeichensetzungsfehler (z.~B. zwei fehlende Kommas) beinhalten, wurde jeder Fehler als \textit{ein} Fehler berechnet. \textit{Zeichensetzungsfehler} können bei allen Regeln auftreten, denn die beiden analysierten Sprachen verfügen über unterschiedliche Zeichensetzungsregelungen. Die \textit{Großschreibung} war insbesondere bei der Regel „Für zitierte Oberflächentexte gerade Anführungszeichen verwenden“ von Bedeutung, denn zitierte Oberflächentexte sollten großgeschrieben werden.

\largerpage
Auf \textit{lexikalischer} Ebene werden die drei möglichen lexikalischen Fehler abgedeckt: Ein Wort im Ausgangssatz fehlte in der Übersetzung (\textit{Wort ausgelassen}); ein Wort wurde zusätzlich übersetzt, obwohl es nicht im Ausgangssatz enthalten war (\textit{Wort extra eingefügt}); ein Wort wurde vom System in der Ausgangssprache wiedergegeben (\textit{Wort unübersetzt geblieben}). Letzteres wird in der MÜ z.~B. zur 1:1-Wiedergabe von Eigennamen oder innovativen fremdsprachlichen Termini eingesetzt. Sollte jedoch ein Wort unübersetzt bleiben, obwohl eine Übersetzung dafür erforderlich und möglich ist, zählt dieser Fall als Fehler. Ferner wurde der Fehlertyp \textit{Konsistenzfehler} miteinbezogen, da er in den analysierten Daten (insbesondere bei der Regel „Eindeutige pronominale Bezüge verwenden“) vorkam. Ein lexikalischer Konsistenzfehler tritt auf, wenn dasselbe Wort im Satz unterschiedlich übersetzt wird (vgl. \citealt{Mertin2006}: 249). Ein Beispiel hierfür ist die Übersetzung des Worts „Gerät“ im Hauptsatz als ‚device‘ und im Nebensatz als ‚appliance‘. Eine mögliche Ursache für diesen MÜ-Fehler ist die Verwendung von Synonymen, um eine Wiederholung zu vermeiden und somit den Stil zu verbessern. Dennoch hat die Konsistenz im Falle von technischen Texten oberste Priorität, da sie die Lesbarkeit und Verständlichkeit unerlässlich fördert.

Auf \textit{grammatischer} Ebene wurden vier Fehlertypen definiert: Die ersten drei Fehlertypen decken die Fälle ab, in denen das MÜ-System falsche Wortformen liefert: erstens auf \textit{Wortartebene}, indem z.~B. die MÜ ein Adverb anstelle eines Adjektivs enthält. Zweitens eine falsche Übersetzung von \textit{Verben} in Zeitform oder Konstruktion. Dieser Fehlertyp trat zwar bei mehreren KS-Regeln auf, jedoch war er besonders relevant bei der Regel „Konstruktionen mit ‚sein + zu + Infinitiv‘ vermeiden“, da sich die Übersetzung der Passiv-Ersatzkonstruktion oft als problematisch erwies. Im dritten Fall geht es um den \textit{Kongruenzfehler}. Hierbei wurde nach Duden\footnote{{{{Online unter: \url{https://www.duden.de/rechtschreibung/Kongruenz}}}}} die „Übereinstimmung zusammengehörender Teile im Satz (in Kasus, Numerus, Genus und Person)“ sowie die „inhaltlich sinnvolle Vereinbarkeit des Verbs mit anderen Satzgliedern“ geprüft. Der letzte grammatische Fehlertyp ist die \textit{falsche Wortstellung}. Ein Wortstellungsfehler kann in verschiedenen Fällen vorkommen, daher ist er für alle KS-Regeln relevant. Bei der Annotation wurde hierbei berücksichtigt, dass der Wortstellungsfehler auf Basis der Mindestzahl an erforderlichen Bewegungen bzw. Anpassungen zur Behebung der Fehler berechnet wird. Sollte der Fehler ein Bestandteil der KS-Stelle sein, bedeutet das, dass weitere Satzteile außerhalb der KS-Stelle ebenfalls falsch positioniert sind. In einem solchen Fall wurde jeweils ein Fehler innerhalb als auch außerhalb der KS-Stelle berechnet.

Auf \textit{semantischer} Ebene wurden drei Fehlertypen annotiert: Erstens die \textit{Verwechslung des Sinns}. Bei diesem Fehlertyp liefert das MÜ-System eine der möglichen Übersetzungen des Worts, jedoch gibt diese Übersetzung kontextuell nicht die zutreffende Bedeutung wieder (z.~B. die Übersetzung des Verbs ‚belegen‘ (in ‚Belegen Sie das Kaufdatum durch eine Kaufquittung‘) als ‚occupy‘ anstelle von ‚verify‘ oder ‚prove‘. Der zweite semantische Fehlertyp ist eine \textit{falsche Wahl}. Dieser Fehlertyp tritt auf, wenn die gelieferte MÜ keine mögliche Übersetzung für das betroffene Wort ist (z.~B. ‚die Firmware-Version‘ wurde als ‚the firmware design‘ übersetzt). Der letzte Fehlertyp ist der Kollokationsfehler. Duden\footnote{Online unter: \url{https://www.duden.de/rechtschreibung/Kollokation}} definiert die Kollokation als die „inhaltliche Kombinierbarkeit sprachlicher Einheiten“ sowie der „Zusammenfall verschiedener Inhalte in einer lexikalischen Einheit“. Nach dieser Definition wurde in den MÜ-Sätzen geprüft, ob vorhandene deutsche Kollokationen ins Englische richtig übersetzt wurden. Dieser Fehlertyp war insbesondere für die KS-Regel „Funktionsverbgefüge vermeiden“ von Bedeutung.

\subsubsection{\label{sec:4.4.4.2} Beschreibung und Ablauf bei der Annotation}

Die Fehlerannotation wurde manuell mithilfe der oben beschriebenen Fehlertaxonomie über zwei Phasen durchgeführt.\footnote{{{{Aufgrund des speziellen Aufbaus der Fehlerannotation, insbesondere bei der Unterscheidung zwischen Fehlern innerhalb und außerhalb der KS-Stelle, konnte die Annotation nicht mit einem klassischen Annotationstool durchgeführt werden. Daher wurde die Annotation mithilfe von Microsoft Excel durchgeführt. In Excel wurden Formeln für die Berechnung der Fehler hinterlegt und sämtliche quantitativen Daten errechnet.}}}} In der ersten Phase wurden die 2.160 MÜ-Sätze von einem DE-EN-Übersetzer mit sechs Jahren Berufserfahrung annotiert. In der zweiten Phase wurde die Annotation des ersten Übersetzers von zwei weiteren erfahrenen DE-EN-Übersetzern (10+~Jahre Berufserfahrung) geprüft. Aufgrund der großen Anzahl der Sätze prüfte jeder Übersetzer (in der zweiten Phase) die Hälfte der Sätze. Die Prüfung fand in Form einer Auswahlaufgabe statt, in der jeder Bewerter ankreuzen musste, ob er der Annotation (in Hinsicht auf die Identifikation eines Fehlers sowie den zugeordneten Fehlertyp) zustimmt oder nicht. Im Fall dass ein Bewerter der Annotation eines MÜ-Satzes nicht zustimmte, musste er die MÜ erneut annotieren. Die Prozentsätze der erneut annotierten MÜ waren 27~\% beim ersten Bewerter bzw. 31~\% beim zweiten. Anschließend musste der andere Bewerter die neue Annotation prüfen und sich für eine der beiden Annotationen (Annotation aus der ersten Phase oder Annotation des anderen Bewerters in der zweiten Phase) entscheiden.

Der Annotator bzw. die Bewerter sollten sich die Mindestzahl an Anpassungen bzw. Bewegungen vorstellen, die zur Korrektur der MÜ-Fehler erforderlich ist und eine Veröffentlichung des Texts ermöglicht. Während der beiden Phasen wurde die Ausgangssätze zur Verfügung gestellt. Andernfalls wäre die Annotation von mehreren Fehlertypen nicht möglich gewesen. Bei der Annotation zählte jeder Fehler als \textit{ein} Fehler. Sollte ein Wort mehrere Fehlertypen aufweisen, wurden alle Fehlertypen berücksichtigt. Beispielsweise kann ein Wort einen Großschreibungsfehler sowie einen Wortstellungsfehler aufweisen. In diesem Fall wurden zwei Fehler berechnet. Je nachdem an welcher Stelle der Fehler auftrat, wurde bei der Analyse zwischen Fehlern innerhalb der KS-Stelle und Fehlern außerhalb der KS-Stelle unterschieden.\footnote{{{{Die KS-Stelle ist der Teil des Ausgangssatzes, der bei dem Einsatz der KS-Regel modifiziert werden muss, und sein Äquivalent im Zielsatz, genauer beschrieben unter \sectref{sec:4.4.2.1}.}}}}

\subsubsection{\label{sec:4.4.4.3} Struktur der Ergebnisse der Fehlerannotation}

Im Rahmen der Fehlerannotation erfolgte der Vergleich der Szenarien vor-KS vs. nach-KS im Hinblick auf die Fehleranzahl und den Fehlertyp sowie dichotom im Sinne von Existenz/Vorhandensein oder Nichtexistenz/Nichtvorhandensein der Fehler. Letzteres wurde bei der Bildung von vier sog. „Annotationsgruppen“ benutzt. Konkret lieferte die Fehlerannotation -- unter Berücksichtigung der Aufteilung „Fehler innerhalb der KS-Stelle“ und „Fehler außerhalb der KS-Stelle“ -- die folgenden Ergebnisse:

\begin{enumerate}[label = {(\arabic*)}, align = left]
\item Die Fehleranzahl vor und nach der Anwendung der einzelnen KS-Regeln. Hierfür wurde der Signifikanztest Wilcoxon verwendet, da die Variablen ordinal sind.
\item Die Fehlertypen, die vor und nach der Anwendung der einzelnen KS-Re\-geln, auftraten bzw. eliminiert wurden. Für den Vergleich der Fehlertypen wurde der Signifikanztest McNemar verwendet, da er ermöglicht, zwei verbundene dichotome Parameter zu vergleichen; somit konnten signifikante Veränderungen bei jedem Fehlertyp vor- vs. nach-KS identifiziert werden.
\item Eine Kategorisierung der Ergebnisse in vier Annotationsgruppen FR, RF, RR und FF:

\begin{enumerate}[label = {(\alph*)}, align = left]
\item MÜ der KS-Stelle ist vor der Anwendung der KS-Regel falsch und nachher richtig (FR).
\item MÜ der KS-Stelle ist vor der Anwendung der KS-Regel richtig und nachher falsch (RF).
\item MÜ der KS-Stelle ist sowohl vor als auch nach der Anwendung der KS-Regel richtig (RR).
\item MÜ der KS-Stelle ist sowohl vor als auch nach der Anwendung der KS-Regel falsch (FF).
\end{enumerate}
\end{enumerate}

Die Häufigkeiten der Annotationsgruppen wurden mit Bootstrapping\footnote{{{{Bootstrapping ist ein statistisches Verfahren zur Schätzung der Stichprobenverteilung eines Schätzers durch erneute Stichprobenerstellung mit Ersatz aus der ursprünglichen Stichprobe. Es wird als ein nützliches Verfahren zum Testen der Modellstabilität betrachtet.~(\citealt{IBMnodate})}}}} berechnet.

\begin{enumerate}[label=(4),align=left]
\item Eine Untersuchung der Fehleranzahl außerhalb der KS-Stelle nach der Anwendung der KS-Regel im Vergleich zu davor bei den RR-Fällen. In anderen Worten sollte die KS-Stelle sowohl vor als auch nach dem Einsatz der KS-Regel fehlerfrei sein, wird der MÜ-Output außerhalb der KS-Stelle in Hinsicht auf die Veränderung in der Fehleranzahl genauer untersucht. Die Untersuchung wurde auf Sprachenpaarebene und MÜ-Systemebene (siehe \sectref{sec:5.2.2} bzw. \sectref{sec:5.4.2}) durchgeführt. Hierfür wurde ebenfalls der Signifikanztest Wilcoxon verwendet.
\end{enumerate}



\subsection{Design der Humanevaluation}
\label{sec:4.4.5}
Die Fehlerannotation lieferte objektive quantitative Daten über die aufgetretenen Fehler, ihre Anzahl und ihren Typ vor und nach dem Einsatz der KS-Regeln. Dennoch bleiben die Fragen offen, ob ein Rückgang der Fehleranzahl zwangsläufig auch auf eine bessere MÜ hindeutet oder ob bestimmte Fehlertypen die Qualität der MÜ verhältnismäßig stärker beeinflussen und wie genau die Qualität beeinflusst wird. Eine quantitative Analyse der Ergebnisse der Fehlerannotation kann alleine keinen Aufschluss über diese Details geben, um das Bild vor-KS versus nach-KS in Bezug auf die Qualität vergleichen zu können. Hierfür ist der Forscher auf subjektive qualitative Bewertungen angewiesen. Daher war die Anwendung einer Humanevaluation erforderlich.

\textit{Ziel der Humanevaluation} ist der Vergleich der Qualität des Inhalts und des Stils der KS-Stelle (nicht der Qualität des ganzen Satzes) vor versus nach dem Einsatz der einzelnen KS-Regeln.\footnote{{{{Die Vergleichswerte der Inhalts- und Stilqualität der KS-Stellen in den vor-KS- und nach-KS-Szenarien konnten ermittelt werden, indem der MÜ-Output vor und nach dem Einsatz der KS-Regeln mit Ausnahme der KS-Stelle vereinheitlicht und die Differenz zwischen den Qualitätsbewertungen (Scores) vor und nach KS berechnet wurde (die genaue Aufbereitung der Zielsätze für die Humanevaluation ist unter \sectref{sec:4.4.3.1} (Schritt [9]) dargestellt).}}}}

Obwohl eine gewisse Korrelation zwischen der Stilqualität und der Inhaltsqualität zu erwarten ist (z. B. ist ein Satz, der einen grammatischen Fehler beinhaltet bzw. unverständlich ist, i. d. R. nicht idiomatisch), wurden die Stilqualität und der Inhaltsqualität getrennt bewertet. Der Grund für diese separate Bewertung liegt darin, dass die einzelnen KS-Regeln die Stilqualität und Inhaltsqualität unterschiedlich beeinflussen können. Bei einigen Regeln (wie z.~B. „Funktionsverbgefüge vermeiden“ und „Partizipialkonstruktionen vermeiden“) kann der Stil eine wesentliche Rolle bei der Qualitätsbewertung spielen. In Szenarien, in denen die Übersetzungen sowohl vor als auch nach dem Einsatz der KS-Regel korrekt sind, kann die Stilqualität variieren. Dementsprechend ist eine Differenzierung bei der Bewertung des Einflusses zentral für die Ergebnisse.

Die Triangulation der Ergebnisse der Fehlerannotation mit denen der Humanevaluation ermöglichte die Untersuchung der Korrelation zwischen den Fehlertypen und der Qualität sowie die Untersuchung der Qualität der Annotationsgruppen. Folglich konnten durch die triangulierten Ergebnisse die Qualitätsveränderung und somit der KS-Einfluss genauer beleuchtet werden. Im Folgenden wird das Testdesign genauer erläutert.

\subsubsection{\label{sec:4.4.5.1}Definition der Qualität}

Wie in der Literaturübersicht unter \sectref{sec:3.3.1} dargestellt, wenden die MÜ-Eva\-lua\-tions\-stu\-dien zahlreiche Synonyme von Qualitätskriterien bzw. sich überlappenden Kriterien an. Das häufigste Qualitätskriterium in den geschilderten Studien ist die Verständlichkeit.

Wie \citet[339]{Lehrndorfer1996b} es beschreibt, ist die Verständlichkeit „[e]in unerschöpfliches Thema in den Diskussionen über technische Dokumentation“. Dabei werden Aspekte (sog. „Verständlichkeitsmacher“) untersucht, die das Verstehen des technischen Inhalts erleichtern (ebd.). Zu den bekanntesten Verständlichkeitsmodellen für die Fachkommunikation zählen: das Hamburger Modell von \citet{LangerEtAl1974}, das vier Merkmalsdimensionen der Verständlichkeit (Einfachheit, Gliederung/Ordnung, Kürze/Prägnanz und zusätzliche Stimulanz) definiert. Dieses Modell wurde jedoch aufgrund seiner textzentrierten Perspektive (d.~h. Leser wird außer Acht gelassen) kritisiert und daraufhin von \citet{Groeben1982} und \citet{Göpferich2001} erweitert. Nach dem interaktionalen Ansatz von \citet{Groeben1982} ist die Textverständlichkeit eine Interaktion zwischen Text und Leser. Die vier Dimensionen der Textverständlichkeit von Groeben (sprachliche Einfachheit, semantische Kürze/Redundanz, kognitive Gliederung/Ordnung und stimulierender kognitiver Konflikt) stimmen weitgehend mit denen des Hamburger Modells überein und erweitern es zugleich. In dem Karlsruher Verständlichkeitskonzept erweiterte \citet{Göpferich2001} beide Modelle um die Kommunikationssituation (kommunikative Funktion des Texts) und entwickelte damit einen „kommunikationsorientiert-integrativen Ansatz zur Bewertung der Verständlichkeit von Texten“ bestehend aus sechs Dimensionen (Struktur, Simplizität, Motivation, Prägnanz, Korrektheit und Perzipierbarkeit). Es ist hier aus Platzgründen nicht möglich, den zahlreichen Modellen und Konzepten der Verständlichkeit gerecht zu werden und diese tiefergehend zu diskutieren. Die Arbeit beschränkt sich daher auf die computerlinguistische Perspektive der Verständlichkeit, wie sie in \citegen{HutchinsSomers1992} bekanntem Werk zur MÜ-Evaluation definiert ist.

Im Gegenteil zur Verständlichkeit wird der Stil nur in wenigen Evaluationen miteinbezogen. \citet[164]{HutchinsSomers1992} finden bei der MÜ-Qua\-li\-täts\-e\-va\-lua\-tion jedoch, dass „the appropriateness of a particular style is an important factor”. Nicht nur für die MÜ-Qualität im Allgemeinen, sondern auch für die technische Dokumentation im Speziellen spielt der Stil eine wesentliche Rolle bei der Förderung der Verständlichkeit und der Genauigkeit, wie \citet[307ff.]{Püschel1996} in seinem Beitrag „Sprachstil -- ein Thema für Technische Redakteure?“ genauer erklärt. Nach \citet[307]{Püschel1996} gehören zum Stil einerseits, „dass der Schreibstil viele Facetten hat“ und andererseits, „dass man das Gleiche nicht auf unterschiedliche Weise sagen kann“. Zur Illustration dieser Eigenschaften führt \citet[317]{Püschel1996} folgende Beispielsätze aus der technischen Dokumentation ein: [1] „\ldots ist das Gerät unbedingt zu entkalken“; [2] „\ldots muss man das Gerät unbedingt entkalken“; [3] „\ldots müssen Sie das Gerät unbedingt entkalken“. In der Infinitivkonstruktion [1] wird die Person, die die Handlung durchführen soll, nicht ausgedrückt. In der syntaktisch vollständigen Version [2] bleibt die handelnde Person vage; sie wird mit ‚man‘ ausgedrückt. In der letzten Version [3] wird die handelnde Person ausgedrückt bzw. direkt mit ‚Sie‘ angesprochen. Mithilfe dieser Beispiele verdeutlicht \citet[317]{Püschel1996}, wie „der Zusammenhang von Gedanke und Stil kein Glasperlenspiel“ ist. Er betrachtet kritisch die Trennung von Verständlichkeit und Stil in der technischen Dokumentation:

\begin{quote}
[w]ann immer Probleme des technischen Schreibens diskutiert werden, geht es auch um die Verständlichkeit von Texten und um die Frage, wie Texte verständlicher gemacht werden können. Es liegt dabei auf der Hand, dass jede Optimierungsmaßnahme einen Eingriff in die Gestalt des Texts bildet; [\ldots], dass mit den Veränderungen im Stil zwangsläufig inhaltliche Veränderungen einhergehen. (ebd.)
\end{quote}

Im Hinblick auf die KS verdeutlicht die Diskussion von Püschel, wie der Stil in der technischen Dokumentation (in den obigen Beispielen bei den Regeln zum Vermeiden des Passivs und des Passiversatzes) eine Rolle spielt und wie der Stil Hand in Hand mit der Verständlichkeit und der Genauigkeit des Inhalts einhergeht und sie entsprechend fördern kann (vgl. \citealt{Püschel1996}: 335). Vor diesem Hintergrund durfte die Stilqualität bei dieser Studie nicht außer Acht gelassen werden.

Neben der Verständlichkeit (vgl. „intelligibility“ bei \citealt{White2003}; vgl. „comprehensibility“ und „clarity“ bei \citealt{VanniMiller2002} sowie \citealt{KingEtAl2003}) und dem Stil (vgl. \citealt{VanniMiller2002} sowie \citealt{KingEtAl2003}) ist die Genauigkeit das dritte Qualitätskriterium (vgl. „fidelity“ bei \citealt{White2003}; vgl. „accuracy“ bei \citealt{Arnold1994}: 169), das in die MÜ-Evaluationen miteinbezogen wird.

Eine Definition der MÜ-Qualität, die die Verständlichkeit, die Genauigkeit und den Stil abdeckt und gleichzeitig die Problematik der Bezeichnungen, Aufteilung und möglichen Überschneidungen der Qualitätskriterien (vgl. \sectref{sec:3.3.1}) berücksichtigt, ist die von \citet[163]{HutchinsSomers1992}. Unter Angabe der zutreffenden Synonyme definieren sie die drei genannten Qualitätskriterien wie folgt:

\begin{quote}
(a) Fidelity or accuracy, the extent to which the translated text contains the ‚same‘’ information as the original; (b) Intelligibility or clarity, the ease with which a reader can understand the translation; and (c) Style, the extent to which the translation uses the language appropriate to its content and intention. (ebd.)
\end{quote}

Damit liefern Hutchins und Somers in sich geschlossene und klar aufgeteilte Qualitätskriterien (vgl. \citealt{FiedererO’Brien2009}). Gleichzeitig ist es nachvollziehbar -- wie die obigen Beispiele von Püschel zeigen --, dass ein gewisser gegenseitiger Einfluss sowie eine potenzielle Überschneidung der Kriterien nicht auszuschließen sind.

Eine weitere aktuelle Qualitätsevaluationsmethode, die häufig angewandt wird, ist das MQM (Multidimensional Quality Metrics) Framework \citep{LommelEtAl2013}. Es handelt sich dabei um ein umfassendes Framework aus den fünf Hauptdimensionen „accuracy, fluency, design, verify und internationalization“ und mehr als 100 unterkategorisierten Fehlertypen (mehr dazu unter \sectref{sec:3.3.3.1}). Ausgehend von der Annahme, dass kein einzelnes festes Schema zur Qualitätsbewertung bei unterschiedlichen Übersetzungsprojekten verwendet werden kann, sollen für jedes Projekt je nach seinen Anforderungen und Erwartungen die relevanten Elemente maßgeschneidert ausgewählt werden. \citep{LommelEtAl2013} Diese Methode wäre insbesondere bei der Analyse einer Vielzahl von KS-Regeln, die feinkörnige bzw. weitere spezifische Fehlertypen erfordern, sinnvoll. Bei der vorliegenden Studie waren die Integrität und das Granularitätsniveau die zwei Faktoren, die zur Auswahl der MÜ-Qualitätsdefinition von  \citet{HutchinsSomers1992} führten.

Die Bewertung in der vorliegenden Studie findet auf inhaltlicher und stilistischer Ebene statt. Da der Stil und der Inhalt der MÜ vom Einsatz der einzelnen KS-Regeln zu unterschiedlichem Grad beeinflusst werden können, ist eine Differenzierung bei der Analyse des Einflusses der einzelnen Regeln auf die detaillierten Kriterien der Stil- und Inhaltsqualität essentiell für die Studienergebnisse. Im Folgenden werden die angewandten Definitionen der Stil- und Inhaltsqualität detailliert dargestellt.

Den Teilnehmern der Humanevaluation wurde folgende Definition der Inhaltsqualität präsentiert (vgl. \citealt{HutchinsSomers1992}: 163):
\begin{quote}
The extent to which the translation reflects the information in the source text accurately; and the extent to which the translation is easy to understand. (ebd.)
\end{quote}

Genauigkeit und Verständlichkeit wurden als Kriterien der Inhaltsqualität (\figref{fig:4:8}~[3b]) getrennt bewertet, damit der Einfluss der KS-Regeln auf die beiden Qualitätskriterien nach Hutchins und Somers beurteilt werden kann. Gleichzeitig wurden die Kriterien Genauigkeit und Verständlichkeit auf der Skala der Inhaltsqualität zusammengeführt, denn auf ihrer Grundlage wurde untersucht, inwiefern der \textit{Inhalt} der MÜ originaltreu und verständlich ist.

Anhand des Inhaltsqualität-Scores auf der Likert-Skala wurde die Inhaltsqualität operationalisiert und entsprechend wurde der Einfluss des Einsatzes der KS-Regeln auf die Inhaltsqualität quantitativ bewertet. Um sicherzustellen, dass die Bewertungen der Verständlichkeit bzw. der Genauigkeit in den Score der Inhaltsqualität einfließen, wurden die Bewerter in den Testanweisungen aufgefordert, erst die relevanten Qualitätskriterien (siehe [3b] in \figref{fig:4:8}) anzukreuzen und ihre Kriterienauswahl zu begründen ([4] in \figref{fig:4:8}) und danach den Score der Inhaltsqualität auf Basis der ausgewählten Kriterien zu vergeben ([2] in \figref{fig:4:8}) (siehe „Testaufgaben“ unter \sectref{sec:4.4.5.2}).

Ein positiver Nebeneffekt der Zusammenführung der beiden Kriterien ist die Reduzierung des Zeitaufwands und der Komplexität der Bewertungsaufgabe für die Bewerter. Die Bewerter müssen zwar einen potenziellen Einfluss auf die einzelnen Kriterien erkennen (siehe [3b] in \figref{fig:4:8}), jedoch nicht den Schwierigkeitsgrad der identifizierten Fehler differenziert einmal in Hinsicht auf die Genauigkeit und getrennt in Bezug auf die Verständlichkeit quantitativ bewerten, was sich aufgrund der großen Anzahl der bewerteten Sätze als vorteilhaft erwies.

Für die Stilqualität wurde den Teilnehmern der Humanevaluation folgende Definition präsentiert (vgl. \citealt{HutchinsSomers1992}: 163; \citealt{FiedererO’Brien2009}: 57):

\begin{quote}
The extent to which the translation sounds natural and idiomatic in Standard Written English, is appropriate to the intention of its content as well as is presented clearly orthographically. (ebd.)
\end{quote}

Die Stilqualitätsdefinition zeigt, wie \citet[335]{Püschel1996} betont, dass der Stil „keine Sprachkosmetik“ sei. Die Definition umfasst drei Qualitätskriterien, die die Verständlichkeit und Genauigkeit fördern können (vgl. obige Beispiele von \citealt{Püschel1996}: 317): die Idiomatik der MÜ, die Eignung der MÜ für die Intention des Inhalts sowie die korrekte bzw. klare orthografische Darstellung der MÜ.\footnote{{Es existieren zahlreiche Definitionen für die Begriffe Stil und Intention, die aus Platzgründen nicht alle diskutiert werden können. Die Studie beschränkt sich auf die computerlinguistische Perspektive dieser Begriffe, wie in diesem Abschnitt erklärt}{{{.}}}} Im Folgenden werden die drei Kriterien der Stilqualität näher erläutert:

\begin{enumerate}[label={(\arabic*)}, align=left]
\item
\textit{Die Idiomatik der MÜ}: Typischerweise wird der MÜ-Output post-editiert. Nach dem klassischen Ablauf wird ein „Light Post-Editing“ zur Korrektur der wesentlichen Fehler durchgeführt mit dem Ziel, den Inhalt in einer verständlichen und genauen Form bereitzustellen (vgl. \citealt{O’BrienEtAl2009}; \citealt{O’Brien2010}). Sollte der Stil optimiert werden, so erfolgt ein „Full Post-Editing“, bei dem aus dem MÜ-Output eine stilistisch vergleichbare Humanübersetzung erzeugt werden soll (vgl. \citealt{Wagner1987}). Das ist bisher der gängige Ablauf, da die früheren Ansätze überwiegend mit schwerwiegenden Fehlern (u.~a. im Bereich der Morphologie und Grammatik, wie z.~B. Wortstellungsfehlern) zu kämpfen haben. Mit der Entwicklung der NMÜ haben sich die Eigenschaften des MÜ-Outputs wesentlich geändert, denn die NMÜ kann hingegen diese Schwierigkeiten lösen und darüber hinaus eine im Wesentlichen flüssige Übersetzung liefern (vgl. \citealt{BentivogliEtAl2016}; \citealt{ToralSanchez-Cartagena2017}). Die hohe Flüssigkeit des NMÜ-Outputs zählt zu den dominanten Stärken dieses Ansatzes (vgl. ebd.). Angesichts dieser Entwicklung darf bei einer entwicklungs- bzw. forschungsstandgemäßen empirischen Untersuchung die Evaluation der Idiomatik nicht fehlen. Anderenfalls könnte die technische Dokumentation von dieser Entwicklung nicht profitieren.

Im Jahr \citeyear{FiedererO’Brien2009} untersuchten \citeauthor{FiedererO’Brien2009} den KS-Einfluss im Zusammenhang mit einem RBMÜ-System nach den Definitionen von Hutchins und Somers ebenfalls unter Berücksichtigung der Idiomatik der MÜ in der technischen Dokumentation (vgl. \citealt{FiedererO’Brien2009}). Mit der vorliegenden Studie, wird die KS und ihr Einfluss auf die maschinelle Übersetzbarkeit in Anbetracht der jüngsten MÜ-Entwicklung nach denselben Qualitätsdefinitionen in einem Vergleich von RBMÜ\nobreakdash-, SMÜ-, HMÜ- und NMÜ-Systemen empirisch wieder aufgegriffen, um die technologische Entwicklung der MÜ zu verfolgen und Implikationen für die technische Dokumentation zu erarbeiten.

\item
\textit{Die Eignung der MÜ für die Intention des Inhalts}: Bei dem zweiten Qualitätskriterium geht es um die Eignung der MÜ für die Intention des Inhalts. Nach der Sprechakttheorie kann eine Aussage mit mehr als einer Illokution verbunden sein (vgl. \citealt{Rehbein1988}). In der technischen Dokumentation gilt gleichzeitig die Eindeutigkeit der intendierten Aussage als „wesentliches Güterkriterium“ (\citealt{Lehrndorfer1996a}: 156). Daher ist „die Reflexion der intendierten Pragmatik der Aussage und ihre Benennung durch den technischen Redakteur von Bedeutung“ (ebd.). Nach Lehrndorfer (ebd.: 155) soll „der technische Redakteur zunächst eine genaue Zielsetzung seiner Aussage reflektieren“ und die jeweilige Aussageintention vor jeder Satzeinheit als Auszeichnungsformat nennen bzw. annotieren.

In der technischen Dokumentation existieren mehrere Klassifizierungen für die Aussageintentionen. Lehrndorfer (ebd.: 155ff.) unterscheidet zwischen den drei Hauptkategorien Handlungsanweisung, Sicherheitshinweis und Produktbeschreibung, denen weitere Unterkategorien zugeordnet sind. Eine weitere Klassifizierung der Aussageintentionen in der technischen Dokumentation bietet \citet[110ff.]{Ley2005}. Diese umfasst die Illokutionen Assertion, Direktiv und Expressiv, denen ebenfalls detaillierte Unterkategorien zugeordnet sind (ebd.). Auf dieser Basis war es essentiell, die Eignung der Übersetzung für die Intention des Inhalts zu evaluieren.

\item
\textit{Die korrekte bzw. klare orthografische Darstellung der MÜ}: Hierbei wird die Orthografie als grafisches Mittel für eine adäquate Darstellung des Inhalts miteinbezogen, was wiederum der Verständlichkeit, Genauigkeit sowie der Vermittlung seiner Intention dienlich ist. Nach \citet[30f.]{Nerius2007} ist die Orthografie (Rechtschreibung)\footnote{{Orthografie kommt aus dem Griechischen; „orthós“ für „recht“ und „gráphein“ für „schreiben“ \citep[30]{Nerius2007}.}} die „Norm der formalen Seite der geschriebenen Sprache, und zwar aller Teilbereiche der Schreibung einschließlich der Interpunktion“. Durch die grafischen Mittel der Orthografie werden unterschiedliche Eigenschaften der Sprachelemente verdeutlicht (ebd.: 91). Vier grafische Mittel der Orthografie waren für die Studie von besonderer Bedeutung:
    \begin{itemize}
    \item    Die Anführungszeichen: „Anführungszeichen haben die besondere Fähigkeit, bestimmte kommunikative Bezüge herzustellen und damit eine besondere Aussageabsicht des Schreibenden zum Ausdruck zu bringen.“ Dementsprechend -- anders als das paarige Komma -- ist deren Aufgabe nicht bloß „die Kennzeichnung der Grenzen eines Einschubs innerhalb des Satzverbandes“, sondern vielmehr „die besondere Charakterisierung dieses Einschubs“, so markieren die Anführungszeichen fremde Äußerungen wie Zitate, Titel und Überschriften (\citealt{Nerius2007}: 253f.), vgl. insb. Regel „Für zitierte Oberflächentexte gerade Anführungszeichen verwenden“ unter \sectref{sec:5.3.1}.
    \item    Der Ergänzungsstrich ist eine Art des Bindestriches, der eine ökonomische (ebd.: 187) und zugleich eine semantische Funktion hat (ebd.: 191). Mithilfe des Ergänzungsstriches wird eine Auslassung markiert (ebd.: 187) und die „Zusammengehörigkeit räumlich getrennter Bestandteile zusammengesetzter oder abgeleiteter koordinierter Wörter innerhalb der Wortgruppe“ signalisiert (ebd.: 191). Somit dient der Ergänzungsstrich der „Monosemierung“ (d. h. der Eindeutigkeit) „der Aussage im Interesse des Lesenden und wirkt damit der Gefahr eines Missverständnisses entgegen“ (vgl. fehlender Ergänzungsstrich im Beispiel „Max Müller, Pinsel und Bürstenmachermeister“). Es gibt keine Regel zur Verwendung bzw. Nicht-Verwendung des Ergänzungsstriches. Die Entscheidung über seine Verwendung liegt bei dem Schreibenden. Entscheidet er keine Wortteile wegzulassen, kann dies „zwar gegen stilistische Normen verstoßen, stellt aber keinen orthographischen Fehler dar“. (ebd.) Vgl. insb. Regel „Keine Wortteile weglassen“ unter \sectref{sec:5.3.9}.
    \item     Die Großschreibung, wodurch die Wortklasse der Substative sowie der Anfang eines Satzes bzw. einer Bezeichnung gekennzeichnet wird (ebd.: 91), vgl. insb. Regel „Für zitierte Oberflächentexte gerade Anführungszeichen verwenden“ unter \sectref{sec:5.3.1}.
    \item     Das paarige Komma, das „eine Klammerfunktion hat, indem es syntaktische Einheiten einschließt und dadurch aus dem übrigen Satzverband heraushebt“ (ebd.: 252), vgl. insb. Regel „Partizipialkonstruktionen vermeiden“ unter \sectref{sec:5.3.5}.
    \end{itemize}
\end{enumerate}

In seinem Werk „Deutsche Orthographie“ beleuchtet \citet[91]{Nerius2007}, wie durch die grafischen Mittel der Orthografie Aspekte der Satzbedeutung auf der grafischen Ebene gekennzeichnet werden und somit das Erkennen der Satzform erleichtert und die rasche Erfassung der Satzbedeutung unterstützt wird. Zudem zeigt Nerius (ebd.: 275ff.), die Rolle, die die grafischen Mittel der Orthografie bei der stilistischen Differenzierung spielen können.\footnote{{Der Effekt der Orthografie z.~B. bei der Kommasetzung wird von \citet[238]{Nerius2007} anhand von Beispielen wie „Peter versprach, der Mutter zu schreiben“ vs. „Peter versprach der Mutter, zu schreiben“ und „Der Kranke drohte, sich ein Leid anzutun“ vs. „Der Kranke drohte sich ein Leid anzutun“ verdeutlicht.}}

Vor diesem Hintergrund ist zu erwarten, dass eine klare bzw. korrekte orthografische Darstellung des Inhalts einen Einfluss auf die MÜ-Qualität hat; durch die stilistische Differenzierung, die sie bewirkt, beeinflusst sie die Verständlichkeit und die Genauigkeit der Aussage. Dementsprechend wurde die klare orthografische Darstellung als Kriterium der Stilqualität betrachtet, letztendlich ist die Orthografie, wie ihre Definition besagt (\citealt{Nerius2007}: 30f.), die Norm der „formalen“ Seite der geschriebenen Sprache.

Anhand des Stilqualität-Scores auf der Likert-Skala wurde die Stilqualität operationalisiert und entsprechend wurde der Einfluss des Einsatzes der KS-Regeln auf die Stilqualität quantitativ bewertet. Analog zur Bewertung der Inhaltsqualität wurden die Bewerter in den Testanweisungen aufgefordert, erst die relevanten Qualitätskriterien (siehe [3a] in \figref{fig:4:8}) anzukreuzen und ihre Kriterienauswahl zu begründen ([4] in \figref{fig:4:8}) und danach den Score der Stilqualität auf Basis der ausgewählten Kriterien zu vergeben (siehe „Testaufgaben“ unter \sectref{sec:4.4.5.2}); Ziel dieser Vorgehensweise war, sicherzustellen, dass die Bewertungen der drei genannten Kriterien in den Score der Stilqualität einfließen.


\subsubsection{\label{sec:4.4.5.2}Testlayout}

In diesem Abschnitt wird das Testlayout genau beschrieben; dies umfasst die Klärung, wie die Evaluationsfragen gestellt wurden, welche Bewertungsskala verwendet wurde, wie die Ausgangssätze, die Zielsätze sowie die Evaluation im Allgemeinen den Teilnehmern präsentiert wurden und wie der Evaluationsablauf sich gestaltete.

\subsubsubsection{Form der Evaluationsfragen}

Wie im Kapitel MÜ (unter \sectref{sec:3.3.3.1}) detailliert dargestellt, sind Ranking und Sco\-ring die dominierenden Bewertungsmethoden im Bereich der Humanevaluation. Unter Berücksichtigung der Vor- und Nachteile der beiden Methoden, wurde in der vorliegenden Studie \textit{Scoring} angewendet. Im Folgenden werden die Gründe dieser Wahl näher erläutert.

In der Humanevaluation der Studie war ein Ranking der Übersetzungen aufgrund der großen Anzahl der MÜ-Sätze pro Ausgangssatz nicht möglich. Auch trotz der Einschränkung der Anzahl der MÜ-Sätze auf 5--6 Übersetzungen pro Ausgangssatz, bleibt die Auswertung der Rankingergebnisse von fehlerhaften MÜ-Sätzen problematisch. Betrachten wir ein Beispiel, in dem zwei Ausgangssätze jeweils fünfmal übersetzt wurden: Die Übersetzungen vom ersten Ausgangssatz umfassen zwei fehlerfreie Übersetzungen und drei fehlerhafte Übersetzungen mit \textit{wenig gewichtigen Fehlern}. Bei dem zweiten Ausgangssatz gibt es ebenfalls zwei fehlerfreie Übersetzungen und drei fehlerhafte Übersetzungen, allerdings mit \textit{gewichtigen Fehlern}. Bei den beiden Ausgangssätzen erzielen die fehlerhaften Übersetzungen vergleichbare schlechte Rankings trotz des unterschiedlichen Schweregrads der Fehler (vgl. \citealt{CostaEtAl2015}).

Der einzige Fall, in dem ein Ranking trotz der Schwierigkeit des Rankings mehrerer Sätze vorteilhaft sein kann, ist das Ranking der MÜ-Sätze der Annotationsgruppe (RR). In dieser Annotationsgruppe sind die MÜ-Sätze sowohl vor als auch nach dem Einsatz der KS-Regeln fehlerfrei. Beim Scoring von solchen Fällen könnten die Bewerter dazu neigen, die Sätze schnell -- ohne Beachtung von stilistischen Feinheiten -- mit 5 Punkten (beste Bewertung) zu bewerten. Durch das Ranking könnte eine solche Bewertung vermieden werden, da die Bewerter die MÜ-Sätze genauer betrachten und vergleichen müssen. Jedoch konnte die Gruppe RR nicht gesondert gerankt werden, denn eine Mischung der beiden Bewertungsmethoden (Scoring bei fehlerhaften Übersetzungen und Ranking bei fehlerfreien Übersetzungen) wäre bei der Auswertung problematisch. Für manche Sätze wiederholten sich die MÜ z.~B. bei zwei Systemen, wobei die MÜ bei einem System innerhalb der Annotationsgruppe RR und bei dem anderen innerhalb der Annotationsgruppe FR (falsche MÜ vor KS; richtige MÜ nach KS) war. In solchen Fällen und bei der Verwendung von zwei Bewertungsmethoden hätte eine und dieselbe Übersetzung zwei Bewertungen jeweils nach einer Methode.

Außerdem bestehen weitere Schwierigkeiten beim Ranking: Erstens, ein Ranking kann auch dazu führen, dass die Teilnehmer mehrere Übersetzungen eines Satzes identisch bewerten, es sei denn, die Testanweisungen verbieten identische Bewertungen, sog. „ties“. Dies bildet eine weitere Schwierigkeit für die Teilnehmer, da oft unterschiedliche Fehlertypen schwer vergleichbar sind, sodass sie in Relation zueinander gerankt werden können. Beispielsweise lassen sich bei der Regel „Anführungszeichen verwenden“ die Übersetzungsvarianten Kleinschreibung sowie Großschreibung mit und ohne Anführungszeichen schwer vergleichen und entsprechend ranken. Aus diesen Gründen wurden die MÜ-Sätze mithilfe einer 5-Punkte-Likert-Skala bewertet.

\subsubsubsection{Bewertungsskala}

Skalen, wie die Likert-Skala, sind ein weit verbreitetes Bewertungsinstrument im Bereich der MÜ-Evaluation. Die Spanne der Likert-Skala liegt in der Regel zwischen fünf und neun Punkten (vgl. \citealt{Porst2011}; \citealt{SaldanhaOBrien2014}: 157). Einige Forscher bevorzugen der Verwendung von Skalen mit geraden Zahlen, um zu verhindern, dass die Teilnehmer vermehrt den Mittelwert der Skala wählen. Auf diese Weise versuchen sie dem Risiko entgegenzuwirken, dass das Endergebnis unentschieden bleibt, was wiederum eine Nichtbeantwortung der Forschungsfrage zur Konsequenz haben könnte. (\citealt{SaldanhaOBrien2014}: 157f.; \citealt{Johnston2015}) Gleichzeitig hat die Verwendung von Skalen mit geraden Zahlen den Nachteil, dass die Teilnehmer keine Möglichkeit haben, die tatsächlich neutralen Fälle entsprechend zu bewerten. Das kann wiederum die Ergebnisse verzerren und die Teilnehmer frustrieren. \citep{Johnston2015} Die unter \sectref{sec:3.3.1} „Qualität der MÜ“ dargestellten Studien führten die Bewertung mithilfe einer Likert-Skala mit vier (\citealt{VanSlype1979}; \citealt{VanniMiller2002}; \citealt{Coughlin2003}) oder fünf Punkten (\citealt{LDC2002}; \citealt{White2003}; \citealt{Hamon2007}) durch.

In der vorliegenden Studie wurden die MÜ-Sätze auf einer 5-Punkte-Likert-Skala bewertet. Die Spanne der Skala war für den erzielten Differenzierungsgrad angemessen. Eine kürzere Skala wäre für diese Studie ungeeignet, da die Erfassung von kleinen Qualitätsdifferenzen für den Vergleich der Szenarien vor-KS vs. nach-KS notwendig ist. Eine noch längere bzw. ausdifferenziertere Skala würde die Ratingkonsistenz beeinträchtigen. Ferner unterstützte die ortsflexible Durchführung der Evaluation über einen Zeitraum von drei bis vier Wochen die Teilnehmer dabei, flexibel und ohne Zeitdruck die Übersetzungen ausdifferenziert zu bewerten, was wiederum dabei unterstützend wirkte, das Risiko einer vermehrten Wahl des Mittelwerts der Skala zu minimieren.

\subsubsubsection{Darstellung der Ausgangssätze}

Bei der Erstellung des Testdesigns stellte sich die Frage, ob die Darstellung des Ausgangstexts erforderlich ist. Eine Darstellung des Ausgangssatzes ist notwendig, damit die Genauigkeit der Übersetzung beurteilt werden kann (z. B. kann der lexikalische Fehler „unübersetztes Wort“ nur durch eine Darstellung des Ausgangssatzes erkannt werden). Auf der anderen Seite können die Verständlichkeit und der Stil ohne Ausgangstext bewertet werden. Diese Hypothese bestätigte \citet[205]{White2003}, als er zunächst die Qualität von MÜ-Sätzen im Allgemeinen (im Sinne von „good English“, „degraded by up ton errors“ or „wrong“) bewertete. Er kam zum Ergebnis, dass ohne eine Darstellung des Ausgangstexts „we do not know anything about where these expressions came from. Are they really translations of anything?“ (ebd.). White (ebd.) ging diese Problematik an, indem er den Ausgangstext den Bewertern zur Verfügung stellte. Dies ermöglichte ihm, die Qualität detailliert gemäß den Attributen „Intelligibility“ und „Fidelity“ zu untersuchen und in der Lage zu sein, „to tell something about the translation issues from looking at both the source and target language” (ebd.: 206). Er nannte gleichzeitig die damit verbundene Schwierigkeit, den Test von Übersetzern durchführen lassen zu müssen, die möglicherweise schwer zu einer Teilnahme zu motivieren sind (ebd.).

Im Vergleich zu Whites Studie sind in der vorliegenden Studie nicht nur die „translation issues“ von Interesse, sondern auch die fehlerfreien Fälle. Eine feine Differenzierung im Stil, Verständlichkeits- und Genauigkeitsgrad von fehlerfreien Übersetzungen muss auch abgedeckt werden und ist -- vor allem aufgrund der großen Anzahl von fehlerfreien Übersetzungen -- von großer Bedeutung, damit ein umfangreicher Vergleich der vor- und nach-KS-Szenarien gewährleistet werden kann.

Wie oben erwähnt, ist eine Darstellung des Ausgangssatzes zur Beurteilung der Genauigkeit (und somit der Inhaltsqualität) erforderlich. Für die Stilqualität wurde die Entscheidung, ob die Ausgangssätze den Bewertern zur Verfügung gestellt werden oder nicht, nach der Durchführung eines Testlaufs getroffen. Der Testlauf bestand aus zwei Phasen: In der ersten Phase erhielten drei Teilnehmer 14 Sätze ohne den Ausgangssatz und hatten die Aufgabe die Stilqualität auf einer 5-Punkte-Likert-Skala zu bewerten und etwaige Fehler zu kommentieren (\figref{figex:4:7}).\footnote{{Im Testlauf wurden die Qualitätsdefinitionen vereinfachter bzw. kürzer (als unter \sectref{sec:4.4.5.1}) angegeben, da es im Test primär um die Darstellung der Ausgangssätze ging.}}


\begin{figure}
\includegraphics[width=\textwidth]{figures/Beispiel7.png}
\caption{\label{figex:4:7}Beispiel aus der ersten Phase des Testlaufs}
\end{figure}

Ein Monat später fand die zweite Phase statt. Hier haben dieselben drei Teilnehmer die Stil- und Inhaltsqualität derselben 14 Sätze bewertet, wobei die Ausgangssätze zur Verfügung standen (\figref{figex:4:8}).

\begin{figure}
\includegraphics[width=\textwidth]{figures/Beispiel8.png}
\caption{\label{figex:4:8}Beispiel aus der zweiten Phase des Testlaufs}
\end{figure}

Ein Vergleich der Ergebnisse der Stilqualität in den beiden Phasen zeigt, dass die Bewertung von 57~\% der Sätze unverändert blieb, von 28~\% der Sätze sich um einen Punkt auf der Skala veränderte und von 15~\% der Sätze sich um mehr als einen Punkt auf der Skala veränderte. Zudem zeigten die Kommentare der Teilnehmer, dass der Ausgangstext bei der Bewertung des Stils eine Rolle spielte. Beispielsweise empfahlen in der zweiten Phase (Ausgangssatz dargestellt) zwei Bewerter einen Passivsatz ins Aktiv zu übersetzen. Auch auf semantischer Ebene war es hilfreich, den Ausgangssatz einzublenden, z.~B. wurde in einem Satz ‚Mängel berichten‘ von einigen Systemen als ‚announce defects‘ übersetzt. Zwei Teilnehmer schlugen anhand des Ausgangssatzes ‚report defects‘ zur Verbesserung der Stil- und Inhaltsqualität vor.

Auf Basis dieser Ergebnisse und aufgrund der großen Anzahl der zu bewertenden MÜ-Sätze (1.100 Sätze) wurde entschieden, die Ausgangsätze bei der Bewertung einzublenden. Eine Bewertung der Inhaltsqualität ohne den Ausgangstext wäre nicht möglich. Eine Aufteilung der Sätze in zwei Phasen, wobei die Teilnehmer in der ersten Phase die Stilqualität ohne Ausgangstext bewerten und nach einem bestimmten Zeitraum die Inhaltsqualität mit Ausgangstext in einer zweiten Phase bewerten, hätte das Finden von Teilnehmern erschwert, die für einen längeren Zeitraum verfügbar gewesen wären, und wäre zudem mit weiteren Kosten verbunden gewesen.

\subsubsubsection{Darstellung der Zielsätze: Markierung der KS-Stelle}

Bei dem Testdesign stellte sich die Frage, ob es für die Bewertung sinnvoll wäre, die KS-Stelle zu markieren und die Teilnehmer aufzufordern, sich bei der Bewertung gezielt auf die markierte Stelle zu beziehen und damit zu verhindern, dass sie von anderen Stellen außerhalb der KS-Stelle abgelenkt werden. Die Technik der Markierung einer bestimmten Stelle innerhalb der MÜ wurde von \citet{Callison-BurchEtAl2007} und \citet[236]{RamirezPolo2012} bei der Humanevaluation eingesetzt. \citet{Callison-BurchEtAl2007} markierten einen bestimmten syntaktischen Bestandteil (im Ausgangs- und Zielsatz) und forderten die Teilnehmer auf, mehrere MÜ eines Ausgangssatzes zu ranken. Bei diesem Ranking berichteten Callison-Burch et al. (ebd.) von einem hohem Interrater- und Intrarater-Agreement sowie schneller Evaluation. In der vorliegenden Studie handelt es sich hingegen um eine Scoring-Aufgabe von einer MÜ und keiner Ranking-Aufgabe von mehreren MÜ desselben Satzes. \citet[236]{RamirezPolo2012} markierte die KS-Stelle (im Ausgangs- und Zielsatz) und bat die Teilnehmer die MÜ auf einer 4er-Skala zu bewerten. Sie beschrieb das Profil der Teilnehmer als „native speakers who worked within the automotive industry and therefore mastered the terminology and the background knowledge necessary to understand the texts“ (ebd.). Aus dieser Profilbeschreibung geht hervor, dass die Teilnehmer erfahrene Beschäftigte der Automobilindustrie sind. Informationen zu ihren linguistischen oder translatorischen Qualifikationen kann man jedoch nicht erschließen. Ramírez Polo macht ebenfalls Gebrauch von der Markierung der KS-Stelle im Ausgangs- und Zielsatz, „in order to direct the evaluator's attention to these fragments“ (ebd.). Die Markierung diente somit der Orientierung, die sich eventuell für ihre Zielgruppe als nützlich erwies. Anders als Ramírez Polos Studie wurde die vorliegende Studie von qualifizierten Übersetzern durchgeführt (siehe \sectref{sec:4.4.5.3}).

Um eine Entscheidung diesbezüglich zu treffen, wurden drei Tests mit insgesamt zehn Teilnehmern durchgeführt: Im ersten Test \textit{Evaluation ohne Markierung} wurden 19 Sätze von vier Teilnehmern ohne Markierung der KS-Stelle bewertet. Im zweiten Test \textit{Evaluation mit langer Markierung} wurden dieselben 19 Sätze von drei Teilnehmern bewertet, nachdem die KS-Stelle zusammen mit anderen Wörtern im Satz markiert wurde. Die Länge der Markierung betrug min. 50~\% des Satzes gemessen auf Basis der Anzahl der Wörter. Im dritten Test \textit{Evaluation mit kurzer Markierung} wurden ebenfalls die KS-Stellen in den 19 Sätzen markiert, aber mit einer kürzeren Markierung, so dass die KS-Stelle zusammen mit zwei weiteren Wörtern markiert wurde (d.~h. mit einem Wort vor und einem Wort nach der KS-Stelle, wenn die KS-Stelle in der Mitte des Satzes lag; oder mit zwei Wörtern nach der KS-Stelle, wenn die KS-Stelle am Anfang des Satzes lag). Die Sätze wurden ebenfalls im letzten Test von drei Teilnehmern bewertet. Die Ergebnisse der drei Tests lauten wie folgt:


\begin{table}
\begin{tabularx}{\textwidth}{QYYY}
\lsptoprule
& KS-Stelle ohne Markierung & KS-Stelle mit langer Markierung & KS-Stelle mit kurzer Markierung\\
\midrule
Mittelwert der Stilqualität & 3,80 & 4,00 & 3,96\\
\tablevspace
Mittelwert der Inhaltsqualität & 4,44 & 4,33 & 4,49\\
\lspbottomrule
\end{tabularx}
\caption{\label{tab:4:10}Ergebnisse des Testlaufs}
\end{table}

Wie \tabref{tab:4:10} zeigt, liegt die Differenz im Mittelwert bei dem Szenario „ohne Markierung“ gegenüber dem Szenario „kurze Markierung“ bei 3,9~\% für die Stilqualität bzw. bei 1~\% für die Inhaltsqualität. Zudem kommentierten die Teilnehmer die KS-Stelle wie folgt: Im ersten Test (\textit{Evaluation ohne Markierung}) konnten die Teilnehmer die KS-Stelle in 74~\% der Sätze identifizieren und kommentierten sie. In ca. 16~\% der Fälle kommentierten die Teilnehmer die KS-Stelle zusammen mit einer weiteren Stelle. In den restlichen Fällen (10~\%) waren die MÜ fehlerfrei, sodass kein Kommentar erforderlich war. Auf der anderen Seite zeigten die Tests mit langer und kurzer Markierung, dass die Teilnehmer Schwierigkeiten hatten, sich bei der Qualitätsbewertung auf einen bestimmten Teil der Übersetzung zu beschränken und einen Qualität-Score in Bezug auf die markierte Stelle zu vergeben. Trotz der Testanweisung, nach der nur die markierte Stelle bei der Bewertung berücksichtigt werden soll, beinhalteten die Kommentare häufig Stellen, die nicht markiert waren. Dies kann auch je nach Satzstruktur, den enthaltenen Konstruktionen und der Art der Fehler nachvollziehbar sein.

Aufgrund des kleinen Unterschieds in den Mittelwerten (\tabref{tab:4:10}) in den Szenarien mit und ohne Markierung sowie der beschriebenen Bewertungsschwierigkeit wurde entschieden, die Übersetzungen bei der Evaluation ohne Markierung darzustellen. Im Gegensatz zu den oben genannten Studien, die die Markierung zur Orientierung einsetzen, sieht die vorliegende Studie eine Gefahr in der Markierung, nämlich dass die Teilnehmer durch die Markierung einen Anreiz erhalten, eine Stelle zu kritisieren, die sie ohne Markierung nicht bemängelt hätten. Diese Gefahr wurde durch das Testing ohne Markierung abgewendet.

\subsubsubsection{Darstellung der Evaluation}

Nachdem die Qualitätsaufteilung und -kriterien sowie die Bewertungsskala festgelegt wurden, folgte eine Präzisierung der Details des Testdesigns. Diese beziehen sich auf die genauen Testanweisungen und -aufgaben sowie das Testlayout.

\paragraph*{Testlayout}

Wie \figref{fig:4:8} zeigt, bestand das Testlayout aus fünf Bereichen: Im Bereich [1] stehen die Ausgangs- und Zielsätze; im Bereich [2] die Likert-Skalen für die Stil- und Inhaltsqualität; in den Bereichen [3a] und [3b] die Kriterien der Stil- bzw. Inhaltsqualität; im Bereich [4] Kommentarfelder; und im Bereich [5] ein Feld für den Vorschlag einer Alternativübersetzung.


\begin{figure}
\includegraphics[width=\textwidth]{figures/d3-img005.png}
\caption{\label{fig:4:8}Testlayout der Humanevaluation}
\end{figure}

\paragraph*{Testaufgaben}

Die Teilnehmer hatten \textit{drei Aufgaben}, die nach den Testanweisungen (\figref{fig:4:9}) in der folgenden \textit{Reihenfolge} beantwortet werden sollten:

\begin{enumerate}[label = {(\arabic*)}, align = left]
\item \textit{Auswahl der zutreffenden Kriterien der Stil- und Inhaltsqualität} (Bereich [3a] und [3b] in \figref{fig:4:8}). Da mehrere Kriterien gleichzeitig zutreffen bzw. sich gegenseitig beeinflussen können, wurden für die einzelnen Kriterien Checkboxen verwendet, die die Auswahl mehrerer Kriterien erlauben.
\item \textit{Begründung der Auswahl bestimmter Qualitätskriterien} (Bereich [4] in \figref{fig:4:8}), indem der Bewerter den für das ausgewählte Kriterium relevanten Teil der Übersetzung angibt, seine Auswahl kurz begründet und eine Korrektur bzw. mögliche Verbesserung vorschlägt. Sollten mehrere Modifikationen erforderlich gewesen sein, gibt der Bewerter eine Alternativübersetzung für den gesamten Satz in Bereich [5] an.
\item \textit{Vergabe der Stil- und Inhaltsqualität-Scores} entsprechend der ausgewählten Qualitätskriterien auf den Likert-Skalen (Bereich [2] in \figref{fig:4:8}).
\end{enumerate}

\paragraph*{Testanweisungen}

Die Bewerter erhielten zu Beginn der Testphase eine kurze Einführung zur Evaluation, die die allgemeinen Evaluationsanweisungen, die Definitionen der Qualität sowie eine Erklärung des Testlayouts anhand eines Beispiels (siehe Anhang~\ref{app:1}) beinhaltete. Es wird in der Regel empfohlen, die Definitionen der Qualität am Ende jeder Seite anzugeben, die als Gedächtnisstütze dienen sollen \citep[193]{Doherty2012}. Die alleinige Angabe der Definitionen birgt das Risiko, dass sie nicht gelesen bzw. umgesetzt werden. Anstatt die Definitionen bei jedem Satz isoliert anzugeben, wurden sie in die anzukreuzenden Kriterien (Bereich [3a] und [3b] in \figref{fig:4:8}) integriert. Dies zusammen mit der darauffolgenden Aufgabe, die angekreuzten Kriterien zu kommentieren (Bereich [4] in \figref{fig:4:8}), zielte darauf ab, dass die Evaluationsdefinitionen umgesetzt werden.

Der Test wurde in Microsoft Excel erstellt,\footnote{{Aufgrund des speziellen Aufbaus der Testaufgaben war eine Durchführung des Tests mit einem klassischen Testtool nicht möglich. Mithilfe von Microsoft Excel konnte das gewünschte Design entworfen werden und die Errechnung sämtlicher quantitativer Daten war realisierbar.}} so dass die erste Seite die Anweisungen beinhaltet, gefolgt von 25 Seiten zur Bewertung jeweils eines Satzes (\figref{fig:4:9}).


\begin{figure}
\includegraphics[width=\textwidth]{figures/Abb9.png}
\caption{\label{fig:4:9}Testanweisungen und Testlayout in Microsoft Excel}
\end{figure}

In den Testanweisungen wurden die Bewerter darauf aufmerksam gemacht, dass (1) ein bestimmter Teil der Übersetzung einen Einfluss auf mehrere Qualitätskriterien haben kann (1:n-Beziehung möglich), siehe Punkt 5 in den Anweisungen; und (2) eine inhaltlich korrekte Übersetzung stilistisch optimierungsbedürftig sein kann (Punkt 6 in den Anweisungen).

Die Testanweisungen beeinflussen zweifellos die Studienvalidität, -reliabilität und Evaluationseffizienz \citep{Doherty2017}. Nach Doherty (ebd.: 11) „Instructions and guidelines should be written clearly and concisely and contain explicitly operationalized definitions appropriate to the evaluator group and the TQA\footnote{{TQA steht für Translation Quality Assessment.}} task“. All diese Faktoren wurden bei dem Testdesign berücksichtigt. Die Klarheit der Testanweisungen bzw. -aufgaben wurden in einem kurzen Testlauf mit drei Probanden\footnote{{Die Probanden des Testlaufs nahmen nicht an der Evaluation teil.}} geprüft. Auf Basis dieses Testablaufs wurden die notwendigen Anpassungen vorgenommen. Die Anzahl der Sätze pro Test wurde auf Basis des Testlaufs festgelegt. Im Testlauf nahm die Bewertung eines Satzes ca. 2 Minuten in Anspruch. Um die Testdauer in einem Zeitraum von weniger als einer Stunde zu halten und somit die Auswirkungen von Müdigkeit oder Langeweile zu reduzieren, wurde die Anzahl der Sätze pro Test auf 25 festgelegt. Auf Seite 1 von 25 und Seite 25 von 25 musste jeder Bewerter die Startzeit bzw. Endzeit angeben. Nach den Bewertungsseiten folgte eine für die Bewerter ausgeblendete Seite, in der alle Ergebnisse sowie Zeitangaben mithilfe von Excel-Formeln zusammengefasst wurden.

\paragraph*{Ablauf der Evaluation}

Die 1.100 Testsätze (siehe Anhang~\ref{app:3}) wurden randomisiert in 44 Tests aufgeteilt. Jeder Bewerter hatte die Möglichkeit, je nach seiner Kapazität festzulegen, ob er einen, zwei oder drei Tests pro Tag bewerten wollte. Grundvoraussetzung war, mindestens einen Test täglich zu bewerten, und somit Unterbrechungen zu vermeiden, die möglicherweise das Intrarater-Agreement negativ beeinflusst hätten. Das Maximum wurde auf drei Tests täglich festgelegt, um möglichst eine Bewertung trotz Ermüdung zu verhindern. Außerdem wurden die Teilnehmer gebeten, eine Pause zwischen den Tests einzulegen. Die Bewerter erhielten die Tests in unterschiedlich randomisierter Reihenfolge (z.~B. erhielt Bewerter 1 Test 15, Test 8, Test 40 hintereinander). Der Tester erhielt jeden Tag die bewerteten Tests und prüfte, ob alle Sätze bewertet und ggf. kommentiert worden waren. Bei fehlenden Bewertungen wurde der Teilnehmer gebeten, sie zu ergänzen. Daraufhin erhielt der Teilnehmer die Tests des folgenden Tages. Dieser Ablauf hatte zum Ziel, die Qualität der Bewertung zu sichern. Als Vergütung erhielten die Teilnehmer 25 Cent pro Satz.

Psychologische Risiken, die mit einer Humanevaluation einhergehen, lassen sich nur schwer vollständig verhindern. Durch die zweifache Randomisierung konnte der Effekt einiger Risiken minimiert werden: Erstens ähneln sich die MÜ-Sätze verschiedener Systeme desselben Ausgangssatzes zu einem gewissen Grad. Eine Randomisierung der Testsätze zielte darauf ab, Qualitätsbewertungen zwischen benachbarten Sätzen so unabhängig wie möglich zu halten (vgl. \citealt{Hamon2007}). Zweitens lief die Testphase über drei bis vier Wochen. Eine Randomisierung der Reihenfolge der von jedem Teilnehmer beantworteten Tests hatte zum Ziel, das Risiko, das von \citet{White2003} als „Maturation“ bezeichnet wird, zu mitigieren. Im Laufe der Testphase „ordinary things can affect someone’s ability to be consistent in their judgments. Specifically, they will get tired, bored, hungry, or fed up with the process of evaluating” (ebd.: 209). Das hat den Effekt, dass „sentences they graded later in the cycle will get a different look than the ones they graded earlier“ (ebd.). Dadurch, dass die Bewerter die Tests in unterschiedlicher Reihenfolge erhielten, wurden keine identischen Sätze von allen Teilnehmern am Ende der Testphase bewertet bzw. wurden die Sätze von den Teilnehmern zu unterschiedlichen Zeitpunkten in Laufe der Testphase bewertet.


\subsubsection{Anzahl und Profil der Teilnehmer}\label{sec:4.4.5.3}

Vorherige Studien der MÜ-Evaluation empfehlen die Evaluation mithilfe von mindestens drei bzw. vier Probanden durchzuführen (vgl. \citealt{DysonHannah1987}: 166; \citealt{Arnold1994}: 171). Nach dem Gesetz des abnehmenden Grenznutzens (law of diminishing marginal utility) von Gossen (1854/1983 zit. nach \citealt{CaplinGlimcher2014}) „each additional unit of gain leads to an ever-smaller increase in subjective value“. Eine Anwendung dieses bewährten Konzepts im Experimentkontext würde Folgendes bedeuten: Mit der Erhöhung der Anzahl der Teilnehmer liefert jeder neue Teilnehmer weniger neuen Input als der vorherige Teilnehmer (vgl. \citealt{Huber2008}: 37). Dies wiederum deutet darauf hin, dass „at some point, additional sampling serves little purpose since these measurements provide negligible new information. The optimal sample size falls in this region of diminished marginal utility” (ebd.).

In dieser Studie wurde das Gesetz des abnehmenden Grenznutzens zur Festlegung der Teilnehmeranzahl herangezogen. In der Testphase wurden zunächst fünf Teilnehmer rekrutiert. Sukzessive wurde die Anzahl der Teilnehmer erhöht bis sich der Mittelwert der akkumulativen Qualitätswerte stabilisierte. Bei acht Teilnehmern veränderte sich der akkumulative Qualitätsmittelwert kaum. Entsprechend wurde die Anzahl der Teilnehmer nicht weiter erhöht.

Die Hauptauswahlkriterien für die Teilnehmer waren:

\begin{enumerate}[label = {(\arabic*)}, align = left]
\item Englischmuttersprachler: Dies stellte das Hauptkriterium dar, da die Zielsprache der MÜ Englisch war.
\item Bachelorstudiengang der Translation bereits abgeschlossen. Mit diesem Kriterium wurden hohe Sprachkompetenz in der deutschen Sprache sowie Translationskompetenz sichergestellt.
\item Masterstudent im letzten Semester des Masterstudiengangs Translation des Fachbereiches Translations-, Sprach- und Kulturwissenschaft der Johannes Gutenberg-Universität Mainz.
\end{enumerate}

Im Rahmen der Evaluationsdesignphase wurden Professionals und Semiprofessionals kontaktiert. Im Vergleich zu Professionals waren die Semiprofessionals einfacher zu erreichen, zeigten eine höhere Bereitschaft über mehrere Wochen verfügbar zu sein und ihre Vergütung war kosteneffizienter. Daher wurde die Studie mithilfe von Semiprofessionals durchgeführt. Dieses Profil hat Vorteile gegenüber einer Evaluation mithilfe von Endnutzern oder Professionals: Endnutzer verfügen zwangsläufig nicht über die linguistischen Kompetenzen, die für die Evaluation erforderlich sind (vgl. \citealt{Roturier2006}: 201). Vorherige MÜ-Evaluationsstudien (vgl. \citealt{Arnold1994}; \citealt{FiedererO’Brien2009}), die den Unterschied zwischen der Qualitätsbewertung von Übersetzern gegenüber der von MÜ-System-Endnutzern adressieren, fanden heraus, dass die Bewertung der Übersetzer aufgrund ihrer linguistischen bzw. translatorischen Qualifikationen strenger und kritischer im Vergleich zu der der Endnutzer ist. Für die Humanevaluation der vorliegenden Studie ist eine strenge bzw. kritische Bewertung der MÜ-Qualität von Vorteil, da die Aufgabenstellung eine Differenzierung zusammen mit einer genauen Begründung der identifizierten Fehler verlangt. Gleichzeitig sind die Teilnehmer dieser Studie Übersetzer mit relativ kurzer Übersetzungserfahrung, daher ist eine angemessen kritische Bewertung zu erwarten.

Die Profildaten der Teilnehmer wurden im Rahmen von einem Pre-Test und einem Post-Test erhoben. Die vollständigen Pre- und Post-Tests sind in Anhang~\ref{app:2} zu finden. Im \textit{Pre-Test} wurden die folgenden Grundprofildaten erfragt: Geschlecht, Herkunftsland, Deutschkenntnisse sowie Dauer und Bereich der Übersetzungserfahrung.

Der \textit{Post-Test} bestand aus den folgenden Teilen:

\begin{enumerate}[label = {Teil \arabic*:}, align = left]
\item Fachliche Fragen zur Einstellung gegenüber der MÜ sowie zu Kenntnissen bzw. Erfahrung mit der KS. Diese Fragen wurden erst nach der Evaluation gestellt, damit die Teilnehmer keinen Hinweis im Vorfeld bekommen, worum es sich bei der Bewertung handelt.

\item Feedback zur Evaluation bzw. zur Testphase
\end{enumerate}

Eine Analyse der Testdaten bzw. der Teilnehmerprofildaten und ihrer Feedbacks ist unter \sectref{sec:5.1.4} aufgeführt.


\subsubsection{\label{sec:4.4.5.4}Struktur der Ergebnisse der Humanevaluation}

Im Rahmen der Humanevaluation erfolgte der Vergleich der Szenarien vor-KS vs. nach-KS im Hinblick auf die Stil- und Inhaltsqualität. Zudem konnten die Korrelationen zwischen den Fehlertypen und der Stil- und Inhaltsqualität untersucht sowie die Stil- und Inhaltsqualität der Annotationsgruppen analysiert werden. Konkret konnte anhand der Humanevaluation Folgendes ermittelt bzw. realisiert werden:

\begin{enumerate}[label = {(\arabic*)}, align = left]
\item Vergleich der Stil- und Inhaltsqualität-Scores der MÜ vor vs. nach der Anwendung der einzelnen KS-Regeln. Hierfür wurde der Signifikanztest Wilcoxon verwendet, da nicht alle Qualitätswerte normalverteilt waren.
\item Berechnung und Analyse der Korrelation zwischen den Fehlertypen und der Stil- und Inhaltsqualität. Die Korrelation wurde mithilfe des Spearman-Korrelationstests berechnet, da die Differenz der Fehleranzahl der einzelnen Fehlertypen ordinal war. Zudem setzt Spearman keine Anforderung an die Verteilung und die Linearität voraus.
\item Untersuchung der einzelnen Kriterien der Stil- und Inhaltsqualität
\item Vergleich der Stil- und Inhaltsqualität auf Annotationsgruppenebene. Zur Messung der Signifikanz wurde der Wilcoxon-Test verwendet, da nicht alle Qualitätswerte normalverteilt waren.
\item Erhalt der Alternativübersetzungen, die für die Ermittlung der MÜ-Qualität mithilfe der automatischen Evaluationsmetriken erforderlich sind.
\end{enumerate}


\subsection{Design der automatischen Evaluation}
\label{sec:4.4.6}
Um die Qualität der MÜ vor und nach dem Einsatz der einzelnen KS-Regeln aus einer anderen Perspektive zu beurteilen, wurde eine automatische Evaluation durchgeführt. Hierfür wurden die im Rahmen der Humanevaluation vorgeschlagenen Alternativübersetzungen herangezogen und als Referenzübersetzung für die Evaluation mithilfe von zwei automatischen Evaluationsmetriken (AEMs), nämlich TERbase und hLEPOR, eingesetzt. Durch die Triangulation der Ergebnisse der Humanevaluation mit denen der automatischen Humanevaluation konnte die Korrelation zwischen den Scores der AEMs und der Stil- und Inhaltsqualität genauer untersucht werden. Anhand der triangulierten Ergebnisse konnte die Richtung der Qualitätsveränderung durch eine weitere Evaluationsmethode belegt werden. In den nachstehenden Abschnitten wird das Testdesign näher erläutert sowie die Auswahl der beiden genannten Evaluationsmetriken begründet.

\subsubsection{\label{sec:4.4.6.1}Testdesign}

Wie das Testdesign der Humanevaluation zeigte, bestand ein Teil der Testaufgaben darin, ein „Light Post-Editing“ durchzuführen. Ein „Light Post-Editing“ zielt darauf ab, den Inhalt in einer verständlichen und genauen Weise bereitzustellen und nur wesentliche Korrekturen vorzunehmen (vgl. \citealt{O’Brien2010}). Kritisiert der Teilnehmer nur einen Teil oder ein Wort innerhalb der Übersetzung, so sollte er diesen Teil korrigieren oder eine Verbesserung dafür vorschlagen. Sollte seine Korrektur bzw. Verbesserung mehrere Stellen in der MÜ betreffen oder den Rest der MÜ grammatisch oder stilistisch beeinflussen, wurde der Teilnehmer aufgefordert, eine Alternativübersetzung für den gesamten Satz anzugeben. Im Folgenden der entsprechende Teil der Testanleitung:

\medskip
\noindent\framebox[\textwidth]{\parbox[t]{.9\textwidth}{\medskip
When is it necessary to enter an alternative translation for the whole sentence at the bottom?

\begin{itemize}
\item If you just recommend replacing a certain part in the translation and agree with the rest of the translation, you do not need to enter an alternative translation.
\item If your suggested modification (e.g. in the main clause) requires a further modification (e.g. in the subordinate clause), an alternative translation for the whole sentence is needed; otherwise, the translation might not sound idiomatic.
\end{itemize}
}}
\medskip

Dementsprechend standen für jeden Ausgangssatz bis zu acht Alternativübersetzungen (eine Alternativübersetzung pro Teilnehmer im Fall, dass alle Teilnehmer die MÜ kritisierten) zur Verfügung. Diese Alternativübersetzungen agierten als Referenzübersetzungen für die AEMs, vergleichbar mit der Vorgehensweise von \citet{SnoverEtAl2006}.

Vergleicht man posteditierte MÜ mit Humanreferenzübersetzungen, die beide korrekt sind, sind die posteditierten MÜ in der Regel den MÜ ähnlicher als die Humanreferenzübersetzung (vgl. \citealt{DenkowskiLavie2012}). Gleichzeitig besteht eines der bekanntesten Probleme der AEMs darin, „that metrics are good at detecting similar translations, but poor at evaluating sentences with different structure and lexical choice“ (ebd.: 6). Vor diesem Hintergrund könnte die Verwendung von Humanreferenzübersetzungen mit schlechten AEM-Scores -- nur aufgrund der Unterschiede zwischen der MÜ und der Humanreferenzübersetzung -- verbunden sein, obwohl die MÜ korrekt ist. Mit der Verwendung der posteditierten MÜ konnte die Studie dieses Risiko umgehen.

Im Folgenden wird die Vorgehensweise bei der Auswahl der Referenzübersetzungen sowie die Vergleichsbasis der Szenarien vor-KS vs. nach-KS aus Sicht der automatischen Evaluation erklärt.


\subsubsection{\label{sec:4.4.6.2}Auswahl der Referenzübersetzungen}

Aus den verfügbaren Alternativübersetzungen aller Teilnehmer wurden Übersetzungen zweier Teilnehmer pro Ausgangssatzpaar (vor und nach KS) ausgewählt, die für die AEMs als Referenzübersetzungen fungierten. Wie \citet{DenkowskiLavie2012} zeigten, kann der MÜ-Akzeptanzgrad des Humanübersetzers stark variieren. Um unterschiedliche PE-Möglichkeiten zu berücksichtigen, wurde die automatische Evaluation auf Basis von Referenzübersetzungen von \textit{zwei} Teilnehmern durchgeführt. In einer weiteren Studie mit dem Titel „Potential and limits of using post-edits as reference translations for MT evaluation“ fanden \citet{PopovićEtAl2016} heraus, dass posteditierte MÜ-Outputs „definitely useful as reference translations” sind. Sie empfahlen gleichzeitig die Verwendung des PE-Outputs einer qualitativen MÜ eines unabhängigen Systems (d. h. PE-Output eines anderen Systems) sowie die Verwendung mehrerer Referenzen (ebd.). Mehr als zwei Referenzübersetzungen pro Ausgangssatz waren nicht immer verfügbar, da z. B. korrekte MÜ kein PE erfordern. Es wurde außerdem, wie von Popovic et al. (ebd.) empfohlen, darauf geachtet, dass das PE eines anderen Systems verwendet wurde (z. B. wurde bei der automatischen Evaluation von Systran das PE von Lucy verwendet). Zudem wurde bei der Auswahl der Referenzübersetzungen berücksichtigt, dass möglichst von allen Teilnehmern gleichermaßen Übersetzungen vertreten sind. Zunächst wurden die Teilnehmer chronologisch nach Abgabedatum des letzten Tests aufgelistet. Die Alternativübersetzung des ersten Teilnehmers wurde, falls vorhanden, ausgewählt. Sollte der erste Teilnehmer keine Alternativübersetzung vorgeschlagen haben, wurde die Übersetzung des zweiten Teilnehmers genommen. Bei dem nächsten Satz wurde die Übersetzung des dritten Teilnehmers ausgewählt usw.

In den Testanweisungen erhielten die Teilnehmer die allgemeine Hintergrundinformation, dass es sich bei der Übersetzung um Benutzerhandbücher und Gebrauchsanweisungen handelt. Es wurden keine weiteren Angaben zum genauen Kontext der einzelnen Testsätze zur Verfügung gestellt. Ziel hierbei war, dass die Teilnehmer aus einer vergleichbaren Ausgangssituation wie die MÜ-Systeme heraus kontextlos handeln und auf dieser Basis die MÜ-Sätze bewerten und ggf. Alternativübersetzungen vorschlagen. Nach den Testanleitungen sollten die Teilnehmer einen Kommentar hinterlassen, wenn sie bei einem bestimmten Satz Kontextinformation benötigten, um die MÜ bewerten oder eine Alternativübersetzung vorschlagen zu können. Im Endeffekt war die Anzahl der Kommentare zu fehlenden Kontextinformationen sehr gering und die Kommentare flossen bei der qualitativen Auswertung mit ein. Gleichzeitig wurden die Alternativübersetzungen mit diesen Kommentaren als Referenzübersetzungen in den AEMs verwendet.

\subsubsection{\label{sec:4.4.6.3}Auswahl der automatischen Evaluationsmetriken}

Wie im Kapitel MÜ (siehe \sectref{sec:3.3.3.2} „Automatische Evaluation“) dargestellt wurde, wurden seit den 90er Jahren zahlreiche AEMs entwickelt. In dieser Studie wurden die AEMs TERbase \citep{SnoverEtAl2006} und hLEPOR \citep{HanEtAl2013} verwendet.

TER\footnote{TER steht für Translation Edit Rate \citep{SnoverEtAl2006}.} \citep{SnoverEtAl2006} zählt zu den populären AEMs zum Systemvergleich für europäische Sprachen (vgl. \citealt{Doherty2017}). In TER haben alle Editierungsvorgänge (Edit operations) (Einfügen, Löschen, Ersetzen und Verschieben) einheitliche Editierungskosten (Edit cost) von eins. TERbase ist eine Variante der Metrik TER (\citealt{GonzàlezGiménez2014}: 19). Anders als TER führt TERbase kein „Synonym Match“ (vgl. ebd.). In der Studie führte die Verwendung von Synonymen bei einer KS-Regel wie z. B. „Eindeutige pronominale Bezüge verwenden“ zu einer lexikalischen Inkonsistenz (siehe \tabref{tabex:05:43}). Entsprechend war es erforderlich, dass ein Punktabzug bei der Verwendung von Synonymen erfolgte.\footnote{Ebenfalls wurde Meteor (\citealt{BanerjeeLavie2005}) aus demselben Grund, nämlich dass kein Punktabzug bei der Verwendung von Synonymen erfolgt, ausgeschlossen.} Aus diesem Grund wurde TERbase verwendet. Die Qualität-Scores von TER bzw. TERbase bewegen sich zwischen $-$ 1 (niedrigster Wert) und 0 (höchster Wert); somit arbeitet TERbase mit negativen Werten. Die Evaluation mithilfe von TERbase wurde mit dem online verfügbaren Tool Asiya\footnote{{{{Asiya ist online zugänglich unter:} }}\url{http://asiya.cs.upc.edu/demo/asiya_online.php}} (\citealt{GonzàlezGiménez2014}) durchgeführt. Das Tool ist benutzerfreundlich und kostenfrei online zugänglich.

Eine weitere sehr verbreitete AEM für die MÜ-Evaluation zwischen europäischen Sprachen ist BLEU\footnote{{BLEU steht für BiLingual Evaluation Understudy \citep{PapineniEtAl2002}.}} \citep{PapineniEtAl2002}. Trotz ihrer weit verbreiteten Nutzung konnte BLEU in der Studie nicht verwendet werden. Die Evaluation im Rahmen dieser Studie erfolgt auf Satzebene, während BLEU auf Dokumentebene funktioniert (ebd.). Zudem zeigen \citet{Callison-BurchEtAl2009}, dass Metriken der neuen Generation BLEU hinsichtlich der Korrelation mit der Humanevaluation übertreffen.

Eine hohe Korrelation mit der Humanevaluation ist ein Hauptkriterium bei der Bewertung der Effektivität von AEMs, wodurch AEMs die kostenintensive Humanevaluation vermehrt ersetzen können (vgl. \citealt{LinOch2004}). Vor diesem Hintergrund war es erforderlich, eine zweite AEM der neuen Generation zu finden, deren Scores mit den Bewertungen im Rahmen von Humanevaluationen stark korrelieren. Im WMT Workshop 2011\footnote{\url{http://www.statmt.org/wmt11}} zeigte die Spearman Korrelation zwischen den Bewertungen der Humanevaluation und Scores der AEM hLEPOR die höchste Korrelation im Vergleich zu denen der AEMs MPF, ROSE, METEOR, BLEU und TER -- sowohl sprachübergreifend (Korrelationskoeffizient von 0,83) als auch für das Sprachenpaar der vorliegenden Studie DE>EN (Korrelationskoeffizient 0,86) \citep{HanEtAl2013}. Das Berechnungsmodell von hLEPOR\footnote{{hLEPOR steht für Harmonic mean of enhanced Length Penalty, Precision, n-gram Position difference Penalty and Recall \citep{HanEtAl2013}.}} basiert auf einem harmonischen Mittelwert von drei Faktoren: einer „enhanced length penalty“, einer „N-gram position difference penalty“ sowie „precision and recall“ \citep{HanEtAl2013}. Die Qualität-Scores von hLEPOR bewegen sich zwischen 0 (niedrigster Wert) und 1 (höchster Wert); somit arbeitet hLEPOR mit positiven Werten. Dank des Entwickler-Teams von hLEPOR wurde ein einfach zu bedienendes Tool mit einem für DE-EN getunten Skript zu Forschungszwecken im Rahmen der Studie kostenfrei zur Verfügung gestellt.

Aus den obengenannten Motiven sowie Kosten- und Effizienzgründen wurde die automatische Evaluation nur mithilfe der AEMs TERbase und hLEPOR durchgeführt. Weitere AEMs, die über vergleichbare Merkmale verfügen, sind somit nicht auszuschließen und können in zukünftigen Analysen zum Vergleich herangezogen werden.

\subsubsection{\label{sec:4.4.6.4}Basis des Vergleichs vor-KS vs. nach-KS zur Ermittlung des KS-Einflusses}

Der Qualitätsvergleich der beiden Szenarien vor-KS vs. nach-KS erfolgte in der automatischen Evaluation mit den beiden AEMs (hLEPOR und TERbase) wie folgt:

\begin{enumerate}[label = {(\arabic*)}, align = left]
\item
Für jedes Szenario wurden die AEM-Scores anhand zweier Referenzübersetzungen berechnet. Beispielsweise wurde für Satz 1 im Szenario vor-KS der hLEPOR-Score auf Basis der Referenzübersetzungen 1 und 2 (Ref.1 und Ref.2) berechnet. Dies ergab die Scores hLEPOR-Ref.1 und hLEPOR-Ref.2).
\item
Der Mittelwert der Scores aus den beiden Referenzübersetzungen wurde berechnet (d.~h. im Beispiel der Mittelwert der Scores hLEPOR-Ref.1 und hLEPOR-Ref.2).
\item
Nach derselben Vorgehensweise wurde der Mittelwert für das Szenario nach-KS berechnet.
\item
Der Einfluss der KS-Regel wurde auf Basis der Differenz „Mittelwert nach-KS“ \textit{minus} „Mittelwert vor-KS“ gemessen. Eine positive Differenz deutet auf eine Verbesserung des AEM-Scores hin, und umgekehrt weist eine negative Differenz auf eine Verschlechterung des AEM-Scores hin.
\end{enumerate}

Obwohl mehrere AEMs die Möglichkeit bieten, parallel diverse Referenzübersetzungen zu verwenden (vgl. \citealt{Han2018}) -- wobei nur die Referenzübersetzung mit dem höheren Match für die Score-Berechnung herangezogen wird --, wurde der Metrik-Score in dieser Studie in zwei Durchgängen jeweils nach einer Referenzübersetzung gesondert berechnet. Ziel dabei war es, die Alternativübersetzungen von zwei Teilnehmern zu berücksichtigen, anstatt die Evaluation auf Basis einer Referenzübersetzung einzuschränken.

Es bestand ferner die Möglichkeit, dass die Teilnehmer Stellen außerhalb der KS-Stelle posteditieren. Daher wurde (1) darauf geachtet, dass die Referenzübersetzungen der vor- und nach-Szenarien vom selben Teilnehmer stammen. Sollte der Teilnehmer eine Stelle außerhalb der KS-Stelle posteditieren, würde diese Stelle in den Referenzübersetzungen (vor- und nach-KS) wiederholt. (2) Die Ermittlung des KS-Einflusses erfolgte auf Basis der Differenz ‚AEM-Score nach-KS‘ \textit{minus} ‚AEM-Score vor-KS‘, damit der im Score enthaltene Anteil von potenziellen Edits außerhalb der KS-Stelle herausgefiltert werden kann. \tabref{tabex:4:9} veranschaulicht die Problematik des PE einer Stelle außerhalb der KS-Stelle und zeigt wie sie gelöst wurde:

In \tabref{tabex:4:9} ging es um die KS-Regel „Für zitierte Oberflächentexte gerade Anführungszeichen verwenden“. Dementsprechend besteht die KS-Stelle aus dem Wort \textit{Raumdruck} ohne Anführungszeichen im Szenario vor-KS bzw. mit Anführungszeichen im Szenario nach-KS (fett hervorgehoben). Die Edits in den Referenzübersetzungen wurden innerhalb der KS-Stelle unterstrichen und außerhalb der KS-Stelle farblich hervorgehoben. Wie unter \sectref{sec:4.4.3.1} ([9] Aufbereitung der Zielsätze für die Humanevaluation) erläutert wurde, wurden die MÜ-Sätze außerhalb der KS-Stelle vor und nach dem Einsatz der KS-Regeln vereinheitlicht. Dies ließ die Teilnehmer-Edits außerhalb der KS-Stelle -- falls Edits erforderlich waren -- ebenfalls einheitlich. Der erste Teilnehmer (Ref.1) editierte in seiner Alternativübersetzung nur die KS-Stelle. Der zweite Teilnehmer (Ref.2) hingegen editierte sowohl die KS-Stelle als auch eine Stelle außerhalb der KS-Stelle (vgl. ‚can be activated‘).


\begin{table}
\begin{tabularx}{\textwidth}{lQr}
\lsptoprule
\multicolumn{2}{c}{} & \textbf{hLEPOR}\\
\textbf{Vor KS} & Die zwei aktivierbaren Raumdruck-Sollwerte sind im Menü \textbf{Raumdruck} definiert. & \\
\textbf{MÜ vor KS} & The two activatable room pressure setpoints are defined in the \textbf{area pressure} menu. & \\
{} & {} & \\
\textbf{Ref.1} & The two activatable room pressure setpoints are defined in the \textbf{\ul{$"$Room} pressure\ul{$"$}} menu. & 0,8794\\
\textbf{Ref.2} & The two room pressure target values that \txgray{can be activated} are defined in the \textbf{\ul{$"$room} pressure\ul{$"$}} menu. & 0,5953\\
\midrule
\multicolumn{2}{c}{} & \textbf{hLEPOR}\\
\textbf{Nach KS} & Die zwei aktivierbaren Raumdruck-Sollwerte sind im Menü \textbf{$"$Raumdruck$"$} definiert. & \\
\textbf{MÜ nach KS} & The two activatable room pressure setpoints are defined in the \textbf{$"$room pressure$"$} menu. & \\
{} & {} & \\
\textbf{Ref.1} & The two activatable room pressure setpoints are defined in the \textbf{$"$\ul{R}oom pressure$"$} menu. & 0,9682\\
\textbf{Ref.2} & The two room pressure target values that \txgray{can be activated} are defined in the \textbf{$"$room pressure$"$} menu. & 0,6947\\
\lspbottomrule
\end{tabularx}
\caption{\label{tabex:4:9} Beispiel 9}
\bspnote{KS{}-Stelle ist \textbf{fett} dargestellt; Edits innerhalb der KS-Stelle sind \bful{unterstrichen}; Edits außerhalb der KS-Stelle sind \txgray{hervorgehoben}.}
\end{table}


\hspace*{-3pt}Der Einfluss der KS errechnete sich in dem Beispiel wie folgt = [(0,9682 +~0,6947) / 2] $-$ [(0,8794 +~0,5953) / 2] = 0,0941. Durch die Subtraktion Score-nach-KS \textit{minus} Score-vor-KS wurde bei der zweiten Referenzübersetzung (Ref.2) der Effekt der außerhalb der KS-Stelle editierten Stelle (‚can be activated‘) herausgefiltert, denn diese Stelle wiederholte sich vor- und nach-KS. Der berechnete positive Differenzwert (0,0941) deutet auf einen positiven Einfluss der KS-Regel hin.


\subsubsection{\label{sec:4.4.6.5}Struktur der Ergebnisse der automatischen Evaluation}

Im Rahmen der automatischen Evaluation erfolgte der Vergleich der Szenarien vor-KS vs. nach-KS mithilfe der Qualitätsmetriken TERbase und hLEPOR. Zudem wurden die Korrelationen zwischen den Scores der Qualitätsmetriken und der Stil- und Inhaltsqualität untersucht. Konkret konnte anhand der automatischen Evaluation Folgendes ermittelt bzw. realisiert werden:

\begin{enumerate}[label = {(\arabic*)}, align = left]
\item Vergleich der Scores der Qualitätsmetriken vor vs. nach der Verwendung der einzelnen KS-Regeln. Hierbei wurde der Signifikanztest Wilcoxon verwendet, da nicht alle Qualitätswerte normalverteilt waren.
\item Berechnung und Analyse der Korrelation zwischen den Scores der Qualitätsmetriken und der Stil- und Inhaltsqualität. Die Korrelation wurde mithilfe des Spearman-Korrelationstests berechnet, da nicht alle Qualitätswerte normalverteilt waren. Zudem setzt Spearman keine Anforderung an die Verteilung und die Linearität voraus.
\end{enumerate}


\section{\label{sec:4.5}{{Fazit}}}

Mithilfe eines dreiphasigen Mixed-Methods-Triangulationsansatzes wird der Einfluss einzelner Regeln der Kontrollierten Sprache auf die Qualität des MÜ-Out\-puts unterschiedlicher MÜ-Systeme untersucht und verglichen. Die implementierten Methoden sind: Fehlerannotation, Humanevaluation und automatische Evaluation. Jede Methode liefert Daten, die für die Analyse in der darauffolgenden Methode erforderlich sind. Auf diese Weise wurde angestrebt, das Ziel der Studie über drei Phasen systematisch zu realisieren und dabei sämtliche Herausforderungen der KS-Untersuchung auf Regelebene zu überwinden. Ferner wurden die Ergebnisse der Analysen trianguliert, um diverse Fragestellungen beantworten zu können. Nicht nur eine methodenexterne Triangulation der drei genannten Methoden sah die angewandte Methodik vor, sondern auch eine methodeninterne Triangulation beim Design der Humanevaluation. Im Testdesign der Humanevaluation waren die Qualitätsdefinitionen in die anzukreuzenden Qualitätskriterien integriert. Somit hatten alle Teilnehmer eine direkte und einheitliche Basis für die Vergabe der Qualität-Scores. Des Weiteren mussten die Teilnehmer vor der Vergabe der Qualität-Scores die angekreuzten Qualitätskriterien kommentieren bzw. posteditieren. Diese Triangulation fördert die interne Konsistenz, optimiert die Zuverlässigkeit und ermöglicht eine genaue Interpretation der Daten. Ferner ist das Testdesign replizierbar. In diesem Kapitel wurde die Methodologie der Studie unter Einbeziehung der Operationalisierung und der Validität präsentiert. Zur Entwicklung des Studiendesigns wurden die theoretische Basis sämtlicher relevanten Aspekte sowie die Ergebnisse vorheriger Studien reflektierend diskutiert. Das daraus resultierende Design wurde bei der Datenerfassung und -analyse umgesetzt. Die Ergebnisse sind im folgenden Kapitel aufgeführt.
