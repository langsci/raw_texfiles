\documentclass[output=paper,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.13383795}
\author{Vilde R. S. Reksnes\orcid{0000-0002-6329-0469}\affiliation{University of Edinburgh} and Alice Rees\orcid{0000-0001-5906-2693}\affiliation{University of Edinburgh} and Chris Cummins\orcid{0000-0001-8708-4515}\affiliation{University of Edinburgh} and Hannah Rohde\orcid{0000-0002-9356-3229}\affiliation{University of Edinburgh}}
\title[Tell me something I don’t know]{Tell me something I don’t know: Speaker salience and style affect comprehenders’ expectations for informativity}

\abstract{A comprehender's estimate of what events or situations are typical in the world is distinct from their estimate of what a speaker is likely to report on. Comprehension and production studies have shown contradicting preferences for which types of estimates are used by comprehenders and by speakers: Typicality is favoured in comprehension (e.g. real-world typical content is associated with processing ease), whereas speakers' production choices favour the inclusion of surprising or informative content (i.e. easily inferable or typical content is disfavoured). We posit that comprehenders are aware of and make use of speakers’ production preferences when anticipating upcoming content. In two studies, we elicit sentence completions as an index of comprehenders’ expectations about upcoming material and evaluate the informativity of these completions (their object typicality, presence of modification or negation, and information theoretic entropy and relative entropy scores). Experiment 1 manipulated the salience of the speaker and found that increased emphasis on the speaker led to an increase in informativity, showing that the more aware comprehenders are made of an intentionally communicating speaker, the more their expectations favour upcoming words that would yield an informative utterance. Experiment 2 further tested the malleability of this informativity bias by familiarising participants with two speakers who differ in the informativity of their utterances. When completing utterances from the two speakers, comprehenders provide more informative completions for the high-informativity speaker, showing that comprehenders are able to adapt their expectations for informativity to individual speakers’ communicative styles. This sensitivity to speakers’ production preferences highlights a role for informativity-driven reasoning about the speaker in models of language processing.}
%\abstract{A comprehender's estimate of what events or situations are typical in the world is distinct from their estimate of what a speaker is likely to report on. Comprehension and production studies tend to show contradicting preferences for which types of estimates are used by comprehenders and by speakers: typicality is favoured in comprehension (e.g. real-world typical content is associated with processing ease); however, speakers tend to make production choices that favour the inclusion of surprising or otherwise informative content (meaning that easily inferable or typical content is disfavoured). We posit that comprehenders are aware of and make use of speakers’ production preferences when anticipating upcoming content. In two Cloze task studies, we elicit sentence completions as an index of comprehenders’ expectations about upcoming material and evaluate the informativity of these completions via several measures (object typicality, presence of modification or negation, and information theoretic entropy and relative entropy scores). The first experiment manipulated the salience of the speaker and found that increased emphasis on the speaker led to an increase in informative sentence completions, showing that the more aware comprehenders are made of an intentionally communicating speaker, the more their expectations favour upcoming words that would yield an informative utterance. The second experiment further tested the malleability of this informativity bias by familiarising participants with two different speakers: one who routinely makes uninformative utterances, and one who produces utterances that are highly informative. When asked to complete utterances from each of the two speakers, comprehenders provide more informative completions for the high-informativity speaker, showing that comprehenders are able to adapt their expectations for informativity to individual speakers’ communicative styles. This sensitivity to speakers’ production preferences highlights a role for informativity-driven reasoning about the speaker in models of language processing.}

\IfFileExists{../localcommands.tex}{
   \addbibresource{../localbibliography.bib}
   \input{../localpackages}
   \input{../localcommands}
   \input{../localhyphenation}
%   \boolfalse{bookcompile}
      \togglepaper[7]%%chapternumber
}{}

\begin{document}
\SetupAffiliations{mark style=none}
\maketitle

\section{Introduction} 
The question of how comprehenders generate expectations regarding upcoming content has received much attention in the field (see e.g. \cite{KuperbergJaeger2016} for a review). In a discourse context, expectations about upcoming content can be understood to reflect what comprehenders believe speakers will choose to talk about. On the one hand, speakers can choose to use language to talk about the world that they encounter, producing utterances that describe the kinds of situations that arise in their daily lives. On the other hand, speakers can choose to convey content that is worth talking about, producing utterances that they think a listener will find interesting and newsworthy. 

The first choice characterises what we will call \textit{transparent} language use, wher\-eby speakers use language to directly narrate what happens to them. If speakers use language transparently, comprehenders ought to come to expect language that describes the kinds of situations that arise frequently and plausibly in the real world. The second choice characterises \textit{filtered} language, whereby speakers limit their utterances to those whose content is novel and informative, inducing listeners to expect descriptions of situations which are interesting by virtue of their infrequency. If speakers do indeed filter available content and do so in ways to achieve informativity-driven communicative goals, and if comprehenders are aware of a speaker's role in this filtering, a prediction arises that a context that increases the emphasis on the speaker (in their role as a filter of potential content to convey) should increase comprehenders' expectations for content about infrequent situations. We will argue that prior work on comprehenders' expectations often focuses on expectations about transparent language use, missing an opportunity to find evidence of expectations that are driven by comprehenders' awareness of the speaker as someone engaged in intentional communication with informativity-driven goals. 

A long-standing claim is that when a comprehender makes guesses about what words are coming next, they rely on their knowledge about the world. Studies have shown that knowledge of what is typical of real-world situations is active during language processing and thus typicality has been linked to processing ease in comprehension. Here, we define typicality as the frequency of a situation or event given the context (e.g. trains are typically present at a train station, a beach typically has sand). As an example of comprehenders' anticipation of upcoming content, \citet{KamideHaywood2003} use a visual world eye-tracking methodology and show that participants expect utterances to convey real-world typical content. For example, for an image depicting a man, a motorbike and a carousel, participants hear \textit{The man will ride the\dots} and look to the motorbike before the noun is uttered, anticipating a continuation about the most real-world typical object in the scene (the motorbike). That work is in keeping with findings about the processing costs associated with encountering surprising events. For example, \citet{KutasHillyard1980} show that strong semantic incongruity caused a peak in the N400 relative to moderate incongruity (\textit{taking a sip from the transmitter} vs \textit{taking a sip from the waterfall}). Measures of sentence recall \citep{MarksMiller1964} and reading with eye-tracking \citep{Morris1994} corroborate these effects.

There are studies showing that real-world typicality can be overridden. For example, \citet{NieuwlandVanBerkum2006} showed processing ease for a real-world non-typical event, namely a peanut singing about being in love, compared to a real-world typical event, namely a peanut being salted \citep[see also][]{TroyerKutas2020}. However, the important thing to note here is that comprehenders adjusted their expectations to fit the constraints of a fictional world, one where peanuts sing about their new girlfriends and their amorous feelings towards them. In other words, they still show a bias towards the typical, but this bias is governed by what is typical in the new ``real world''. Overall, these studies suggest that comprehenders have a preference for typicality when they are processing language, i.e. a bias towards transparent language use.
 
Conversely, research on production shows that speakers do impose a filter on their language use: They tend to make production choices that favour the inclusion of surprising or otherwise informative content about a situation \citep[e.g.][]{BrownDell1987, LockridgeBrennan2002}. In doing so, they show a dispreference for real-world typical content: Speakers will often choose to omit altogether information that is inferable or typical, and instead highlight non-typical information. In \citeauthor{BrownDell1987}’s \citeyearpar{BrownDell1987} classic production study, participants read short stories that involved either a typical instrument or an atypical instrument, for example, a stabbing that happened either with a knife or an icepick. Participants were then asked to retell the stories to test whether they chose to mention the instrument in their event description. The results showed a preference to omit mention of the instrument if it was typical (knife being a typical stabbing instrument) and to include a mention of the instrument if it was atypical (icepick being an atypical stabbing instrument). In keeping with this pattern whereby inferable or typical content can be omitted or reduced, there is evidence that high frequency words are produced with shorter acoustic duration than low frequency words (\cite{AylettTurk2004}; see also \cite{LevyJaeger2007, Jaeger2010, Kravtchenko2014, lemke2021}). In other contexts, speakers are shown to include colour adjectives more often when referring to an object if the colour is atypical, such as a pink banana, than if it is typical \citep{Sedivy2003, WesterbeekEtAl2015, RubioFernandez2016, DegenEtAl2020}, and to mention the material of an object when atypical, such as a wool bowl compared to a ceramic bowl \citep{MitchellVanDeemter2013}. Notably, \citet{DegenEtAl2020} found that speakers are less likely to mention the colour of an object if it is typical (e.g. a yellow banana) even when the colour is necessary for disambiguating the intended referent (i.e. when a brown banana is also present). The authors take this to indicate that the speakers are relying on the listener to infer that the most typical object was intended. In contrast, if the target object was an atypical colour (a blue banana, in this case), the colour was always mentioned when a competitor object was present.

The pictures that emerge from these two bodies of literature thus seem to conflict. Evidence from comprehension studies strongly suggests that comprehenders estimate upcoming content with a bias in favour of sentences about typical situations (transparent language use), while results from production studies demonstrate a preference for informativity or newsworthiness in content selection (filtered language use). According to this literature, it would therefore appear as if comprehenders’ expectations are out of line with the predicted behaviour of speakers according to Gricean accounts; that is, speakers’ contributions are expected to be appropriately informative and relevant \citep{Grice1975}. However, if comprehenders are rational and sensitive to speakers’ production preferences, these preferences should also bear upon their estimates regarding upcoming content: That is, comprehenders should expect the kind of content that cooperative speakers are likely to mention, rather than the kind of content that is likely to be the case in the real world. For example, you would probably expect there to be trains in a train station but generally not expect someone to tell you that fact out of the blue. However, if you receive a call from a friend who is at your local train station and they tell you the Hogwarts Express has just pulled up, you would be surprised by the fact that this particular train has made a visit to your local station, but likely not by the fact that your friend chose to communicate the information, given that the situation occurred. 
 
On this view, a comprehender's estimate of what is typical in the world is distinct from their estimate of what the speaker is likely to say \citep{RohdeLucas2021}. In principle, either of these estimates might be activated when a comprehender makes guesses about an upcoming utterance. In the current study, we examine the nature of comprehenders’ predictions by using a sentence completion paradigm (a so-called Cloze task; \cite{Taylor1953}). We use this task to elicit specific sentence completions to assess what words comprehenders predict a speaker will produce, rather than testing whether comprehension is speeded/slowed by available content, as in previous work. Although the Cloze task involves a participant producing words, it can also be understood  as a comprehension-oriented process, in that the task probes what a comprehender thinks is coming next. This approach follows work on coreference that often uses sentence completion tasks to estimate comprehenders' interpretation biases and their expectations about next mention \citep[e.g.][]{StevensonKleinman1994}. In fact, the original Cloze study describes the task in comprehension terms as an ``attempt to reproduce accurately a part deleted from a `message' (any language product) by deciding, from the context that remains, what the missing part should be'' and notably describes the participants as readers/listeners \citep[][416]{Taylor1953}. 

In Experiment 1, we manipulate the emphasis on the presence of a speaker to test whether participants' estimates are modulated by how salient the speaker is. If this manipulation is sufficient to make participants consider the speaker's production process, their guesses about upcoming content should be influenced by their inferences about what the speaker's goals are and why they are choosing to speak. For example, completing the sentence \textit{I’m at the train station, and there's \_\_\_\_} with the words \textit{a train} yields a description of a likely situation, in that a scenario with a train would be a typical occurrence given the location. However, if comprehenders consider what a speaker would deem worth reporting on, they might not expect such  an utterance, precisely given the high typicality of the scenario described. The presence of a train is so typical as to be inferable without mention, so a comprehender might assume that a speaker would not choose to mention it. Consequently, something more informative such as \textit{street performers} or a similarly low-typicality occurrence might be a more likely completion when the presence of a speaker is emphasised. 
 
An emerging line of research shows that comprehenders do keep track of speakers' production preferences. \citet{RohdeLucas2021} established the presence of informativity-driven effects in comprehension, showing that sentences containing newsworthy content were preferred in certain contexts over those that contained real-world typical content. In particular, they observed processing ease for atypical content in a natural dialogue setting with no contextual manipulations; participants were faster reading a newsworthy message about socks that cost \$100 than a message about more standardly priced \$2 socks. \citeauthor{RohdeLucas2021} take this to demonstrate that a communicative context with an intentionally communicating speaker behind an utterance is sufficient to induce a comprehension preference in favour of newsworthy content. Their studies show that comprehenders experience difficulty in integrating inexplicably uninformative content, but do not test what content comprehenders specifically expect or what factors might modulate such expectations. The current study aims to examine the nature of this predicted content in a more fine-grained way. 

Corroborating the findings in \citet{RohdeLucas2021}, \citet{RohdeFranke2022} conducted a series of forced-choice tasks in which participants were asked to estimate what a character in a story would say or what they might think; they found that the character’s choice to produce an utterance unprompted, as opposed to responding to a question or thinking uncommunicated thoughts, cued participants to estimate a more atypical meaning, demonstrating that comprehenders favour sentences that convey newsworthy information. In fact, when comprehenders come across content that does not meet this standard for newsworthiness, they try to reconcile the absence of sufficiently informative content by recasting the utterance in a way that makes it an informative contribution \citep{KravtchenkoDemberg2022}. In \citeauthor{KravtchenkoDemberg2022}'s experiment, participants read short stories about stereotyped activities. In general, comprehenders’ script knowledge about such activities allows speakers to omit typical or inferable information. Upon encountering an utterance about an easily inferable event, e.g. \textit{Mary ate} in the context of going to a restaurant, participants made inferences that this generally typical action was nonetheless unusual for the protagonist. In other words, the study highlights cases in which a discrepancy emerges between the understanding of a speaker as having goals to be cooperatively informative and the uncooperative typicality of the situation being communicated. This discrepancy can be resolved if comprehenders change their prior beliefs about what was typical in this context, thereby ensuring that the utterance meets expectations for informativity. 
 
Relatedly, \citet{LemkeSchäferEtAl2021} make the point that script knowledge provides a way to quantify the likelihood of upcoming content as it may approximate extralinguistic knowledge, which is used when language users make predictions about upcoming events and, thereby, content. They argue for the importance of such knowledge when modelling anticipation, specifically for the production of ellipsis and fragments \citep{LemkeReichEtAl2021, SchäferEtAl2021}. The extralinguistic context is particularly relevant discourse-initially when linguistic context is not available to inform comprehenders’ anticipations. \citet{VenhuizenBrouwer2019} similarly argue that world knowledge has not been hitherto sufficiently incorporated in computational models of comprehension. They develop a model that derives a preference for informative content in processing. The studies presented here expand on such work by assessing the effect of speaker awareness, another aspect of extralinguistic knowledge, on comprehenders’ expectations.
 
Awareness of the speaker can be understood to underlie comprehenders' gues\-ses about the reason behind a speaker's production choice. In particular, there is a longer standing body of work that assesses comprehenders’ guesses about the reason behind a speaker’s production choice, and in particular what referential expressions speakers choose when tasked with picking out a specific object among a set of possible referents (see e.g. \cite{DaviesArnold2019} for a review). Many of these studies show comprehenders reasoning specifically about the informativity of a speaker’s utterances \citep[e.g.][]{GrodnerSedivy2011, PogueTanenhaus2016, RyskinBrown‐Schmidt2019}. For example, if a speaker reliably uses adjectives contrastively (e.g. produces \textit{click on the tall glass} when there is another glass present), participants are able to use that information to identify the referent more quickly; however, participants stop relying on the adjective to indicate contrast if a speaker habitually uses adjectives non-contrastively \citep[e.g. produces \textit{click on the tall glass} even when there is only one glass;][]{GrodnerSedivy2011}. Although this work shows comprehenders adjusting their expectations to different speakers with regards to informativity, these are specifically referential expectations; the pragmatic reasoning does not tap into expectations in the way we outline above, namely expectations for the selection of sentence content. Instead, this work tends to focus on tasks in which the communicative goal is referential success – the comprehender knows that the speaker has an intended referent they need to talk about and the speaker’s choice lies in what forms to use for describing the referent. A comprehender can thus use their awareness of the speaker’s referential communicative goal when interpreting the message \citep{Sedivy2003}. 

In contrast, the studies presented here test expectations for message content when an utterance is conveyed out of the blue, i.e. when content newsworthiness is more likely to be the goal rather than referential identification within a larger utterance. In this way, we aim to assess comprehenders’ expectations for informativity more generally: What content do comprehenders expect cooperative speakers to mention? The current study also departs from the reference resolution studies discussed above in how the effect of speaker identity is manipulated. Past work on reference has examined this variable by contrasting two types of speakers, those who are pragmatically reliable and those who are unreliable. While our Experiment 2 implements an analogous manipulation to this, contrasting an informative and an uninformative speaker, our Experiment 1 investigates a more fundamental contrast by comparing the presence vs. absence of a speaker in order to test whether comprehenders' awareness of the speaker can itself influence their expectations about content newsworthiness. 

There are current models of language use that, in line with the Gricean approach, take into account listeners’ expectations for speakers to make informative contributions. For example, the Rational Speech Act (RSA) framework models a speaker who makes production choices based on a listener who in turn is able to make inferences about the speaker and their communicative goals (see e.g. \cite{GoodmanFrank2016} for an overview). The RSA model captures the interplay between speakers and listeners and how they reason about each other's linguistic choices. It follows that the speaker plays a key role when a listener processes an utterance. This point is tested explicitly by \citet{KreissDegen2020}, who build an RSA model based on empirically elicited referring expressions from speakers to predict listeners' behaviour in a subsequent referential identification experiment with or without contrast objects present (e.g. another banana when the target is a yellow banana). They show that when listeners hear partial referring expressions (e.g. \textit{click on the yellow}) they make inferences about how likely a speaker is to use a modifier based not only on the presence of a contrast object, which is what other contrastive inference studies have consistently found, but also on the typicality of a competitor (e.g. a yellow strawberry). Their RSA model accurately predicts this listener behaviour. In other words, listeners identify the intended referent based not only on linguistic or cognitive factors related to the referents themselves, but also based on the listener’s reasoning about the speaker’s production probabilities. Note that again, this study is one where referential success is the goal of the communicative interaction. The current work is consistent with the principles of RSA, and adds to this framework by explicitly testing the salience of the speaker and its effect on comprehenders’ expectations for content. 
 
In sum, whereas a range of prior studies have emphasised the importance of (real-)world typicality, an emerging body of work aims to bring attention to the role informativity plays in processing. Our study adds to this informativity-driven account of processing, and shows that expectations for informativity also matter for the estimates comprehenders make regarding upcoming content. In anticipating what someone will say next, comprehenders may be tracking several distributions of probabilities about the speaker, such as the probability of particular situations that a speaker might encounter, the likelihood of choosing to formulate an utterance about that situation (as opposed to staying silent), and even the choice among available formulations for expressing that meaning. On this view, anticipating upcoming content must be understood to rely on (at least) two components: One concerns the situation and one concerns the speaker and how and whether they will filter the scene. We argue that in order to accurately capture the processes at play, there needs to be a role for informativity-driven reasoning about the speaker in models of language processing.
 
\section{Experiment 1: Awareness of speaker intentions in comprehension}
Our main thesis, in keeping with the findings discussed above, is that comprehenders can and do take into account speakers’ production preference to be informative during processing. This in turn means that certain contexts may emphasise the relevance of speakers’ production preferences because they highlight the presence of the speaker themselves. The prediction is that increasing the salience of an intentionally communicating speaker should lead to an increase in comprehenders’ expectations for informative content. Experiment 1 tests this claim using a Cloze task paradigm to elicit sentence completions as an indicator of what the participant expects a speaker would say. Specifically, we ask participants to complete statements about what is present at a particular location, e.g. a train station. A total of 20 different locations were used.\footnote{The locations are: Bakery, library, forest, living room, beach, petrol station, cinema, office, playground, mountain, bathroom, park, bedroom, post office, train station, garden, golf course, restaurant, hospital, and stationery shop.\label{fn:locations}} In a between-participants design we manipulate the salience of the speaker across four conditions. As shown in \figref{Figure1}, all conditions use the \textit{there’s \mbox{\_\_\_\_}} prompt. The \textsc{bare} condition only mentions the location (e.g. \textit{At the train station, there’s \mbox{\_\_\_\_}}); the \textsc{\textsc{third person}} condition describes someone at the location (\textit{They’re at the train station, and there’s \_\_\_\_}); the \textsc{first person} condition directly mentions the speaker (\textit{I’m at the train station, and there’s \mbox{\_\_\_\_}}); and the \textsc{visible speaker} condition uses first person and adds a photograph of a person speaking, with the text prompt being identical to the \textsc{first person} condition. The manipulation is intended to vary the perceived communicative intent of the context, such that our most speaker-salient condition, \textsc{visible speaker}, is the one where communication is most strongly foregrounded by emphasising the presence of a speaker who has uttered the sentence. The expectation is that such contexts will elicit more completions about the presence of non-typical or otherwise unexpected entities in the target location.

\begin{figure}
\includegraphics[width=0.90\textwidth]{7_Exp1-ExamplesTrials.png}
\caption{Example target sentences for the train station location in each of the four conditions}
\label{Figure1}
\end{figure}

\hspace*{-2.4pt}If speaker salience influences comprehenders’ guesses, the least speaker-salient condition is predicted to reveal comprehenders’ reliance on real-world knowledge\footnote{Barring any construction of a fictional world, we assume comprehenders tend to rely on their real-world knowledge for their typicality estimates.} of typical entities or situations that are likely to be present at the different locations, in keeping with prior comprehension studies demonstrating comprehenders' reliance on real-world knowledge (e.g. \textit{a train} for the train station location). We in turn expect an increase in informative completions across the four conditions, with the most informativity expected in the \textsc{visible speaker} condition (e.g. \textit{a celebrity}). The possibility that the less speaker-salient conditions will be distinguished from each other is predicted by work in embodied cognition showing that people are sensitive to the use of different pronouns when taking perspective; people adopt an actor's perspective when first person is used, and an external perspective with the third person \citep{BorghiCimatti2010}. Our goal in including a gradation of speaker salience conditions is to test whether -- and via what properties -- an emphasis on the speaker can be achieved. These manipulations themselves are less of interest than the possibility that, if a speaker-oriented perspective can be achieved, comprehenders' guesses of what that speaker will say next may favour utterances conveying more informative content. We will be using a set of measures to capture different senses of informativity (see measures and data analysis below). An increase in informativity would indicate that rather than simply invoking typicality when they anticipate upcoming content, comprehenders make use of their awareness of speakers’ production preferences and filter the possible options through the lens of the speaker -- in that way estimating what content a cooperative speaker would consider worth uttering.

\subsection{Methods}
\subsubsection{Participants}

400 participants with English as their first language, no known language impairments, location in the US or the UK, and of minimum 18 years of age (\textit{M} = 36.09, \textit{SD} = 11.73, range = 18–78), were recruited on Prolific (\url{https://www.prolific.com/}). We tested an equal number of participants in each of the four conditions, i.e. 100 participants per condition, each providing 20 data points. Participants who failed two or more attention checks or reported another language than English as their first language were excluded and more participants were recruited to reach the desired number per condition. Participants were paid £7.12/hour on average.\footnote{The details of the cost were established by Prolific based on the median time taken to complete the experiment per submission for each group.} In addition, 22 participants were recruited through the University of Edinburgh’s Experiment Volunteer Panel for a pre-test, used to assess the typicality of a variety of objects in the item locations (see measures). These participants were students who received course credits as compensation.

\subsubsection{Design}

Experiment 1 tests whether speaker salience influences comprehenders' expectations about sentence completions. In order to implement this speaker salience manipulation, each of the four conditions increasingly highlight the speaker, as exemplified in \figref{Figure1}. The \textsc{bare} condition only mentions the location; the \textsc{\textsc{third person}} condition invokes a speaker talking about someone at the location; the \textsc{first person} condition directly mentions the speaker; and the \textsc{visible speaker} condition uses first person and adds a photograph of a person speaking, with the sentence embedded in a speech bubble. This last condition is intended as the one where communicative intent and therefore awareness of speakers’ production preferences is most emphasised.

Participants in the \textsc{bare}, \textsc{\textsc{third person}} and \textsc{first person} conditions are not given any information about the communicative context, whereas participants in the \textsc{visible speaker} condition are told that each utterance is the beginning of a phone call where the speaker has called someone to tell them something. In other words, the speaker has not been prompted by a question and is speaking out of the blue.

\subsubsection{Materials}

The 20 target items each mentioned a different location (see Footnote \ref{fn:locations}). Locations were chosen to be places that would have an adequate number of typical and therefore expected objects present (e.g. trains, platforms, passengers, etc. for train station) but also allowed for the possibility of less typical entities being mentioned without raising doubts about the speaker's reliability.

Each target sentence followed the same format within a condition, mentioning a location in the first clause and providing a sentence-final blank space for the participant to fill. The target sentence templates for each condition were as follows: \textsc{bare}: \textit{At the [location], there’s \mbox{\_\_\_\_}}, \textsc{third person}: \textit{They’re at the [location], and there’s \mbox{\_\_\_\_}}, \textsc{first person} and \textsc{visible speaker}: \textit{I’m at the [location], and there’s \mbox{\_\_\_\_}} (all exemplified with \textit{train station} in \figref{Figure1}). The \textsc{visible speaker} condition additionally displayed the target sentence in a speech bubble alongside a picture of a person talking on the phone. A total of 20 different speaker images were used, 10 portraying female speakers and 10 portraying male speakers. Speaker images were allocated to specific items, but participants only saw each speaker for a single target item (e.g. the man in \figref{Figure1} is seen talking about the train station location). The speaker images were reused in the fillers, so that a given list in the \textsc{visible speaker} condition contained the same speaker image a total of three times. The experiment included 40 fillers in each condition. For \textsc{bare}, \textsc{third person} and \textsc{first person} conditions, fillers were adapted Cloze task items from \citet{AltarribaEtAl1996} with the blank space appearing either initially, in the middle or at the end of the sentence. For \textsc{visible speaker}, fillers were created that were more natural-sounding as conversation-initial utterances. Two fillers in each condition served as catch trials as they had only one or two likely completions, e.g. \textit{I’m going to the swimming \_\_\_\_ this afternoon} in \textsc{visible speaker} condition, where \textit{pool} is the expected completion. See OSF for a full list of materials for the \textsc{visible speaker} condition.\footnote{\url{https://osf.io/7h5qs/}}

\subsubsection{Procedure}

The experiment was presented using Qualtrics (\url{https://www.qualtrics.com/}). For \textsc{bare}, \textsc{third person} and \textsc{first person} conditions, participants were told they would read sentences where a part was missing and their task was to type in the word or words they thought should be in the sentence. We suggested an upper limit of 3 words in the instructions at the beginning of the experiment with the aim of discouraging participants from expanding the sentence beyond a single target noun phrase. For \textsc{visible speaker}, the instructions also emphasised the communicative intent of the sentences by telling participants that they were seeing someone who had just called up a friend and their task was to fill in the missing part with what they thought the speaker could have said. They were also instructed that what they saw was the first thing said in the conversation, to further highlight that these are unprompted utterances. Items were presented in fully randomised order. Participants were presented with one item at a time with a text field below the item stating \textit{Fill in the blank}, in which they typed in their completions. They then clicked an arrow at the bottom right of the screen to proceed to the next item. Any one participant saw only one condition, i.e. a total of 60 items (20 of which were target items). The median completion times in each condition were between 10 and 15 minutes.
 
\subsubsection{Measures and data analysis} 

Prior to data analyses, responses were standardised, i.e. manually cleaned to have matching case, punctuation and spelling, initial articles (\textit{a/an/the}) were removed, and any spelling differences were collapsed (e.g. \textit{doughnut/donut}). Where participants listed more than one response (e.g. \textit{doctors/nurses}), only the first item mentioned was included. For each response, the main noun was identified as well as any modification and negation used. A total of 10 continuations were non-meaningful and therefore excluded from analysis. The continuations included ranged from 1 word in length to 11 words, with a mean of 1.61. 
 
The pre-test was a non-communicative task in which a separate group of participants ($N=22$) were asked to list a minimum of 3 and maximum of 10 things they would be likely to find at each of the locations mentioned in the target sentences. The pre-test provides a measure of what is considered typical at the different locations. Main nouns were extracted from responses and collated into ranked lists, ranging from highly expected\slash common (e.g. \textit{train} for \textit{train station}) to less expected\slash common (e.g. \textit{bustle, musician, pigeon}). Each noun in a given location was then given a typicality score estimated as the proportion of pre-test participants who mentioned that noun (e.g. for the train station location, 0.91 and 0.00 for \textit{train} and \textit{delay}, respectively).
 
The informativity of participants’ completions is assessed with five measures. Each of these measures is intended to capture a different sense of how a continuation provided in the experiment may be informative: 
\begin{enumerate}
\item Variability of responses (mean entropy score per condition to compare consistency versus unpredictability of responses)
\item Kullback-Leibler (KL) divergence of responses (i.e. relative entropy of responses with \textsc{bare} condition as baseline)
\item Inclusion of modification (which may make otherwise typical content like a train more newsworthy, e.g. \textit{steam train})
\item Inclusion of negation to mark the absence of something (often something typical, e.g. \textit{no train})
\item Typicality of objects mentioned (compared to responses elicited in the pre-test; specifically, how many participants in the pre-test listed object X for location Y)
\end{enumerate}

Entropy is an information-theoretic measure for quantifying the variation in a distribution of outcomes. As such, entropy provides a measure of the average amount of information needed to represent an outcome drawn from a given distribution. In a condition where participants' sentence continuations are strongly biased to a few possible outcomes, the entropy for that distribution of outcomes will be low (i.e. a given outcome is, on average, not very informative as it is highly predictable). Conversely, in a condition with more unique outcomes or more variability across outcomes, the entropy for the distribution will be high (i.e. a given outcome is, on average, very informative as it is difficult to predict).

Entropy scores were calculated using participants' full text responses.\footnote{Where $x$ is a particular response provided by one or more participants (e.g. \textit{a train}), we computed entropy as follows for each location and each condition: $\text{Entropy} = \sum_{x} p(x) \log (p(x))$.} First, an entropy score was calculated for each location in each condition (e.g. for train station in \textsc{visible speaker} condition), from which we derived a mean entropy score per condition. By testing an equal number of participants (=100) per condition, we avoid a concern that the probability computations which contribute to the entropy scores would be distorted by unequal sample sizes. Such distortion could arise, for example, if one condition had fewer responses than another, such that the singleton responses (those produced only once) would be assigned higher probability in the small-sample-size condition than singleton responses in a condition with more responses.
 
KL divergence was also calculated using participants' full text responses. Relative entropy provides a measure of comparison between two probability distributions. Comparing the distribution of responses from  each condition relative to the \textsc{bare} condition can therefore be seen as a proxy for comparing comprehenders’ estimates of what objects or events they expect a speaker to mention versus what objects or events are expected to be present in a location. Relative entropy complements the entropy measure described above, which only characterises a single distribution rather than drawing a comparison between the response distribution between conditions. Also, using the entropy measure alone may risk missing potential differences between distributions; two distributions could have similar entropy but very different properties (e.g. the {\textsc{bare}} condition for the train station location might favour \textit{train} with the same probability that another condition favours \textit{delay}; in that case, if the remaining alternatives have similar probabilities, the two distributions would have similar entropy and any differences in distributions would remain undetected). Since all responses did not appear in all conditions, responses with a zero probability in {\textsc{bare}} condition (i.e. responses that were not provided by any of the participants in this condition but did occur in one or more of the other conditions) were given a +1 smoothing. We opted to calculate relative entropy over the top 4 most frequent responses and bin the remaining responses in \textit{Other}. This method avoids giving undue weight to any of the responses that were not observed in, for example, the \textsc{bare} condition (i.e. responses that were smoothed), which may only have a very low probability of being observed in, for example, the \textsc{visible speaker} condition. That is, the \textit{Other} bin ensures that no non-occurring response would be given the same probability as a low-probability response.

A speaker’s use of modification and negation in a continuation provides a way of adding more information. For the analyses of modification and negation, the presence of either element was manually coded as present (1) or not (0).
 
Typicality is about what is surprising and atypical for a given situation. For the typicality measure, the main noun in each response was checked against main nouns extracted in the pre-test data set and assigned a score based on the popularity rank of that response in the pre-test.

\subsection{Results}

As predicted, \textsc{visible speaker} yielded the most informative completions on all five measures, as shown in Figures \ref{Exp1KL} to \ref{Exp1Typ}. For an illustration of the variation in responses across the 4 conditions, see OSF for figures displaying the distribution of responses in the train station location.\footnote{\url{https://osf.io/7h5qs/}} 

The mean number of distinct responses per location in each of the 4 conditions were as follows: \textsc{visible speaker}: 70.5, \textsc{first person}: 47.1, \textsc{third person}: 49.0, and \textsc{bare}: 39.6. Entropy scores were analysed with paired Wilcoxon signed-rank tests across the items. Completions showed higher entropy for \textsc{visible speaker} (mean: 4.00), \textsc{first person} (mean: 3.14) and \textsc{third person} (mean: 3.23) conditions compared to the baseline \textsc{bare} (mean: 2.80; $p < 0.001$ for all three pairwise comparisons), and also the entropy of \textsc{visible speaker} condition was higher than that of \textsc{first person} ($p < 0.001$), as seen in the left panel of \figref{Exp1KL}. Although there is a numeric difference between \textsc{first person} and \textsc{third person} conditions in the opposite direction to the prediction, this difference was not significant ($p = 0.17$). However, the difference between \textsc{visible speaker} and the numerically adjacent \textsc{third person} is significant ($p < 0.001$). In order to test whether the difference in entropy was independent of use of modification or negation in participants' responses, we also conducted the analysis on the subset of responses not containing any modification and negation.\footnote{Thank you to an anonymous reviewer for suggesting this analysis. We note that while the subset analysis eliminates the matching number of responses across conditions, it eliminates more responses from the \textsc{visible speaker} condition than any other, where the most use of modification and negation was registered. If higher entropy were to emerge simply due to the presence of more singleton unique responses, the subset analysis reduces the chance of seeing such an increase in \textsc{visible speaker}. Hence, we do not believe this introduces a confound where analysing this subset would inadvertently yield higher entropy in the predicted \textsc{visible speaker} condition.} The above pattern is somewhat altered, with \textsc{visible speaker} still showing the highest entropy (mean: 2.70) but the remaining conditions showing a different descending order across \textsc{third person} (mean: 2.60), followed by \textsc{bare} (mean: 2.40), and then lowest entropy in \textsc{first person} condition (mean: 2.33). \textsc{visible speaker} differs signi\-ficantly from \textsc{bare} and \textsc{first person}, but only numerically from \textsc{third person}. There is a significant difference between \textsc{bare} and \textsc{first person}, but none between \textsc{bare} and \textsc{third person}.

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[height=.25\textheight]{7_Exp1-Entropy.png} % first figure itself
        %\caption{Mean entropy for each condition}
        %\\captionof{figure}{Mean entropy for each condition}
        \label{Exp1Entropy}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[height=.25\textheight]{7_Exp1-KL.png} % second figure itself
        \captionof{figure}{The left panel shows mean entropy for each condition; the right panel shows KL divergence as a comparison between the distribution in the listed condition and that of the baseline \textsc{bare} condition.}
        \label{Exp1KL}
    \end{minipage}
\end{figure}

\begin{figure}%[htb]
    \begin{floatrow}
    \ffigbox
    {\includegraphics[width=\linewidth]{7_Exp1-Modification.png}}
    {\caption{Proportion of use of modification across conditions}\label{Exp1Mod}}
    \ffigbox
    {\includegraphics[width=\linewidth]{7_Exp1-Negation.png}}
    {\caption{Proportion of use of negation across conditions}\label{Exp1Neg}}
    \end{floatrow}
\end{figure}

\begin{figure}%[htb]
    \includegraphics[width=0.5\textwidth]{7_Exp1-Typicality.png}
    \caption{Mean typicality score of responses across conditions}\label{Exp1Typ}
\end{figure}

The results for relative entropy confirm a role for speaker salience, as seen in the right panel of \figref{Exp1KL}\footnote{See OSF for examples of a list of the binned responses for each of the locations across conditions.}: Across items, the KL divergence between \textsc{bare} and \textsc{visible speaker} conditions (mean: 0.74) is greater than that between \textsc{bare} and \textsc{first person} (0.10, paired $t$-test: $p > 0.001$) or between \textsc{bare} and \textsc{third person} (0.13, $p > 0.001$). The comparisons between \textsc{bare} and \textsc{first person} and between \textsc{bare} and \textsc{third person} did not show a significant difference. As with entropy above, we also analysed the subset of responses that did not contain any modification or negation. In this case, the gradation of results stays the same: The KL divergence for \textsc{bare} and \textsc{visible speaker} (mean: 0.44) is higher than \textsc{bare} and \textsc{first person} (0.08, $p > 0.001$), as well as than \textsc{bare} and \textsc{third person} (0.12, $p > 0.001$). However, we also see a significant difference in the comparison between \textsc{bare} and \textsc{first person} and between \textsc{bare} and \textsc{third person} ($p > 0.05$). In sum, the distribution of participants’ completions in \textsc{visible speaker} condition is the one that is most clearly different from the distribution in \textsc{bare} condition.
 
The binary outcomes of modification and negation were analysed with generalised mixed effects models\footnote{modification\sim condition + (1 + condition | subjectID) + (1 + condition | location)\newline negation\sim  condition + (1 + condition | subjectID) + (1 + condition | location)} \citep[GLMM:][]{Jaeger2008} using the lme4 package \citep{BatesWalker2015} in R \citep{RCoreTeam}, treating condition as a fixed effect, with random slopes and intercepts of condition for participants and locations \citep{BarrTily2013}. In order to test whether we observe the predicted gradual increase in use of modification and negation across conditions, we forward coded the condition prior to analysis. This allows us to compare each condition to the adjacent one: \textsc{bare} vs. \textsc{third person}, \textsc{third person} vs. \textsc{first person}, and \textsc{first person} vs. \textsc{visible speaker}. To achieve convergence for the modification and negation models with maximal random effects structure, we used the optimiser \texttt{optimx} with the method \texttt{bobyqa} which led to convergence. For each fixed effect, we report a $p$-value as generated by the lmer logistic regression which is based on the Wald Z statistic \citep{Agresti2003}. As seen in \figref{Exp1Mod}, modification rates were highest in \textsc{visible speaker} condition. \textsc{visible speaker} was significantly higher than \textsc{first person} ($\beta = 2.81, \text{SE} = 0.38, z = 7.46, p < 0.001$), and \textsc{third person} was higher than \textsc{bare} ($\beta = 2.46, \text{SE} = 0.47, z = 5.26, p < 0.001$). Although there is a difference between \textsc{first person} and \textsc{third person} in the predicted direction, this difference was not significant ($\beta = -0.13, \text{SE} = 0.41, z = -0.32, p = 0.75$). For negation (\figref{Exp1Neg}), \textsc{visible speaker} shows significantly higher proportions than \textsc{first person} ($\beta = 2.63, \text{SE} = 0.29, z = 9.01, p < 0.001$) and \textsc{first person} is higher than \textsc{third person} ($\beta = 3.78, \text{SE} = 0.85, z = 4.44, p < 0.001$); however, although numerically the predicted pattern is present for \textsc{third person} compared to \textsc{bare}, this difference was not significant ($\beta = 11.18, \text{SE} = 7.60, z = 1.47, p = 0.142$).\footnote{Using \texttt{bobyqa} produced a warning, but did not stop the models from converging. If we simplified the models by removing random slopes of condition for participant and for location, we achieve convergence without \texttt{bobyqa} and encountering no warnings. For modification the results stay the same (\textsc{visible speaker} higher than \textsc{first person} ($\beta = 2.56, \text{SE} = 0.35, z = 7.29,\allowbreak p < 0.001$), no difference between \textsc{first person} and \textsc{third person} ($\beta = 0.19, \text{SE} = 0.36, z = 0.52,\allowbreak p < 0.60$), and \textsc{third person} higher than \textsc{bare} ($\beta = 1.67, \text{SE} = 0.38, z = 4.44, p < 0.001$)), for negation we see significance in all three comparisons (\textsc{visible speaker} higher than \textsc{first person} ($\beta = 2.08, \text{SE} = 0.30, z = 7.03, p < 0.001$), \textsc{first person} higher than \textsc{third person} ($\beta = 0.93, \text{SE} = 0.35, z = 2.66, p < 0.01$) and \textsc{third person} higher than \textsc{bare} ($\beta = 2.54,\allowbreak \text{SE} = 0.72, z = 3.53, p  < 0.001$)).}
 
For the measure of typicality, we calculated a score for each main noun based on the number of pre-test participants who mentioned that object for the relevant location (meaning that a response given in Experiment 1 that was not provided in the pre-test receives a score of 0). Results are shown in \figref{Exp1Typ}. To analyse these typicality scores we again forward coded the condition (comparing each condition to the adjacent one; \textsc{bare} vs. \textsc{third person}, \textsc{third person} vs. \textsc{first person}, \textsc{first person} vs. \textsc{visible speaker})\footnote{Since the observed means do not pattern with the predicted order, we conducted an additional analysis setting the forward contrast coding to order the conditions to match the observed order; \textsc{bare} vs. \textsc{first person}, \textsc{first person} vs. \textsc{third person}, \textsc{third person} vs. \textsc{visible speaker} (as seen in \figref{Exp1Typ}). The result remains the same.} and used a linear mixed effects model\footnote{typicality\sim condition + (1 + condition | subjectID) + (1 + condition | location)} with condition as a fixed effect and random slopes and intercepts of condition for participants and locations \citep{BarrTily2013}, as above. The significance of the fixed effect of condition was determined via a likelihood ratio test comparing the fit of the model to one with the same random effects structure but no fixed effect. Again, to achieve convergence with the models with maximal random effects structure, we used the optimiser \texttt{optimx} with the method \texttt{bobyqa}. We see a main effect of condition in the model comparison ($p < 0.001$), showing that the model with condition as a fixed effect significantly improved the model fit. Typicality was significantly lower in \textsc{visible speaker} ($\beta = -0.25, \text{SE} = 0.03, t = -7.93$) compared to \textsc{first person}, and \textsc{third person} was lower than \textsc{bare} ($\beta = -0.09, \text{SE} = 0.03, t = -2.86$). The difference between \textsc{first person} and \textsc{third person} was not significant ($\beta = 0.03, \text{SE} = 0.03, t = 1.20$).

\subsection{Discussion}

The results of Experiment 1 show that participants' expectations about informativity, as elicited by an utterance completion task, vary according to how and whether the speaker is made salient. In the conditions which were intended to make the speaker more salient, participants’ completions score higher in informativity as shown by the five measures: Higher entropy, greater KL divergence for \textsc{visible speaker} condition compared to the \textsc{bare} condition, more use of modification and of negation, and fewer typical entities provided in the \textsc{visible speaker} condition compared to the other conditions. We take this increase in informativity to reflect an increased awareness of the speaker, inducing participants to consider the speaker's production preferences when estimating possible utterance completions. 

We find only partial support for the hypothesis that continuations gradually increase in informativity across the four conditions: The use of modification and negation follows the expected pattern numerically with a gradual increase from the least to most speaker-salient conditions, but there is no significant difference between \textsc{third person} and \textsc{first person} conditions. For entropy, KL divergence and typicality, the pattern is somewhat less clear: The results pattern as predicted except for in \textsc{third person} and \textsc{first person} conditions, which are mirrored numerically compared to the predicted direction. However, these differences are also not significant. Overall, we interpret this to show that the manipulation generally works in the intended way; it affects a participant's probability of using the second estimate discussed above, i.e. one that takes into account a speaker's production preference to convey informative content. The gradual pattern observed in the data might thus reflect individuals' varying sensitivity to the manipulation: The \textsc{visible speaker} condition most strongly cues participants to consider the speaker's production process, but evidently some participants seem to already be engaging their awareness of the speaker in the less speaker-salient conditions. The manipulation seems to affect the measures differently, suggesting that the mechanism for comprehenders to change perspectives in this way may be more complex than our experimental design can account for. Nevertheless, it seems that whatever the process may be that generates candidate expressions when anticipating content in a sentence completion task, this process is substantially modulated by comprehenders' awareness of the speaker.

\section{Experiment 2: Awareness of speaker style in comprehension}\largerpage

Having established in Experiment 1 that comprehenders appear to use their awareness of speakers' production biases in generating expectations about what a speaker will say next, Experiment 2 tests whether such expectations also reflect properties of the speaker.\footnote{This experiment was preregistered: \url{https://osf.io/r2h7d}} We ask whether comprehenders have fixed expectations for speakers to convey newsworthy content, or whether such expectations are malleable. This study is different from the studies on referential expectations discussed in the introduction, where the communicative goal is identifying the intended referent. Experiment 2 instead aims to test speaker-specific expectations for informativity regarding upcoming message content where the communicative goal is unspecified and likely to be communicative interest. For example, comprehenders may estimate that different speakers have different thresholds for what counts as an informative contribution. Comprehenders may in turn allow for variation in the way a speaker delivers newsworthy content depending on what they know about the speaker. 

Studies on speaker adaptation in a range of linguistic domains show that comprehenders use cues from a speaker’s previous discourse when anticipating upcoming content and when interpreting it. For example, a speaker’s accent or dialect can lead comprehenders to shift their perception of phonemes \citep{HayEtAl2006,HayDrager2010}; when encountering temporarily ambiguous words comprehenders make anticipatory looks to a more frequently mentioned referent based on the speaker’s previous behaviour \citep{CreelTanenhaus2008}; in the early stages of processing, comprehenders are influenced by a speaker’s either literal or non-literal style when disambiguating metaphorical polysemous words \citep{DaviesEtAl2022}; and comprehenders adapt their interpretations of scalar quantifiers such as \textit{some} and \textit{many} following exposure to different speakers \citep{YildirimJaeger2016}. There are also formalised accounts of, e.g. phonetic adaptation \citep{KleinschmidtJaeger2015} and semantic/pragmatic adaptation \citep{SchusterDegen2020} which capture comprehenders' ability to track subtle characteristics of different speakers and adjust their interpretations accordingly. 
 
Using the \textsc{visible speaker} condition from Experiment 1, our second experiment tests whether speaker-specific effects are observable in comprehenders’ expectations for the informativity of upcoming content. Two different speakers with different communication styles were introduced in an exposure phase: One who produces a mix of informative and uninformative utterances and one who produces only informative utterances. Following the exposure phase, participants completed sentences purportedly uttered by these two speakers. If comprehension reflects speaker-specific expectations for informativity, comprehenders are predicted to provide more informative completions for the \textsc{high-informativ\-ity} speaker compared to the \textsc{low-informativity} speaker, showing that they can modify their expectation about what someone is going to say next after learning about the specific production preferences of the individual speakers. Such a finding would indicate that in anticipating upcoming content, we are able to use awareness of production dynamically as we get to know the communication style of particular speakers.

\subsection{Methods}
\subsubsection{Participants}

200 participants were recruited on Prolific with the same criteria as in Experiment 1: English as first language, no known language impairments, location in the US or the UK, and minimum 18 years of age (\textit{M} = 36.07, \textit{SD} = 14.20, range = 18–76). Additionally, people who had taken part in Experiment 1 were blocked from participating in Experiment 2. As in Experiment 1, participants who failed two or more attention checks or reported another language than English as their first language were excluded and replaced by other participants to reach the desired number of 100 participants per condition. Participants were paid £7.60/hour on average.\footnote{The details of the cost were established by Prolific based on the median time taken to complete the experiment per submission for each group.}

\begin{figure}%[h!]
\includegraphics[width=0.49\textwidth]{7_Exp2-ExposureSuzy1.png}
\hfill
\includegraphics[width=0.49\textwidth]{7_Exp2-ExposureSuzy2.png}
\caption{Example of a filler item for the \textsc{low-informativity} speaker in exposure phase. All items in exposure consisted of two parts, shown here in two boxes. Image from Freepik (\url{https://freepik.com}).}\label{ExposureSuzy}
\end{figure}

\begin{figure}%[h!]
\includegraphics[width=0.49\textwidth]{7_Exp2-ExposureAnna1.png}
\hfill
\includegraphics[width=0.49\textwidth]{7_Exp2-ExposureAnna2.png}
\caption{Example of a critical item for the \textsc{high-informativity} speaker in exposure phase. All items in exposure consisted of two parts, shown here in two boxes. Image from Pixabay (\url{https://pixabay.com}).}\label{ExposureAnna}
\end{figure}

\subsubsection{Design}

Experiment 2 tests whether comprehenders adapt their expectations for informativity based on the speaking style of two different speakers: One who produces a mix of informative and uninformative utterances and one who produces informative utterances only. The experiment has two conditions (\textsc{high-informativity} vs. \textsc{low-informativity} speaker style), manipulated within participants. The task consisted of two phases; an exposure phase and a test phase. In the exposure phase, participants were familiarised with two different speakers, Suzy and Anna, and their differing communication styles (see an example for each of the speakers in Figures \ref{ExposureSuzy} and \ref{ExposureAnna}). The test phase is a near replication of the \textsc{visible speaker} condition in Experiment 1 in which participants write completions for the two different speakers (examples in Figures \ref{TestSuzy} and \ref{TestAnna}). We designed this paradigm to encourage participants to feel like they were engaged in a series of phone calls. In the exposure phase, the participant's task is to type in utterances that contribute their own turns in the dialogue. In the test phase, on the other hand, they are instructed to fill in parts of the caller's utterances. The exposure phase thus ensures that they witness the kind of utterances produced by the two callers, whereas the test phase asks them to indicate what they think each of the callers would be likely to say. 

\begin{figure}%[h!]
\includegraphics[width=0.95\textwidth]{7_Exp2TestSuzy.png}
\caption{Example of a critical item for the \textsc{low-informativity} speaker in test phase}\label{TestSuzy}
\end{figure}
\begin{figure}%[h!]
\includegraphics[width=0.95\textwidth]{7_Exp2TestAnna.png}
\caption{Example of a critical item for the \textsc{high-informativity} speaker in test phase}\label{TestAnna}
\end{figure}

Completions for the \textsc{high-informativity} speaker are expected to be more informative than those for the \textsc{low-informativity} speaker. Such an observed difference in the measures between \textsc{low-informativity} speaker condition and \textsc{high-informativity} speaker condition would reflect a change in participants’ evaluation of what counts as informative for each of the speakers, in line with our thesis that comprehenders are able to dynamically use their knowledge of speakers’ communicative styles in their estimates of likely upcoming content.

\subsubsection{Materials}

Materials were constructed to depict a series of beginnings of phone calls, with a picture of the speaker (the person calling), and speech bubbles showing different turns of the conversation. The critical items (training items in the exposure phase and test items in the test phase) are in the form of \textit{I’m at the [location], and there’s \_\_\_\_}, as in Experiment 1, using the same 20 locations split across the two phases.

In the exposure phase, participants see the start of a phone call and a contribution from either the \textsc{high-informativity} speaker or the \textsc{low-informativity} speaker. For the critical items in the exposure phase, the \textsc{high-informativity} speaker only utters training sentences about non-typical situations (e.g. \textit{I’m at the golf course, and there’s a celebrity here.}), whereas the \textsc{low-informativity} speaker utters some training sentences about non-typical situations as well as some about typical situations (e.g. \textit{I’m at the cinema, and there’s popcorn.}). For the critical training items in the exposure phase, the speakers’ utterances are taken from responses in Experiment 1: The \textsc{high-informativity} speaker has 2 critical training items, both using completions from the \textsc{visible speaker} condition, whereas the \textsc{low-informativity} speaker has 8 critical training items, 2 with completions from the \textsc{visible speaker} condition and 6 from the \textsc{bare} condition. Note that this setup ensures that both speakers provide the same number of interesting contributions overall, to avoid participants inferring that one speaker encounters more non-typical situations than the other. The different communication styles of the speakers is then emphasised by the fact that the \textsc{low-informativity} speaker additionally reports on 6 typical situations. In addition to the critical training items, the exposure phase includes 18 filler items; 6 for the \textsc{high-informativity} speaker and 12 for the \textsc{low-informativity} speaker. This setup means that overall the participant answers calls from the \textsc{low-informativity} speaker noticeably more often than the \textsc{high-informativity} speaker (20 vs. 8 items). Filler items were either utterances requesting information or conveying information. These were constructed to match the communication style of the speakers, such that the fillers for the \textsc{high-informativity} speaker targeted newsworthy information (e.g. \textit{Do you know when Lisa is arriving next week?}), whereas the \textsc{low-informativity} speaker called about mundane events (e.g. \textit{What did you have for dinner?}). Across the critical and filler items in the exposure phase, participants were encouraged to engage in the task and to see the setup as a communicative context, by requiring them to provide either a starting utterance for the conversations or a response to the speaker’s utterance.
 
All the items in the exposure phase consisted of two images (see Figures~\ref{ExposureSuzy} and~\ref{ExposureAnna}). The first shows a calendar with a day and a time next to a hand holding a ringing phone with the caller’s name clearly visible. The day and time was included to reinforce the manipulation that the \textsc{low-informativity} speaker calls more often; they call every day and occasionally several times a day, whereas the \textsc{high-informativity} speaker calls a maximum of once a day and not every day. The second image shows the speaker and a series of speech bubbles indicating the first, second and third turn of the dialogue. The speaker’s speech bubble, which is always the second turn, is always fully visible in order to expose the participant to each speaker’s communicative style, whereas the speech bubbles for the callee (the participant) contain fill-in-the-blank spaces in either the first or the third turn.
 
For the test phase, the speakers’ utterances include a fill-in-the-blank space to elicit participants’ guesses about what each of the speakers is likely to say. All items in the test phase were similar to those in the \textsc{visible speaker} condition of Experiment 1, showing the speaker and a speech bubble with an utterance that includes a blank; here in Experiment 2, the test items additionally depict a ringing phone and the callee’s greeting in a speech bubble appearing to one side (see examples Figures \ref{TestSuzy} and \ref{TestAnna}). A participant sees an equal number of test items from each speaker (5 from the \textsc{low-informativity} speaker and 5 from the \textsc{high-informativity} speaker), plus 10 filler items from each speaker as well. Filler items were the same as in Experiment 1, including the two that served as attention checks.
 
Critical training and test items (i.e. the 20 locations from Experiment 1) were counterbalanced across lists such that a location that appeared in the exposure phase in one list appeared as a test item in another list. Similarly, participants saw the \textsc{high-informativity} (HI) and \textsc{low-informativity} (LI) speaker depicted as one of the two speaker images, counterbalanced across participants, to avoid any potential bias associated with the appearance of the speakers. Counterbalancing items and pictures of speakers resulted in 4 lists:
\begin{itemize}
    \item LI speaker: dark-haired, HI speaker: blonde + exposure items: target set A
    \item LI speaker: blonde, HI speaker: dark-haired + exposure items: target set A
    \item LI speaker: dark-haired, HI speaker: blonde + exposure items: target set B
    \item LI speaker: blonde, HI speaker: dark-haired + exposure items: target set B
\end{itemize}

Under this counterbalancing, a given item in the exposure phase was always associated with the same speaker style (e.g. \textit{train station} was always uttered by the \textsc{low-informativity} speaker in exposure), and likewise a given item in the test phase was always associated with the same speaker style (e.g. \textit{restaurant} was always uttered by the \textsc{high-informativity} speaker in test).

\subsubsection{Procedure}

The experiment was presented using Qualtrics. In the exposure phase, participants were instructed to imagine answering calls from each of two speakers, filling in an empty speech bubble from the participant’s side of the conversation in each call. Each trial consisted of two parts (example trials in Figures~\ref{ExposureSuzy} and~\ref{ExposureAnna}). First, participants see a day and time and a phone showing the caller's name; second, they see the caller talking into a phone and the dialogue in speech bubbles, and a text box for filling in the missing content. The items varied between the participant having to complete the very first utterance (the greeting) or the response to the caller’s first utterance. This variation was intended to ensure participants stayed engaged and read the utterances fully, as well as to distract them from the potentially unnatural sounding utterances from the caller by allowing them to influence the dialogue by contributing their own turns. Items in the exposure phase were presented in a fixed order, so that every participant within a list saw the same item order. As in Experiment 1, participants were presented with one item at a time with a text field below the item where they typed their completions, and they proceeded to the next trial by clicking the arrow on screen.
 
In the test phase, participants were again instructed that they would be seeing the beginnings of a series of phone calls from the same two speakers, but this time they would be completing the callers’ utterances. The test phase was essentially a replication of the \textsc{visible speaker} condition in Experiment 1, with the only difference being the edits explained above. The test phase used simpler visualisations of the phone calls compared to the exposure phase, as shown in Figures \ref{ExposureSuzy} and \ref{ExposureAnna} versus \ref{TestSuzy} and \ref{TestAnna}; this simplification was intended to make the second phase of the experiment less tiresome for the participant by reducing the number of images they had to click through. Presentation of items in the exposure phase was fully randomised.

After the test phase, there were three questions intended to assess whether participants had paid attention and were sensitive to the different speaker styles. The first of these was a speaker line-up where participants had to choose which of five speakers they had talked to. The speaker line-up consisted of the images of the two speakers flipped horizontally, and three images of other speakers taken from Experiment 1. The second question asked if the participant noticed a difference between the two speakers, and if so, what that difference was. The third asked them to rate how interesting they found each of the two speakers on a scale from 1 (not at all interesting) to 7 (very interesting). The median completion times in each list were between 17 and 19 minutes.

\subsubsection{Measures and data analysis}

For Experiment 2, we used the same coding procedure and the same measures as in Experiment 1: Entropy as a measure of variability in participant responses, KL divergence to measure how the distribution of responses in the two conditions here compares to that of a baseline condition (the \textsc{bare} condition from Experiment 1), modification and negation as measures of participants’ enhancement of their responses, and typicality of the responses’ main noun as compared to the nouns elicited in the Experiment 1 pre-test. A total of 2 continuations were non-meaningful and therefore excluded from analysis. The included continuations ranged from 1 word in length to 6 words, with a mean of 1.55.

\begin{figure}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[height=.25\textheight]{7_Exp2-entropy.png} % first figure itself
        %\caption{Mean entropy for each condition}
        \label{Exp2Entropy}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[height=.25\textheight]{7_Exp2-KL.png} % second figure itself
        \caption{The left panel shows mean entropy for each speaker informativity condition; the right panel shows KL divergence as a comparison between the distribution in the listed condition and that of the baseline \textsc{bare} condition.}
        \label{Exp2KL}
    \end{minipage}
\end{figure}

\subsection{Results}

The mean number of distinct responses in the \textsc{high-informativity} condition was 58.0, compared to 44.7 in the \textsc{low-informativity} condition. As predicted for the entropy measure, the \textsc{high-informativity} speaker yielded a distribution of responses with higher entropy (mean: 3.60) compared to the \textsc{low-informativity} speaker (mean: 3.00; Wilcoxon signed-rank test: $p < 0.005$), as seen in the left panel of \figref{Exp2KL}. For KL divergence, there was a numeric difference between \textsc{high-informativity} vs. \textsc{bare} (1.37) and \textsc{low-informativity} vs. \textsc{bare} (1.23), but the difference was not significant (right panel, \figref{Exp2KL}). As in Experiment 1, we also calculated the entropy of the subset of responses not containing any modification or negation. For entropy, the pattern still holds, with \textsc{high-informativity} (mean: 2.80) being significantly higher than \textsc{low-informativity} (mean: 2.20, $p < 0.05$). For KL divergence, the pattern above remained numeric only (\textsc{high-informativity} vs. \textsc{bare} (1.14) and \textsc{low-informativity} vs. \textsc{bare} (1.10)).
 
As in Experiment 1, the binary variables of modification and negation were analysed with generalised mixed effects models treating condition (here, \textsc{high-informativity} vs. \textsc{low-informativity}) as a fixed effect and with random slopes and intercepts of condition for participants and items. For the model for modification, we again used the optimiser \texttt{optimx} with the method \texttt{bobyqa} to achieve convergence with the full random effects structure. As predicted and as can be seen in \figref{Exp2Mod}, modification was used more frequently in completions of utterances from the \textsc{high-informativity} speaker compared to the \textsc{low-informativity} speaker ($\beta = -0.88$, $\text{SE} = 0.19$, $z = -4.57$, $p < 0.001$). The model for negation converged with full random effects structure and the default optimiser; however, although numerically the predicted pattern is observable (see \figref{Exp2Neg}), the difference between \textsc{high-informativity}/\textsc{low-informativity} speakers was not significant ($\beta = -0.58$, $\text{SE} = 0.57$, $z = -1.02$, $p = 0.306$).
 
Typicality was analysed with the same approach as in Experiment 1, and again the predicted pattern was confirmed (\figref{Exp2Typ}). We constructed a linear mixed effects model with condition (\textsc{high-informativity} vs. \textsc{low-informativity}) as a fixed effect and random slopes and intercepts of condition for participants and items, with the optimiser \texttt{optimx} combined with the method \texttt{bobyqa} to achieve convergence. The model confirms that typical main nouns were used less often for the \textsc{high-informativity} speaker compared to \textsc{low-informativity} speaker ($\beta = -0.13$, $\text{SE} = 0.06$, $t = -2.32$). To derive a $p$-value, we used model comparison via a likelihood ratio test and found a main effect ($p < 0.05$), showing that the model with condition as a fixed effect provides a significantly better fit than the model with same random effects structure but no fixed effect.

\begin{figure}
    \begin{floatrow}
    \ffigbox[.3\textwidth]
    {\includegraphics[width=.3\textwidth]{7_Exp2-Modification.png}}
    {\caption{Proportion of use of modification across conditions}\label{Exp2Mod}}
    \ffigbox[.3\textwidth]
    {\includegraphics[width=.3\textwidth]{7_Exp2-Negation.png}}
    {\caption{Proportion of use of negation across conditions}\label{Exp2Neg}}
    \ffigbox[.3\textwidth]
    {\includegraphics[width=.3\textwidth]{7_Exp2-Typicality.png}}
    {\caption{Mean typicality score of responses across conditions}\label{Exp2Typ}}
    \end{floatrow}
\end{figure}

\subsection{Discussion}

In line with the claim that comprehension reflects speaker-specific expectations for informativity, the results of Experiment 2 show that participants’ completions were higher in informativity for the \textsc{high-informativity} speaker compared to the \textsc{low-informativity} speaker. The \textsc{high-informativity} speaker completions were more variable as shown by the higher entropy, and elicited more modification and lower typicality, showing that participants adapt their expectations to the individual speakers’ communication styles as predicted. For KL divergence, there was a numeric difference in the predicted direction, but this was not significant. The results for the rate of negation were numerically in line with the prediction that the \textsc{high-informativity} speaker is more likely to comment on the \textit{lack} of something typical compared to the \textsc{low-informativity} speaker, but this effect was also not significant. This may reflect the fact that none of the critical items in the exposure phase explicitly demonstrate how the two speakers use negation, whereas some of both filler and critical items in the exposure phase did use modification.

\section{General discussion}

The goal of this study was to investigate the effect of comprehenders’ awareness of a speaker’s production preferences and communication style on their anticipation of upcoming content. Previous work on comprehension and production tends to show contradicting preferences for comprehenders and speakers, such that typicality is favoured in comprehension (e.g. being associated with processing ease), whereas speakers tend to make production choices that favour the inclusion of surprising or otherwise informative content (meaning easily inferable or typical content is disfavoured). However, we posited that comprehenders are aware of and make use of speakers’ production preferences in their expectations for upcoming content, such that they have a bias towards filtered language use rather than a simple expectation of transparent language use. The two Cloze task studies presented here tested this by tapping into the comprehender’s expectations, when the comprehender does not have access to the scene being described.
 
Experiment 1 addressed the question of whether increasing the emphasis on the speaker would lead to an increased expectation for informative content. The higher entropy of completions in the \textsc{visible speaker} condition and also \textsc{first person} and \textsc{third person} compared to \textsc{bare}, show that participants’ completions were more variable and thus less predictable in the conditions where the speaker was more salient. Furthermore, KL divergence showed that the distribution of participants’ completions in \textsc{visible speaker} condition was the most clearly different from the distribution in \textsc{bare} condition. We take this as evidence that participants are tuning in to speakers’ production preferences and expecting speakers to tell them interesting things by providing more unique completions in these conditions compared to the \textsc{bare} condition. For example, while \textit{train} was a common completion for the train station location in the \textsc{bare} condition, it was much less frequent in the \textsc{visible speaker} condition, suggesting that participants are taking into account what a speaker might consider worth talking about when they provide their completions.
 
Our predictions were upheld for the other three measures. Rather than looking at the overall variability in participants’ completions, these measures aim to capture properties of the content of individual completions. Modification and negation are taken to be strategies that participants can employ to make something that might otherwise be considered typical for a location more non-typical and therefore worthy of reporting. Participants made use of modification such as descriptive adjectives (e.g. \textit{a cute dog}), quantifying expressions (e.g. to communicate unusual amounts of an entity: \textit{so much choice}) or modification to specify a less typical subtype of an entity (e.g. \textit{steam train}). Similarly, negation can be a way to mark the absence of something that one would typically expect to find at a location, such as a train at the train station, and reporting this absence (e.g. \textit{no trains}) would therefore likely be considered informative. As for typicality, the results showed that participants produced completions mentioning typical entities most often in the \textsc{bare} condition, and significantly less often in the \textsc{visible speaker} condition. These measures and the progressive differences across conditions are taken to reflect comprehenders’ sensitivity to the experimental manipulation of speaker salience and, more broadly, to reveal their expectation for speakers to convey non-typical and informative content.

These findings have potential repercussions for the use of Cloze tasks in other psycholinguistic research since the kinds of completions that participants provide are evidently malleable. Given the differences in response distributions that we observed across conditions, researchers using Cloze tasks may need to tune their experimental tasks to their precise research goals. This might require using standard Cloze task phrasing and procedures to elicit completions that reflect participants' estimates of what is typical in the world or using more situated tasks that depict a communicative context in order to elicit completions that reflect participants' estimates of what a speaker is likely to talk about.

Experiment 2 addressed the question of whether this expectation is further malleable depending on properties of the speaker, specifically whether comprehenders are sensitive to different communication styles. Using completions provided in Experiment 1, participants were exposed to two speakers whose utterances were either high or low in informativity. Similar to the speaker salience manipulation in Experiment 1, the manipulation of speaker properties in Experiment 2 allows us to test whether comprehenders take into account properties of the speaker in guessing upcoming content (transparent language use vs. filtered language use). Participants showed a bias to expect the \textsc{high-informativity} speaker to produce utterances conveying more informative content than the \textsc{low-informativity} speaker. Entropy was higher for the \textsc{high-informativity} speaker, showing more variability in completions. Although the KL divergence measure did not show a significant difference between the two conditions (when the distribution of responses in each speaker condition is compared to that of the {\textsc{bare}} condition in Experiment 1), there was a numeric difference in the predicted direction. Modification was higher for the \textsc{high-informativity} speaker, and although not significant, we also saw an increase in use of negation with the \textsc{high-informativity} speaker. Lastly, mention of typical entities was more frequent for the \textsc{low-informativity} speaker. We take this to show that comprehenders are able to estimate that different speakers have different thresholds for what counts as an informative contribution and adjust their expectations accordingly.

A challenge when discussing how we anticipate upcoming content is to clearly distinguish the relevant concepts involved. Typicality, plausibility and (im)possi-bility all seem interwoven, and although they all likely play a role in anticipating what someone is going to say next, we have focussed on typicality in this study. We have defined typicality as the frequency of an event or situation; the frequency with which something occurs is conceptually and empirically relatively straightforward to measure. As we observed in this study, participants expect utterances to convey less typical content when the speaker is emphasised. Importantly, however, no participant seemingly contributed responses that were so extremely non-typical that they crossed the threshold into being impossible (e.g. a steam train in the kitchen). One could imagine scenarios where utterances about highly non-typical situations would be felicitous \citep[e.g. in descriptions of fictional or dream worlds;][]{FoyGerrig2014, TroyerKutas2020}, and perhaps even the most expected in some contexts, but there seems to be an intuition that utterances should stay within the realm of what is plausible. 

One way of conceiving of the interplay between typicality and plausibility could be that the latter provides a range within which situations can happen, whereas whether or not those situations are typical will depend on their frequency. For instance, taking a sip from a transmitter is implausible; drinking from a waterfall, however, certainly is plausible, but significantly less typical than drinking from a tap. Since continuations in our current study all seemed to fall within the range of plausible utterances, our data cannot speak to the role of plausibility in processing and anticipating upcoming content. There is work attempting to tease apart the effects of impossibility and implausibility on processing \citep[see e.g.][]{WarrenMcConnell2007} and other work that makes a distinction between real-world plausibility and word-predictability \citep[e.g.][]{AlbuEtAl2023}. As future work looks at more fine-grained processing in relation to anticipation of upcoming content and speaker salience, it could become important to make clear distinctions between all these concepts; typicality, plausibility and possibility. 

As discussed above, previous work has emphasised the importance of typicality in processing \citep[e.g.][]{MarksMiller1964, KutasHillyard1980, Morris1994, KamideHaywood2003}. When looking at the psycholinguistic literature, one might think that the default approach is to see sentences describing implausible or non-typical situations as anomalies that need to be reparsed \citep[see e.g.][]{CaiPickering2022}. However, such work sidesteps the role of pragmatic reasoning, failing to incorporate a comprehender’s awareness of the possibility that such an utterance has been produced precisely because its content is surprising, in particular in contexts where the communicative goal of the interaction is unspecified. Of course, it is reasonable to assume that most language users will expect a contribution to stay within a certain range of plausibility – if a contribution is too implausible, one would expect processing to suffer because it may be hard for comprehenders to reconcile what the speaker has said with what they know about the world. However, the results presented here suggest that comprehenders do have expectations that speakers will talk about interesting and non-typical things, highlighting a role for informativity-driven reasoning about the speaker in models of language processing.

  
%\section*{Abbreviations}
%\begin{tabularx}{.45\textwidth}{lQ}
%... & \\
%... & \\
%\end{tabularx}
%\begin{tabularx}{.45\textwidth}{lQ}
%... & \\
%... & \\
%\end{tabularx}


\section*{Acknowledgements}
We thank editor Robin Lemke and two anonymous reviewers for very helpful comments.

%\section*{Contributions}
%John Doe contributed to conceptualization, methodology, and validation. 
%Jane Doe contributed to writing of the original draft, review, and editing.

\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
\end{document}
