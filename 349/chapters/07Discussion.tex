\chapter{Discussion}\label{ch:discussion}\largerpage

In Chapters \ref{ch:cos}, \ref{ch:psy} and \ref{ch:observations}, I have presented studies of productively formed contemporary \textit{-ment} derivatives based on COS verbs and psych verbs. 
In the present chapter, I will now summarize and discuss which general insights can be gained from my results. I will do so by first providing answers to my research questions (\sectref{sec:disc-RQs}), and then evaluating my methodological decisions (\sectref{sec:disc-evaluation}).

\section{Answers to my research questions}
\label{sec:disc-RQs}

\subsection{Which readings are possible in newly formed \textit{-ment} derivatives?}

According to the existing literature (e.g. \citealt{Bauer.2013,Lieber.2016}, see \sectref{sec:cos-output-ment}), \textit{-ment} is a versatile suffix, being attested in the eventive readings \textsc{state} and \textsc{event} as well as in the non-eventive readings \textsc{in\-stru\-ment\slash means, result, product, location} and [−animate] \textsc{patient\slash theme}.\footnote{Note that some labels differ between authors.} Therefore, it is not surprising that I found a range of different readings for my \textit{-ment} neologisms as well. 

Compared to previous literature, however, I made use of more fine-grained distinctions, which allowed me to paint a more detailed picture of \textit{-ment}.
In my studies, I distinguished a total of 16 different attested readings; ten eventive and six non-eventive. Table \ref{tab:ReadingsComparison} contrasts my categories with those from previous studies. 

\begin{table}[t]
\caption{Established readings produced by \textit{-ment} according to the literature, compared with the readings identified in this study.}
\label{tab:ReadingsComparison}
\begin{tabular}{ll}
\lsptoprule
Literature                & This book                    \\
\midrule
 event      & change-of-(psych-)state causation \\
                          & psych-state causation             \\
                          & experiencer psych-action          \\
                          & causing event                     \\
                          & change-of-(psych-)state           \\
state/condition           & psych-state                       \\
                          & result (psych-)state              \\
instrument/means          & instrument     \\
						& causer \\
						& stimulus \\
result                    & result                            \\
product                   & implicit product                  \\
location                  & n.a.                                 \\
 {[−animate]} patient/theme & [−animate] patient                \\
\lspbottomrule
\end{tabular}
\end{table}

Let us first have a look at the eventive readings.
First of all, I distinguish between transpositional eventive readings and shifts to eventive nodes that are embedded more deeply in the semantic representation (e.g. transpositional \textsc{change-of-state causation} and the shifts to its subevents \textsc{causing event} and \textsc{change-of-state}).
Second, with regard to transpositional readings, I distinguish between different kinds of transposition, depending on the frame type. For example, the psych base verb \textit{uplift} is modeled with a frame typed as \textit{change-of-psych-state causation}, while \textit{annoy} is modeled with a \textit{psych-state causation} frame. This distinction carries over into the nominalization frames, resulting in two separate transpositional readings \textsc{change-of-psych-state causation} (for \textit{uplift}) and \textsc{psych-state causation} (for \textit{annoy}). 

Regarding non-eventive readings, I have confirmed those which have been posited in previous literature, with the exception of \textsc{location}. 
Furthermore, I have refined the \textsc{product} category, finding that only the subtype \textsc{implicit product} is attested in my data. 
Both gaps, \textsc{location} and \textsc{explicit product}, can be traced back to the semantics of the base verbs investigated here: Neither COS verbs nor psych verbs have \textsc{location} as one of their core attributes, nor do they denote the creation of an explicit product. 
Finally, I have identified two new readings, namely \textsc{causer} and \textsc{stimulus}. These have so far not been distinguished from the related categories \textsc{instrument/means} and \textsc{agent}.

The semantic versatility of \textit{-ment} is also observable on the level of the individual nominalizations: Every single derivative in my data set is polysemous, exhibiting a range of possible readings. 
The fewest readings can be found for SE psych nouns of the subtype psych-state, which can be found in two readings. The largest number of readings can be found for the noun class I have dubbed \textit{instrument/causer/result-COS nouns}, with as many as seven different readings per type. In a specific context, one or more of these possible readings is selectable. Importantly, a quantitative exploration of my data has shown that the context is often not able to fully disambiguate a newly formed derivative: I have not been able to find unambiguous attestations for as much as 44\% of all combinations of nominalization and reading. 

From these results, it is obvious that \textit{-ment} is an extremely versatile suffix. In new derivatives, it can produce a semantically diverse range of readings, while also heeding a clearly identifiable set of constraints. These constraints are provided partly by the base verb semantics, and partly by the suffix itself. This brings us to research question 2.

\subsection{What are the semantic contributions of the base and of the affix?}

I started out this study from the assumption that semantically similar base verbs will have semantically similar derivatives. While the initial categorization of base verbs needed to be rather significantly revised (see \sectref{sec:disc-evaluation} for an evaluation), we can conclude that this assumption is correct. 
More precisely, I have found transpositional readings as well as shifts to components which are more deeply embedded in the verbs' semantics. These components are of three kinds:
First, components that form the event structure can be targeted (e.g. \textsc{change-of-state} or \textsc{result-state}).
Second, the target of a shift can be a participant that frequently occurs in the direct syntactic neighborhood of the base verb (e.g. \textsc{stimulus}).\footnote{Where to draw the line between a frequent and an infrequent participant, however, remains to be seen (see also the discussion of \textsc{instrument} and \textsc{causer} readings in \sectref{sec:cos-output-survey-instrument}).}
Finally, I have found shifts to non-argumental participants (i.e. \textsc{result} and \textsc{implicit product}). I have thus been able to show that the readings in which a deverbal nominalization is attested provide an informative basis for the base verb's semantic representation. An investigation of derivational semantics can therefore complement insights gained from syntax-based word classifications such as VerbNet or FrameNet. 

The semantic contribution of the base is also manifested in negative evidence. Thus, according to the literature, \textit{-ment} can produce \textsc{location} and \textsc{product} (in my terminology: \textsc{explicit product}) readings. However, as expected, I have not identified these readings because \textsc{location} and \textsc{product} are not elements of the semantic representation of my base verbs.\footnote{Both \textsc{location} and \textsc{explicit product} are attested in a number of neologisms which for reasons of space were not discussed in this book. For example, \textsc{location} can be denoted by \textit{embedment, emplacement} and \textit{trapment}. These are based on \HighlightText{verbs of putting} (see \citealt[111--122]{Levin.1993}) \textendash{} a verb class which has a change of location in its semantics.
\textsc{Explicit product}, on the other hand, is attested for instance in derivatives based on \HighlightText{build verbs} (p. 173--174). Example neologisms which did not make it into the data set presented here are \textit{carvement, knitment} and \textit{whittlement}.}

In both studies, I have also observed that not all components which are present in the semantic representation of a given base verb are also attested as readings in its derivative. This can mainly be attributed to properties of derivation with \textit{-ment}. The three unattested readings are \textsc{agent}, [+animate] \textsc{patient,} and \textsc{experiencer}, which is a finding that corresponds to previous literature (e.g. \citealt{Lieber.2016}).\footnote{There are further readings which are neither part of the base verbs frames, nor were they expected to be attested with \textit{-ment} in the first place, namely \textsc{path, measure, collective, abstract, behavior, inhabitant/language, belief,} and \textsc{adherent} (see \citealt{Lieber.2016}).} The three readings share the feature [+animate], which has led me to the conclusion that \textit{-ment} generally disprefers [+animate] readings. 

From this interplay of base and affix, an array of possible readings arises for each nominalization in my data set. For example, the psych noun \textit{confoundment} can have the transpositional reading \textsc{psych-state causation}, the eventive readings \textsc{psych-state} and \textsc{causing event}, and the participant reading \textsc{stimulus}.
Out of these possibilities, then, one or more is selected in context.

So far in this discussion, I have addressed two central aspects of derivational semantics, namely affix polysemy and the compositionality of the derivational process. In answer to research question 3, I will now review the formal aspects of my analysis.

\subsection{How can the semantics of derivation be modeled in a frame approach?}

In order to model the process of nominalization with \textit{-ment} on COS and psych verb bases, I applied an approach in which the semantics of the base verbs and that of the resulting nouns are modeled in separate frames. A type signature specifies the properties of and relations between all components used in the frames.  
The frames for the base verbs and for the nominalizations are then integrated into a lexeme formation rule (LFR), which expresses the relation between the two. Due to the extensive polysemy of \textit{-ment} and the wide variety of base verb components identified in my study, the most feasible approach is to model the derivation of the individual readings in one LFR each. All LFRs are then incorporated in an inheritance hierarchy.  

The range of possible readings for a given derivative is determined by an interplay between the type signature and the inheritance hierarchy for \textit{-ment}. 
Where no incompatibilities arise, inheritance is successful, resulting in a range of possible readings for the \textit{-ment} derivative. 
For instance, a shift from the psych verb \textit{annoy} to a \textsc{result-state} reading in \textit{annoyment} is possible because the attribute \textsc{result-state} is compatible with psych verbs, as is defined in the type signature, and with \textit{-ment}, as is fixed in the inheritance hierarchy. 
When incompatibilities do arise, inheritance fails and certain readings are excluded. 
For example, a shift from \textit{annoy} to an \textsc{experiencer} reading in \textit{annoyment} fails because the range of the attribute \textsc{experiencer} is fixed to [+animate] entities, so that the animacy constraint blocks the inheritance mechanism.  

\section{Methodological issues}
\label{sec:disc-evaluation}
This section presents a critical evaluation of the methodological decisions I have made in the course of this study. More precisely, I will reflect on issues related to the investigation of neologisms (\sectref{sec:disc-evaluation-neol}), to semantic categorizations (\sectref{sec:disc-evaluation-semcat}), and to the frame approach (\sectref{sec:disc-evaluation-frames}).

\subsection{Neologism data}
\label{sec:disc-evaluation-neol}

I have investigated neologisms in order to set the focus on actual speaker intuition, rather than on lexical idiosyncrasies that have developed over decades or even centuries. This choice led to a number of methodological issues:
First, the status of a formation as a neologism is not always clear. I therefore relied on a number of external measures to decide whether a given derivative could be considered a neologism (e.g. frequency band in the \citetalias{OED}, attestation as a hapax legomenon). 
Second, it is in the nature of neologisms that they are less frequently attested than lexicalized words. I therefore extended my corpus study to corpora in the wider sense (i.e., Google and Twitter). As a consequence, I had to be very conservative regarding any clues that a given attestation may not have been produced by a native speaker. 
Third, despite this expansion of the data base, the available data was scarce for most nominalizations, and finding attestations proved to be laborious. However, I was able to find most expected combinations of nominalization and reading, with only 11\% gaps (15\% for COS nouns and 6\% for psych nouns).
The final issue presented by the neologism data was that, while speakers of English do have an intuition of what a new \textit{-ment} formation may mean out of context, its full range of readings becomes available only in context. Therefore, in the semantic categorization of my nominalization data, the annotators had to rely completely on contextual cues, vague as they may be. 
Of course, this is exactly what happens when a speaker encounters a neologism ``in the wild.'' Therefore, the annotation process was more similar to reality than one might think, but also less categorial and straightforward than one might like.

The question arises: Was investigating neologisms worthwhile? The short answer is: Yes. In my study, I have been able to shape an image of the contemporary, productive process of \textit{-ment} derivation, despite the difficulties just described. 

However, what I have also found is that there is no difference between the two subcategories of my data set (neologism data versus supplementary data, see \sectref{sec:meth-revis-select}). The formations which, in the \citetalias{OED}, are categorized as uncommon but recognizable to speakers of English exhibit the same range of possible readings as those that are extremely rare (including those that are not listed in the \citetalias{OED} in the first place). 
For example, the two psych-state-causation nouns in the supplementary data set, \textit{bemusement} and \textit{convincement}, behave exactly like their counterparts in the neologism data set (e.g. \textit{annoyment} or \textit{affrightment}).

In future research, it would therefore be interesting to conduct a quantitative investigation, examining which frequency effects can be detected: 
Does a higher frequency of a derivative relate to the (non-)availability of readings? 
To what extent do frequent (lexicalized) readings of a given derivative block its usage in rare (or even unexpected) readings? 
For example, even very frequent derivatives like \textit{government} or \textit{equipment} are sometimes attested in atypical readings:

\begin{exe}
	\ex 
	\begin{xlist}
	\item \label{ex:government} An anonymous author [...] wrote that part of the ``natural liberty'' Englishmen and [...] other individuals did not part with when they entered into a ``state of \textbf{government}'' was ``the right that every one has to speak his sentiments openly [...]'' \\ {\small(Google ACAD scholarship.law.columbia.edu)}
	\item \label{ex:equipment} Fundamental to this purpose was Allah's \textbf{equipment} of the female with an instinctive desire and a strong natural passion {\small(\ac{iWeb} ACAD iupui.edu)}
	\end{xlist}
\end{exe}

\noindent Thus, \textit{government} in (\ref{ex:government}) does not exhibit one of its lexicalized readings (e.g. `the governing power in a country or state' or `a period of rule,' \citetalias{OED}), but rather a \textsc{state} reading. Likewise, \textit{equipment} in (\ref{ex:equipment}) does not reflect the usual definition of `anything used in equipping' (\citetalias{OED}), but a clearly transpositional reading.\footnote{The transpositional reading is actually listed in the OED, but a manual inspection of COCA shows that this reading is uncommon: The first 100 hits of the search string 〈equipment〉 do not contain a single instance of a transpositional reading.}

\subsection{Semantic categorization}
\label{sec:disc-evaluation-semcat}

The core of this book is formed by the semantic categorization of the base verbs and their nominalizations, and the semantic decomposition based on these categories.
Now, I would like to look back and evaluate two aspects of this approach: the usefulness of nominalization readings as a tool to access base verb semantics, and the simplifying nature of categorical distinctions.

\subsubsection{Nominalization readings and verb semantics}
A notorious issue in semantic investigations is the arbitrariness of the applied semantic categories. I approached this issue by starting my investigation from a set of clearly defined semantic categories which were based on previous research. For participants of events, I relied on VerbNet, and for the event structure, I consulted a wide range of (formal) literature. 
I then adapted my VerbNet-based semantic decomposition in accordance with my observations from the corpus data.

I found that the verb categorization offered by \citet{Levin.1993} and VerbNet does not suffice to predict all participant readings that I found in the nominalizations. This is because the basis of their classification is syntactic, so that non-argumental participants (such as, for instance, \textsc{implicit product}) are not included. 
Obviously, the assumption that syntactically similar verbs are also semantically similar does hold to some extent, resulting in intuitively reasonable verb classes such as \textit{psych verbs} or \textit{COS verbs}. However, the Levin/VerbNet classification does not offer a complete semantic decomposition (nor do the authors claim that it should). In this book, however, I have shown that decomposition is necessary for an understanding of the process of nominalization and of the resulting noun semantics.

All changes that I made to the initial set of central participants are strictly data-driven. 
For example, I introduced \textsc{product} with the additional distinction between \textsc{implicit product} and \textsc{explicit product} because I noticed that \textsc{product}, but not all kinds of \textsc{product}, is attested in my data. 
Another example is the feature [$\pm$animate]. In order for the animacy constraint to work, I introduced this feature, resulting in such distinctions as [+animate] \textsc{patient} versus [−animate] \textsc{patient}.
Other distinctions were irrelevant for my data, or even made wrong predictions. For example, I eliminated the requirement [+solid] on the \textsc{patient} and \textsc{instrument} roles of c/i reversible COS verbs.

Apart from the identification of central participants, my results offer valuable insights with regard to the event-semantic decomposition of verbs and nouns. 
More precisely, I found that \textit{-ment} derivation can induce semantic shifts to sub\-event nodes (e.g. the \textsc{change-of-state} reading), and I identified clues in the contexts of some nominalizations that indicate a complex event structure for the respective base verbs as well (e.g. contexts of the kind \textit{x's V-ment of y with z}). 
For COS verbs, I have been able to support the traditionally assumed complex event structure. 
For psych verbs, I have contributed a new perspective in the discussion of which event types are adequate to model them. 
Thus, I have been able to show that the range of readings in which a deverbal nominalization is attested provides an informative, additional basis for decomposing the base verb's semantics. 

Conversely, however, it is obvious that the VerbNet-based verb frames did not suffice to predict all possible nominalization readings: I based these frames on VerbNet, which only takes into consideration those participants which frequently figure syntactically in a verb's contexts.  
Therefore, for more predictive power of the base verb frames, further sources of information are required. In the conclusion of this book (\chapref{ch:conclusion}), I will delineate which other frameworks could complement the frame-semantic approach to reach this goal.

\subsubsection{Categorical decisions}

Any categorization means controlled loss of information. In this book, this was an especially pronounced issue, since I made a number of categorical decisions: 
I partitioned my data set into verb classes and subclasses, I proposed one or more frame representations for each subclass, and I used semantic labels such as [$\pm$solid] as well as semantic categories such as \textsc{instrument}. In doing so, I had to assume a number of clear-cut distinctions, which at times proved problematic. 

Take, for example, the subcategorization of COS verbs. Based on VerbNet, there are three subcategories represented in my data set. 
My findings, however, point to a total of ten different groupings of base verbs, based on the availability of certain readings in the nominalizations (see \sectref{sec:cos-output-formal-frames} for details). 
For example, apart from those readings shared by all COS nouns, \textit{diminish} produces \textsc{causer} and \textsc{result} readings in its \textit{-ment} derivative, while \textit{congeal} produces \textsc{instrument} and \textsc{patient} readings instead, and \textit{disperse} produces \textsc{instrument} and \textsc{result}.

Based on this complex distribution of readings, the first reaction may be to assume that the best predictor for a COS noun's range of readings is the individual base verb, and that assuming subcategories of COS base verbs does not make very much sense at all. However, some of the emerging groups are intuitively reasonable. For example, the inherently scalar verbs \textit{diminish, increase} and \textit{worsen} show the same pattern in their nominalizations, as do \textit{bedraggle, befoul} and \textit{debauch} (forming something like a \textit{staining} group).

Another example of a problem with clear-cut semantic categories is the distribution of \textsc{instrument} and \textsc{causer} readings. I hypothesized that we are dealing with a gradient phenomenon, with the (non-)availability of these readings being related to the frequency with which \textsc{instrument} and \textsc{causer} participants are attested with a given base verb (see \sectref{sec:cos-output-formal}): The more frequently either of the two is attested with a given base, the more likely its nominalization may be to exhibit the corresponding reading.

\begin{sloppypar}
These examples do not illustrate problems that arise specifically with the frame approach, or with my data, but rather they represent a fundamental issue with categorical formalizations in general: An attribute is represented in a frame, or it is not; a feature is plus, or minus \textendash{} all categorical approaches have to break down gradient phenomena into distinct categories at some point.\footnote{Of course, there is a whole debate on gradience in linguistics. For a general overview see, for example, \citet{Hay.2005}, \citet{Aarts.2007} and \citet{Lappin.2015}.} 
\end{sloppypar}

\subsection{Frames}
\label{sec:disc-evaluation-frames}

After having applied the frame approach in two extensive studies, my overall assessment is that frames provide a useful tool for the modeling of derivational semantics (see also my answer to research question 3, \sectref{sec:disc-RQs}). Due to their flexible, recursive structure, they allow a detailed and expressive deconstruction of lexical semantics, and can straightforwardly be combined with other formalisms (here: LFRs and inheritance hierarchies) in order to model and comprehend complex linguistic mechanisms. That said, there are several issues that need to be considered when working with frames.  

One question that often arises when discussing frame semantics with other researchers is whether frames are indeed too flexible. While the procedure of creating a frame representation is built on a number of regulations (e.g. the uniqueness conditions, see \sectref{sec:fr-dus-basic-unique}), it often appears as though attributes and values can be added to a frame to suit the researcher's fancy. 
In this book, I have addressed this issue by implementing a data-driven approach: While revising the VerbNet-based frames, I included only those elements which are required to model the \textit{-ment} nominalization readings in my data set. That is, the final frames contain nodes which are targets of referential shifts, and the attribute paths leading to them. On the other hand, those frame elements in the VerbNet-based frames which are not involved in derivation with \textit{-ment} were confirmed or contested by examining the \textit{-ment} derivatives' contexts.

Additionally, because the frame format is so flexible, it is necessary to explicitly preclude arbitrary attribute-value combinations. 
For the purposes of this book, I have chosen to spell out the pertinent appropriateness conditions in a type signature. While this adds to the transparency of my approach, it is no trivial task, and I had to apply a number of simplifications and shorthands in order to reduce the type signature to a manageable size. 

At the outset of this book, I formulated a number of prerequisites for a framework to be useful for modeling affix polysemy: Such a framework needs to allow for semantic composition and decomposition, thus allowing the researcher to model the semantic contribution of the base in the process of derivation. It needs to be flexible enough to incorporate all possible nominalization readings, and at the same time restricted enough to preclude impossible ones. 
All things considered, I have made the case that the frame approach is an appropriate, even excellent tool to model affix polysemy. 
