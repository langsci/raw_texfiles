\chapter{Conclusion and outlook}\label{ch:conclusion}\largerpage[2]

In this book, I have investigated affix polysemy by analyzing corpus attestations of a data set of English \textit{-ment} neologisms with psych verb and COS verb bases. For this, I combined a decompositional frame-semantic approach with LFRs and inheritance hierarchies. 
Based on an in-depth study of the semantic contributions of both the base and the affix, I have been able to determine how a derivational process acts on the semantics of a given base. 
In this, I have shown that an explicit semantic decomposition of the base is essential for the analysis of the resulting derivative's semantics. From the perspective of the derivative, I have demonstrated that identifying possible readings provides evidence for the semantics of the base verb as well.

My results show that \textit{-ment} can target a highly restricted set of elements in the frame of a given base verb. By doing so, the suffix produces a range of possible readings in the derivative, which becomes ultimately interpretable only within a specific context.  
The derivational process is governed by an interaction of properties of the affix (e.g. the animacy constraint) and of the base (i.e. the presence or absence of nodes).
For instance, a shift from the psych verb \textit{annoy} to a \textsc{result-state} reading in \textit{annoyment} is possible because the frame attribute \textsc{result-state} is compatible with psych verbs, as defined in the type signature, and with \textit{-ment}, as fixed in the inheritance hierarchy. 
Meanwhile, a shift from \textit{annoy} to an \textsc{experiencer} reading in \textit{annoyment} fails because the value range of the attribute \textsc{experiencer} is fixed to [+animate] entities, so that \textit{-ment}'s animacy constraint blocks the inheritance mechanism.

Furthermore, a quantitative analysis of gaps in my data set reveals that the availability of data is surprisingly high. Thus, despite having analyzed neologisms, I have found most expected combinations of nominalization and reading. Within the subset of attested combinations, ambiguity is widespread: Both from an onomasiological and from a semasiological point of view, there are comparatively few unambiguous attestations.
Interestingly, gaps and ambiguity in my data are especially pronounced for one group of readings, which I dubbed \textsc{originator} readings (i.e. \textsc{instrument, causing event, causer} and \textsc{stimulus}). Therefore, a further finding of the quantitative analysis is that \textsc{originator} readings are likely being partly blocked for new \textit{-ment} derivatives by more standardly subject-denoting suffixes such as \textit{-er} or \textit{-ant}, but further research is needed to corroborate this finding statistically. 

Overall, I have shown that a decompositional frame-semantic approach applied to corpus data succeeds in modeling the derivational process of one suffix on two kinds of base verb. In order to devise a comprehensive model for derivation, the next step is to broaden the scope of research to different kinds of bases (e.g. nouns or adjectives) and derivational processes (e.g. category-preserving derivation, which includes prefixation). 
This way, further constraints and properties of derivation can be identified, and it can be determined whether my findings for deverbal \textit{-ment} nominalization conform to more general principles.
I have made the case that, in this endeavor, it will be essential to make the semantics of the respective base explicit. While studies following this approach do exist (see e.g. \citealt{Zinova.2016}, investigating Russian prefixation), more research is needed to be able to identify both language-specific and \textendash{} possibly \textendash{} universal principles in the interaction of base and affix. 

In addition, my corpus-based, frame-semantic approach should be sup\-ple\-ment\-ed with other methods in order to validate my findings and to refine my proposed frame representations. Specifically, I suggest to use computational tools and experimental methods to tackle some issues which have remained unresolved in this book. 

First, my frames and the predictions they yield should be further tested by implementing them computationally. A framework which has already been successfully applied to do this for English nominal \textit{-al} is eXtensible MetaGrammar (XMG, \citealt{Crabbe.2013}; see \citealt{Andreou.inpress}). This implementation has shown that, by introducing type constraints (such as my animacy constraint) into the frame architecture, those readings which are possible for a given combination of base and affix can indeed be predicted and generated. 

Second, I propose to add probabilistic elements to the frame representations in order to achieve a more fine-grained model.  
In this book, I have predicted nominalization readings based on the presence or absence of frame attributes, governed by constraints. Here, more detailed predictions could be achieved by including probabilistic information about the base, for instance based on co-occurrence frequencies of the base verb with its participants (as sketched for the prediction of \textsc{instrument} versus \textsc{causer} readings in COS nouns in \sectref{sec:cos-output-formal-frames}).
In this vein, frames could also be combined with analogical models (AM, e.g. \citealt{Skousen.2007}).
In an AM approach, a computational algorithm uses a lexicon of stored forms and properties to predict an item's probability for a given property. The main challenge here would be to devise a detailed but constrained set of cogent input properties, especially since AM has only rarely been applied in the field of semantics. 

Third, including probabilistic information can be beneficial also with regard to the assessment of possible readings.
In this book, I have assessed the range of possible nominalization readings based on the availability or non-availability of attestations. In other words, I distinguished only between possible and (presumably) impossible readings, without considering the proportions between readings. A quantitative analysis of randomly sampled corpus data would not only offer a more realistic representation of the available data, but the identified proportions could also be compared to the AM results, testing the model's predictive power. On the other hand, very infrequent readings may easily be missed in a random sampling approach, as opposed to the purposeful sampling approach applied here.   

The fourth issue is also related to the predictive power of the base frames: My initial categorization of the base verbs, based on the semantic-syntactic approach in VerbNet, did not capture all semantic distinctions that turned out to be relevant for nominalization semantics. 
The accuracy of base verb categorization could be improved with Distributional Semantics Models (DSM, e.g. \citealt{Boleda.2019,Marelli.2015}).
In a DSM approach, context is used to model a vector for a word's meaning. According to the \textit{distributional hypothesis}, semantically similar words are thought to occur in similar contexts (e.g. words with eventive readings occurring with temporal modifiers such as \textit{continuous}). Similar words will thus have vectors that are close together in the vector space. DSM could thus be used to group the base verbs semantically.
Here, an advantage would be that the researcher does not have to devise a set of properties; rather, the semantic similarity of the words would be computed automatically. However, determining the actual semantic properties of the resulting groups of words is an intricate and laborious task involving both the manual inspection of contexts (see e.g. \citealt{Lapesa.2018}) and/or quantitative approaches (see e.g. \citealt{Wauquier.2020}).

The final issue I want to mention is related to my finding that the context plays a critical role in derivational semantics, being responsible for the final disambiguation of readings. The question arises how the disambiguation of polysemy can be distinguished from coercion, that is, context-induced,  post-lexical meaning shifts. 
In my two studies, I used introspection and consulted with my fellow annotators. In future research, this approach may be complemented by using experimental methods (as suggested by \citealt[195--196]{Loebner.2008}). For example, based on fMRI or reaction time experiments, it has been reported that coerced readings require more cognitive effort (see e.g. \citealt{Lai.2017}). The crux for such an investigation of my data set, however, would be to distinguish the processing cost of coercion from the high cognitive load that comes with processing neologisms. 

To conclude, the present book shows that the semantics of derivation can be successfully modeled by using a decompositional frame-semantic approach. It is also clear, however, that my two studies have merely scratched the surface of what is possible and desired, and that further research is needed to devise a comprehensive model of derivational semantics. 

