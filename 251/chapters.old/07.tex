\chapter{Discussion \& conclusions}\label{ch:Summary}

The aim of \chapref{ch:Phoneme-Split} was to develop an explanatory
model of a specific type of sound change: phoneme split, or phoneme
genesis. Yet, in the course of developing that model, the change being
modeled itself underwent a certain kind of transformation. When phoneme
split was first introduced in \sectref{sec:Actuation-1} it was
described as allophone becoming phoneme. The implication, particularly
in the case of vowel nasalization, was that a completely new phoneme
category had to be created, something that had not been previously
modeled. The classical representations for the synchronic and diachronic
rules are given below.
\begin{covexample}
\label{allophonic rule}$/V/\rightarrow[\tilde{V}]/$\_\_$N$
\end{covexample}
\begin{covsubexamples}
\item \label{split-1}$/VN/>/\tilde{V}/$
\item \label{split-2}$[\tilde{V}]>/\tilde{V}/$
\end{covsubexamples}
In scenario (\ref{split-1}), the loss of the nasal context \emph{(N)}
is the precipitating event, critical to the emergence of the phoneme.
In scenario (\ref{split-2}) the loss of the nasal context is irrelevant;
the phoneme arises through some other mechanism.

Immediately, the actuation problem arises – the problem of determining
why phoneme split sometimes happens and sometimes does not (\citealt{Labov1968}).
If the conditioning context can be lost without phoneme genesis, then
it cannot be the loss alone that creates the phoneme (\ref{split-1}).
But if the loss of context is irrelevant and coincidental, then contextually
predictable phonemes are possible and we have no way to determine, or predict, the status of such sounds (\ref{split-2}).

The solution to this impasse suggested by the Multiple-Parse
Model is that phonemes are nothing other than hypotheses made by individual
listener/speakers about how to break up word-level units, hypotheses
that may change from moment to moment and from token to token. Once
such a hypothesis is made it acquires its own representational reality
– at least for that listener/speaker. Because allophonic relationships only
exist as corollaries of a given phonemic analysis, they are automatically
generated under one hypothesis, and automatically missing under the
other. 

However, even under the “allophonic” analysis, allophones never
actually surface in this model. The process that generates what linguists
would label as an allophone does not occur at the same representational
level as the phoneme; it occurs in the region shared between two adjacent
phonemes.\footnote{Incidentally, this reveals another hidden assumption of the generative
notation: the fact that coarticulation appears to only affect one
of the segments involved. Nasalization occurs on the vowel, but vocalization
should also occur on the nasal. This bias is most likely based in
perception, but articulation-wise, the allophonic relationship may
be relatively symmetric.} It is predictable in the sense that nasality is predictable when
the velum is lowered. But it is meaningless to talk about bigram predictability
– the predictability of vowel nasality from the subsequent nasal –
because the listener does not hear a sequence of phones. Under one
parsing of the input, nasality will be attributed to gestural
overlap between adjacent phonemes, under another it will be attributed
to gestural overlap within a single phoneme. In either case it will
be entirely predictable. 

The sound change in question, therefore, does not actually involve
the generation of a new phoneme category. If we assume that all possible
hypotheses are entertained for all ambiguous inputs, then all phonemes
exist at all times, and it is only their probabilities that might
change over time.\footnote{This does not preclude the merger of phonetic values in the pronunciation
of two sounds that were previously distinct (e.g. the so-called PIN/PEN
merger in certain dialects of American English).} This reframing avoids the representational paradoxes discussed
earlier. Actuation now pertains to factors that affect the probability
distribution over the hypothesis space. Such factors are likely to
be numerous, and undoubtedly include aspects of language processing
not explored here. In the same vein, the vowel nasalization model
is not to be taken as applicable to all types of sound change, nor
even as a model of all aspects of vowel nasalization change. In
the next two sections some other factors are briefly discussed, along
with possible extensions of the current work.

\section{Additional implications \& future work}

The Multiple-Parse Model is a model of the internal dynamics of a
single word category in isolation. In this model the assumption is
that sub-lexical categories are derived from words, rather than the
other way around (see, e.g. \citealp{beckman2000ontogeny}). Once
such categories arise, however, they are expected to exert influence
in the other direction (English orthography is likely to produce a
similar effect). Even without the influence of explicit phoneme categories,
we expect word-level representations to be linked in some way that
reflects their similarity to each other. Therefore, the evolution of a given
word cannot truly occur in isolation. 

Sound change is typically taken to refer to change at the phoneme level.
In the Multiple-Parse Model change is taken to occur at a less abstract
level: sub-lexical, but specific to an individual word. I assume that
a generalization stage is necessary, likely requiring multiple, semi-independent
changes at the word level.\footnote{Not to mention the spread of change to all members of a speech community, which I assume is another necessary stage of change, but well beyond the scope of the present work.} The dynamics of such a model are not trivial, and require, among
other constraints, that the phoneme-to-word feedback bias be strong
enough to allow generalization to occur across all words containing
that phoneme, but not so strong as to prevent changes at the level
of the individual word. 

One interesting consequence of adopting the position that word categories
precede phoneme categories, is that phonetic regularities must begin
as \noun{states} (stored articulatory variables), rather than \noun{processes}
(the result of combining two or more linguistic units), in the infant
learner. Processes are potentially inferred gradually, over sufficient
amounts of variable data (e.g. \citealp{goodman1997inseparability}),
but individual \noun{state} representations might persist, as \noun{process}
ones do in the simulations of the previous section. 

The opposite course of development might be expected to occur in the
domain of morphology, where explicit concatenation requires a \noun{process}
model, but \noun{state} analyses become available over time. In fact,
the “competition” between the Analysis-1 parse and the Analysis-2 parse bears a high degree of similarity to dual-route theories of
morphology (e.g. \citealt{caramazza1988lexical,frauenfelder1992constraining}).
Classically, transparent morphological alternations are assumed to
be rule-based, analogously to allophonic alternations. However, it
is evident from the historical record that morphological affixations
that were once productive can fall out of use, resulting in a few
artifactual forms that are unlikely to be decomposed into their constituents
by modern speakers. Additionally, some highly frequent forms, although
transparently decomposable, may behave as though they have unique
lexical entries (e.g. \citealt{baayen1997singulars}. \footnote{See \citealt{levelt1999theory}
for a review of frequency-based storage, and \citealt{burani1987representation,baayen1993frequency}
for further discussion of factors affecting morphological storage).}
This parallelism does not seem to be coincidental, and is especially
relevant to allophonic alternations that occur precisely at morpheme
boundaries.

Morphophonological alternations are, in fact, often taken to comprise
the best evidence of an active phonological rule. This is because
the morphological process involved is assumed to be productive. That
is, it is assumed to be a \noun{process}. Yet, the change that led
to the phonological alternation may only have come about due to representations
becoming more \noun{state}-like, as is implied by the behavior of
the Multiple-Parse Model. If this is on the right track, then truly
phonological, truly productive alternations may only arise when \noun{state}
and \noun{process} representations are balanced in such a way as to
preserve this tension. Determining the necessary conditions for this
to happen presents an interesting area for future research.

\section{Types of sound change}

In the modeling of sound change, the term “phonetic bias” seems
to have been used as a cover term to refer to phonetically-based sound
change of more or less any kind. Thus it has been (or can be) applied
to word reduction, vowel lengthening, vowel nasalization, and nasal
place assimilation (or loss), among others. However, there is no \emph{a priori}
reason to expect all phonetically-based sound change to operate in
the same way. And part of an ultimate theory of sound change
will include a taxonomy both of the source of a given change, as well
as its actuation mechanism. 

The Multiple-Parse Model of vowel nasalization presented in Chapter ~\ref{ch:Phoneme-Split} is based on the hypothesis that coarticulatory nasalization
is \emph{not} best analyzed as a phonetic bias; that is, as a constant
pressure acting in a fixed direction. Instead, the source of nasalization
is taken to be an inherent property of motor planning involving the
temporal overlap between adjacent articulatory gestures. Synchronically,
overlap degree is assumed to vary as a function of speaking rate,
among potentially many factors, all of them contributing to a stable
distribution with a certain degree of variance. In the implemented
model, a change in the resting activation of a word-level category
acts to shift both the absolute durations of the articulatory parameters,
as well as the proportion of overlap. Words become shorter, with a
higher degree of overlap, as resting activation increases. This follows
from the assumption that activation level directly affects not only
the speed with which words are accessed and initiated, but also the
speed at which articulation unfolds. The utility of this model is
only as good as this assumption, and will need to be revised if our
understanding of the frequency effect changes.\footnote{The correlation between speaking rate and degree of coarticulation,
as well as the correlation between word-frequency and degree of coarticulation,
appear to be quite robust. It is less clear, however, what the exact
mechanism is that mediates between activation level and degree of
coarticulation. Without this link, we run the risk of modeling an
epiphenomenon, rather than the phenomenon itself.} However, actuation is achievable by any mechanism that can shift
the overlap distribution as a whole.

The Multiple-Parse Model, of course, is meant to be not just a model
of vowel nasalization, but of all linguistic phenomena that are functionally
equivalent to vowel nasalization. Establishing this class is not trivial,
and I will only hypothesize here that phenomena involving articulatory
overlap, articulatory blending, and articulatory masking will generally
be possible to model in this way. True phonetic biases can also be
incorporated into the general model. Consonants occurring before other
consonants (rather than vowels) can be considered to be in a perceptually
disadvantaged position. This is especially true for stops, since most
of the cues to their identity actually occur in the transitions to
a following vowel (e.g. \citealt{liberman1954role}), but likely
holds to some extent for most consonants. Articulatorily, the velum
gesture attributed to the nasal in a word like “camp” will be
overlapped to some extent not only with the preceding vowel, but also
with the following consonant. The overlap with the preceding vowel
is highly audible, while the overlap with the following stop is much
less so, due to the complete closure in the oral cavity. The stop
context, relative to a vowel context (such as in the word “camo”),
can be thought of as biasing for nasal deletion (or a nasal vowel).
This can be implemented as a factor that raises the probability of
the single-segment parse.\footnote{In fact, the word-final context modeled in \chapref{ch:Phoneme-Split}
does not constitute a homogeneous phonetic environment. Unless the
target word is in absolute phrase-final position it will be followed
by another word, beginning with either a consonant or a vowel. Because
the two different possibilities consist of different perceptual environments,
segment loss might only occur in the former, resulting in a type of
liaison (e.g. \citealt{Tranel1981}). There is also some evidence
to suggest that changes restricted to specific words can be attributed
to their historically higher occurrence rates in the perceptually
disadvantaged environment (e.g. \citealt{brown2012discourse})}

Velar palatalization was briefly discussed in \sectref{sec:Competing-targets}
as an example of gesture blending. Faster productions will result
in more overlap between consonant and vowel, which should merge the
two gestures more completely, as well as render the combined production
shorter. Both phonetic properties should lead to an increase in the
probability of the single-segment analysis. The many different ways
in which palatalization can be realized in different languages (e.g.
{k\textgreater t͡ʃ}, {k\textgreater kʲ}, {kj\textgreater kʲ},
etc.) suggests a number of possible influencing factors, as well
as an inherently larger space of possible parses. One such parse results
in a two-segment analysis, with an intermediate tongue position for
the consonantal gesture (see \figref{fig:/k=0002B2+i/}); another
results in a single-segment analysis with a complex two-target gesture
(see \figref{fig:Palatalizationc}). Perceptual asymmetries have
been found with respect to the rate of misidentification of {[}ki{]}
sequences in noise and fast speech (as {[}ti{]} and {[t͡ʃi]},
most commonly) suggesting that phonetic bias plays a role in this
change (\citealt{Guion1998,Chang2001}).

In contrast, the phenomenon of vowel lengthening (\sectref{subsec:Model-2:-Lengthening})
does not appear to be the direct result of overlap, blending, or masking.
There is, however, no consensus in the literature regarding the phonetic
source of this effect. In fact, there is not even agreement about
whether the process is one of lengthening before voiced obstruents,
or shortening before voiceless ones (\citealt{gimson1970introduction,wells1982accents}).
Of the hypotheses proposed, most have an articulatory basis (e.g.
\citealt{belasco1958variations,delattre1962some,chen1970vowel,lisker1974explaining,Klatt1976,moreton2004realization,schwartz2010phonology}),
but auditory/perceptual accounts have been offered as well (e.g.
\citealt{lisker1957closure,javkin1977phonetic,Kluender1988}). None
of these have been firmly established empirically, and strong arguments
have been made against many of them. Without some idea of what the
mechanism for the actual increase (or decrease) in length is, it is
not possible to produce an insightful model. Work in progress suggests,
in fact, that the apparent lengthening effect may be epiphenomenal:
the result of partial temporal compensation, resulting from an upper
limit on the duration of voiced obstruents (Morley \& Smith \emph{in
prep.}). If this is correct then it suggests another type of misparsing
that can occur when multiple sources affect the same phonetic dimension
in roughly the same way. In the case of vowel length, contributing
sources include phrase-final lengthening, lengthening due to slowed
speaking rate, and greater length due to an inherently longer vowel,
creating ambiguity as to how the observed duration should be attributed.\footnote{This theory requires that some type of normalization be carried out
– even if it is just a comparison between neighboring segments. If
pure duration is the dimension of contrast, then it is hard to see
how segments could be classified as ``long'' or ``short'' unless speaking
rate, at minimum, is taken into account.}

Other kinds of change, such as transphonologization, or chain shifting,
suggest yet other potential sources, but it is beyond the scope
of this book to speculate about their exact nature. However, given
a hypothesis regarding the source of the phenomenon and the representational
level at which it acts, it is possible to create an implemented model.
Such a model may, or may not, bear much resemblance to those proposed
in this work, yet the basic questions about the relationship between
theory and model, and between model and implementation will remain
the same. 

\section{Summary \& conclusions}

Computational models allow us to run experiments with language that
are not possible in the real world, such as those at the timescale
of diachronic change. They present a powerful and useful tool for making
explicit tests of our current theories. Computational models can be
used to establish existence proofs, demonstrating that it is possible
to solve a problem in a particular way. On the flip side, modeling requires extensive simplification of the complex factors
at play in language use and comprehension and there is never any
guarantee that the simplifications have not altered the problem to
the point that the results no longer shed light on the phenomenon
of interest. Implemented models are often tailored to specific problems,
and may prove to be inconsistent with other known aspects of language.
In order to get a model to run, there are various implementational
choices that must be made, choices that may, in fact, contain hidden
theoretical assumptions. Thus, the interpretation of modeling results,
just like the interpretation of the more traditional type of experimental
results, must include serious consideration of potential confounds. 

The purpose of the present work has been to bring the theoretical
issues to the fore via explicit links between different implementational
approaches and the types of representational structures they embody.
In this way a number of representational inconsistencies, or paradoxes,
were uncovered. The more transparent of these were the cases in which
tokens were assigned two different underlying representations, or
where an explicitly separate (i.e. stored) category was also subject
to a process, giving the phenomenon a hybrid \noun{state-process}
status. In fact, there may be a paradox lurking in applying a process
(e.g. knowing that tokens should be lengthened in a particular context)
but failing to account for the effects of that process (lengthening)
when adding the produced token back to the perceptual exemplar cloud. 

Two apparent successes of the basic iterative exemplar model – accounting
for frequency-based lenition, and phonetic similarity effects – were
called into question. \sectref{subsec:Model-1:-Context-Free}
demonstrated that, depending on the specifics of how word frequency
is represented, successive reduction of tokens does not necessarily
produce the observed negative correlation between frequency and word
length. Retention of fine-grained phonetic detail (without retention
of production context) was shown to actually disrupt predictable phonetic
allophony. Depending on other representational decisions, the result
was either a single variant that occurred in all contexts, multiple
variants that occurred unpredictably, or a continuously moving target
(\sectref{sec:Context-Dependent-Iterativity}). 

Developing exemplar models that make the right kinds of predictions
requires some force for constraining the powerful iterativity mechanism
of the perception-production feedback loop. This is often accomplished
in practice by filtering out tokens that fall between two existing,
contrastive categories. But in the absence of contrast, something
else is required to keep the categories bounded. There seems to be
a common misconception that exemplar models do not require underlying
representations, or targets. But models may in fact implement what
amounts functionally to a soft target, or attractor, even if it is
not identified as such (\sectref{subsec:Soft-Targets}). Such
a target may, in fact, be necessary to produce bounded behavior.

Furthermore, the standard assumption of an identity mapping between
perception and production obscures the complexity of the speech processing
problem. In fact, differences between what speakers intend to produce,
and what listeners perceive, are likely to play a large role in diachronic
change that arises from synchronic variation, the very thing these
models are trying to explain. Nor is it the case that iterative application
of an articulation-based bias (such as anticipatory feature spread)
can be assumed to lead to cumulativity on the acoustic side (Chapter
\ref{ch:Perception-Production}). To produce the type of gradual
increase that is desired, a change in the relative timing of articulators
may be required. The explanation as to why such a change would occur
is the answer to the actuation question itself.

A proposal was offered in \chapref{ch:Phoneme-Split} for one
way to account for phoneme genesis arising from allophonic split.
The model was designed in a way that prioritized representational
consistency, capturing both change and stability, and implementing
a plausible mechanism for change at both local and global scales.
The “correct” sub-lexical representations were not assumed,
and therefore, neither was the allophonic rule (or production bias).
Instead, the equivalent of an articulatory representation was decided
independently for each token. Feedback occurred in the dependence
of the parsing probabilities on the values of the articulatory parameters.
In the reported simulations there was only one choice to be made by
the speaker/listener, whether to store or generate the degree of overlap
between the two articulatory gestures. Stability was achieved by a
general-purpose force (speaking rate), acting bi-directionally, to
both lengthen and shorten tokens. Different stable states resulted
from different resting activation levels, which affected the rapidity
with which the words were produced. This result hinged on two properties
of the model: the dependence of the speaking rate effect on word duration
(longer tokens were lengthened more than shorter tokens for the same
decrease in rate), and the implementation of resting activation as
a shift in the mean of the speaking rate distribution. Numerous other
implementational choices are possible, but only a small fraction of
them lead to a theoretically coherent, cognitively plausible, empirically
adequate outcome. Thus, the existence proof embodied in the Multiple-Parse
Model has merit in and of itself. The results also raise the possibility
that certain consistently intractable problems in the study of change
and actuation may be artifacts of the overt and covert assumptions
of the traditional notational system.

On the one hand, the work in this book represents relatively minor
variations on existing proposals and models: the basic exemplar architecture
in which sub-phonemic detail is retained; the role of word frequency
in sound change; ambiguity in surface forms as the driver of variation;
etc. Its primary innovation may be in bringing together and explicitly
implementing those elements. Yet, the result is a radical re-conceptualization
of basic phonological tenets. I have suggested that 1) phoneme split is neither phoneme
creation, nor allophone loss, 2) neither allophonic rules
nor phonemic inventories actually exist as traditionally described, and 3) phonological
rules as we typically understand them may only arise under restricted
conditions, requiring morphological antecedents and a more explicit
stage of learner generalization. This conceptual shift was largely
a consequence of forcing diachronic and synchronic representations
to match, revealing that questions about how sound categories change
are really questions about what sound categories are – how they are
mentally represented –, and that neither question can be adequately
answered without the other.
