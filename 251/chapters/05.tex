\chapter{The relationship between perception and production}\label{ch:Perception-Production}

In the models examined to this point, the tokens of \isi{perception} have
been assumed to be identical to the tokens of \isi{production}. This assumption
obscures the fact that targets in \isi{production} are necessary in order
for sounds to be produced at all, i.e., that read-out of a stored
set of acoustic values is not possible. It also conflates biases that
act in \isi{production} with those that act in \isi{perception}, requiring them
to act on the same units. Furthermore, this assumption requires that
complex \isi{articulatory} dynamics be uniquely and transparently realized
acoustically. For a dimension like segment duration, this assumption
may not be too unreasonable. However, the correspondence between articulation
and acoustics is well known to be a many-to-many mapping. Invariant
cues to abstract phonemes have failed to be discovered in either domain. 

Starting at least with \citet{Goldinger1996}, it has been assumed
that experienced exemplars are stored as motor plans without intermediate
processing. While this may be adopted largely as an implementational
convenience, it is based on the assumption that the true details of
the mapping will not significantly affect the mechanism of change,
or the model outcomes (see \citealt{Pierrehumbert2000}). Perhaps
the most critical assumption is that they will not affect the feedback
loop that is the driving mechanism of such models. However, as this
chapter will demonstrate, a non-trivial perception-to-\isi{production} mapping
is not just an additive factor that can be slotted into existing models,
but a shift in perspective that affects all aspects of modeling, up
to and including what we take to be the source of sound change itself.
The following sections will make these ramifications explicit for
three cases representing three different types of \isi{phonetic} \isi{bias} (two
of which have been previously modeled): vowel \isi{lengthening} (duration-based
targets); \isi{vowel nasalization} (sequencing of different articulators);
and \isi{velar palatalization} (sequencing of different targets for the
same \isi{articulator}). 

\section{Duration-based targets}

The lack of motivation for a process by which a \isi{production}, at some
random point along the relevant dimension, is moved only a small amount
towards its \isi{target}, was mentioned briefly in \sectref{sec:Model-Interpretation}.
Failure to completely achieve a \isi{target} may not seem paradoxical at
first glance, because it suggests a well-known \isi{articulatory} phenomenon,
known as “undershoot”, in which targets fail to be completely
achieved (e.g. \citealt{Lindblom1963}). But this is not an equivalent
process.\footnote{The centering \isi{bias} in \citet{Wedel2008} is characterized as a \isi{lenition}
\isi{bias} towards the center of each segment dimension. Because Wedel's
categories lack underlying targets (they are randomly generated and
evolve as poor, or ambiguous, tokens are discarded), his \isi{lenition}
\isi{bias} is the mechanism that prevents categories from dispersing indefinitely.
For a two-dimensional \isi{phonetic} vowel space composed of the first and
second formant frequencies, a centralizing \isi{bias} is fairly consistent
with undershoot. However, this \isi{bias} is implemented as a fixed attractor
location, rather than a process that shifts the vowel formants a small
amount towards the center of formant space on each \isi{production}. This
suggests that there is a \isi{target}, or ideal, vowel location from which
all vowels are perturbed by other forces. }

Undershoot can occur if over-all speech rate is rapid, not allowing
enough time to overcome the \isi{inertia} inherent in the physical articulators,
or if sequential targets involving the same \isi{articulator} (e.g. tongue
body) are far apart in the mouth. A duration \isi{target}, however, cannot
be undershot in the same way. In the first place, segment duration
\emph{per se} is not specified on individual articulators or their
configurations. Furthermore, duration is not absolute. Thus, although
a faster \isi{speaking rate} will lead to shorter vowel durations, it will
also shorten all segments in all contexts, meaning that the relative
difference between vowel durations in pre-\isi{voiced} versus pre-\isi{voiceless}
contexts will not necessarily be affected (unless duration values
are at floor or ceiling). A \isi{speaking rate} transformation effectively
changes the location of the \isi{target} itself in absolute terms; it does
not affect the speaker's ability to reach that \isi{target} for any given
token. Length-based features are arguably better modeled by targets
that are a function of \isi{speaking rate}. 

With respect to the effect of \isi{frequency} on duration, the mechanism,
and its interaction with \isi{speaking rate}, remains somewhat unclear.
In models of \isi{frequency} effects, \isi{speaking rate} does not seem to be
considered. Yet the parallels between the two are clear. The conceptualization
of \isi{frequency} of use as repeated practice suggests that there exists
a maximally fluent, or optimal, \isi{production} \isi{target}. While increased
\isi{frequency} should not reduce any word below that \isi{target}, increased
\isi{speaking rate} might. If \isi{frequency} of use translates to higher resting
activation, on the other hand, and higher \isi{resting activation} leads
to faster \isi{production}, successive shortening should only occur if listeners
fail to \isi{normalize} for \isi{speaking rate}; and if they fail to \isi{normalize}
for \isi{speaking rate}, then the stored distribution will reflect the typical
variance in \isi{speaking rate}. This issue will be taken up in Chapter
\ref{ch:Phoneme-Split}, with two different implementations of the
\isi{frequency} effect.

\section{Coordination of independent articulators}

In the \isi{vowel nasalization} example of \sectref{subsec:Model-3:-Nasalization}
it was assumed that \isi{nasalization} occurred when a given vowel token
was produced adjacent to a nasal consonant, transforming from completely
oral ($[-nasal]$), to completely nasal ($[+nasal${]}). This simulation
was useful for illustrating the \isi{context mismatch} that would result
from nasalized tokens being produced in a non-nasal context (which
occurs\linebreak whether \isi{nasality} is considered binary or not). Our current
purpose, however, is to consider how an \isi{articulatory} phenomenon like
\isi{nasality} could be modeled iteratively with the classic perception-\isi{production}
loop. 

In most \isi{exemplar} models ``\isi{phonetic} \isi{bias}'' is taken to apply without
limit, and without regard to input values. That is, \isi{lengthening} will
occur regardless of how long the vowel already is, provided it occurs
in a pre-\isi{voiced} context. For the phenomenon of \isi{vowel nasalization}
this requires some partial \isi{nasalization} that applies whenever a vowel
is produced preceding a nasal consonant, a partial \isi{nasalization} that
is additive in nature. This is schematized in (\ref{Ex: Iterative Nasality}). 
\begin{covexample}
\label{Ex: Iterative Nasality}$\textit{Nasalization}(V)\rightarrow V^{+N}$

$\textit{Nasalization}(V^{+N})\rightarrow V^{+2N}$

$\textit{Nasalization}(V^{+2N})\rightarrow V^{+3N}$
\end{covexample}
But this type of acoustic cumulativity is only possible under a very
specific, and unlikely, \isi{production} model. 

As first described in \sectref{subsec:Model-3:-Nasalization},
\isi{phonetic} \isi{vowel nasalization} is the product of the coarticulation that
occurs throughout normal speech. Sounds are not produced in strict
sequence but overlap considerably with their neighbors. In the case
of a vowel-nasal sequence, the \isi{velum}, or soft palate, is raised in
anticipation of the nasal segment before the vowel gesture has completed,
resulting in airflow through the nasal cavity during at least part
of the vowel's \isi{production}. To represent the \isi{articulatory} side of this
phenomenon, and draw a clear distinction between perceived tokens
and their correspondents in \isi{production}, I will make use of the representational
tools of Articulatory Phonology (AP) (\citealt{Browman1986,Browman1990}). 

In AP, the abstract representational units of speech are taken to
be analogous to musical scores, which indicate the coordination and
ordering of a series of physical movements (\isi{articulatory} gestures).
Those gestures involve a set of active articulators – the tongue,
\isi{velum}, glottis, etc. – usually in relation to a set of passive \isi{articulator}
locations – the teeth, lips, hard palate, etc. Scores consist of a
series of \isi{target} locations for each active \isi{articulator} (e.g. the
alveolar ridge behind the teeth), and timing relations between those
movements (e.g. begin movement of \isi{tongue tip} at midpoint of open
glottis gesture). \figref{fig:Normal nasalization} depicts a \isi{gestural}
score for nasal coarticulation, based on the specific sequence {/æm/}.
Time is represented along the x-axis, and the active articulators
are shown on the y-axis ({\scshape tb}\,=\,Tongue Body; {\scshape vel}\,=\,\isi{velum}). The box adjacent
to each active \isi{articulator} represents the time span during which that
\isi{articulator} is activated: gradually moving towards it \isi{target} position,
then away to a subsequent \isi{target}, or resting state. The interval during
which the boxes overlap indicates the period when the two articulators
are active at the same time. This overlap, indicated by the space
between the dotted lines in \figref{fig:Normal nasalization}, is
the source of the \isi{vowel nasalization} of interest. 

\begin{figure}[H]
\begin{subfigure}[t]{.3\textwidth}
        \includegraphics[width=\linewidth]{figures/nasalization1.pdf}
        \caption{\label{fig:Normal nasalization}Nasalization of underlyingly oral vowel token}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{.3\textwidth}
        \includegraphics[width=\linewidth]{figures/nasalization2.pdf}
        \caption{\label{fig:nasalized vowel}Underlyingly nasalized vowel token (unnormalized)}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{.3\textwidth}
        \includegraphics[width=\linewidth]{figures/nasalization3.pdf}
        \caption{\label{fig:extra nasalization}Nasalization of underlyingly nasal vowel token}
    \end{subfigure}
    
% % \subfloat[\label{fig:Normal nasalization}Nasalization of underlyingly oral
% % vowel token ]{\includegraphics[width=0.25\textwidth]{figures/\isi{nasalization}̘1.pdf}}\hfill{}\subfloat[\label{fig:nasalized vowel}Underlyingly nasalized vowel token (\isi{unnormalized})]{\includegraphics[width=0.25\textwidth]{figures/\isi{nasalization}̘2.pdf}}\hfill{}\subfloat[\label{fig:extra nasalization}Nasalization of underlyingly nasal
% % vowel token]{\includegraphics[width=0.25\textwidth]{figures/\isi{nasalization}̘3.pdf}}

\caption{\label{fig:Coarticulation}Coarticulation involving different articulators}
\end{figure}

The \isi{gestural score} indicated in \figref{fig:Normal nasalization}
produces the acoustic realization {[æ̃m]}. The vocalic
portion of this token, if stored without \isi{normalization}, is represented
by {/æ̃/}. The two different types of brackets are used
here in exactly their usual sense: square brackets indicate a surface
form, an instance of speech, while forward slashes indicate an underlying
form, a form used to generate a speech act. Before such an acoustic
token can be produced, however, it must be converted to an \isi{articulatory}
representation. This is shown in \figref{fig:nasalized vowel}.
Note that, despite the fact that the \isi{nasalization} is now a property
of the vowel itself, the same two \isi{articulatory} gestures are still
required in \isi{production}.

At some still later model cycle, when the token represented in \figref{fig:nasalized vowel}
is chosen for \isi{production} in the identical nasal context, the combined
\isi{articulatory} score is realized as \figref{fig:extra nasalization}.
Under error-free \isi{perception} and \isi{production}, the \isi{velum} gesture associated
with the vowel (indicated by diagonal fill lines), and the \isi{velum} gesture
associated with the nasal will overlap completely. And because there
is only one \isi{velum}, there will be only one \isi{velum} gesture. Acoustically,
this will result in exactly the same amount of \isi{nasality} on the vowel
as before ({[æ̃]}). The feedback loop is, in fact, halted
after a single iteration. The only way that the vowel could become
successively more nasalized is if the \isi{velum} were to begin raising
earlier and earlier in time – in other words, if a successive change
in the timing relationship were to occur.\footnote{Differences in the amount of \isi{velar} opening and degree of \isi{velar} airflow
can be found among different types of nasalized vowels (\citealt{bell1993understanding,hajek2000vowel}).
But this is a property of a given vowel. There is no reason for the
greater degree of \isi{velar} opening for a low vowel, for example, to be
increased further each time that vowel is produced preceding a nasal.} But a change of this nature requires independent motivation. In
other words, the iterative result does not come for free when the
acoustic-\isi{articulatory} mapping is no longer an identity relation. 

\section{\label{sec:Competing-targets}Competing targets for the same articulator}

The final case of perception-to-\isi{production} mapping considered here
is one that contains conflicting consecutive specifications for a
single \isi{articulator}. A common phenomenon of this type is \isi{palatalization},
which involves the tongue shifting towards the hard palate (either
forward or backward) due to the influence of a following or preceding
segment (\citealt{Guion1998,Keating1993}). Palatalization often occurs
in sequences of obstruent consonants and high vowels. For example,
in the articulation of the sequence {/ki/}, the \isi{articulatory}
\isi{target} of the {/k/} is the \isi{velum}, or soft palate, where the
\isi{tongue body} makes contact, briefly creating a complete closure in
the oral cavity. The \isi{articulatory} \isi{target} for the vowel is closer to
the hard palate, where the \isi{tongue body} should reach its highest point,
but without making contact. As a result of the upcoming \isi{tongue body}
specification for the {/i/}, the tongue position for the
{/k/} is shifted forwards – away from the soft palate, and
towards the hard palate. The result is a “blend”, something
that is in between where the two gestures would be in isolation (\citealt{Browman1986,Zsiga2000}). 

The blended \isi{production} for the palatalized \isi{velar} is depicted in \figref{fig:/k+i}. 
At the bottom of the figure, the boxes represent
temporal extent as before, this time of the single Tongue Body \isi{articulator}.
Diagonal fill lines represent the duration when the {/k/}
\isi{target} is active, and the semi-opaque white, the duration of the active
{/i/} articulation. Above, the trajectory of the highest
point of the \isi{tongue body} relative to the two \isi{target} locations is indicated
by the dotted line. The \isi{tongue body} is assumed to start from a resting
position that places its highest point somewhere in between the hard
and soft palates. With the start of the {/k/} gesture, movement
of the \isi{tongue body} is initiated towards the soft palate. However,
because the gesture of the following {/i/} is anticipated,
a shift in direction takes place before this \isi{target} is reached, causing
both targets to be only partially achieved (the solid curves indicate
the \isi{target} trajectories for each segment in isolation). 

\begin{figure}[H]
\begin{subfigure}[t]{.60\textwidth}
        \centering\includegraphics[height=3.5cm]{figures/palatalizationa.pdf}
        \caption{\label{fig:/k+i}/k+i/}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{.40\textwidth}
        \centering\includegraphics[height=3.5cm]{figures/palatalizationb.pdf}
        \caption{\label{fig:/k=0002B2+i/} {/kʲ+i/}}
\end{subfigure}
    
    
% % \subfloat[\label{fig:/k+i}/k+i/]{\includegraphics[width=0.4\textwidth]{figures/palatalizationa.pdf}}\hfill{}\subfloat[\label{fig:/k=0002B2+i/} {/kʲ+i/}]{\includegraphics[width=0.4\textwidth]{figures/palatalizationb.pdf}}

\caption{Coarticulation involving the same articulator. The dark solid lines
represent the trajectories of each segment in isolation. The dotted
line represents the actual trajectory. The Tongue Body (\textsc{tb}) is taken
to start and finish in a resting position in between the two targets. }
\end{figure}

I will take the acoustic counterpart to this \isi{production} token to\largerpage
correspond to a sequence of partially palatalized \isi{velar} and high front vowel, 
{[kʲ]}, {[i]}. The \isi{articulatory} representation associated with the acoustic
representation {/kʲ/} contains a \textsc{tb} gesture located at the
minimum of the dotted curve in \figref{fig:/k+i}. On a subsequent
\isi{production} cycle in which this token is produced in the context of
a following {/i/} ({/kʲ+i/}), the “palatalizing
\isi{bias}” will result in something like the dotted curve in \figref{fig:/k=0002B2+i/}.
Effectively, the strength of the \isi{bias} has been reduced. This is because
the amount of \isi{bias} depends on the distance between the two targets,
and the \isi{target} of the {/kʲ/} is closer to the \isi{target} of
the {/i/}. In fact, it may now be possible to reach both
targets via a slight modification in the \isi{gestural} timing. In other
words, there is no clear necessity of (continuously) shifting the
\isi{target} location for the obstruent closer to the hard palate, resulting
in successively more palatalized tokens. 

There is actually more than one plausible acoustic interpretation
of the output of \figref{fig:/k+i}, and thus more than one \isi{articulatory}
mapping for tokens derived from the original \isi{production} of the {/k+i/}
sequence. The \isi{target} locations of both the consonant and the vowel
may be altered, or the perceived boundary between the two segments
may be shifted, or both. \figref{fig:Palatalizationc} depicts a
scenario in which the entire sequence has been stored as an \isi{exemplar}
of the original {/k/} category: a composite segment consisting
of two sequenced targets.\footnote{Using the IPA to represent acoustic correspondents is not ideal, due
not only to the conflation of acoustic and \isi{articulatory} information,
but because it is not fine-grained enough to capture all the relevant
differences among the \isi{gestural} scores. The composite analysis could
alternatively be represented as {/kʲ/} (as opposed to the
original {/kʲ/+/i/}). A change in both targets might look
like {/kʲ/+/ɪ/}. Other possibilities include: {/k/+/j/+/i/},
{/k/+/j/}, {/k͡j/}.} This particular type of mapping is of considerable interest because
it is not structure-preserving at the \isi{phoneme} level. If the \isi{perception}-to-\isi{production} mapping itself is the cause of the loss or the
gain of a \isi{phoneme}, then \isi{phoneme split} may be possible without an independent
change that eliminates conditioning context – it may, in fact, follow
directly from a merger of the \isi{allophone} with the \isi{allophonic} context. 

\begin{figure}[H]
\centering{}\includegraphics[height=3.5cm]{figures/palatalizationc.pdf}\caption{\label{fig:Palatalizationc}Possible production token derived from
perception token {[}{kʲi}{]}}
\end{figure}


\section{\label{subsec:Misperception-=000026-Misarticulation}Misperception \& misarticulation}\largerpage

A well-established tradition in laboratory phonology attributes \isi{phonetic}
and \isi{phonological} sound change to mishearing and misspeaking on the
part of individual speakers and listeners \citep{Ohala1980,Ohala1981,ohala1983origin,Ohala1990}.
Many such changes are traced to coarticulation in \isi{production}, which
can create perceptual ambiguity, and the possibility that what the
listener recovers is not what the speaker intended. In certain theories
of change, the rarity of sound change is attributed to the fact that
most speech takes place in a mode where speakers provide sufficient
cues for listeners, and listeners accurately reverse the effects of
coarticulation. Only rarely do listeners switch to a ``non-speech''
mode, in which they take the perceived forms at face value, or randomly
decide to keep a poor category \isi{exemplar}, rather than discard it (e.g.
\citealp{lindblom1990explaining,Garrett2013}). In other theories,
discrepancies between speaker and listener are more common, and the
rarity of language-wide change is attributed to the listener's access
to other sources of information about the ``correct'' form of a word
(and/or the low likelihood of other speakers adopting and spreading
an individual's novel variant, e.g. \citealt{Ohala1980}). 

Perception biases emerge when segment \emph{x} is more likely to be
misheard as segment \emph{y}, than segment \emph{y} to be misheard
as segment \emph{x}. Production biases, in some cases, can be attributed
to the masking of overlapping \isi{articulatory} gestures in rapid or casual
speech. These biases are of the same kind as those adopted in the
preceding models. Yet a fundamental aspect of the nature of these
misanalyses has been lost in implementation. Even in models that explicitly
invoke the Evolutionary Phonology framework (see \citealt{Blevins2004}),
the mechanism is typically realized at a very coarse grain.\footnote{Although \citet{Boer2000} uses a non-trivial mapping from acoustic
data to \isi{production} targets, it is not a model of sound change, but
of structure emergence in vowel systems. In a similar type of model,
\citet{oudeyer2006self} relies on the same units (neurons) being
used in \isi{perception} and \isi{production}. However, this mapping is mediated
by the distributed nature of the representations (over a network of
neurons), and the fact that neurons are ``tuned'' by experienced input,
via a non-linear activation function. } For example, the model in \citet{wedel2017category} (also described
in \citealt{Blevins2009}) is driven by \isi{misperception} error (or “variant
trading”); this occurs as a binary decision between neighboring
lexical categories. As already discussed, \citet{Garrett2013} implement
an all-or-nothing \isi{normalization} mechanism.\footnote{Although they make an explicit distinction between a word-level perceptual
token space, and a segment-level \isi{production} token space, no transformation
algorithm is provided. They also suggest that the \isi{articulatory} ``speech''
mode is sometimes available for \isi{perception}, so the exact relationship
between the two ``modes'' of processing is somewhat unclear. In practice,
the models seem to be implemented using a single abstract \isi{phonetic}
dimension.} In \citet{Kirby2014}, ``\isi{misparsing}'' is more gradient; for any given
token, a random amount of the \isi{target} segment may be mis-attributed
to the preceding segment. However, the \isi{misparsing} doesn't depend on
the \isi{phonetic} properties of the input, and different outcomes are only
possible by ``turning off'' the \isi{misparsing}. \citet{morley2014implications}
uses a bi-directional \isi{misperception} term in a model of \isi{velar palatalization},
but \isi{misperception} only applies to feature \isi{parsing}, and is segment
preserving. 

In the next chapter a new model of \isi{vowel nasalization} is developed,\largerpage
guided by the goal of avoiding the theoretical and implementational
pitfalls laid out in this, and preceding, chapters. This model will
contain an explicit listener analysis stage in which each token is parsed into its 
constituent units and the number of possible units is not fixed. Neither \isi{misparsing} 
nor multiple processing modes is required because the input is not assumed to come pre-segmented 
at the \isi{phoneme} level. Therefore, surface variation is directly reflected in underlying 
variation and change in one leads to change in the other.
